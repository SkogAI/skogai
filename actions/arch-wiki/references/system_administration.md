# Arch-Wiki - System Administration

**Pages:** 225

---

## Netboot

**URL:** https://wiki.archlinux.org/title/Netboot

**Contents:**
- BIOS
  - Using ipxe.lkrn
  - Using ipxe.pxe
- UEFI
  - Installation with efibootmgr
  - Boot from a USB flash drive
    - In GNU/Linux
    - In Windows
- Troubleshooting
  - Booting EFI binary gives "device error"

Netboot images are small (< 1 MiB) images that can be used to download the latest Arch Linux release on the fly upon system boot. It is unnecessary to update the netboot image, the newest release will be available automatically. Netboot images can be downloaded from the Arch Linux Netboot page.

To use netboot on a BIOS-based computer, you need either the ipxe.lkrn or ipxe.pxe image.

The ipxe.lkrn image can be booted like a Linux kernel. Any Linux boot loader (like GRUB or Syslinux) can be used to load it from your hard drive, a CD or a USB drive. For example, the Syslinux wiki gives instructions to install[2] and configure[3] Syslinux on a bootable medium.

You can make flash drive that boots ipxe.lkrn with the following steps:

Alternatively, you can also try the image with QEMU by running the following command:

The ipxe.pxe image is a PXE image. It can be chainloaded from an existing PXE environment. This allows configuring a DHCP server such that booting from the network will always boot into Arch Linux netboot.

Alternatively, you can also chainload it from existing pxe loader such as pxelinux. This is a menu entry example:

For this example to work you must have pxechn.c32 copied to the directory where your pxelinux.0 resides.

The ipxe-arch.efi image can be used to launch Arch Linux netboot in UEFI mode. Only 64-bit UEFI is supported. The image can be added as a boot option via efibootmgr, launched from a boot manager, like systemd-boot or rEFInd, or directly from the UEFI shell. You can also boot it from a standalone USB stick on UEFI systems.

First install the efibootmgr package. Assuming your EFI system partition (ESP) is /dev/sdd1 and mounted under esp, you should move it as follows—let us also give it a more friendly name:

Then you can create a boot entry as follows:

If you want to boot the Netboot EFI binary from a USB flash drive, copy it to the default/fallback boot path (/EFI/BOOT/BOOTx64.EFI) on a FAT formatted partition. It does not require creating a EFI system partition on the drive as all UEFI will happily boot any FAT volume from USB flash drives. The most compatible setup would be using the MBR partition table with a single active (bootable) primary partition of type 0c "W95 FAT32 (LBA)".[4]

The image should be then loaded automatically by UEFI systems.

For example, assuming /dev/sdX as the flash drive, prepare the USB flash drive as follows:

Prepare the USB flash drive as follows:

If booting the EFI binary results in Failed to execute ... device error, make sure the network stack is enabled in your UEFI settings. It may require initializing the network interface controller's (NIC) option ROM, so additionally look for settings like "OnBoard LAN Boot ROM", "Launch PXE OpROM Policy" or similar.

**Examples:**

Example 1 (unknown):
```unknown
boot/syslinux
```

Example 2 (unknown):
```unknown
# mount /dev/sdc /mnt
# mkdir -p /mnt/boot/syslinux
# cp ipxe.lkrn /mnt/boot
```

Example 3 (unknown):
```unknown
/mnt/boot/syslinux/syslinux.cfg
```

Example 4 (unknown):
```unknown
DEFAULT arch_netboot
   SAY Booting Arch over the network.
LABEL arch_netboot
   KERNEL /boot/ipxe.lkrn
```

---

## Linux console

**URL:** https://wiki.archlinux.org/title/Console_fonts

**Contents:**
- Implementation
  - Virtual consoles
  - Text mode
  - Framebuffer console
- Keyboard shortcuts
- Fonts
  - Preview and temporary changes
  - Persistent configuration
- Cursor appearance
  - Cursor size

According to Wikipedia:

This article describes the basics of the Linux console and how to configure the font display. Keyboard configuration is described in the /Keyboard configuration subpage. For alternative console solutions offering more features (full Unicode fonts, modern graphics adapters etc.), see KMSCON or similar projects.

The console, unlike most services that interact directly with users, is implemented in the kernel. This contrasts with terminal emulation software, such as Xterm, which is implemented in user space as a normal application. The console has always been part of released Linux kernels, but has undergone changes in its history, most notably the transition to using the framebuffer and support for Unicode.

Despite many improvements in the console, its full backward compatibility with legacy hardware means it is limited compared to a graphical terminal emulator. The main difference between the Linux console and graphical terminal emulators is the shells in the Linux console are attached directly to TTY devices (/dev/tty*), whereas the shells in a graphical terminal emulator are attached to pseudo-TTYs (/dev/pty*).

Also, graphical terminal emulators can have many more features than the Linux console, including a richer set of available fonts, multiple tabs/windows, split views, scrollback buffers/sliders, background colors/images (optionally with transparency), etc. Some of these features can be used in the Linux console with terminal multiplexers, such as Tmux or GNU Screen, or in certain text user interface programs (TUI) typically relying on libraries such as ncurses and the like, e.g. Vim, nano, or Emacs. These can also be used in graphical terminal emulators, if desired.

The console is presented to the user as a series of virtual consoles. These give the impression that several independent terminals are running concurrently; each virtual console can be logged in with different users, run its own shell and have its own font settings. The virtual consoles each use a device /dev/ttyX, and you can switch between them by pressing Alt+Fx (where x is equal to the virtual console number, beginning with 1). The device /dev/console is automatically mapped to the active virtual console.

See also chvt(1), openvt(1) and deallocvt(1).

Since Linux originally began as a kernel for PC hardware, the console was developed using standard IBM CGA/EGA/VGA graphics, which all PCs supported at the time. The graphics operated in VGA text mode, which provides a simple 80x25 character display with 16 colours. This legacy mode is similar to the capabilities of dedicated text terminals, such as the DEC VT100 series. It is still possible to boot in text mode (with vga=0 nomodeset) if the system hardware supports it, but almost all modern distributions (including Arch Linux) use the framebuffer console instead.

As Linux was ported to other non-PC architectures, a better solution was required, since other architectures do not use VGA-compatible graphics adapters, and may not support text modes at all. The framebuffer console was implemented to provide a standard console across all platforms, and so presents the same VGA-style interface regardless of the underlying graphics hardware. As such, the Linux console is not a terminal emulator, but a terminal in its own right. It uses the terminal type linux, and is largely compatible with VT100.

See also console_codes(4).

The Linux console uses UTF-8 encoding by default, but because the standard VGA-compatible framebuffer is used, a console font is limited to either a standard 256, or 512 glyphs. If the font has more than 256 glyphs, the number of colours is reduced from 16 to 8. In order to assign correct symbol to be displayed to the given Unicode value, a special translation map, often called unimap, is needed. Nowadays, most of the console fonts have the unimap built-in; historically, it had to be loaded separately.

By default, the virtual console uses the kernel built-in font with a CP437 character set[1], but this can be easily changed. The kernel offers about 15 built in fonts to choose from, from which the officially supported kernels provide two: VGA 8x16 font (CONFIG_FONT_8x16) and Terminus 16x32 font (CONFIG_FONT_TER16x32). The kernel chooses the one to use based on its evaluation of the screen resolution. Another builtin font can be forced upon by kernel parameters boot parameter setting such as fbcon=font:TER16x32.

The kbd package provides tools to override the kernel decision for virtual console font and font mapping. Available fonts are provided in the /usr/share/kbd/consolefonts/ directory; those ending with .psfu or .psfu.gz have a Unicode translation map built-in.

Keymaps, the connection between the key pressed and the character used by the computer, are found in the subdirectories of /usr/share/kbd/keymaps/; see /Keyboard configuration for details.

shows a table of glyphs or letters of a font.

setfont temporarily changes the font if passed a font name (in /usr/share/kbd/consolefonts/) such as

Font names are case-sensitive. With no parameter, setfont returns the console to the default font.

So to have a small 8x8 font, with that font installed like seen below, use e.g.:

To have a bigger font, the Terminus font (terminus-font) is available in many sizes, such as ter-132b which is large.

You can also append -d for double size. This would be using a 64*64 font:

The FONT variable in /etc/vconsole.conf is used to set the font at boot, persistently for all consoles. See vconsole.conf(5) for details.

For displaying characters such as Č, ž, đ, š or Ł, ę, ą, ś using the font lat2-16.psfu.gz:

It means that second part of ISO/IEC 8859 characters are used with size 16. You can change font size using other values (e.g. lat2-08). For the regions determined by 8859 specification, look at the Wikipedia:ISO/IEC 8859#The parts of ISO/IEC 8859.

Since mkinitcpio v33, the font specified in /etc/vconsole.conf gets automatically loaded during early userspace by default via the consolefont hook, which adds the font to the initramfs. See Mkinitcpio#HOOKS for more information.

You may also need to restart systemd-vconsole-setup.service after changing /etc/vconsole.conf.

If the fonts appear to not change on boot, or change only temporarily, it is most likely that they got reset when graphics driver was initialized and console was switched to framebuffer. By default, all in-tree kernel drivers are loaded early, NVIDIA users should see NVIDIA#Early loading to load their graphics driver before /etc/vconsole.conf is applied.

This subject is poorly documented. You should read the following articles:

The console cursor can be adjusted with Device Attributes (DA) control function. The sequence of parameters must be preceded by a single question mark (despite console_codes(4) says the opposite).

Here is an example for full block non-blinking green cursor with black symbols under it:

The same can be expressed with the octal and characters instead of hex codes:

The same can be applied as permanent configuration with kernel parameter:

The first parameter, despite it's name cursor size, with number 16 (the rightmost two hex digits of the kernel parameter are 10) means "use software cursor."

If you want to change hardware cursor shape - use corresponding number (from 0 to 6, see the table above).

The second parameter, called toggle mask, flips corresponding bits of the color.

(symbol under the cursor)

In our case the second parameter is 15 (the middle hex digits of the kernel parameter are 0f), so all four foreground (symbol) bits will be flipped. The most important rule is: toggling (the second parameter) is applied after the setting (the third parameter).

The third parameter is called set mask. It sets corresponding character attribute bits. We use 47 (the leftmost hex digits of the kernel parameter are 2f) in our example, which means two things:

See HiDPI#Linux console (tty).

**Examples:**

Example 1 (unknown):
```unknown
/dev/console
```

Example 2 (unknown):
```unknown
vga=0 nomodeset
```

Example 3 (unknown):
```unknown
Ctrl+Alt+Del
```

Example 4 (unknown):
```unknown
/usr/lib/systemd/system/ctrl-alt-del.target
```

---

## Domain name resolution

**URL:** https://wiki.archlinux.org/title/Name_Service_Switch

**Contents:**
- Name Service Switch
  - Resolve a domain name using NSS
- Glibc resolver
  - Overwriting of /etc/resolv.conf
    - Alternative using nmcli
  - Limit lookup time
  - Hostname lookup delayed with IPv6
  - Local domain names
- Lookup utilities
- Resolver performance

In general, a domain name represents an IP address and is associated to it in the Domain Name System (DNS). This article explains how to configure domain name resolution and resolve domain names.

This article or section needs expansion.

The Name Service Switch (NSS) facility is part of the GNU C Library (glibc) and backs the getaddrinfo(3) API, used to resolve domain names. NSS allows system databases to be provided by separate services, whose search order can be configured by the administrator in nsswitch.conf(5). The database responsible for domain name resolution is the hosts database, for which glibc offers the following services:

systemd provides three NSS services for hostname resolution:

NSS databases can be queried with getent(1). A domain name can be resolved through NSS using:

The glibc resolver reads /etc/resolv.conf for every resolution to determine the nameservers and options to use.

resolv.conf(5) lists nameservers together with some configuration options. Nameservers listed first are tried first, up to three nameservers may be listed. Lines starting with a number sign (#) are ignored.

Network managers tend to overwrite /etc/resolv.conf, for specifics see the corresponding section:

To prevent programs from overwriting /etc/resolv.conf, it is also possible to write-protect it by setting the immutable file attribute:

This article or section is a candidate for merging with NetworkManager#/etc/resolv.conf.

If you use NetworkManager, nmcli(1) can be used to set persistent options for /etc/resolv.conf. Change "Wired" to the name of your connection. Example:

For more options have a look at the man pages of nmcli(1), nm-settings-nmcli(5) and resolv.conf(5).

If you are confronted with a very long hostname lookup (may it be in pacman or while browsing), it often helps to define a small timeout after which an alternative nameserver is used. To do so, put the following in /etc/resolv.conf.

If you experience a 5 second delay when resolving hostnames it might be due to a DNS-server/Firewall misbehaving and only giving one reply to a parallel A and AAAA request.[1] You can fix that by setting the following option in /etc/resolv.conf:

To be able to use the hostname of local machine names without the fully qualified domain name, add a line to /etc/resolv.conf with the local domain such as:

That way you can refer to local hosts such as mainmachine1.example.org as simply mainmachine1 when using the ssh command, but the drill command still requires the fully qualified domain names in order to perform lookups.

To query specific DNS servers and DNS/DNSSEC records you can use dedicated DNS lookup utilities or those shipped with DNS servers. These tools implement DNS themselves and do not use NSS.

Some DNS server packages ship with DNS lookup utilities that can be used without running the DNS server:

The Glibc resolver does not cache queries. To implement local caching, use systemd-resolved or set up a local caching DNS server and use it as the name server by setting 127.0.0.1 and ::1 as the name servers in /etc/resolv.conf or in /etc/resolvconf.conf if using openresolv.

This article or section needs expansion.

The DNS protocol (Do53) is unencrypted and does not account for confidentiality, integrity or authentication, so if you use an untrusted network or a malicious ISP, your DNS queries can be eavesdropped and the responses manipulated. Furthermore, DNS servers can conduct DNS hijacking.

You need to trust your DNS server to treat your queries confidentially. DNS servers are provided by ISPs and third-parties. Alternatively you can run your own recursive name server (a.k.a. recursive resolver, a.k.a. DNS recursor), which however takes more effort. If you use a DHCP client in untrusted networks, be sure to set static name servers to avoid using and being subject to arbitrary DNS servers, or alternatively, use a VPN to connect to a secure network and use its DNS servers. To secure your communication with a remote DNS server you can use an encrypted protocol, provided that both the upstream server and your local resolver support the protocol. Common encrypted DNS protocols are:

To verify that responses are actually from authoritative name servers, you can validate DNSSEC, provided that both the upstream server(s) and your local resolver support it.

Although one may use an encrypted DNS resolver, a TLS connection still leaks the domain names in the Server Name Indication (SNI) when requesting the domain certificate. This leak can be checked using the Wireshark filter tls.handshake.extensions_server_name_len > 0, or using the following tshark command:

A proposed solution is to use the Encrypted Client Hello (ECH), a TLS 1.3 protocol extension.

Be aware that some client software, such as major web browsers[2][3], are starting to implement DNS over HTTPS. While the encryption of queries may often be seen as a bonus, it also means the software sidetracks queries around the system resolver configuration.[4]

Firefox provides configuration options to enable or disable DNS over HTTPS and select a DNS server. Mozilla has setup a Trusted Recursive Resolver (TRR) programme with transparency information on their default providers. It is notable that Firefox supports and automatically enables the Encrypted Client Hello (ECH) for TRR providers, see Firefox/Privacy#Encrypted Client Hello.

Chromium will examine the user's system resolver and enable DNS over HTTPS if the system resolver addresses are known to also provide DNS over HTTPS. See this blog post for more information and how DNS over HTTPS can be disabled.

Mozilla has proposed universally disabling application-level DNS if the system resolver cannot resolve the domain use-application-dns.net. Currently, this is only implemented in Firefox.

Oblivious DNS over HTTPS (ODoH)—RFC 9230—is a system which addresses a number of DNS privacy concerns. See Cloudflare's article for more information. It added DNS over HTTPS to the academic Oblivious DNS design. See the Improving the privacy of DNS and DoH with oblivion article for a discussion of the differences.

This article or section needs expansion.

Communication between recursive resolvers and root servers is not encrypted and the root server operators are against implementing it. For encrypted communication with authoritative servers there is the experimental RFC 9539 which allows the opportunistic use of DNS over TLS and DNS over QUIC.

There are various third-party DNS services. Wikipedia has a list of "notable" public DNS service operators while the curl project's wiki has a more extensive list of publicly available DNS over HTTPS servers (a lot of which also support DNS over TLS). The systemd package configures fallback DNS for systemd-resolved when no DNS servers are configured (manually or via DHCP/RA).

You can use dnsperftest to test the performance of the most popular DNS resolvers from your location. dnsperf.com provides global benchmarks between providers.

Some DNS services also provide dedicated software:

DNS servers can be authoritative and recursive. If they are neither, they are called stub resolvers and simply forward all queries to another recursive name server. Stub resolvers are typically used to introduce DNS caching on the local host or network. Note that the same can also be achieved with a fully-fledged name server. This section compares the available DNS servers, for a more detailed comparison, refer to Wikipedia:Comparison of DNS server software.

It is possible to use specific DNS resolvers when querying specific domain names. This is particularly useful when connecting to a VPN, so that queries to the VPN network are resolved by the VPN's DNS, while queries to the internet will still be resolved by your standard DNS resolver. It can also be used on local networks.

To implement it, you need to use a local resolver because glibc does not support it.

In a dynamic environment (laptops and to some extents desktops), you need to configure your resolver based on the network(s) you are connected to. The best way to do that is to use openresolv because it supports multiple subscribers. Some network managers support it, either through openresolv, or by configuring the resolver directly. NetworkManager supports conditional forwarding without openresolv.

**Examples:**

Example 1 (unknown):
```unknown
/etc/resolv.conf
```

Example 2 (unknown):
```unknown
$ getent ahosts domain_name
```

Example 3 (unknown):
```unknown
/etc/resolv.conf
```

Example 4 (unknown):
```unknown
/etc/resolv.conf
```

---

## Secure Shell

**URL:** https://wiki.archlinux.org/title/Ssh

**Contents:**
- Software
  - Server only
  - Client only
- Securing
- See also

This article or section is a candidate for merging with OpenSSH.

According to Wikipedia:

Examples of services that can use SSH are Git, rsync and X11 forwarding. Services that always use SSH are SCP and SFTP.

An SSH server, by default, listens on the standard TCP port 22. An SSH client program is typically used for establishing connections to an sshd daemon accepting remote connections. Both are commonly present on most modern operating systems, including macOS, GNU/Linux, Solaris and OpenVMS. Proprietary, freeware and open source versions of various levels of complexity and completeness exist.

---

## XDG Base Directory

**URL:** https://wiki.archlinux.org/title/XDG_Base_Directory

**Contents:**
- Specification
  - User directories
  - System directories
- Support
  - Contributing
  - Supported
  - Partial
  - Hardcoded
- Tools
- Libraries

This article summarizes the XDG Base Directory specification in #Specification and tracks software support in #Support.

Please read the full specification. This section will attempt to break down the essence of what it tries to achieve.

Only XDG_RUNTIME_DIR is set by default through pam_systemd(8). It is up to the user to explicitly define the other variables according to the specification. Changing it might cause issues with pipewire and screen sharing on chromium.

See Environment variables#Globally for information on defining variables.

This article or section is out of date.

This article or section needs expansion.

This section exists to catalog the growing set of software using the XDG Base Directory Specification introduced in 2003. This is here to demonstrate the viability of this specification by listing commonly found dotfiles and their support status. For those not currently supporting the Base Directory Specification, workarounds will be demonstrated to emulate it instead.

The workarounds will be limited to anything not involving patching the source, executing code stored in environment variables or compile-time options. The rationale for this is that configurations should be portable across systems and having compile-time options prevent that.

Hopefully this will provide a source of information about exactly what certain kinds of dotfiles are and where they come from.

When contributing make sure to use the correct section.

Nothing should require code evaluation, patches or compile-time options to gain support and anything which does must be deemed hardcoded. Additionally, if the process is error prone or difficult, it should also be classified as hardcoded.

If present ~/.actrc will be merged with the XDG path config.

Location overview by Google does not mention XDG - paths could be hardcoded instead of using the proper variable, though that is unlikely as Intellij IDEA, which Android Studio is based on, implements it properly as well

The BITWARDENCLI_APPDATA_DIR environment variable takes precedence.

Currently contains a single data.json file with all the vault data, so it ought to belong in XDG_DATA_HOME

XDG_CONFIG_HOME/byobu

Legacy path takes precedence if present, or if XDG_CONFIG_HOME is not set.

If the legacy path ~/.calcurse is present, it will take precedence.

XDG_CACHE_HOME/clangd

Project specific configuration can be specified in proj/.clangd. Configuration is combined when this is sensible. In case of conflicts, user config has the highest precedence, then inner project, then outer project.

If the legacy path is present, it will take precedence.

See Ctags Option files.

libcups added XDG support in v3 (still in beta). The version in the official repositories is still hardcoded to ~/.cups.

Legacy paths have precedence over XDG paths. Emacs will never create XDG_CONFIG_HOME/emacs/. Workaround for 26.3 or older: It's possible to set HOME, but it has unexpected side effects.

legacy path can be used with FreeCAD --keep-deprecated-paths

The old method of export STACK_ROOT="$XDG_DATA_HOME"/stack still works and takes priority [55][dead link 2024-07-30—HTTP 404].

XDG_CONFIG_HOME/latexmk/latexmkrc

XDG_CONFIG_HOME/lesskey

XDG_STATE_HOME/lesshst or XDG_DATA_HOME/lesshst

If the legacy path ~/.luarocks is present, it will take precedence.

1b99570 0b71156 ce401d7

mkdir -p "$XDG_DATA_HOME"/newsboat "$XDG_CONFIG_HOME"/newsboat

XDG_CONFIG_HOME/utop/utoprc

github.com/osc/pull/940

XDG_CONFIG_HOME/osc/oscrc XDG_STATE_HOME/osc/cookiejar

Legacy path takes precedence if it exists

87f1e8f a9020c6 3b22f0f 0a012ae

a0be0cc7 15e1fc92 e9d1be0e

59a8618 87ae830 9ab510a 4c195bc

fd8686e 66d704b 51cff01

mkdir "$XDG_CONFIG_HOME"/scummvm/ "$XDG_DATA_HOME"/scummvm mv ~/.scummvmrc "$XDG_CONFIG_HOME"/scummvm/scummvm.ini mv ~/.scummvm "$XDG_DATA_HOME"/scummvm/saves

See Shellcheck RC Files for more info.

3e4591d bd8c427 f57fc71

For Qt programs, GTK or Qt programs on Wayland, to use cursors in XDG_DATA_HOME/icons, the XCURSOR_PATH environment variable needs to be configured.

See :h xdg-base-dir for more details.

The viminfo file can be set with :set viminfofile=$XDG_STATE_HOME/vim/viminfo

9fc6b37[dead link 2024-07-30—HTTP 404] eaccf70[dead link 2024-07-30—HTTP 404]

[156][dead link 2024-07-30—HTTP 404]

Alternatively, it always respects XMONAD_CACHE_DIR, XMONAD_CONFIG_DIR, and XMONAD_DATA_DIR.

The remote's ~/.ansible/tmp can be moved by setting remote_tmp = ${XDG_CONFIG_HOME}/ansible/tmp in an appropriate ansible.cfg. [170] [171]

HOME="$XDG_DATA_HOME" btcli

or use the fork that has native XDG support: [188]

export CRAWL_DIR="$XDG_DATA_HOME"/crawl/

Despite this, clusterssh will still create ~/.clusterssh/.

Undocumented, though actively used: export DISCORD_USER_DATA_DIR="${XDG_DATA_HOME}"

Source: <discord_system_package_root>/resources/app.asar.

export MIX_XDG="true"

mkdir "$XDG_CONFIG_HOME"/erlang mv ~/.erlang.cookie "$XDG_CONFIG_HOME"/erlang

The environment variable GHCUP_USE_XDG_DIRS can be set to any non-empty value. See [211].

export GR_PREFS_PATH="$XDG_CONFIG_HOME"/gnuradio

GNU Radio Companion: export GRC_PREFS_PATH="$XDG_CONFIG_HOME"/gnuradio/grc.conf

Note that this currently does not work out-of-the-box using systemd user units and socket-based activation, since the socket directory changes based on the hash of $GNUPGHOME. You can get the new socket directory using gpgconf --list-dirs socketdir and have to modify the systemd user units to listen on the correct sockets accordingly. You also have to use the following gpg-agent.service drop-in file (or otherwise pass the GNUPGHOME env var to the agent running in systemd), or you might experience issues with "missing" private keys:

If you use GPG as your SSH agent, set SSH_AUTH_SOCK to the output of gpgconf --list-dirs agent-ssh-socket instead of some hardcoded value.

If GOMODCACHE is not set, it defaults to $GOPATH/pkg/mod (see [217]). GOCACHE is supported and defaults to $XDG_CACHE_HOME/go-build (see [218]).

PASSWORD_STORE_DIR is supported only during initialization.

If Lxappearance is used, ~/.gtkrc-2.0 may keep being created because it is where clicking "Apply" customizations writes to. The path is hardcoded in Lxappearance, but simply being an output file, the settings can be repeatedly moved to the location.

To prevent KDE Plasma from creating this file, disable the "GNOME/GTK Settings Synchronization" background service.

The value of this variable must include the substring __HVER__, which will be replaced at run time with the current MAJOR.MINOR version string.

export JUPYTER_CONFIG_DIR="$XDG_CONFIG_HOME"/jupyter

v5.0.0 <= python-jupyter-core < v6.0.0:

export JUPYTER_PLATFORM_DIRS="1" (see [229])

python-jupyter-core >= v6.0.0: full support (via python-platformdirs) enabled by default

KDE4 uses KDEHOME. It is not recommended to set the variable for newer versions.

to change the m2 repo location used by leiningen look here: Leiningen#m2_repo_location

Make sure XDG_CACHE_HOME is set beforehand to directory user running Xorg has write access to.

Do not use XDG_RUNTIME_DIR as it is available after login. Display managers that launch Xorg (like GDM) will repeatedly fail otherwise.

However, no way to change the location of this configuration file.

, mvn -gs "$XDG_CONFIG_HOME"/maven/settings.xml and set <localRepository> as appropriate in settings.xml

Used to be MATHEMATICA_USERBASE, see Upgrading from Mathematica to Wolfram.

Creates a further .minikube directory in MINIKUBE_HOME for whatever reason.

~/.my.cnf only supported for mysql-server, not mysql-client [251]

~/.mylogin.cnf unsupported

export TERMINFO="$XDG_DATA_HOME"/terminfo, export TERMINFO_DIRS="$XDG_DATA_HOME"/terminfo:/usr/share/terminfo

prefix is unnecessary (and unsupported) if Node.js is installed by nvm.

Both configuration and state data are stored in OPAMROOT, so this solution is not fully compliant.

The local_list option must be given an absolute path.

Currently it hard-codes ~/.local/share.

Log files may still be written to ~/pt/logs regardless of this setting until the .packettracer file is recreated manually.

export PHIVE_HOME="$XDG_DATA_HOME/phive"

It is required to create both directories: mkdir "$XDG_CONFIG_HOME/pg" && mkdir "$XDG_STATE_HOME"

PYTHON_HISTORY: export PYTHON_HISTORY=$XDG_STATE_HOME/python_history PYTHONPYCACHEPREFIX: export PYTHONPYCACHEPREFIX=$XDG_CACHE_HOME/python PYTHONUSERBASE: export PYTHONUSERBASE=$XDG_DATA_HOME/python

For more info see Bundler: bundle config.

Both configuration and data files are stored in SDKMAN_DIR, so this solution does not fully conform to the XDG Base Directory Specification.

mv ~/.ssr "$XDG_CONFIG_HOME"/simplescreenrecorder

export SPACEMACSDIR="$XDG_CONFIG_HOME"/spacemacs, mv ~/.spacemacs "$SPACEMACSDIR"/init.el

Other files need to be configured like Emacs.

tiptop -W "$XDG_CONFIG_HOME"/tiptop

Setting this makes the editor look for the contents of .config/Code - OSS in $VSCODE_PORTABLE/user-data.

You can also run Visual Studio with the --extensions-dir flag, such as code --extensions-dir "$XDG_DATA_HOME/vscode". This is documented and probably will not break as unexpectedly, as it has other use cases.

You can also edit the value of dataFolderName in product.json file to .local/share/codium or the path you want. But this workaround will have to be applies after every update of the pacakge, so you can install vscodium-xdg-dir-patchAUR that does it automatically.

The directory needs to be created manually

mkdir "$XDG_CONFIG_HOME/wakatime"

mkdir -p "$XDG_DATA_HOME"/wineprefixes, export WINEPREFIX="$XDG_DATA_HOME"/wineprefixes/default

App also creates ~/.x3270connect but this is currently unsupported.

Note that LightDM does not allow you to change this variable. If you change it nonetheless, you will not be able to login. Use startx instead or configure LightDM. According to [311] SLiM has ~/.Xauthority hardcoded.

The SDDM Xauthority path can be changed in its own configuration files as shown below. Unfortunately, it is relative to the home directory.

On Wayland, overriding this may cause Xorg programs to fail to connect to the Xwayland server. For example, both kwin and mutter use a randomized name, so it cannot be set to a static value.

Depending on where you have configured your $XDG_CACHE_HOME, you might need to expand the paths yourself.

Unlike most other examples in this table, actual X11 init scripts will vary a lot between installations.

Finally, if you use zsh as a login shell and chose to rely on either of the startup files ~/.zshenv or, ~/.zprofile or, ~/.zlogin to set important environment variables such as ZDOTDIR, to bootstrap, there is no way around having the one file that sets ZDOTDIR be in the default location. For context, an exception is if your wider system configuration does set the ZDOTDIR environment variable before that.

In the above config file, some locations can be customized using options like newsrc-path= and address-book=.

Specify the new directories used by Arduino CLI in arduino-cli.yaml as mentioned in the documentation here. alias arduino-cli='arduino-cli --config-file $XDG_CONFIG_HOME/arduino15/arduino-cli.yaml'

export HISTFILE="$XDG_STATE_HOME"/bash/history

bashrc can be sourced from a different location in /etc/bash.bashrc. Specify --init-file <file> as an alternative to ~/.bashrc for interactive shells.

Generated by the maintenance scripts eval.php and sql.php.

export OLLAMA_MODELS=$XDG_DATA_HOME/ollama/models

Note that this requires root privileges and will change the location of ~/.sbclrc for all users. This can be mitigated by checking for an existing ~/.sbclrc inside the lambda form.

export VIMPERATOR_RUNTIME="$XDG_CONFIG_HOME"/vimperator

The tool xdg-ninjaAUR detects unwanted files/directories in $HOME which can be moved to XDG base directories. See README for examples.

The tool boxxy can be used to wrap applications which do not respect the XDG base directories and redirect any unwanted files.

The tool ephemeral can be used to link chromium/electron caches that normally live in XDG_CONFIG_HOME to locations in XDG_CACHE_HOME.

For directories which cannot be relocated, some desktop environments such as KDE allow you to hide them:

path is the path of the file/directory, relative to the parent directory of .hidden.

**Examples:**

Example 1 (unknown):
```unknown
XDG_RUNTIME_DIR
```

Example 2 (unknown):
```unknown
XDG_CONFIG_HOME
```

Example 3 (unknown):
```unknown
$HOME/.config
```

Example 4 (unknown):
```unknown
XDG_CACHE_HOME
```

---

## SLiM

**URL:** https://wiki.archlinux.org/title/SLiM

**Contents:**
- Installation
- Configuration
  - Enabling SLiM
  - Environments
  - Set default username
  - Enable Autologin
  - Theming
    - Custom background
    - Dual screen setup
- Tips and tricks

This article or section is out of date.

SLiM is an acronym for Simple Login Manager. Lightweight and easily configurable, SLiM requires minimal dependencies, and none from the GNOME or KDE desktop environments. It therefore contributes towards a lightweight system for users that also like to use lightweight desktops such as Xfce, Openbox, and Fluxbox.

Install the slim package.

SLiM can automatically detect installed desktop environments and window managers through the use of sessiondir /usr/share/xsessions/ in /etc/slim.conf. Those upgrading from a version before 1.3.6-2 must amend /etc/slim.conf and xinitrc, accordingly. See below.

Enable the SLiM service slim.service. This assumes a previously enabled display manager was disabled first. Otherwise, change the default target.

To configure SLiM 1.3.6-2 (or later) to load an environment, edit both /etc/slim.conf and ~/.xinitrc.

First, edit /etc/slim.conf: If you only use a single environment, you can hash out sessiondir /usr/share/xsessions/. This will disable automatic detection of installed environments:

If you wish to automatically detect installed desktop environments, leave the line un-commented.

Second, edit xinitrc:

SLiM can be configured to automatically set a desired username, which will therefore already be completed. The password field will also already be focused by default. Change the following line in /etc/slim.conf:

Uncomment this line, and change "simone" to the username of choice:

Edit /etc/slim.conf to uncomment the auto_login command and replace no with yes:

Install the slim-themes package. The archlinux-themes-slim packages contains several different themes (slimthemes.png). Look in the directory of /usr/share/slim/themes to see the themes available. Enter the theme name on the current_theme line in /etc/slim.conf:

You can preview a theme while Xorg is running with:

To close, type "exit" in the Login line and press Enter.

Additional theme packages can be found in the AUR. See the theme documentation for how to customize your theme or make your own. SLiM does not support alternative theme directories, so it is recommended to create a package for your custom theme so that pacman is aware of it.

SLiM is hard-coded to load background.png or background.jpg (in that order) from your theme directory. Simply overwrite the appropriate file

You can customize the slim theme in /usr/share/slim/themes/<your-theme>/slim.theme to turn these percents values. The box itself is 450 pixels by 250 pixels:

If your theme has a background picture, you should use the background_style setting (stretch, tile, center or color) to get it correctly displayed.

After installing, edit /etc/slim.conf and uncomment the line:

This will give you a normal arrow instead. This setting is forwarded to xsetroot -cursor_name. You can look up the possible cursor names or in /usr/share/icons/your-cursor-theme/cursors/.

To change the cursor theme being used at the login screen, see Cursor themes#The default cursor theme.

To share a wallpaper between SLiM and your desktop, rename the used theme background, then create a link from your desktop wallpaper file to the default SLiM theme:

You may shutdown, reboot, suspend, exit or even launch a terminal from the SLiM login screen. To do so, use the values in the username field, and the root password in the password field:

If you use Splashy and SLiM, sometimes you cannot power-off or reboot from menu in GNOME, Xfce, LXDE or others. Check your /etc/slim.conf and /etc/splash.conf; set the DEFAULT_TTY=7 same as xserver_arguments vt07.

If your power off tray icon fails, it could be due to not having root privileges. To start a tray icon with root privileges, be sure to have SLiM start the program. Edit /etc/slim.conf as follows:

By default, SLiM fails to log logins to utmp and wtmp which causes who, last, etc. to misreport login information. To fix this edit your slim.conf as follows:

You can also use the sessionstart_cmd/sessionstop_cmd in /etc/slim.conf to log specific infomation, such as the session, user, or theme used by slim:

Or if you want to play a song when slim loads (and you have the beep program installed)

See GNOME/Keyring#Using the keyring to use GNOME Keyring in a custom session.

The Xorg server generally picks up the DPI but if it does not you can specify it to SLiM. If you set the DPI with the argument -dpi 96 in /etc/X11/xinit/xserverrc it will not work with SLiM. To fix this change your slim.conf from:

Use the current_theme variable as a comma separated list to specify a set from which to choose. Selection is random.

If tty terminals 3-6 are not used and commented out (You may use screen and therefore only need one terminal), change /etc/slim.conf to move the X server:

Simply change the vt07 to for example vt03 as no agetty is started there.

To automatically mount an encrypted partition on user login with SLiM, configure pam_mount as follows:

Edit /etc/X11/xorg.conf.d/10-evdev.conf, find the following section, add the two bolded lines, and replace dvorak with your preferred keymap:

Slim includes slimlock, a screen lock feature. To use it, just run slimlock.

Slimlock reads some configuration from /etc/slim.conf and its own configuration file /etc/slimlock.conf.

To prevent VT switching whilst locked, set tty_lock to 1 in slimlock.conf. This also requires that that you have write access to /dev/console and that slimlock has the sys_sys_tty_config capability. One way to achieve this is set slimlock to suid root:

An alternative is to setcap and permit your uid to write to /dev/console.

You can use xss-lock to lock the screen automatically:

There is a bug or known issue with the combination of SLiM, Xfce and systemd that does not let the system to properly shutdown and systemd waits for the SLiM service to end, but eventually is terminated.

To accelerate the shutdown process these lines might help when editing slim.service:

If your password contains non-ASCII characters (é, è, ç, à, etc.) and the locale of your system is in Unicode (fr_FR.UTF-8 for example), you will not be able to log in to your session with the package from the official repository (bugs found on Debian, bug#532060 and on NixOS, bug#29802).

A fixed version available on AUR brings Unicode support and solves this problem: slim-unicodeAUR.

When the Screen section of xorg.conf contains DefaultDepth 30, Slim immediately crashes in _XPutPixel32() libX11 function called from Image::createPixmap().

**Examples:**

Example 1 (unknown):
```unknown
sessiondir /usr/share/xsessions/
```

Example 2 (unknown):
```unknown
/etc/slim.conf
```

Example 3 (unknown):
```unknown
/etc/slim.conf
```

Example 4 (unknown):
```unknown
slim.service
```

---

## Bash

**URL:** https://wiki.archlinux.org/title/Alias

**Contents:**
- Invocation
  - Configuration files
  - Shell and environment variables
- Command line
  - Tab completion
    - Single-tab
    - Common programs and options
    - Customize per-command
  - History
    - History completion

Bash (Bourne-Again SHell) is a command-line shell/programming language by the GNU Project. Its name alludes to its predecessor, the long-deprecated Bourne shell. Bash can be run on most Unix-like operating systems, including GNU/Linux.

Bash is the default command-line shell on Arch Linux.

Bash behaviour can be altered depending on how it is invoked. Some descriptions of different modes follow.

If Bash is spawned by login in a TTY, by an SSH daemon, or similar means, it is considered a login shell. This mode can also be engaged using the -l/--login command line option.

Bash is considered an interactive shell when its standard input, output and error are connected to a terminal (for example, when run in a terminal emulator), and it is not started with the -c option or non-option arguments (for example, bash script). All interactive shells source /etc/bash.bashrc and ~/.bashrc, while interactive login shells also source /etc/profile and ~/.bash_profile.

Bash will attempt to execute a set of startup files depending on how it was invoked. See the Bash Startup Files section of the GNU Bash manual for a complete description.

The behavior of Bash and programs run by it can be influenced by a number of environment variables. Environment variables are used to store useful values such as command search directories, or which browser to use. When a new shell or script is launched it inherits its parent's variables, thus starting with an internal set of shell variables[1].

These shell variables in Bash can be exported in order to become environment variables:

Environment variables are conventionally placed in ~/.profile or /etc/profile so that other Bourne-compatible shells can use them.

See Environment variables for more general information.

Bash command line is managed by the separate library called Readline. Readline provides emacs and vi styles of shortcuts for interacting with the command line, i.e. moving back and forth on the word basis, deleting words etc. It is also Readline's responsibility to manage history of input commands. Last, but not least, it allows you to create macros.

Tab completion is the option to auto-complete typed commands by pressing Tab (enabled by default).

It may require up to three tab-presses to show all possible completions for a command. To reduce the needed number of tab-presses, see Readline#Faster completion.

By default, Bash only tab-completes commands, filenames, and variables. The package bash-completion extends this by adding more specialized tab completions for common commands and their options, which can be enabled by sourcing /usr/share/bash-completion/bash_completion (which has been already sourced in Arch's /etc/bash.bashrc). With bash-completion, normal completions (such as ls file.* Tab Tab) will behave differently; however, they can be re-enabled with compopt -o bashdefault program (see [2] and [3] for more detail).

By default, Bash only tab-completes file names following a command. You can change it to complete command names using complete -c:

or complete command names and file names with -cf:

See bash(1) § Programmable Completion for more completion options.

You can bind the up and down arrow keys to search through Bash's history (see: Readline#History and Readline Init File Syntax):

or to affect all readline programs:

The HISTCONTROL variable can prevent certain commands from being logged to the history.

To stop logging of consecutive identical commands:

To remove all but the last identical command:

To avoid saving commands that start with a space:

To avoid saving consecutive identical commands, and commands that start with a space:

To remove all but the last identical command, and commands that start with a space:

See bash(1) § HISTCONTROL for details.

To disable the bash history only temporarily:

The commands entered now are not logged to the $HISTFILE.

For example, now you can hash passwords with printf secret | sha256sum, or hide GPG usage like gpg -eaF secret-pubkey.asc and your secret is not written to disk.

To disable all bash history:

... and just to make sure, destroy your old histfile forever:

Zsh can invoke the manual for the command preceding the cursor by pressing Alt+h. A similar behaviour is obtained in Bash using this Readline bind:

This assumes are you using the (default) Emacs editing mode.

atuin replaces your existing shell history with an SQLite database, and records additional context for your commands. Additionally, it provides optional and fully encrypted synchronization of your history between machines, via an Atuin server.

Enable bash history timestamps (export HISTTIMEFORMAT="%F %T ") before syncing. Atuin works well with tools like blesh-gitAUR and cmd-wrapped to provide an enhanced terminal experience across machines.

alias is a command, which enables a replacement of a word with another string. It is often used for abbreviating a system command, or for adding default arguments to a regularly used command.

Personal aliases can be stored in ~/.bashrc or any separate file sourced from ~/.bashrc. System-wide aliases (which affect all users) belong in /etc/bash.bashrc. See [4] for example aliases.

For functions, see Bash/Functions.

See Bash/Prompt customization.

ble.sh (Bash Line Editor), packed as blesh-gitAUR, is a command line editor written in pure Bash, which is an alternative to GNU Readline. It has many enhanced features like syntax highlighting, autosuggestions, menu-completion, abbreviations, Vim editing mode, and hook functions. Other interesting features include status line, history share, right prompt, transient prompt, and xterm title.

After installing it, source it in an interactive session.

Configurations are explained in depth in the ~/.blerc file and at the wiki. The stable bleshAUR package is also available.

pkgfile includes a "command not found" hook that will automatically search the official repositories, when entering an unrecognized command.

You need to source the hook to enable it, for example:

Then attempting to run an unavailable command will show the following info:

You can disable the Ctrl+z feature (pauses/closes your application) by wrapping your command like this:

Now, when you accidentally press Ctrl+z in adomAUR instead of Shift+z, nothing will happen because Ctrl+z will be ignored.

To clear the screen after logging out on a virtual terminal:

Bash can automatically prepend cd when entering just a path in the shell. For example:

But after adding one line into .bashrc file:

autojump-gitAUR is a python script which allows navigating the file system by searching for strings in a database with the user's most-visited paths.

zoxide is an alternative which has additional features and performance improvements compared to the original autojump and can serve as a drop-in replacement for autojump.

For the current session, to disallow existing regular files to be overwritten by redirection of shell output:

This is identical to set -C.

To make the changes persistent for your user:

To manually overwrite a file while noclobber is set:

pushd and popd can be used to push or pop directories to a stack while switching to them. This can be useful for "replaying" your navigation history.

See bash(1) § DIRSTACK.

When resizing a terminal emulator, Bash may not receive the resize signal. This will cause typed text to not wrap correctly and overlap the prompt. The checkwinsize shell option checks the window size after each command and, if necessary, updates the values of LINES and COLUMNS.

If you have set the ignoreeof option and you find that repeatedly hitting ctrl-d causes the shell to exit, it is because this option only allows 10 consecutive invocations of this keybinding (or 10 consecutive EOF characters, to be precise), before exiting the shell.

To allow higher values, you have to use the IGNOREEOF variable.

The package shellcheck analyzes bash (and other shell) scripts, prints possible errors, and suggests better coding.

There is also the web site shellcheck.net of the same purpose, based on this program.

**Examples:**

Example 1 (unknown):
```unknown
bash script
```

Example 2 (unknown):
```unknown
/etc/bash.bashrc
```

Example 3 (unknown):
```unknown
/etc/profile
```

Example 4 (unknown):
```unknown
~/.bash_profile
```

---

## Sudo

**URL:** https://wiki.archlinux.org/title/Sudo

**Contents:**
- Installation
- Usage
  - Login shell
- Configuration
  - Defaults skeleton
  - View current settings
  - Using visudo
  - Example entries
  - Sudoers default file permissions
- Tips and tricks

Sudo allows a system administrator to delegate authority to give certain users—or groups of users—the ability to run commands as root or another user while providing an audit trail of the commands and their arguments.

Sudo is an alternative to su for running commands as root. Unlike su, which launches a root shell that allows all further commands root access, sudo instead grants temporary privilege elevation to a single command. By enabling root privileges only when needed, sudo usage reduces the likelihood that a typo or a bug in an invoked command will ruin the system.

Sudo can also be used to run commands as other users; additionally, sudo logs all commands and failed access attempts to the journal for security auditing.

Install the sudo package.

To begin using sudo as a non-privileged user, it must be properly configured. See #Configuration.

To use sudo, simply prefix a command and its arguments with sudo and a space:

For example, to use pacman:

See sudo(8) for more information.

You cannot run every command as an other user simply by prepending sudo. In particular when using a redirection and command substitution, you must use a login shell, which can be easily accessed with sudo -iu user (one can omit -u user if the desired user is root).

In the following example command substitution would work in a full shell, but fails with prepending sudo:

This article or section needs expansion.

sudoers(5) § SUDOERS OPTIONS lists all the options that can be used with the Defaults command in the /etc/sudoers file.

See [1] for a list of options (parsed from the version 1.8.7 source code) in a format optimized for sudoers.

See sudoers(5) for more information, such as configuring the password timeout.

Run sudo -ll to print out the current sudo configuration, or sudo -lU user for a specific user.

The configuration file for sudo is /etc/sudoers. It should always be edited with the visudo(8) command. visudo locks the sudoers file, saves edits to a temporary file, and checks it for syntax errors before copying it to /etc/sudoers.

The default editor for visudo is vi. The sudo package is compiled with --with-env-editor and honors the use of the SUDO_EDITOR, VISUAL and EDITOR variables. EDITOR is not used when VISUAL is set.

To establish nano as the visudo editor for the duration of the current shell session, export EDITOR=nano; to use a different editor just once simply set the variable before calling visudo:

Alternatively you may edit a copy of the /etc/sudoers file and check it using visudo -c /copy/of/sudoers. This might come in handy in case you want to circumvent locking the file with visudo.

To change the editor permanently, see Environment variables#Per user. To change the editor of choice permanently system-wide only for visudo, add the following to /etc/sudoers (assuming nano is your preferred editor):

To allow a user to gain full root privileges when they precede a command with sudo, add the following line:

To allow a user to run all commands as any user but only on the machine with hostname HOST_NAME:

To allow members of group wheel sudo access:

To disable asking for a password for user USER_NAME:

Enable explicitly defined commands only for user USER_NAME on host HOST_NAME:

Enable explicitly defined commands only for user USER_NAME on host HOST_NAME without password:

A detailed sudoers example is available at /usr/share/doc/sudo/examples/sudoers. Otherwise, see the sudoers(5) for detailed information.

The owner and group for the sudoers file must both be 0. The file permissions must be set to 0440. These permissions are set by default, but if you accidentally change them, they should be changed back immediately or sudo will fail.

A common annoyance is a long-running process that runs on a background terminal somewhere that runs with normal permissions and elevates only when needed. This leads to a sudo password prompt which goes unnoticed and times out, at which point the process dies and the work done is lost or, at best, cached. Common advice is to enable passwordless sudo, or extend the timeout of sudo remembering a password. Both of these have negative security implications. The prompt timeout can also be disabled and since that does not serve any reasonable security purpose it should be the solution here:

The following is only relevant if the bash completion is not available (either full or reduced as described above): Aliases in Zsh and Bash are normally only expanded for the first word in a command. This means that your aliases will not normally get expanded when running the sudo command. One way to make the next word expand is to make an alias for sudo ending with a space. Add the following to your shell's configuration file:

zshmisc(1) § ALIASING describes how this works:

As well as bash(1) § ALIASES:

To draw attention to a sudo prompt in a background terminal, users can simply make it echo a bell character:

Note the ^G is a literal bell character. E.g. in vim, insert using the sequence Ctrl+v Ctrl+g. If Ctrl+v is mapped, e.g. for pasting, one can usually use Ctrl+q instead. In nano, Alt+v Ctrl+g.

This article or section needs expansion.

Another option is to set the SUDO_PROMPT environment variable. For example, add the following to your shell configuration file:

If you are annoyed by sudo's defaults that require you to enter your password every time you open a new terminal, set timestamp_type to global:

If you are annoyed that you have to re-enter your password every 5 minutes (default), you can change this by setting a longer value for timestamp_timeout (in minutes):

If you are using sudo commands in a long script and you do not want to wait for user input when the timeout expires, it is possible to refresh the timeout by separately running sudo -v in a loop (whereas sudo -K revokes it immediately).

If you have a lot of environment variables, or you export your proxy settings via export http_proxy="...", when using sudo these variables do not get passed to the root account unless you run sudo with the -E/--preserve-env option.

The recommended way of preserving environment variables is to append them to env_keep:

Users can configure sudo to ask for the root password instead of the user password by adding targetpw (target user, defaults to root) or rootpw to the Defaults line in /etc/sudoers:

To prevent exposing your root password to users, you can restrict this to a specific group:

Users may wish to disable the root login. Without root, attackers must first guess a user name configured as a sudoer as well as the user password. See for example OpenSSH#Deny.

The account can be locked via passwd:

A similar command unlocks root.

Alternatively, you can use the following command to delete the password and then lock the root account :

To enable root login again:

Note that this merely disables password-based login. The user may still be able to login using another authentication token (e.g. an SSH key). To disable the account use:

The factual accuracy of this article or section is disputed.

In case of system emergency, the recovery prompt is going to ask you for a root password, making it impossible to log into recovery shell. To automatically unlock the root account in case of emergency add SYSTEMD_SULOGIN_FORCE=1 environment variable to rescue.service using a drop-in file:

kdesu may be used under KDE to launch GUI applications with root privileges. It is possible that by default kdesu will try to use su even if the root account is disabled. Fortunately one can tell kdesu to use sudo instead of su. Create/edit the file ~/.config/kdesurc:

or use the following command:

Let us say you create 3 users: admin, devel, and archie. The user "admin" is used for journalctl, systemctl, mount, kill, and iptables; "devel" is used for installing packages, and editing configuration files; and "archie" is the user you log in with. To let "archie" reboot, shutdown, and use netctl we would do the following:

Edit /etc/pam.d/su and /etc/pam.d/su-l. Require user be in the wheel group, but do not put anyone in it.

Limit SSH login to the 'ssh' group. Only "archie" will be part of this group.

Restart sshd.service.

Add users to other groups.

Set permissions on configs so devel can edit them.

With this setup, you will almost never need to login as the root user.

"archie" can connect to their home Wi-Fi.

"archie" can not use netctl as any other user.

When "archie" needs to use journalctl or kill run away process they can switch to that user.

But "archie" cannot switch to the root user.

If "archie" want to start a gnu-screen session as admin they can do it like this:

sudo parses files contained in the directory /etc/sudoers.d/. This means that instead of editing /etc/sudoers, you can change settings in standalone files and drop them in that directory. This has two advantages:

The format for entries in these drop-in files is the same as for /etc/sudoers itself. To edit them directly, use visudo -f /etc/sudoers.d/somefile. See sudoers(5) § Including other files from within sudoers for details.

The files in /etc/sudoers.d/ directory are parsed in lexicographical order, file names containing . or ~ are skipped. To avoid sorting problems, the file names should begin with two digits, e.g. 01_foo.

sudo provides the sudoedit command (equivalent to sudo -e). This is useful for editing files which can be edited by root only while still running the editor as a normal user, and using that user’s configuration.

To edit a file, set SUDO_EDITOR to the name of the editor and pass the file name to sudoedit. For example:

See #Using visudo and sudo(8) § e for ways to set the editor, but beware of possible security issues.

If multiple names are passed to sudo, all files are opened in the editor in a single invocation. A feature useful for merging files:

Users can enable the insults easter egg in sudo by adding the following line in the sudoers file with visudo.

Upon entering an incorrect password, this will replace Sorry, try again. message with humorous insults.

By default, there is no visual feedback when you input a password. That is done on purpose for extra security. However, if you wish to have visual input, you can enable it by adding this line:

To customize the password prompt with colors and/or bold fonts, set the SUDO_PROMPT environment variable in your shell initialization file and use tput(1).

For example, to set the password prompt to display Password: in bold red, use this:

Or use different colors with the default message like so:

See more on Color output in console and Bash/Prompt customization

U2F is great to use with sudo, as it can effectively eliminate the risk of shoulder surfing in public areas while still giving you conscious control to approve the prompt with a simple physical touch.

See Universal 2nd Factor#Passwordless sudo.

When using sudo, you may want to write to protected files. Using tee allows such a separation:

when a simple >/>> would not have worked.

A similar concept is useful when you forgot to start Vim with sudo when editing a file owned by an other user. In this case you can do the following inside Vim to save the file:

You can add this to your ~/.vimrc to make this trick easy-to-use with :w!! mapping in command mode:

The > /dev/null part explicitly throws away the standard output since we do not need to pass anything to another piped command.

More detailed explanation of how and why this works can be found in How does the vim “write with sudo” trick work? article on StackOverflow.

You can use sudo-rs as a standalone replacement for sudo, without requiring the sudo package.

Create /etc/pam.d/sudo following sudo's default configuration.

Also create /etc/pam.d/sudo-i for sudo -i:

Create or edit /etc/sudo-rs/config.toml:

sudo-rs supports both /etc/sudoers and /etc/sudoers-rs and uses the latter if it exists.

Optionally, you can replace sudo with sudo-rs by symlinking it from a higher priority PATH directory:

This article or section is a candidate for merging with #Configuration.

SSH does not allocate a tty by default when running a remote command. Without an allocated tty, sudo cannot prevent the password from being displayed. You can use ssh's -t option to force it to allocate a tty.

The Defaults option requiretty only allows the user to run sudo if they have a tty.

This article or section is a candidate for merging with #Configuration.

Sudo will union the user's umask value with its own umask (which defaults to 0022). This prevents sudo from creating files with more open permissions than the user's umask allows. While this is a sane default if no custom umask is in use, this can lead to situations where a utility run by sudo may create files with different permissions than if run by root directly. If errors arise from this, sudo provides a means to fix the umask, even if the desired umask is more permissive than the umask that the user has specified. Adding this (using visudo) will override sudo's default behavior:

This sets sudo's umask to root's default umask (0022) and overrides the default behavior, always using the indicated umask regardless of what umask the user as set.

**Examples:**

Example 1 (unknown):
```unknown
$ sudo pacman -Syu
```

Example 2 (unknown):
```unknown
sudo -iu user
```

Example 3 (unknown):
```unknown
$ sudo wpa_supplicant -B -i interface -c <(wpa_passphrase MYSSID passphrase)
```

Example 4 (unknown):
```unknown
Successfully initialized wpa_supplicant
Failed to open config file '/dev/fd/63', error: No such file or directory
Failed to read or parse configuration '/dev/fd/63'
```

---

## Intel graphics

**URL:** https://wiki.archlinux.org/title/Intel_graphics

**Contents:**
- Installation
- Loading
  - Early KMS
  - Enable GuC / HuC firmware loading
- Xorg configuration
  - With the modesetting driver
  - With the Intel driver
    - AccelMethod
    - Using Intel DDX driver with recent GPUs
    - Disabling TearFree, TripleBuffer, SwapbuffersWait

Since Intel provides and supports open source drivers, Intel graphics are essentially plug-and-play.

For a comprehensive list of Intel GPU models and corresponding chipsets and CPUs, see Wikipedia:Intel Graphics Technology and Gentoo:Intel#Feature support.

Also see Hardware video acceleration.

The Intel kernel module should load fine automatically on system boot.

If it does not happen, then:

Kernel mode setting (KMS) is supported by the i915 and xe drivers, and is enabled early since mkinitcpio v32, as the kms hook is included by default. For other setups, see Kernel mode setting#Early KMS start for instructions on how to enable KMS as soon as possible at the boot process.

Starting with Gen9 (Skylake and onwards), Intel GPUs include a Graphics micro (μ) Controller (GuC) which provides the following functionality:

To use this functionality, first ensure that linux-firmware-intel is installed, as it provides the GuC and HuC firmware files.

Next, the GuC firmware must be loaded. With regards to HuC support, some video features (e.g. CBR rate control on SKL low-power encoding mode) require loading the HuC firmware as well [6].

The new experimental xe driver enables Guc and Huc functionality by default.

For the i915 driver, GuC functionality is controlled by the enable_guc kernel module parameter. Its usage is as follows:

The factual accuracy of this article or section is disputed.

If GuC submission or HuC firmware loading is not enabled by default for your GPU, you can manually enable it.

Set the enable_guc= kernel module parameter. For example, with:

Regenerate the initramfs, on next boot you can verify both GuC and HuC are enabled by using dmesg:

If they are not supported by your graphics adapter you will see:

Alternatively, check using:

Note that the related warning is not fatal, as explained in [10]:

There is generally no need for any configuration to run Xorg.

However, to take advantage of some driver options or if Xorg does not start, you can create an Xorg configuration file.

If you have installed xf86-video-intel but want to load the modesetting driver explicitly instead of letting the DDX driver take priority, for example when trying to compare them:

Create an Xorg configuration file similar to the one below:

Additional options are added by the user on new lines below Driver. For the full list of options, see the intel(4) man page.

You may need to indicate Option "AccelMethod" when creating a configuration file, the classical options are UXA, SNA (default) and BLT.

If you experience issues with default SNA (e.g. pixelated graphics, corrupt text, etc.), try using UXA instead, which can be done by adding the following line to your configuration file:

See the "AccelMethod" option under intel(4) § CONFIGURATION DETAILS.

For Intel GPUs starting from Gen8 (Broadwell), the Iris Mesa driver is needed:

If you use a compositor (the default in modern desktop environment like GNOME, KDE Plasma, Xfce, etc.), then TearFree, TripleBuffer and SwapbuffersWait can usually be disabled to improve performance and decrease power consumption.

The i915 kernel module allows for configuration via module options. Some of the module options impact power saving.

A list of all options along with short descriptions and default values can be generated with the following command:

To check which options are currently enabled, run

You will note that many options default to -1, resulting in per-chip powersaving defaults. It is however possible to configure more aggressive powersaving by using module options.

Framebuffer compression (FBC) is a feature that can reduce power consumption and memory bandwidth during screen refreshes.

The feature will be automatically enabled if supported by the hardware. You can use the command below to verify whether it is is enabled:

If the parm is set to -1, you do not need to do anything. Otherwise, to force-enable FBC, use i915.enable_fbc=1 as kernel parameter or set in /etc/modprobe.d/i915.conf:

Enabling frame buffer compression on pre-Sandy Bridge CPUs results in endless error messages:

The goal of Intel Fastboot is to preserve the frame-buffer as setup by the BIOS or boot loader to avoid any flickering until Xorg has started.[16][17]

To force enable fastboot on platforms where it is not the default already, set i915.fastboot=1 as kernel parameter or set in /etc/modprobe.d/i915.conf:

See Intel GVT-g for details.

Starting with Gen6 (Sandy Bridge and onwards), Intel GPUs provide performance counters used for exposing internal performance data to drivers. The drivers and hardware registers refer to this infrastructure as the Observation Architecture (internally "OA") [18], but Intel's documentation also more generally refers to this functionality as providing Observability Performance Counters [19][dead link 2023-09-16—HTTP 404] [20][dead link 2023-09-16—HTTP 404].

By default, only programs running with the CAP_SYS_ADMIN (equivalent to root) or CAP_PERFMON capabilities can utilize the observation architecture [21] [22]. Most applications will be running without either of these, resulting in the following warning:

To enable performance support without using the capabilities (or root), set the kernel parameter as described in sysctl.

This can be useful for some full screen applications:

If it does not work, try:

where param is one of "Full", "Center" or "Full aspect".

The MESA_GL_VERSION_OVERRIDE environment variable can be used to override the reported OpenGL version to any application. For example, setting MESA_GL_VERSION_OVERRIDE=4.5 will report OpenGL 4.5.

See Hardware video acceleration#Verification.

To try the (experimental) new Xe driver, you need:

Note your PCI ID with:

Then add the following to your Kernel parameters with the appropriate PCI ID:

Make sure you have an alternate solution to boot in order to revert if necessary.

The SNA acceleration method causes tearing on some machines. To fix this, enable the TearFree option in the xf86-video-intel driver by adding the following line to your configuration file:

See the original bug report for more info.

TearFree support was added to the modesetting driver [27][28]. As the last release for the non-XWayland servers was the 21.1 release in 2021, this patch has not reached a stable release, so you will need xorg-server-gitAUR until then.

The intel-driver uses Triple Buffering for vertical synchronization; this allows for full performance and avoids tearing. To turn vertical synchronization off (e.g. for benchmarking) use this .drirc in your home directory:

DRI3 is the default DRI version in xf86-video-intel. On some systems this can cause issues such as this. To switch back to DRI2 add the following line to your configuration file:

For the modesetting driver, this method of disabling DRI3 does not work. Instead, one can set the environment variable LIBGL_DRI3_DISABLE=1.

Should you experience missing font glyphs in GTK applications, the following workaround might help. Edit /etc/environment to add the following line:

See also FreeDesktop bug 88584.

If you experience corrupted and/or frozen graphics in some applications (such as random colors filling the application window, extreme unreasonable blurriness, an application failing to update its graphics at all while performing other tasks without lag, etc), try running the application with OpenGL instead of Vulkan. This has occurred on some configurations with Intel Arc GPUs.

Some issues with X crashing, GPU hanging, or problems with X freezing, can be fixed by disabling the GPU usage with the NoAccel option - add the following lines to your configuration file:

Alternatively, try to disable the 3D acceleration only with the DRI option:

This issue is covered on the Xrandr page.

If after resuming from suspend, the hotkeys for changing the screen brightness do not take effect, check your configuration against the Backlight article.

If the problem persists, try one of the following kernel parameters:

Also make sure you are not using fastboot mode (i915.fastboot kernel parameter), it is known for breaking backlight controls.

If you experience corruption, unresponsiveness, lags or slow performance in Chromium and/or Firefox some possible solutions are:

A few seconds after X/Wayland loads the machine will freeze and journalctl will log a kernel crash referencing the Intel graphics as below:

This can be fixed by disabling execlist support which was changed to default on with kernel 4.0. Add the following kernel parameter:

This is known to be broken to at least kernel 4.0.5.

The video output of a Windows guest in VirtualBox sometimes hangs until the host forces a screen update (e.g. by moving the mouse cursor). Removing the enable_fbc=1 option fixes this issue.

Panel Self Refresh (PSR), a power saving feature used by Intel iGPUs is known to cause flickering in some instances FS#49628 FS#49371 FS#50605. A temporary solution is to disable this feature using the kernel parameter i915.enable_psr=0 or xe.enable_psr=0.

This can solve error messages like [i915] *ERROR* CPU pipe A FIFO underrun.

The classic mesa driver for Gen 3 GPUs included in the mesa-amber package reports OpenGL 2.0 by default, because the hardware is not fully compatible with OpenGL 2.1.[29] OpenGL 2.1 support can be enabled manually by setting /etc/drirc or ~/.drirc options like:

One of the low-resolution video ports may be enabled on boot which is causing the terminal to utilize a small area of the screen. To fix, explicitly disable the port with an i915 module setting with video=SVIDEO-1:d in the kernel command line parameter in the boot loader. See Kernel parameters for more info.

If that does not work, try disabling TV1 or VGA1 instead of SVIDEO-1. Video port names can be listed with xrandr.

According to a Linux kernel issue, sound will not be output through HDMI if intel_iommu=on. To fix this problem, use the following kernel parameter:

Or alternatively, disable IOMMU:

The factual accuracy of this article or section is disputed.

Low-powered Intel processors and/or laptop processors have a tendency to randomly hang or crash due to the problems with the power management features found in low-power Intel chips. If such a crash happens, you will not see any logs reporting this problem. Adding the following Kernel parameters may help to resolve the problem.

ahci.mobile_lpm_policy=1 fixes a hang on several Lenovo laptops and some Acer notebooks due to problematic SATA controller power management. That workaround is strictly not related to Intel graphics but it does solve related issues. Adding this kernel parameter sets the link power management from firmware default to maximum performance and will also solve hangs when you change display brightness on certain Lenovo machines but increases idle power consumption by 1-1.5 W on modern ultrabooks. For further information, especially about the other states, see the Linux kernel mailing list and Red Hat documentation.

i915.enable_dc=0 disables GPU power management. This does solve random hangs on certain Intel systems, notably Goldmount and Kaby Lake Refresh chips. Using this parameter does result in higher power use and shorter battery life on laptops/notebooks. If this helps, you can try finer-grained DC limitations as documented in modinfo i915 | grep enable_dc.

intel_idle.max_cstate=1 limits the processors sleep states, it prevents the processor from going into deep sleep states. That is absolutely not ideal and does result in higher power use and lower battery life. However, it does solve random hangs on many Intel systems. Use this if you have a Intel Baytrail or a Kaby Lake Refresh chip. Intel "Baytrail" chips were known to randomly hang without this kernel parameter due to a hardware flaw, theoretically fixed 2019-04-26. More information about the max_cstate parameter can be found in the kernel documentation and about the cstates in general on a writeup on GitHub.

If you try adding intel_idle.max_cstate=1 i915.enable_dc=0 ahci.mobile_lpm_policy=1 in the hope of fixing frequent hangs and that solves the issue you should later remove one by one to see which of them actually helped you solve the issue. Running with cstates and display power management disabled is not advisable if the actual problem is related to SATA power management and ahci.mobile_lpm_policy=1 is the one that actually solves it.

Check Linux Reviews for more details.

In case you infrequently wake up to a black screen, but the system otherwise properly resumes with CPU pipe A FIFO underrun messages in the journal and limiting intel_idle.max_cstate=1 reliably prevents that, you can use Suspend and hibernate#Sleep hooks and cpupower-idle-set(1) to effectively control the C-state around the suspend cycle with -D0 and -E to not permanently run the CPU in the lowest C-state.

This article or section is a candidate for merging with Kernel mode setting#Forcing modes and EDID.

For some 165Hz monitors, xrandr might not display the 165Hz option, and the fix in #Adding undetected resolutions does not solve this. In this case, see i915-driver-stuck-at-40hz-on-165hz-screen.

Then append edid in HOOKS of /etc/mkinitcpio.conf, Just like this:

Then regenerate the initramfs.

Users with Raptor Lake and Alder Lake-P 12th gen mobile processor laptops from various vendors experienced freeze and black-screen after waking up from suspending. It is because many laptop vendors ship an incorrect VBT (Video BIOS Table), as described in freedesktop issues 5531 6401, that wrongly describe the actual ports connected to the iGPU. In this case, all of the documented cases concern duplicate eDP entries.

Considering most vendors will not publish a BIOS update for a laptop with a properly working Windows OS, Linux users could only address the issue on the kernel side. There are two methods for a user to prevent the duplicate eDP entries from affecting the kernel: patching the kernel or loading a modified VBT.

For patching the kernel, the duplicate eDP entry needs to be identified by analyzing the output of:

This shows that there is in fact a duplicate eDP, and the kernel should ignore the second entry, but the user is still encouraged to check this. This can then be patched with the following kernel patch in which the index of the duplicate screen can be substituted for ignoreEntry = 1 if it needs be.

A second way to solve this is to edit the VBT by directly erasing the duplicate entry from the VBT.

This works by copying the VBT and editing it with a hex editor and changing the device type corresponding with the duplicate device handle to 00 00:

The modified VBT can then be loaded by copying it to /lib/firmware/modified_vbt passing i915.vbt_firmware=modified_vbt as a kernel parameter and, if required, regenerate the initramfs.

By default some monitors might not be recognized properly by the Intel GPU and have washed out colors because it's not in full-range RGB mode.

If you are using GNOME, an alternative is to add <rgbrange>full</rgbrange> to the ~/.config/monitors.xml configuration. For example:

When getting an error like this when running some programs (eg. vainfo, falkon, mpv...):

The probable cause could be disabled ReBar (Resizable BAR) in BIOS/UEFI. Some motherboards may allow to enable ReBar only when UEFI mode is active, without legacy support.

**Examples:**

Example 1 (unknown):
```unknown
Ctrl+Alt+Fn
```

Example 2 (unknown):
```unknown
/etc/modprobe.d/
```

Example 3 (unknown):
```unknown
/usr/lib/modprobe.d/
```

Example 4 (unknown):
```unknown
enable_guc=3
```

---

## Dual boot with Windows

**URL:** https://wiki.archlinux.org/title/Dual_boot

**Contents:**
- Important information
  - Windows UEFI vs BIOS limitations
  - Boot loader UEFI vs BIOS limitations
  - UEFI Secure Boot
  - Fast Startup and hibernation
    - Windows settings
      - Disable Fast Startup and disable hibernation
      - Disable Fast Startup and enable hibernation
      - Enable Fast Startup and enable hibernation
  - Windows filenames limitations

This is an article detailing different methods of Arch/Windows coexistence.

Microsoft imposes limitations on which firmware boot mode and partitioning style can be supported based on the version of Windows used:

In case of pre-installed systems, all systems pre-installed with Windows 8/8.1, 10 and 11 boot in UEFI/GPT mode. Up to Windows 10, the firmware bitness matches the bitness of Windows, ie. x86_64 Windows boot in x86_64 UEFI mode and 32-bit Windows boot in IA32 UEFI mode.

An easy way to detect the boot mode of Windows is to do the following[1]:

In general, Windows forces type of partitioning depending on the firmware mode used, i.e. if Windows is booted in UEFI mode, it can be installed only to a GPT disk. If Windows is booted in Legacy BIOS mode, it can be installed only to an MBR disk. This is a limitation enforced by Windows Setup, and as of April 2014 there is no officially (Microsoft) supported way of installing Windows in UEFI/MBR or BIOS/GPT configuration. Thus Windows only supports either UEFI/GPT boot or BIOS/MBR configuration.

Such a limitation is not enforced by the Linux kernel, but can depend on which boot loader is used and/or how the boot loader is configured. The Windows limitation should be considered if the user wishes to boot Windows and Linux from the same disk, since installation procedure of boot loader depends on the firmware type and disk partitioning configuration. In case where Windows and Linux dual boot from the same disk, it is advisable to follow the method used by Windows, ie. either go for UEFI/GPT boot or BIOS/MBR boot. See https://support.microsoft.com/kb/2581408 for more information.

Most of the Linux boot loaders installed for one firmware type cannot launch or chainload boot loaders of the other firmware type. That is, if Arch is installed in UEFI/GPT or UEFI/MBR mode in one disk and Windows is installed in BIOS/MBR mode in another disk, the UEFI boot loader used by Arch cannot chainload the BIOS installed Windows in the other disk. Similarly if Arch is installed in BIOS/MBR or BIOS/GPT mode in one disk and Windows is installed in UEFI/GPT in another disk, the BIOS boot loader used by Arch cannot chainload UEFI installed Windows in the other disk.

The only exceptions to this are GRUB in Apple Macs in which GRUB in UEFI mode can boot BIOS installed OS via appleloader command (does not work in non-Apple systems), and rEFInd which technically supports booting legacy BIOS OS from UEFI systems, but does not always work in non-Apple UEFI systems as per its author Rod Smith.

However if Arch is installed in BIOS/GPT in one disk and Windows is installed in BIOS/MBR mode in another disk, then the BIOS boot loader used by Arch can boot the Windows in the other disk, if the boot loader itself has the ability to chainload from another disk.

Windows Setup creates a 100 MiB EFI system partition (except for Advanced Format 4K native drives where it creates a 300 MiB ESP), so multiple kernel usage is limited. Workarounds include:

All pre-installed Windows 8/8.1, 10 and 11 systems by default boot in UEFI/GPT mode and have UEFI Secure Boot enabled by default. This is mandated by Microsoft for all OEM pre-installed systems.

Arch Linux install media does not support Secure Boot yet. See Secure Boot#Booting an installation medium.

It is advisable to disable UEFI Secure Boot in the firmware setup manually before attempting to boot Arch Linux. Windows 8/8.1, 10 and 11 SHOULD continue to boot fine even if Secure boot is disabled. The only issue with regards to disabling UEFI Secure Boot support is that it requires physical access to the system to disable secure boot option in the firmware setup, as Microsoft has explicitly forbidden presence of any method to remotely or programmatically (from within OS) disable secure boot in all Windows 8/8.1 and above pre-installed systems

The factual accuracy of this article or section is disputed.

Secure Boot changes should not affect BitLocker: the issue might be self-signing the windows boot loader and chain-loading it from a boot loader: as long as the Windows boot loader stays signed with MS keys, and the Microsoft certs enrolled it should be fine. Is the issue disabling it and booting Windows or disabling, reenabling it and then booting Windows? The first one would be understandable, the second one is something to warn about.

If you intend to use Secure Boot for Linux as well, you may need to perform changes to the Secure Boot settings. Those changes prevent unlocking the BitLocker disk without the recovery key, leading to permanent data loss. Before proceeding, check if this is the case and save your BitLocker recovery key if not already done. This is especially important if Windows was preinstalled by the vendor.

There are two OSs that can be hibernated, you can hibernate Windows and boot Linux (or another OS), or you can hibernate Linux and boot Windows, or hibernate both OSs.

For the same reason, if you share one EFI system partition between Windows and Linux, then the EFI system partition may be damaged if you hibernate (or shutdown with Fast Startup enabled) Windows and then start Linux, or hibernate Linux and then start Windows. Check the respective section in EFI system partition for mitigation strategies.

ntfs-3g added a safe-guard to prevent read-write mounting of hibernated NTFS filesystems, but the NTFS driver within the Linux kernel has no such safeguard.

Windows cannot read filesystems such as ext4 by default that are commonly used for Linux. These filesystems do not have to be considered, unless you install a Windows driver for them.

Fast Startup is a feature in Windows 8 and above that hibernates the computer rather than actually shutting it down to speed up boot times.

There are multiple options regarding the Windows settings for Fast Startup and hibernation that are covered in the next sections.

The procedure of disabling Fast Startup is described in the tutorials for Windows 8, Windows 10 and Windows 11. In any case if you disable a setting, make sure to disable the setting and then shut down Windows, before installing Linux; note that rebooting is not sufficient.

This is the safest option, and recommended if you are unsure about the issue, as it requires the least amount of user awareness when rebooting from one OS into the other. You may share the same EFI system partition between Windows and Linux.

In a Windows command-line shell with administrator privileges:

This option requires user awareness when rebooting from one OS into the other. If you want to start Linux while Windows is hibernated, which is a common use case, then

The same considerations apply as in case "Disable Fast Startup and enable hibernation", but since Windows can not be shut down fully, only hibernated, you can never read-write mount any filesystem that was mounted by Windows while Windows is hibernated.

Windows is limited to filepaths being shorter than 260 characters.

Windows also puts certain characters off limits in filenames for reasons that run all the way back to DOS:

These are limitations of Windows and not NTFS: any other OS using the NTFS partition will be fine. Windows will fail to detect these files and running chkdsk will most likely cause them to be deleted. This can lead to potential data-loss.

NTFS-3G applies Windows restrictions to new file names through the windows_names option: ntfs-3g(8) § Windows_Filename_Compatibility (see fstab).

The recommended way to set up a Linux/Windows dual booting system is to first install Windows, only using part of the disk for its partitions. When you have finished the Windows setup, boot into the Linux install environment where you can create and resize partitions for Linux while leaving the existing Windows partitions untouched. The Windows installation will create the EFI system partition which can be used by your Linux boot loader. If you are installing Windows from scratch, do note that the EFI System partition created by Windows Setup will be too small for most use cases. See #The EFI system partition created by Windows Setup is too small.

If you already have Windows installed, it will already have created some partitions on a GPT-formatted disk:

Using the Disk Management utility in Windows, check how the partitions are labelled and which type gets reported. The Reserved Partition may not be visible in the Disk Management utility in which case it can be identified using diskpart in windows cmd. This will help you understand which partitions are essential to Windows, and which others you might repurpose. The Windows Disk Management utility can also be used to shrink Windows (NTFS) partitions to free up disk space for additional partitions for Linux.

You can then proceed with partitioning, depending on your needs. The boot loader needs to support chainloading other EFI applications to dual boot Windows and Linux. An additional EFI system partition should not be created, as it may prevent Windows from booting.

Simply mount the existing partition.

Computers that come with newer versions of Windows often have Secure Boot enabled. You will need to take extra steps to either disable Secure Boot or to make your installation media compatible with secure boot (see above and in the linked page).

Even though the recommended way to set up a Linux/Windows dual booting system is to first install Windows, it can be done the other way around. In contrast to installing Windows before Linux, you will have to set aside a partition for Windows, say 40GB or larger, in advance. Or have some unpartitioned disk space, or create and resize partitions for Windows from within the Linux installation, before launching the Windows installation.

Windows will use the already existing EFI system partition. Follows an outline, assuming Secure Boot is disabled in the firmware.

The following assumes GRUB is used as a boot loader (although the process is likely similar for other boot loaders) and that Windows 10 will be installed on a GPT block device with an existing EFI system partition (see the "System partition" section in the Microsoft documentation for more information).

Create with program gdisk on the block device the following three new partitions. See [5] for more precise partition sizes.

Create NTFS file systems on the new Microsoft basic data and Windows RE (recovery) partitions using the mkntfs program from package ntfs-3g.

Reboot the system into a Windows 10 installation media. When prompted to install select the custom install option and install Windows on the Microsoft basic data partition created earlier. This should also install Microsoft EFI files in the EFI system partition.

After installation (set up of and logging into Windows not required), reboot into Linux and generate a GRUB configuration for the Windows boot manager to be available in the GRUB menu on next boot.

See #Windows UEFI vs BIOS limitations.

See Unified Extensible Firmware Interface#Windows changes boot order.

If you have a GPT-partitioned disk and erased (e.g. with mkfs.fat -F32 /dev/sdx) the EFI system partition, you will notice that Windows Boot Manager will either disappear from your boot options, or selecting it will send you back to the UEFI.

To remedy it, boot with a Windows installation media, press Shift+F10 to open the console (or click NEXT > Repair Computer > Troubleshoot... > Advanced > Command Prompt), then start the diskpart utility:

Select the appropriate hard drive by typing:

Make sure that there is a partition of type system (the EFI system partition):

Select this partition:

and assign a temporary drive letter to it:

To make sure that drive letter is correctly assigned:

Navigate to C:\ (or what your system drive letter is):

Next is the "magic" command, which recreate the BCD store (with /s for the mount point, /f for firmware type, optionally add /v for verbose):

You should now have Windows Boot Manager working as a boot option, and thus have access to Windows. Just make sure to never format your EFI system partition again!

See [6], [7] and [8].

By default, Windows Setup creates a 100 MiB EFI system partition (except for Advanced Format 4K native drives where it creates a 300 MiB ESP). This is generally too small to fit everything you need. You can replace the existing EFI system partition with a new, larger one.

If you are installing Windows from scratch, you can dictate the size of the EFI system partition during installation[9]:

Once Windows is installed, you can resize the primary partition down within Windows and then reboot and go about your usual Arch install, filling the space you just created.

Alternatively, you can use the Arch install media to create a single EFI system partition of your preferred size before you install Windows on the drive. Windows Setup will use the EFI system partition you made instead of creating its own.

On BIOS systems, Windows cumulative updates may fail with the error We couldn’t complete the updates. Undoing changes. Don’t turn off your computer. In such case, while in Windows, you need to set the Windows partition as active.

After successfully installing the Windows update, mark back your Linux partition as active, using the commands above.

When it comes to pairing Bluetooth devices with both the Linux and Windows installation, both systems have the same MAC address, but will use different link keys generated during the pairing process. This results in the device being unable to connect to one installation, after it has been paired with the other. To allow a device to connect to either installation without re-pairing, follow Bluetooth#Dual boot pairing.

**Examples:**

Example 1 (unknown):
```unknown
appleloader
```

Example 2 (unknown):
```unknown
esp/EFI/Microsoft/Boot/Fonts/
```

Example 3 (unknown):
```unknown
esp/EFI/Microsoft/Boot/
```

Example 4 (unknown):
```unknown
COMPRESSION="xz"
COMPRESSION_OPTIONS=(-9e)
MODULES_DECOMPRESS="yes"
```

---

## Xorg

**URL:** https://wiki.archlinux.org/title/Video_drivers

**Contents:**
- Installation
  - Driver installation
    - AMD
- Running
- Configuration
  - Using .conf files
  - Using xorg.conf
- Input devices
  - Input identification
  - Mouse acceleration

X.Org Server — commonly referred to as simply X — is the X.Org Foundation implementation of the X Window System (X11) display server, and it is the most popular display server among Linux users. Its ubiquity has led to making it an ever-present requisite for GUI applications, resulting in massive adoption from most distributions.

For the alternative and successor, see Wayland.

Xorg can be installed with the xorg-server package.

Additionally, some packages from the xorg-apps group are necessary for certain configuration tasks. They are pointed out in the relevant sections.

Finally, an xorg group is also available, which includes Xorg server packages, packages from the xorg-apps group and fonts.

This article or section is a candidate for moving to Graphics processing unit.

The Linux kernel includes open-source video drivers and support for hardware accelerated framebuffers. However, userland support is required for OpenGL, Vulkan and 2D acceleration in X11.

First, identify the graphics card (the Subsystem output shows the specific model):

Then, install an appropriate driver. You can search the package database for a complete list of open-source Device Dependent X (DDX) drivers:

However, hardware-specific DDX is considered legacy nowadays. There is a generic modesetting(4) DDX driver in xorg-server, which uses kernel mode setting and works well on modern hardware. The modesetting DDX driver uses Glamor[1] for 2D acceleration, which requires OpenGL.

If you want to install another DDX driver, note that Xorg searches for installed DDX drivers automatically:

In order for video acceleration to work, and often to expose all the modes that the GPU can set, a proper video driver is required:

Other DDX drivers can be found in the xorg-drivers group.

Xorg should run smoothly without closed source drivers, which are typically needed only for advanced features such as fast 3D-accelerated rendering for games. The exceptions to this rule are recent GPUs (especially NVIDIA GPUs) not supported by open source drivers.

For a translation of model names (e.g. Radeon RX 6800) to GPU architectures (e.g. RDNA 2), see Wikipedia:List of AMD graphics processing units#Features overview.

The Xorg(1) command is usually not run directly. Instead, the X server is started with either a display manager or xinit.

Xorg uses a configuration file called xorg.conf and files ending in the suffix .conf for its initial setup: the complete list of the folders where these files are searched can be found in xorg.conf(5), together with a detailed explanation of all the available options.

The /etc/X11/xorg.conf.d/ directory stores host-specific configuration. You are free to add configuration files there, but they must have a .conf suffix: the files are read in ASCII order, and by convention their names start with XX- (two digits and a hyphen, so that for example 10 is read before 20). These files are parsed by the X server upon startup and are treated like part of the traditional xorg.conf configuration file. Note that on conflicting configuration, the file read last will be processed. For this reason, the most generic configuration files should be ordered first by name. The configuration entries in the xorg.conf file are processed at the end.

For option examples to set, see Fedora:Input device configuration#xorg.conf.d.

Xorg can also be configured via /etc/X11/xorg.conf or /etc/xorg.conf. You can also generate a skeleton for xorg.conf with:

This should create a xorg.conf.new file in /root/ that you can copy over to /etc/X11/xorg.conf.

Alternatively, your proprietary video card drivers may come with a tool to automatically configure Xorg: see the article of your video driver, NVIDIA or AMDGPU PRO, for more details.

For input devices the X server defaults to the libinput driver (xf86-input-libinput), but xf86-input-evdev and related drivers are available as alternative.[2]

Udev, which is provided as a systemd dependency, will detect hardware and both drivers will act as hotplugging input driver for almost all devices, as defined in the default configuration files 10-quirks.conf and 40-libinput.conf in the /usr/share/X11/xorg.conf.d/ directory.

After starting X server, the log file will show which driver hotplugged for the individual devices (note the most recent log file name may vary):

If both do not support a particular device, install the needed driver from the xorg-drivers group. The same applies, if you want to use another driver.

To influence hotplugging, see #Configuration.

For specific instructions, see also the libinput article, the following pages below, or Fedora:Input device configuration for more examples.

See Keyboard input#Identifying keycodes in Xorg.

See Mouse acceleration.

See libinput or Synaptics.

See Keyboard configuration in Xorg.

For a headless configuration, the xf86-video-dummy driver is necessary; install it and create a configuration file, such as the following:

See main article Multihead for general information.

You must define the correct driver to use and put the bus ID of your graphic cards (in decimal notation).

To get your bus IDs (in hexadecimal):

The bus IDs here are 0:2:0 and 1:0:0.

By default, Xorg always sets DPI to 96 since 2009-01-30. A change was made with version 21.1 to provide proper DPI auto-detection, but reverted.

The DPI of the X server can be set with the -dpi command line option.

Having the correct DPI is helpful where fine detail is required (like font rendering). Previously, manufacturers tried to create a standard for 96 DPI (a 10.3" diagonal monitor would be 800x600, a 13.2" monitor 1024x768). These days, screen DPIs vary and may not be equal horizontally and vertically. For example, a 19" widescreen LCD at 1440x900 may have a DPI of 89x87.

To see if your display size and DPI are correct:

Check that the dimensions match your display size.

If you have specifications on the physical size of the screen, they can be entered in the Xorg configuration file so that the proper DPI is calculated (adjust identifier to your xrandr output):

If you only want to enter the specification of your monitor without creating a full xorg.conf, create a new configuration file. For example (/etc/X11/xorg.conf.d/90-monitor.conf):

If you do not have specifications for physical screen width and height (most specifications these days only list by diagonal size), you can use the monitor's native resolution (or aspect ratio) and diagonal length to calculate the horizontal and vertical physical dimensions. Using the Pythagorean theorem on a 13.3" diagonal length screen with a 1280x800 native resolution (or 16:10 aspect ratio):

This will give the pixel diagonal length, and with this value you can discover the physical horizontal and vertical lengths (and convert them to millimeters):

For RandR compliant drivers (for example the open source ATI driver), you can set it by:

To make it permanent, see Autostarting#On Xorg startup.

You can manually set the DPI by adding the option under the Device or Screen section:

GTK very often overrides the server's DPI via the optional X resource Xft.dpi. To find out whether this is happening to you, check with:

With GTK library versions since 3.16, when this variable is not otherwise explicitly set, GTK sets it to 96. To have GTK apps obey the server DPI you may need to explicitly set Xft.dpi to the same value as the server. The Xft.dpi resource is the method by which some desktop environments optionally force DPI to a particular value in personal settings. Among these are KDE and TDE.

DPMS is a technology that allows power saving behaviour of monitors when the computer is not in use. This will allow you to have your monitors automatically go into standby after a predefined period of time.

The Composite extension for X causes an entire sub-tree of the window hierarchy to be rendered to an off-screen buffer. Applications can then take the contents of that buffer and do whatever they like. The off-screen buffer can be automatically merged into the parent window, or merged by external programs called compositing managers. For more information, see Wikipedia:Compositing window manager.

Some window managers (e.g. Compiz, Enlightenment, KWin, marco, metacity, muffin, mutter, Xfwm) do compositing on their own. For other window managers, a standalone composite manager can be used.

This section lists utilities for automating keyboard / mouse input and window operations (like moving, resizing or raising).

See also Clipboard#Tools and an overview of X automation tools.

This article or section is out of date.

To run a nested session of another desktop environment:

This will launch a Window Maker session in a 1024 by 768 window within your current X session.

This needs the package xorg-server-xnest to be installed.

A more modern way of doing a nested X session is with Xephyr.

See xinit#Starting applications without a window manager.

See main article: OpenSSH#X11 forwarding.

With the help of xinput you can temporarily disable or enable input sources. This might be useful, for example, on systems that have more than one mouse, such as the ThinkPads and you would rather use just one to avoid unwanted mouse clicks.

Install the xorg-xinput package.

Find the name or ID of the device you want to disable:

For example in a Lenovo ThinkPad T500, the output looks like this:

Disable the device with xinput --disable device, where device is the device ID or name of the device you want to disable. In this example we will disable the Synaptics Touchpad, with the ID 10:

To re-enable the device, just issue the opposite command:

Alternatively using the device name, the command to disable the touchpad would be:

You can disable a particular input source using a configuration snippet:

device is an arbitrary name, and driver_name is the name of the input driver, e.g. libinput. device_name is what is actually used to match the proper device. For alternate methods of targeting the correct device, such as libinput's MatchIsTouchscreen, consult your input driver's documentation. Though this example uses libinput, this is a driver-agnostic method which simply prevents the device from being propagated to the driver.

Run script on hotkey:

Dependencies: xorg-xprop, xdotool

See also #Killing an application visually.

To block tty access when in an X add the following to xorg.conf:

This can be used to help restrict command line access on a system accessible to non-trusted users.

To prevent a user from killing X when it is running add the following to xorg.conf:

When an application is misbehaving or stuck, instead of using kill or killall from a terminal and having to find the process ID or name, xorg-xkill allows to click on said application to close its connection to the X server. Many existing applications do indeed abort when their connection to the X server is closed, but some can choose to continue.

Xorg may run with standard user privileges instead of root (so-called "rootless" Xorg). This is a significant security improvement over running as root. Note that some popular display managers do not support rootless Xorg (e.g. LightDM or XDM).

You can verify which user Xorg is running as with ps -o user= -C Xorg.

See also Xorg.wrap(1), systemd-logind(8), Systemd/User#Xorg as a systemd user service, Fedora:Changes/XorgWithoutRootRights and FS#41257.

To configure rootless Xorg using xinitrc:

Note that executing startx directly without exec leaves the shell open in the case of a xorg crash. Since some lock screens are executed inside xorg, this can lead to full access to the executing user.

GDM will run Xorg without root privileges by default when kernel mode setting is used.

When Xorg is run in rootless mode, Xorg logs are saved to ~/.local/share/xorg/Xorg.log. However, the stdout and stderr output from the Xorg session is not redirected to this log. To re-enable redirection, start Xorg with the -keeptty flag and redirect the stdout and stderr output to a file:

Alternatively, copy /etc/X11/xinit/xserverrc to ~/.xserverrc, and append -keeptty. See [5].

As explained above, there are circumstances in which rootless Xorg is defaulted to. If this is the case for your configuration, and you have a need to run Xorg as root, you can configure Xorg.wrap(1) to require root:

Wayback is an X11 compatibility layer which allows for running full X11 desktop environments (and window managers) using Wayland components. It's available from the AUR as wayback-x11AUR package.

If a problem occurs, view the log stored in either /var/log/ or, for the rootless X default since v1.16, in ~/.local/share/xorg/. GDM users should check the systemd journal. [6]

The logfiles are of the form Xorg.n.log with n being the display number. For a single user machine with default configuration the applicable log is frequently Xorg.0.log, but otherwise it may vary. To make sure to pick the right file it may help to look at the timestamp of the X server session start and from which console it was started. For example:

X creates configuration and temporary files in current user's home directory. Make sure there is free disk space available on the partition your home directory resides in. Unfortunately, X server does not provide any more obvious information about lack of disk space in this case.

If you use a Matrox card and DRI stopped working after upgrading to Xorg, try adding the line:

to the Device section that references the video card in xorg.conf.

X fails to start with the following log messages:

To correct, uninstall the xf86-video-fbdev package.

Error message: unable to load font `(null)'.

Some programs only work with bitmap fonts. Two major packages with bitmap fonts are available, xorg-fonts-75dpi and xorg-fonts-100dpi. You do not need both; one should be enough. To find out which one would be better in your case, try xdpyinfo from xorg-xdpyinfo, like this:

and use what is closer to the shown value.

If Xorg is set to boot up automatically and for some reason you need to prevent it from starting up before the login/display manager appears (if the system is wrongly configured and Xorg does not recognize your mouse or keyboard input, for instance), you can accomplish this task with two methods.

Depending on setup, you will need to do one or more of these steps:

If you are getting Client is not authorized to connect to server, try adding the line:

to /etc/pam.d/su and /etc/pam.d/su-l. pam_xauth will then properly set environment variables and handle xauth keys.

If the filesystem (specifically /tmp) is full, startx will fail. The log file will contain:

Make some free space on the relevant filesystem and X will start.

Your color depth is set wrong. It may need to be 24 instead of 16, for example.

If X terminates with error message SocketCreateListener() failed, you may need to delete socket files in /tmp/.X11-unix. This may happen if you have previously run Xorg as root (e.g. to generate an xorg.conf).

That error means that only the current user has access to the X server. The solution is to give access to root:

That line can also be used to give access to X to a different user than root.

**Examples:**

Example 1 (unknown):
```unknown
$ lspci -v -nn -d ::03xx
```

Example 2 (unknown):
```unknown
$ pacman -Ss xf86-video
```

Example 3 (unknown):
```unknown
/usr/share/X11/xorg.conf.d/
```

Example 4 (unknown):
```unknown
/etc/X11/xorg.conf.d/
```

---

## Xorg

**URL:** https://wiki.archlinux.org/title/X_server

**Contents:**
- Installation
  - Driver installation
    - AMD
- Running
- Configuration
  - Using .conf files
  - Using xorg.conf
- Input devices
  - Input identification
  - Mouse acceleration

X.Org Server — commonly referred to as simply X — is the X.Org Foundation implementation of the X Window System (X11) display server, and it is the most popular display server among Linux users. Its ubiquity has led to making it an ever-present requisite for GUI applications, resulting in massive adoption from most distributions.

For the alternative and successor, see Wayland.

Xorg can be installed with the xorg-server package.

Additionally, some packages from the xorg-apps group are necessary for certain configuration tasks. They are pointed out in the relevant sections.

Finally, an xorg group is also available, which includes Xorg server packages, packages from the xorg-apps group and fonts.

This article or section is a candidate for moving to Graphics processing unit.

The Linux kernel includes open-source video drivers and support for hardware accelerated framebuffers. However, userland support is required for OpenGL, Vulkan and 2D acceleration in X11.

First, identify the graphics card (the Subsystem output shows the specific model):

Then, install an appropriate driver. You can search the package database for a complete list of open-source Device Dependent X (DDX) drivers:

However, hardware-specific DDX is considered legacy nowadays. There is a generic modesetting(4) DDX driver in xorg-server, which uses kernel mode setting and works well on modern hardware. The modesetting DDX driver uses Glamor[1] for 2D acceleration, which requires OpenGL.

If you want to install another DDX driver, note that Xorg searches for installed DDX drivers automatically:

In order for video acceleration to work, and often to expose all the modes that the GPU can set, a proper video driver is required:

Other DDX drivers can be found in the xorg-drivers group.

Xorg should run smoothly without closed source drivers, which are typically needed only for advanced features such as fast 3D-accelerated rendering for games. The exceptions to this rule are recent GPUs (especially NVIDIA GPUs) not supported by open source drivers.

For a translation of model names (e.g. Radeon RX 6800) to GPU architectures (e.g. RDNA 2), see Wikipedia:List of AMD graphics processing units#Features overview.

The Xorg(1) command is usually not run directly. Instead, the X server is started with either a display manager or xinit.

Xorg uses a configuration file called xorg.conf and files ending in the suffix .conf for its initial setup: the complete list of the folders where these files are searched can be found in xorg.conf(5), together with a detailed explanation of all the available options.

The /etc/X11/xorg.conf.d/ directory stores host-specific configuration. You are free to add configuration files there, but they must have a .conf suffix: the files are read in ASCII order, and by convention their names start with XX- (two digits and a hyphen, so that for example 10 is read before 20). These files are parsed by the X server upon startup and are treated like part of the traditional xorg.conf configuration file. Note that on conflicting configuration, the file read last will be processed. For this reason, the most generic configuration files should be ordered first by name. The configuration entries in the xorg.conf file are processed at the end.

For option examples to set, see Fedora:Input device configuration#xorg.conf.d.

Xorg can also be configured via /etc/X11/xorg.conf or /etc/xorg.conf. You can also generate a skeleton for xorg.conf with:

This should create a xorg.conf.new file in /root/ that you can copy over to /etc/X11/xorg.conf.

Alternatively, your proprietary video card drivers may come with a tool to automatically configure Xorg: see the article of your video driver, NVIDIA or AMDGPU PRO, for more details.

For input devices the X server defaults to the libinput driver (xf86-input-libinput), but xf86-input-evdev and related drivers are available as alternative.[2]

Udev, which is provided as a systemd dependency, will detect hardware and both drivers will act as hotplugging input driver for almost all devices, as defined in the default configuration files 10-quirks.conf and 40-libinput.conf in the /usr/share/X11/xorg.conf.d/ directory.

After starting X server, the log file will show which driver hotplugged for the individual devices (note the most recent log file name may vary):

If both do not support a particular device, install the needed driver from the xorg-drivers group. The same applies, if you want to use another driver.

To influence hotplugging, see #Configuration.

For specific instructions, see also the libinput article, the following pages below, or Fedora:Input device configuration for more examples.

See Keyboard input#Identifying keycodes in Xorg.

See Mouse acceleration.

See libinput or Synaptics.

See Keyboard configuration in Xorg.

For a headless configuration, the xf86-video-dummy driver is necessary; install it and create a configuration file, such as the following:

See main article Multihead for general information.

You must define the correct driver to use and put the bus ID of your graphic cards (in decimal notation).

To get your bus IDs (in hexadecimal):

The bus IDs here are 0:2:0 and 1:0:0.

By default, Xorg always sets DPI to 96 since 2009-01-30. A change was made with version 21.1 to provide proper DPI auto-detection, but reverted.

The DPI of the X server can be set with the -dpi command line option.

Having the correct DPI is helpful where fine detail is required (like font rendering). Previously, manufacturers tried to create a standard for 96 DPI (a 10.3" diagonal monitor would be 800x600, a 13.2" monitor 1024x768). These days, screen DPIs vary and may not be equal horizontally and vertically. For example, a 19" widescreen LCD at 1440x900 may have a DPI of 89x87.

To see if your display size and DPI are correct:

Check that the dimensions match your display size.

If you have specifications on the physical size of the screen, they can be entered in the Xorg configuration file so that the proper DPI is calculated (adjust identifier to your xrandr output):

If you only want to enter the specification of your monitor without creating a full xorg.conf, create a new configuration file. For example (/etc/X11/xorg.conf.d/90-monitor.conf):

If you do not have specifications for physical screen width and height (most specifications these days only list by diagonal size), you can use the monitor's native resolution (or aspect ratio) and diagonal length to calculate the horizontal and vertical physical dimensions. Using the Pythagorean theorem on a 13.3" diagonal length screen with a 1280x800 native resolution (or 16:10 aspect ratio):

This will give the pixel diagonal length, and with this value you can discover the physical horizontal and vertical lengths (and convert them to millimeters):

For RandR compliant drivers (for example the open source ATI driver), you can set it by:

To make it permanent, see Autostarting#On Xorg startup.

You can manually set the DPI by adding the option under the Device or Screen section:

GTK very often overrides the server's DPI via the optional X resource Xft.dpi. To find out whether this is happening to you, check with:

With GTK library versions since 3.16, when this variable is not otherwise explicitly set, GTK sets it to 96. To have GTK apps obey the server DPI you may need to explicitly set Xft.dpi to the same value as the server. The Xft.dpi resource is the method by which some desktop environments optionally force DPI to a particular value in personal settings. Among these are KDE and TDE.

DPMS is a technology that allows power saving behaviour of monitors when the computer is not in use. This will allow you to have your monitors automatically go into standby after a predefined period of time.

The Composite extension for X causes an entire sub-tree of the window hierarchy to be rendered to an off-screen buffer. Applications can then take the contents of that buffer and do whatever they like. The off-screen buffer can be automatically merged into the parent window, or merged by external programs called compositing managers. For more information, see Wikipedia:Compositing window manager.

Some window managers (e.g. Compiz, Enlightenment, KWin, marco, metacity, muffin, mutter, Xfwm) do compositing on their own. For other window managers, a standalone composite manager can be used.

This section lists utilities for automating keyboard / mouse input and window operations (like moving, resizing or raising).

See also Clipboard#Tools and an overview of X automation tools.

This article or section is out of date.

To run a nested session of another desktop environment:

This will launch a Window Maker session in a 1024 by 768 window within your current X session.

This needs the package xorg-server-xnest to be installed.

A more modern way of doing a nested X session is with Xephyr.

See xinit#Starting applications without a window manager.

See main article: OpenSSH#X11 forwarding.

With the help of xinput you can temporarily disable or enable input sources. This might be useful, for example, on systems that have more than one mouse, such as the ThinkPads and you would rather use just one to avoid unwanted mouse clicks.

Install the xorg-xinput package.

Find the name or ID of the device you want to disable:

For example in a Lenovo ThinkPad T500, the output looks like this:

Disable the device with xinput --disable device, where device is the device ID or name of the device you want to disable. In this example we will disable the Synaptics Touchpad, with the ID 10:

To re-enable the device, just issue the opposite command:

Alternatively using the device name, the command to disable the touchpad would be:

You can disable a particular input source using a configuration snippet:

device is an arbitrary name, and driver_name is the name of the input driver, e.g. libinput. device_name is what is actually used to match the proper device. For alternate methods of targeting the correct device, such as libinput's MatchIsTouchscreen, consult your input driver's documentation. Though this example uses libinput, this is a driver-agnostic method which simply prevents the device from being propagated to the driver.

Run script on hotkey:

Dependencies: xorg-xprop, xdotool

See also #Killing an application visually.

To block tty access when in an X add the following to xorg.conf:

This can be used to help restrict command line access on a system accessible to non-trusted users.

To prevent a user from killing X when it is running add the following to xorg.conf:

When an application is misbehaving or stuck, instead of using kill or killall from a terminal and having to find the process ID or name, xorg-xkill allows to click on said application to close its connection to the X server. Many existing applications do indeed abort when their connection to the X server is closed, but some can choose to continue.

Xorg may run with standard user privileges instead of root (so-called "rootless" Xorg). This is a significant security improvement over running as root. Note that some popular display managers do not support rootless Xorg (e.g. LightDM or XDM).

You can verify which user Xorg is running as with ps -o user= -C Xorg.

See also Xorg.wrap(1), systemd-logind(8), Systemd/User#Xorg as a systemd user service, Fedora:Changes/XorgWithoutRootRights and FS#41257.

To configure rootless Xorg using xinitrc:

Note that executing startx directly without exec leaves the shell open in the case of a xorg crash. Since some lock screens are executed inside xorg, this can lead to full access to the executing user.

GDM will run Xorg without root privileges by default when kernel mode setting is used.

When Xorg is run in rootless mode, Xorg logs are saved to ~/.local/share/xorg/Xorg.log. However, the stdout and stderr output from the Xorg session is not redirected to this log. To re-enable redirection, start Xorg with the -keeptty flag and redirect the stdout and stderr output to a file:

Alternatively, copy /etc/X11/xinit/xserverrc to ~/.xserverrc, and append -keeptty. See [5].

As explained above, there are circumstances in which rootless Xorg is defaulted to. If this is the case for your configuration, and you have a need to run Xorg as root, you can configure Xorg.wrap(1) to require root:

Wayback is an X11 compatibility layer which allows for running full X11 desktop environments (and window managers) using Wayland components. It's available from the AUR as wayback-x11AUR package.

If a problem occurs, view the log stored in either /var/log/ or, for the rootless X default since v1.16, in ~/.local/share/xorg/. GDM users should check the systemd journal. [6]

The logfiles are of the form Xorg.n.log with n being the display number. For a single user machine with default configuration the applicable log is frequently Xorg.0.log, but otherwise it may vary. To make sure to pick the right file it may help to look at the timestamp of the X server session start and from which console it was started. For example:

X creates configuration and temporary files in current user's home directory. Make sure there is free disk space available on the partition your home directory resides in. Unfortunately, X server does not provide any more obvious information about lack of disk space in this case.

If you use a Matrox card and DRI stopped working after upgrading to Xorg, try adding the line:

to the Device section that references the video card in xorg.conf.

X fails to start with the following log messages:

To correct, uninstall the xf86-video-fbdev package.

Error message: unable to load font `(null)'.

Some programs only work with bitmap fonts. Two major packages with bitmap fonts are available, xorg-fonts-75dpi and xorg-fonts-100dpi. You do not need both; one should be enough. To find out which one would be better in your case, try xdpyinfo from xorg-xdpyinfo, like this:

and use what is closer to the shown value.

If Xorg is set to boot up automatically and for some reason you need to prevent it from starting up before the login/display manager appears (if the system is wrongly configured and Xorg does not recognize your mouse or keyboard input, for instance), you can accomplish this task with two methods.

Depending on setup, you will need to do one or more of these steps:

If you are getting Client is not authorized to connect to server, try adding the line:

to /etc/pam.d/su and /etc/pam.d/su-l. pam_xauth will then properly set environment variables and handle xauth keys.

If the filesystem (specifically /tmp) is full, startx will fail. The log file will contain:

Make some free space on the relevant filesystem and X will start.

Your color depth is set wrong. It may need to be 24 instead of 16, for example.

If X terminates with error message SocketCreateListener() failed, you may need to delete socket files in /tmp/.X11-unix. This may happen if you have previously run Xorg as root (e.g. to generate an xorg.conf).

That error means that only the current user has access to the X server. The solution is to give access to root:

That line can also be used to give access to X to a different user than root.

**Examples:**

Example 1 (unknown):
```unknown
$ lspci -v -nn -d ::03xx
```

Example 2 (unknown):
```unknown
$ pacman -Ss xf86-video
```

Example 3 (unknown):
```unknown
/usr/share/X11/xorg.conf.d/
```

Example 4 (unknown):
```unknown
/etc/X11/xorg.conf.d/
```

---

## Core dump

**URL:** https://wiki.archlinux.org/title/Systemd-coredump

**Contents:**
- Disabling automatic core dumps
  - Using sysctl
  - Using systemd
  - Using PAM limits
  - Using ulimit
- Making a core dump
  - Where do they go?
- Managing the core dump files
  - Cleanup of core dump files
- Analyzing a core dump

A core dump is a file containing a process's address space (memory) when the process terminates unexpectedly. Core dumps may be produced on-demand (such as by a debugger), or automatically upon termination. Core dumps are triggered by the kernel in response to program crashes, and may be passed to a helper program (such as systemd-coredump(8)) for further processing. A core dump is not typically used by an average user, but developers could use it as a post-mortem snapshot of the program's state at the time of the crash, especially if the fault is hard to reliably reproduce.

Users may wish to disable automatic core dumps for a number of reasons:

sysctl can be used to set the kernel.core_pattern to nothing to disable core dump handling. Create this file

To apply the setting immediately, use sysctl:

systemd's default behavior is defined in /usr/lib/sysctl.d/50-coredump.conf, which sets kernel.core_pattern to call systemd-coredump. It generates core dumps for all processes in /var/lib/systemd/coredump. systemd-coredump behavior can be overridden by creating a configuration snippet in the /etc/systemd/coredump.conf.d/ directory with the following content (See coredump.conf(5) § DESCRIPTION, [1]):

Then reload the systemd manager configuration with daemon-reload.

See systemd-coredump(8) § Disabling coredump processing.

See limits.conf#core.

Command-line shells such as bash or zsh provide a builtin ulimit command which can be used to report or set resource limits of the shell and the processes started by the shell. See bash(1) § SHELL BUILTIN COMMANDS or zshbuiltins(1) for details.

To disable core dumps in the current shell:

If the system is setup to pipe coredumps into a program such as systemd-coredump using kernel.core_pattern, the Linux kernel itself ignores the ulimit setting (see core(5)), so then it depends on the program the dump gets piped to whether this setting is respected or not (systemd-coredump will still use it).

For programs not using the ulimit setting of the crashed process, dumpable prctl(2) can be used to disable coredump processing for selected processes.

To generate a core dump of an arbitrary process, first install the gdb package. Then attach to this process by following Debugging/Getting traces#Attaching to an existing process.

Then at the (gdb) prompt:

Now you have a coredump file called core.2071.

The kernel.core_pattern sysctl decides where automatic core dumps go. By default, core dumps are sent to systemd-coredump which can be configured in /etc/systemd/coredump.conf. By default, all core dumps are stored in /var/lib/systemd/coredump (due to Storage=external) and they are compressed with zstd (due to Compress=yes). Additionally, various size limits for the storage can be configured.

To retrieve a core dump from the journal, see coredumpctl(1).

Use coredumpctl to find the corresponding dump. Note that regular users can run coredumpctl without special privileges to manage core dumps of their processes.

The core dump files stored in /var/lib/systemd/coredump/ will be automatically cleaned by systemd-tmpfiles --clean, which is triggered daily with systemd-tmpfiles-clean.timer. Core dumps are configured to persist for at least 2 weeks, see systemd-tmpfiles --cat-config.

See Journal#Clean journal files manually to remove the entries.

First, you need to uniquely identify the relevant dump. This is possible by specifying a PID, name of the executable, path to the executable or a journalctl predicate (see coredumpctl(1) and journalctl(1) for details). To see details of the core dumps:

Pay attention to "Signal" row, that helps to identify crash cause. For the analysis one usually examine the backtrace using a debugger (gdb(1) by default):

When gdb is started, use the bt command to print the full backtrace:

In many cases, the output will contain question marks as placeholders for missing debugging symbols. See Debugging/Getting traces for how to obtain them.

**Examples:**

Example 1 (unknown):
```unknown
kernel.core_pattern
```

Example 2 (unknown):
```unknown
/etc/sysctl.d/50-coredump.conf
```

Example 3 (unknown):
```unknown
kernel.core_pattern=|/bin/false
```

Example 4 (unknown):
```unknown
# sysctl -p /etc/sysctl.d/50-coredump.conf
```

---

## NTFS

**URL:** https://wiki.archlinux.org/title/NTFS

**Contents:**
- Tips and tricks
  - Improving performance
  - Prevent creation of names not allowed by Windows
- Known issues
  - Explicit file system type required to mount
- Troubleshooting
  - File system mounts read-only
  - unknown filesystem type 'ntfs'
  - Unable to mount with ntfs3 with partition marked dirty
- See also

The ntfs3 kernel driver provides read and write support for the file system.

There are no userspace utilities alongside the kernel driver. To format partitions or perform maintenance you still need a Windows machine or external tools like NTFS-3G.

You can enable the prealloc mount(8) option to decrease fragmentation in case of parallel write operations (most useful for HDD).

NTFS itself does not have restrictions for characters and names used, but Windows does.

ntfs3 supports windows_names mount(8) option. Use it to strictly maintain compatibility.

ntfs3 may require the file system type to mount, or may otherwise mount read-only, see #File system mounts read-only.

To be able to mount the file system, specify its type as ntfs3. For example, using mount(8)'s -t/--types option:

The kernel from linux package has enabled CONFIG_NTFS_FS compatibility option [1]. It mimics the legacy driver behavior and mounts the file system read-only when ntfs type is used for mount.

To mount the file system read-write, use ntfs3 type. See #Explicit file system type required to mount.

When mounting NTFS, you can encounter an error such as:

See #Explicit file system type required to mount.

If you want to use ntfs3 as the default driver for ntfs partitions, such udev rule does the trick:

Although, this method is not recommended and can confuse some 3rd party tools.

When trying to mount a good NTFS partition (i.e. which successfully mounts with NTFS-3G and for which ntfsfix --no-action does not report any error), you may get the following error:

ntfs3 will not mount a partition where the volume is marked dirty without the force option. dmesg explicitly helps recognizing the situation, saying:

You can try passing the --clear-dirty argument to ntfsfix(8) to clean it. [2]

**Examples:**

Example 1 (unknown):
```unknown
windows_names
```

Example 2 (unknown):
```unknown
# mount -t ntfs3 /dev/sdxY /mnt
```

Example 3 (unknown):
```unknown
/dev/sdxY /mnt ntfs3 defaults 0 0
```

Example 4 (unknown):
```unknown
CONFIG_NTFS_FS
```

---

## sshguard

**URL:** https://wiki.archlinux.org/title/Sshguard

**Contents:**
- Installation
- Configuration
  - firewalld
  - UFW
  - iptables
  - nftables
- Usage
  - systemd
  - syslog-ng
- Configuration

sshguard is a daemon that protects SSH and other services against brute-force attacks, similar to fail2ban.

sshguard is different from the latter in that it is written in C, is lighter and simpler to use with fewer features while performing its core function equally well.

sshguard is not vulnerable to most (or maybe any) of the log analysis vulnerabilities that have caused problems for similar tools.

Install the sshguard package.

sshguard works by monitoring /var/log/auth.log, syslog-ng or the systemd journal for failed login attempts. For each failed attempt, the offending host is banned from further communication for a limited amount of time. The default amount of time the offender is banned starts at 120 seconds, and is increases by a factor of 1.5 every time it fails another login. sshguard can be configured to permanently ban a host with too many failed attempts.

Both temporary and permanent bans are done by adding an entry into the "sshguard" chain in iptables that drops all packets from the offender. The ban is then logged to syslog and ends up in /var/log/auth.log, or the systemd journal if the latter is being used.

You must configure one of the following firewalls to be used with sshguard in order for blocking to work.

This article or section is out of date.

sshguard can work with firewalld. Make sure you have firewalld enabled, configured and setup first. To make sshguard write to your zone of preference, issue the following commands:

If you use ipv6, you can issue the same command but substitute sshguard4 with sshguard6. Finish with

You can verify the above with

Finally, in /etc/sshguard.conf, find the line for BACKEND and change it as follows

If UFW is installed and enabled, it must be given the ability to pass along DROP control to sshguard. This is accomplished by modifying /etc/ufw/before.rules and /etc/ufw/before6.rules to contain the following lines which should be inserted just after the section for loopback devices.

Restart ufw after making this modification.

The main configuration required is creating a chain named sshguard, where sshguard automatically inserts rules to drop packets coming from bad hosts:

Then add a rule to jump to the sshguard chain from the INPUT chain. This rule must be added before any other rules processing the ports that sshguard is protecting. Use the following line to protect FTP and SSH or see [1] for more examples.

Change the value of BACKEND to the following:

When you start/enable the sshguard.service, two new tables named sshguard in the ip and ip6 address families are added which filter incoming traffic through sshguard's list of IP addresses. The chains in the sshguard table have a priority of -10 and will be processed before other rules of lower priority. See sshguard-setup(7) and nftables for more information.

Enable and start sshguard.service.

If you have syslog-ng installed, you may start sshguard directly from the command line instead.

Configuration is done in /etc/sshguard.conf which is required for sshguard to start. A commented example is located at /usr/share/doc/sshguard/sshguard.conf.sample or can also be found on Bitbucket sshguard.conf.sample.

By default in the Arch-provided configuration file, offenders become permanently banned once they reach a "danger" level of 120 (or 12 failed logins; see attack dangerousness for more details). This behavior can be modified by prepending a danger level to the blacklist file.

The 200: in this example tells sshguard to permanently ban a host after achieving a danger level of 200.

Finally, restart sshguard.service.

A slightly more aggressive banning rule than the default one is proposed here to illustrate various options:

For some users under constant attack, a more aggressive banning policy can be adopted. If you are confident that accidental failed logins are unlikely, you can instruct SSHGuard to permanently ban hosts after a single failed login. Modify the parameters in the configuration file in the following way:

Finally restart sshguard.service.

Also, to prevent multiple authentication attempts during a single connection, you may want to change /etc/ssh/sshd_config by defining:

Restart sshd.service for this change to take effect.

If you ban yourself, you can wait to get unbanned automatically or use iptables or nftables to unban yourself.

You will also need to remove the IP address from /var/db/sshguard/blacklist.db in order to make unbanning persistent.

First check if your IP is banned by sshguard:

Then use the following command to unban, with the line-number as identified in the former command:

Remove your IP address from the attackers set:

where family is either ip or ip6.

To see what is being passed to sshguard, examine the script in /usr/lib/systemd/scripts/sshguard-journalctl and the systemd service sshguard.service. An equivalent command to view the logs in the terminal:

**Examples:**

Example 1 (unknown):
```unknown
/var/log/auth.log
```

Example 2 (unknown):
```unknown
/var/log/auth.log
```

Example 3 (unknown):
```unknown
# firewall-cmd --permanent --zone=public --add-rich-rule="rule source ipset=sshguard4 drop"
```

Example 4 (unknown):
```unknown
# firewall-cmd --reload
```

---

## Environment variables

**URL:** https://wiki.archlinux.org/title/Environment_variable

**Contents:**
- Utilities
- Defining variables
  - Globally
    - Using shell initialization files
    - Using pam_env
  - Per user
    - Graphical environment
      - Per desktop environment session
      - Per Xorg session
      - Per Wayland session

An environment variable is a named object that contains data used by one or more applications. In simple terms, it is a variable with a name and a value. The value of an environmental variable can for example be the location of all executable files in the file system, the default editor that should be used, or the system locale settings. Users new to Linux may often find this way of managing settings a bit unmanageable. However, environment variables provide a simple way to share configuration settings between multiple applications and processes in Linux.

The coreutils package contains the programs printenv and env. To list the current environmental variables with values:

The env utility can be used to run a command under a modified environment. The following example will launch xterm with the environment variable EDITOR set to vim. This will not affect the global environment variable EDITOR.

The shell builtin set(1p) allows you to change the values of shell options, set the positional parameters and to display the names and values of shell variables.

Each process stores their environment in the /proc/$PID/environ file. This file contains each key value pair delimited by a nul character (\x0). A more human readable format can be obtained with sed, e.g. sed 's:\x0:\n:g' /proc/$PID/environ.

To avoid needlessly polluting the environment, you should seek to restrict the scope of variables. In fact, graphical sessions and systemd services require you to set variables in certain locations for them to take effect. The scopes of environment variables are broken down into the contexts they affect:

Most Linux distributions tell you to change or add environment variable definitions in /etc/profile or other locations. Keep in mind that there are also package-specific configuration files containing variable settings such as /etc/locale.conf. Be sure to maintain and manage the environment variables and pay attention to the numerous files that can contain environment variables. In principle, any shell script can be used for initializing environmental variables, but following traditional UNIX conventions, these statements should only be present in some particular files.

The following files can be used for defining global environment variables on your system, each with different limitations:

The following Bash helper function can be used to append a number of directories to the PATH environment variable. Add the function at the top of the file where you define your environment (e.g. ~/.bashrc). The function will only add directories that actually exist on the filesystem, and it will avoid creating duplicate entries.

Most shells (including Bash, Zsh, and fish) allow adding variables to the environment using the export command. This allows defining environment variables in a common file such as ~/my-environment.sh:

This file can then be sourced from shell startup files:

The PAM module pam_env(8) loads the variables to be set in the environment from the following files in order: /etc/security/pam_env.conf and /etc/environment.

/etc/environment must consist of simple VARIABLE=value pairs on separate lines, for example:

/etc/security/pam_env.conf has the following format:

@{HOME} and @{SHELL} are special variables that expand to what is defined in /etc/passwd. The following example illustrates how to expand the HOME environment variable into another variable:

The format also allows to expand already defined variables in the values of other variables using ${VARIABLE} , like this:

VARIABLE=value pairs are also allowed, but variable expansion is not supported in those pairs. See pam_env.conf(5) for more information.

You do not always want to define an environment variable globally. For instance, you might want to add /home/my_user/bin to the PATH variable but do not want all other users on your system to have that in their PATH too. Local environment variables can be defined in many different files:

To add a directory to the PATH for local usage, put following in ~/.bash_profile:

To update the variable, re-login or source the file: $ source ~/.bash_profile.

If an environment variable only affects graphical applications, you may want to restrict the scope of it by only setting it within the graphical session. In order of decreasing scope:

Some graphical environments, (e.g. KDE Plasma) support executing shell scripts at login: they can be used to set environment variables. See KDE#Autostart for example.

The procedure for modifying the environment of the Xorg session depends on how it is started:

Though the end of the script depends on which file it is, and any advanced syntax depends on the shell used, the basic usage is universal:

Since Wayland does not initiate any Xorg related files, GDM and KDE Plasma source systemd user environment variables instead.

No other display managers supporting Wayland sessions (e.g. SDDM) provide direct support for this yet. However, LightDM and SDDM source startup scripts for login shells on Wayland sessions too.

greetd also sources /etc/profile and ~/.profile - this behavior is controlled by its source_profile setting, enabled by default.

If your display manager sources startup scripts like ~/.bash_profile and you want to use environment.d, you can source it like so:

To set environment variables only for a specific application instead of the whole session, edit the application's .desktop file. See Desktop entries#Modify environment variables for instructions.

For Steam games, you can configure a program's environment by editing its launch options; see Steam#Launch options.

Sometimes only temporary variables are required. One might wish to temporarily run executables from a specific directory without typing the absolute PATH each time, or use the PATH in a short temporary shell script.

For example, to add a session-specific directory to PATH, use:

To add only a shell-specific directory to PATH, use:

In Bash, PATH is already exported by default, so both of the above will remain visible to subprocesses unless overwritten. To better illustrate the difference between exported and non-exported variables, consider the following:

The following section lists a number of common environment variables used by a Linux system and describes their values.

**Examples:**

Example 1 (unknown):
```unknown
$ env EDITOR=vim xterm
```

Example 2 (unknown):
```unknown
/proc/$PID/environ
```

Example 3 (unknown):
```unknown
sed 's:\x0:\n:g' /proc/$PID/environ
```

Example 4 (unknown):
```unknown
/etc/profile
```

---

## Improving performance/Boot process

**URL:** https://wiki.archlinux.org/title/Improving_performance/Boot_process

**Contents:**
- Analyzing the boot process
  - Using systemd-analyze
  - Using bootchart2
- Compiling a custom kernel
- Initramfs
- Choose the adequate way to start for services
- Staggered spin-up
- Filesystem mounts
- Less output during boot
- Changing boot loader

Improving the boot performance of a system can provide reduced boot wait times and serves as a means to learn more about how certain system files and scripts interact with one another. This article attempts to aggregate methods on how to improve the boot performance of an Arch Linux system.

systemd provides a tool called systemd-analyze that can be used to show timing details about the boot process, including an svg plot showing units waiting for their dependencies. You can see which unit files are causing your boot process to slow down. You can then optimize your system accordingly.

To see how much time was spent in kernelspace and userspace on boot, simply use:

To list the started unit files, sorted by the time each of them took to start up:

At some points of the boot process, things can not proceed until a given unit succeeds. To see which units find themselves at these critical points in the startup chain, do:

You can also create an SVG file which describes your boot process graphically, similar to Bootchart:

See systemd-analyze(1) for details.

You could also use Bootchart2 to visualize the boot sequence.

Compiling a custom kernel can reduce boot time and memory usage. Though with the standardization of the 64-bit architecture and the modular nature of the Linux kernel, these benefits may not be as great as expected. See Kernel#Compilation for more info.

Compression level of modules of official kernel is built with ZSTD_CLEVEL=19. But ZSTD_CLEVEL=1 may be better for SSD.

It is recommended to make modules for storage and file system for your root volume built-in to allow running without initramfs.

If possible in your setup, running without initramfs should provide the fastest way.

mkinitcpio uses the base and udev hooks by default. Faster boot times can be achieved by replacing them with systemd. See Mkinitcpio#Common hooks for more details. See also Fsck#Boot time checking if replacing the fsck hook.

In a similar approach to #Compiling a custom kernel, the initramfs can be slimmed down. A simple way is to include the mkinitcpio autodetect hook. Booster generates initramfs smaller than mkinitcpio or dracut with fast single binary init. See Minimal initramfs or Booster#Removing modules.

Depending on your hardware (processor and storage speed), using lz4 instead of the default zstd compression option may be quicker since the faster decompression speed at boot time usually offsets the slightly larger size of the initramfs that has to be read from disk. See Mkinitcpio#COMPRESSION.

You can also minimize Microcode image [1] by intel-ucode if you use running without initramfs or Booster:

One central feature of systemd is D-Bus and socket activation. This feature should be preferred for most cases as it causes services to be started only when they are first accessed and is generally a good thing (e.g. having cups.service enabled at boot time is usually not useful for desktop use, enable instead cups.socket which will only start the service when actually printing).

However, if you know that a service (like upower) will always be started during boot, then the overall boot time might be reduced by starting it as early as possible. This can be achieved (if the service file is set up for it, which in most cases it is) by enabling upower.service.

This will cause systemd to start UPower as soon as possible, without causing races with the socket or D-Bus activation.

Some hardware implements staggered spin-up, which causes the OS to probe ATA interfaces serially, which can spin up the drives one-by-one and reduce the peak power usage. This slows down the boot speed, and on most consumer hardware provides no benefits at all since the drives will already spin-up immediately when the power is turned on. To check if SSS is being used:

If it was not used during boot, there will be no output.

To disable it, add the libahci.ignore_sss=1 kernel parameter.

Thanks to mkinitcpio's fsck hook, you can avoid a possibly costly remount of the root partition by changing ro to rw on the kernel line: options can be set with rootflags=rw,other_mount_options. The entry must be removed from the /etc/fstab file, otherwise the systemd-remount-fs.service will continue to try applying these settings. Alternatively, one could try to mask that unit.

If Btrfs is in use for the root filesystem, there is no need for a fsck on every boot like other filesystems. If this is the case, mkinitcpio's fsck hook can be removed. You may also want to mask the systemd-fsck-root.service, or tell it not to fsck the root filesystem from the kernel command line using fsck.mode=skip. Without mkinitcpio's fsck hook, systemd will still fsck any relevant filesystems with the systemd-fsck@.service

You can also remove API filesystems from /etc/fstab, as systemd will mount them itself (see pacman -Ql systemd | grep '\.mount$' for a list). It is not uncommon for users to have a /tmp entry carried over from sysvinit, but you may have noticed from the command above that systemd already takes care of this. Ergo, it may be safely removed.

Other filesystems, like /home or EFI system partition, can be mounted with custom mount units. Adding noauto,x-systemd.automount to mount options will buffer all access to that partition, and will fsck and mount it on first access, reducing the number of filesystems it must fsck/mount during the boot process.

For some systems, particularly those with an SSD, the slow performance of the TTY is actually a bottleneck, and so less output means faster booting. See the Silent boot article for suggestions.

Changing your boot loader (e.g. a simpler boot loader such as systemd-boot) may reduce boot time by seconds.

If your setup allows it, try using only an EFI boot stub for even shorter boot times.

The best way to reduce boot time is not booting at all. Consider suspending your system to RAM instead.

**Examples:**

Example 1 (unknown):
```unknown
systemd-analyze
```

Example 2 (unknown):
```unknown
$ systemd-analyze
```

Example 3 (unknown):
```unknown
$ systemd-analyze blame
```

Example 4 (unknown):
```unknown
$ systemd-analyze critical-chain
```

---

## systemd/FAQ

**URL:** https://wiki.archlinux.org/title/Systemd/FAQ

**Contents:**
- FAQ
  - Why do I get log messages on my console?
  - How do I change the default number of gettys?
  - How do I get more verbose output during boot?
  - How do I avoid clearing the console after boot?
  - What kernel options are required for systemd?
  - What other units does a unit depend on?
  - My computer shuts down, but the power stays on
  - How can I make a script start during the boot process?
  - Service unit active state is "active (exited)" in green

For an up-to-date list of known issues, look at the upstream TODO.

You must set the kernel loglevel yourself. Historically, /etc/rc.sysinit did this for us and set dmesg's loglevel to 3, which was a reasonably quiet loglevel. Either add loglevel=3 or quiet to your kernel parameters.

Currently, only one getty is launched by default. If you switch to another TTY, a getty will be launched there (socket-activation style). In other words, Ctl+Alt+F2 will launch a new getty on tty2.

By default, the number of auto-activated gettys is capped at six. Thus F7 through F12 will not launch a getty.

If you want to change this behavior, then edit /etc/systemd/logind.conf and change the value of NAutoVTs. If you want all Fx keys to start a getty, increase the value of NAutoVTs to 12. If you are forwarding journald to tty12, increase the value of NAutoVTs to 11 (thus leaving tty12 free).

You can also pre-activate gettys which will be running from boot.

To add another pre-activated getty, enable and start getty@ttyX.service.

To remove a getty, disable and stop the relevant getty@ttyX.service.

systemd does not use the /etc/inittab file.

If you see no output at all in console after the initram message, this means you have the quiet parameter in your kernel line. It is best to remove it, at least the first time you boot with systemd, to see if everything is ok. Then, you will see a list [ OK ] in green or [ FAILED ] in red.

Any messages are logged to the system log and if you want to find out about the status of your system, use systemctl or look at the boot/system log with journalctl.

Create a directory called /etc/systemd/system/getty@.service.d and place nodisallocate.conf in there to override the TTYVTDisallocate option to no.

Kernels prior to 3.0 are unsupported.

If you use a custom kernel, you will need to make sure that systemd's options are selected.

If you are compiling a new kernel for use with an installed version of systemd, the required and recommended options are listed in the systemd README file /usr/share/doc/systemd/README.

If you are preparing to install a new version of systemd and are running a custom kernel, the most recent version of the file can be found in the systemd GitHub repository.

For example, if you want to figure out which services a target like multi-user.target pulls in, use something like this:

Instead of Wants you might also try WantedBy, Requires, RequiredBy, Conflicts, ConflictedBy, Before, After for the respective types of dependencies and their inverse.

Use systemctl poweroff instead of systemctl halt.

Create a new file as /etc/systemd/system/myscript.service and add the following contents:

This example assumes you want your script to start up when the target multi-user is launched. Make sure the script is executable.

Enable myscript.service to start the service at boot.

This is the expected behaviour for oneshot services that also use RemainAfterExit=yes. See systemd.service(5) § OPTIONS for more information. Some examples of oneshot services that behave this way are systemd-user-sessions.services, nftables.service, and iptables.service.

You may encounter the following error when attempting to enable a unit:

This can occur when the symlink created by enabling a unit already exists in /etc/systemd/system/. This typically happens when switching from one display manager to another one (for instance GDM to SDDM, which can be enabled with gdm.service and sddm.service, respectively) and the corresponding symlink /etc/systemd/system/display-manager.service already exists.

To solve this problem, either first disable the relevant display manager before enabling the new one, or use the -f/--force option when enabling the new one to overwrite any existing conflicting symlinks (per systemctl(1) § OPTIONS).

**Examples:**

Example 1 (unknown):
```unknown
/etc/rc.sysinit
```

Example 2 (unknown):
```unknown
/etc/systemd/logind.conf
```

Example 3 (unknown):
```unknown
getty@ttyX.service
```

Example 4 (unknown):
```unknown
getty@ttyX.service
```

---

## Laptop/HP

**URL:** https://wiki.archlinux.org/title/Laptop/HP

**Contents:**
- Model list
  - Compaq
  - EliteBook
  - Envy
  - Other
  - Pavilion
  - ProBook
  - Spectre
  - ZBook
- Troubleshooting

Tablet Mode Detection/Automatic On Screen Keyboard Automatic Screen Rotation Microphone Mute Button and LED Volume Mute Button LED

Even if UEFI, Arch Linux and (e.g.) GRUB are correctly configured and with the correct UEFI NVRAM variables set, the system may not boot from the HDD/SSD. The problem is that HP hard coded the paths for the OS boot manager in their UEFI boot manager to \EFI\Microsoft\Boot\bootmgfw.efi to boot Microsoft Windows, regardless of how the UEFI NVRAM variables are changed. There are two workarounds:

The latest HP firmware allows defining a “Customized Boot” path in the UEFI pre-boot graphical environment. Select the “Customized Boot” option in the UEFI pre-boot graphical environment under “Boot Options” and set the path to your OS boot loader on the ESP (see EFI system partition), e.g.:

\EFI\grub\grubx64.efi

Always verify the correct path to the .efi file. Also, adjust the device boot order (also in the UEFI pre-boot graphical environment) to boot this entry first.

This article or section is being considered for removal.

Change the UEFI application path of the OS boot loader to that hard coded path. On your EFI system partition; e.g. with esp being the EFI system partition mountpoint:

Since Linux 4.1x, laptop's fan may not spin down to a lower rev step (and noise) effectively appearing stuck at higher spinning speed with no apparent temperature reason. Possible workarounds are loading a 3D application, a quick suspend to ram or power off for more than 10 minutes. If those tricks do not work, the max CPU frequency can be set to a lower one. See CPU frequency scaling#Setting maximum and minimum frequencies. Related: [2] [3]

Follow the steps outlined in Suspend and hibernate#Hibernation. The suspend to disk process works correctly, but the laptop does not power itself off. To fix this, create the following file:

This file tells systemd to write shutdown instead of platform to /sys/power/disk before writing disk to /sys/power/state.

For making the mute LED work, append model=alc295-hp-x360 to snd-hda-intel's kernel module parameters.

For making the mute LED work, append model=hp-mute-led-mic3 to snd-hda-intel's kernel module parameters.

For making the mute LED work, append model=hp-dock to snd-hda-intel's kernel module parameters.

This Section applies if running lsusb shows a line containing 06cb:00df Synaptics, Inc.. There's at least two firmware issues that may make the fingerprint reader not work. Both can be solved with fwupd. Running fwupdmgr get-devices shows two sub-devices belonging to the fingerprint reader: The "Prometheus" device and its IOTA Config.

Upgrade to kernel >= 6.2.7 and add the following to your kernel boot parameters:

Check it the parameter was applied successfully with:

Driver issue tracker: GitLab

For EliteBook 845 G10 with AMD Ryzen 7 PRO 7840HS, the CPU frequency may be locked at 400-544MHz when switched to battery, a workaround is to blacklist the `amd_pmf` module.

Driver issue tracker: Bugzilla

If the hardware buttons for volume don't work, add enable_5_button_array=1 to intel_hid's kernel module parameters.

After shuting down from a Linux distribution, HP Omen 16 does not fully shutdown all the devices causing battery draining or energy draining when plugged. This may heat up the PC over 85ºC overnight possibly breaking the components. Unfortunately the latest BIOS (F.06) does not resolve the issue.

To be able to use Arch Linux (and any Linux distribution) there are two ways:

**Examples:**

Example 1 (unknown):
```unknown
AutoAddDevices
```

Example 2 (unknown):
```unknown
echo 1 > /sys/bus/pci/rescan
```

Example 3 (unknown):
```unknown
echo 1 > /sys/bus/pci/rescan
```

Example 4 (unknown):
```unknown
acpi.ec_no_wakeup=1
```

---

## Autostarting

**URL:** https://wiki.archlinux.org/title/Autostarting

**Contents:**
- On bootup / shutdown
- On user login / logout
- On device plug in / unplug
- On time events
- On filesystem events
- On shell login / logout
- On Xorg startup
- On desktop environment startup
- On window manager startup

This article links to various methods to launch scripts or applications automatically when some particular event is taking place.

Enable the relevant systemd services. If an application does not provide a systemd service, write your own.

Enable the relevant user unit.

Periodically at certain times, dates or intervals:

Once at a date and time:

Use an inotify event watcher:

See Command-line shell#Configuration files.

Most desktop environments implement XDG Autostart.

If the desktop environments has an article, see its Autostart section.

Many window managers (and compositors) implement XDG Autostart.

If it has an article, see its Autostart section.

---

## FUSE

**URL:** https://wiki.archlinux.org/title/FUSE

**Contents:**
- Unmounting
- List of FUSE filesystems
- See also

Filesystem in Userspace (FUSE) is a mechanism for Unix-like operating systems that lets non-privileged users create their own file systems without editing kernel code. This is achieved by running file system code in user space, while the FUSE kernel module provides only a "bridge" to the actual kernel interfaces.

FUSE filesystems can be unmounted using fusermount3(1) provided by fuse3 or using fusermount(1) provided by fuse2. E.g.:

**Examples:**

Example 1 (unknown):
```unknown
$ fusermount3 -u mountpoint
```

Example 2 (unknown):
```unknown
mount --bind
```

---

## System maintenance

**URL:** https://wiki.archlinux.org/title/Enhance_system_stability

**Contents:**
- Check for errors
  - Failed systemd services
  - Log files
- Backup
  - Configuration files
  - List of installed packages
  - Pacman database
  - Encryption metadata
  - System and user data
- Upgrading the system

Regular system maintenance is necessary for the proper functioning of Arch over a period of time. Timely maintenance is a practice many users get accustomed to.

Check if any systemd services have failed:

See systemd#Using units for more information.

Look for errors in the log files located in /var/log/, as well as messages logged in the systemd journal:

See Xorg#Troubleshooting for information on where and how Xorg logs errors.

Having backups of important data is a necessary measure to take, since human and machine processing errors are very likely to generate corruption as time passes, and also the physical media where the data is stored is inevitably destined to fail.

See Synchronization and backup programs for many alternative applications that may better suit your case. See Category:System recovery for other articles of interest.

It is highly encouraged to automate backups and test the recovery process to ensure everything works as intended. For automation see System backup#Automation.

Before editing any configuration files, create a backup so that you can revert to a working version in case of problems. Editors like vim and emacs can do this automatically. On a larger scale, consider using a configuration manager.

For dotfiles (configuration files in the home directory), see dotfiles#Tracking dotfiles directly with Git.

Maintain a list of all installed packages so that if a complete re-installation is inevitable, it is easier to re-create the original environment.

See pacman/Tips and tricks#List of installed packages for details.

See pacman/Tips and tricks#Back up the pacman database.

See Data-at-rest encryption#Backup for disk encryption scenarios.

It is recommended to perform full system upgrades regularly via pacman#Upgrading packages, to enjoy both the latest bug fixes and security updates, and also to avoid having to deal with too many package upgrades that require manual intervention at once. When requesting support from the community, it will usually be assumed that the system is up to date.

Make sure to have the Arch install media or another Linux "live" CD/USB available so you can easily rescue your system if there is a problem after updating. If you are running Arch in a production environment, or cannot afford downtime for any reason, test changes to configuration files, as well as updates to software packages, on a non-critical duplicate system first. Then, if no problems arise, roll out the changes to the production system.

If the system has packages from the AUR, carefully upgrade all of them.

pacman is a powerful package management tool, but it does not attempt to handle all corner cases. Users must be vigilant and take responsibility for maintaining their own system.

Before upgrading, users are expected to visit the Arch Linux home page to check the latest news, or alternatively subscribe to the RSS feed or the arch-announce mailing list. When updates require out-of-the-ordinary user intervention (more than what can be handled simply by following the instructions given by pacman), an appropriate news post will be made.

Before upgrading fundamental software (such as the kernel, xorg, systemd, or glibc) to a new version, look over the appropriate forum to see if there have been any reported problems.

Users must equally be aware that upgrading packages can raise unexpected problems that could need immediate intervention; therefore, it is discouraged to upgrade a stable system shortly before it is required for carrying out an important task. Instead, wait to upgrade until there is enough time available to resolve any post-upgrade issues.

Avoid doing partial upgrades. In other words, never run pacman -Sy; instead, always use pacman -Syu.

Generally avoid using the --overwrite option with pacman. The --overwrite option takes an argument containing a glob. When used, pacman will bypass file conflict checks for files that match the glob. In a properly maintained system, it should only be used when explicitly recommended by the Arch Developers. See the #Read before upgrading the system section.

Avoid using the -d option with pacman. pacman -Rdd package skips dependency checks during package removal. As a result, a package providing a critical dependency could be removed, resulting in a broken system.

Arch Linux is a rolling release distribution. That means when new library versions are pushed to the repositories, the Developers and Package Maintainers rebuild all the packages in the repositories that need to be rebuilt against the libraries. For example, if two packages depend on the same library, upgrading only one package might also upgrade the library (as a dependency), which might then break the other package which depends on an older version of the library.

That is why partial upgrades are not supported. Do not use:

When refreshing the package database, always do a full upgrade with pacman -Syu.

Be very careful when using IgnorePkg and IgnoreGroup for the same reason. If the system has locally built packages (such as AUR packages), users will need to rebuild them when their dependencies receive a soname bump.

If a partial upgrade scenario has been created, and binaries are broken because they cannot find the libraries they are linked against, do not "fix" the problem simply by symlinking. Libraries receive soname bumps when they are not backwards compatible. A simple pacman -Syu to a properly synced mirror will fix the problem as long as pacman is not broken.

When upgrading the system, be sure to pay attention to the alert notices provided by pacman. If any additional actions are required by the user, be sure to take care of them right away. If a pacman alert is confusing, search the forums or check the latest news on the Arch Linux homepage (see #Read before upgrading the system) for more detailed instructions.

When pacman is invoked, .pacnew and .pacsave files can be created. Pacman provides notice when this happens and users must deal with these files promptly. Users are referred to the pacman/Pacnew and Pacsave wiki page for detailed instructions.

Also, think about other configuration files you may have copied or created. If a package had an example configuration that you copied to your home directory, check to see if a new one has been created.

Upgrades are typically not applied to existing processes. You must restart processes to fully apply the upgrade.

The archlinux-contrib package provides a script called checkservices which runs pacdiff to merge .pacnew files then checks for processes running with outdated libraries and prompts the user if they want them to be restarted.

The kernel is particularly difficult to patch without a reboot. A reboot is always the most secure option, but if this is very inconvenient kernel live patching can be used to apply upgrades without a reboot.

If a package update is expected/known to cause problems, packagers will ensure that pacman displays an appropriate message when the package is updated. If experiencing trouble after an update, double-check pacman's output by looking at /var/log/pacman.log.

At this point, only after ensuring there is no information available through pacman, there is no relevant news on https://archlinux.org/, and there are no forum posts regarding the update, consider seeking help on the forum or over IRC. Downgrading the offending package to revert broken updates should be considered as a last resort.

After upgrading you may now have packages that are no longer needed or that are no longer in the official repositories.

Use pacman -Qtd to check for packages that were installed as a dependency but now, no other packages depend on them. If an orphaned package is still needed, it is recommended to change the installation reason to explicit. Otherwise, if the package is no longer needed, it can be removed. See pacman/Tips and tricks#Removing unused packages (orphans) for details.

Additionally, some packages may no longer be in the remote repositories, but they still may be on your local system. To list all foreign packages use pacman -Qm. Note that this list will include packages that have been installed manually (e.g., from the AUR). To exclude packages that are (still) available on the AUR, use the script from BBS#288205 or try the ancient-packagesAUR tool.

Pacman does a much better job than you at keeping track of files. If you install things manually you will, sooner or later, forget what you did, forget where you installed to, install conflicting software, install to the wrong locations, etc.

To clean up improperly installed files, see pacman/Tips and tricks#Identify files not owned by any package.

Always try open source drivers before resorting to proprietary drivers. Most of the time, open source drivers are more stable and reliable than proprietary drivers. Open source driver bugs are fixed more easily and quickly. While proprietary drivers can offer more features and capabilities, this can come at the cost of stability. To avoid this dilemma, try to choose hardware components known to have mature open source driver support with full features. Information about hardware with open source Linux drivers is available at linux-drivers.org.

Use precaution when using packages from the AUR or an unofficial user repository. Most are supplied by regular users and thus may not have the same standards as those in the official repositories. Always check PKGBUILDs for sanity and signs of mistake or malicious code before building and/or installing the package.

To simplify maintenance, limit the amount of unofficial packages used. Make periodic checks on which are in actual use, and remove (or replace with their official counterparts) any others. See pacman/Tips and tricks#Maintenance for useful commands. Following system upgrade, use rebuild-detector to identify any unofficial packages that may need to be rebuilt.

Update pacman's mirrorlist, as the quality of mirrors can vary over time, and some might go offline or their download rate might degrade.

See mirrors for details.

Programs that help with this can be found in List of applications/Utilities#Disk cleaning.

When looking for files to remove, it is important to find the files that take up the most disk space. Programs that help with this can be found in List of applications/Utilities#Disk usage display.

Remove unwanted .pkg files from /var/cache/pacman/pkg/ to free up disk space.

See pacman#Cleaning the package cache for more information.

Remove unused packages from the system to free up disk space and simplify maintenance.

See #Check for orphans and dropped packages.

Old configuration files may conflict with newer software versions, or corrupt over time. Remove unneeded configurations periodically, particularly in your home directory and ~/.config. For similar reasons, be careful when sharing home directories between installations.

Look for the following directories:

See XDG Base Directory support for more information about these directories.

To keep the home directory clean from temporary files created at the wrong place, it is a good idea to manage a list of unwanted files and remove them regularly, for example with rmshit.py.

rmlint-gitAUR can be used to find and optionally remove duplicate files, empty files, recursive empty directories and broken symlinks.

Old, broken symbolic links might be sitting around your system; you should remove them. Examples on achieving this can be found here and here. However, you should not blindly delete all broken symbolic links, as some of them serve a purpose [1].

To quickly list all the broken symlinks of permanent files on your system, use:

Then inspect and remove unnecessary entries from this list.

The following tips are generally not required, but certain users may find them useful.

Arch's rolling releases can be a boon for users who want to try the latest features and get upstream updates as soon as possible, but they can also make system maintenance more difficult. To simplify maintenance and improve stability, try to avoid cutting edge software and install only mature and proven software. Such packages are less likely to receive difficult upgrades such as major configuration changes or feature removals. Prefer software that has a strong and active development community, as well as a high number of competent users, in order to simplify support in the event of a problem.

Avoid any use of the testing repository, even individual packages from testing. These packages are experimental and not suitable for a stable system. Similarly, avoid packages which are built directly from upstream development sources. These are usually found in the AUR, with names including things like: "dev", "devel", "svn", "cvs", "git", etc.

The linux-lts package is an alternative Arch kernel package, and is available in the core repository. This particular kernel version has long-term support (LTS) from upstream, including security and bug fixes. It is useful if you use out-of-tree kernel modules and want to ensure their compatibility or if you want a fallback kernel in case a new kernel version causes problems.

To make it available as a boot option, you will need to update your boot loader's configuration file to use the LTS kernel and ram disk: vmlinuz-linux-lts and initramfs-linux-lts.img.

**Examples:**

Example 1 (unknown):
```unknown
$ systemctl --failed
```

Example 2 (unknown):
```unknown
# journalctl -b
```

Example 3 (unknown):
```unknown
pacman -Syu
```

Example 4 (unknown):
```unknown
--overwrite
```

---

## Arch boot process

**URL:** https://wiki.archlinux.org/title/Boot_process

**Contents:**
- Firmware types
  - UEFI
  - BIOS
- System initialization
  - UEFI
    - Multibooting
  - BIOS
- Boot loader
  - Feature comparison
- Kernel

In order to boot Arch Linux, a Linux-capable boot loader must be set up. The boot loader is responsible for loading the kernel and initial ramdisk before initiating the boot process. The procedure is quite different for BIOS and UEFI systems.

The firmware is the very first program that is executed once the system is switched on.

The Unified Extensible Firmware Interface has support for reading both the partition table as well as file systems. UEFI does not launch any boot code from the Master Boot Record (MBR) whether it exists or not, instead booting relies on boot entries in the NVRAM.

The UEFI specification mandates support for the FAT12, FAT16, and FAT32 file systems (see UEFI specification version 2.11, section 13.3.1.1), but any conformant vendor can optionally add support for additional file systems; for example, HFS+ or APFS in some Apple's firmwares. UEFI implementations also support ISO 9660 for optical discs.

UEFI launches EFI applications, e.g. boot loaders, boot managers, UEFI shell, etc. These applications are usually stored as files in the EFI system partition. Each vendor can store its files in the EFI system partition under the /EFI/vendor_name directory. The applications can be launched by adding a boot entry to the NVRAM or from the UEFI shell.

The UEFI specification has support for legacy BIOS booting with its Compatibility Support Module (CSM). If CSM is enabled in the UEFI, the UEFI will generate CSM boot entries for all drives. If a CSM boot entry is chosen to be booted from, the UEFI's CSM will attempt to boot from the drive's MBR bootstrap code.

A BIOS or Basic Input-Output System is in most cases stored in a flash memory in the motherboard itself and independent of the system storage. Originally created for the IBM PC to handle hardware initialization and the boot process, it has been replaced progressively since 2010 by UEFI which does not suffer from the same technical limitations.

System switched on, the power-on self-test (POST) is executed. See also Modern CPUs have a backstage cast by Hugo Landau.

If Secure Boot is enabled, the boot process will verify authenticity of the EFI binary by signature.

Since each OS or vendor can maintain its own files within the EFI system partition without affecting the other, multi-booting using UEFI is just a matter of launching a different EFI application corresponding to the particular operating system's boot loader. This removes the need for relying on the chain loading mechanisms of one boot loader to load another OS.

See also Dual boot with Windows.

A boot loader is a piece of software started by the firmware—UEFI or BIOS. It is responsible for loading the kernel with the wanted kernel parameters and any external initramfs images.

A boot manager presents a menu of boot options, or provides some other way to control the boot process—i.e. it just runs other EFI executables.

In the case of UEFI, the kernel itself can be directly launched by the UEFI using the EFI boot stub. A separate boot loader or a boot manager can still be used for the purpose of editing kernel parameters before booting.

Systems with 32-bit IA32 UEFI require a boot loader that supports mixed mode booting.

Since almost no boot loader supports such stacked block devices and since file systems can introduce new features which may not yet be supported by any boot loader (e.g. archlinux/packaging/packages/grub#7, FS#79857, FS#59047, FS#58137, FS#51879, FS#46856, FS#38750, FS#21733 and fscrypt encrypted directories), using a separate /boot partition with a universally supported file system, such as FAT32, is oftentimes more feasible.

See also Wikipedia:Comparison of boot loaders.

The boot loader boots the vmlinux image containing the kernel.

The kernel functions on a low level (kernelspace) interacting between the hardware of the machine and the programs. The kernel initially performs hardware enumeration and initialization before continuing to userspace. See Wikipedia:Kernel (operating system) and Wikipedia:Linux kernel for a detailed explanation.

An initramfs (initial RAM file system) image is a cpio archive providing the necessary files for early userspace (see below) to successfully start the late userspace. This predominantly means all kernel modules, user space tools, associated libraries, supporting files like udev rules, etc. required to locate, access and mount the root file system. With the concept of initramfs it is possible to handle even more complex setups, like e.g. booting from an external drive, stacked devices (logical volumes, software RAIDs, compression, encryption) or running a tiny SSH server in early userspace for remote unlocking or maintenance tasks of the root file system.

The majority of modules will be loaded during later stages of the init process by udev after having switched root to the root file system.

The process is as follows:

Initramfs images are Arch Linux' preferred method for setting up the early userspace and can be generated with mkinitcpio, dracut or booster.

Since 6.13.8 officially supported kernels have Btrfs and Ext4 drivers built-in [4].

This makes it possible for the kernel to use a root partition with these file systems directly and load the rest of external modules needed from there. Although, there are some quirks to keep in mind:

Another thing you really need initramfs for is early microcode loading. But it is not necessary to build full image for that, Arch provides microcode in separate initramfs files, which could be used independently.

If no initramfs image is provided, the kernel always contains still an empty image to start from [8]. So there should be no issues with root partition pinning.

The early userspace stage, a.k.a. the initramfs stage, takes place in rootfs consisting of the files provided by the #initramfs. Early userspace starts by the kernel executing the /init binary as PID 1.

The function of early userspace is configurable, but its main purpose is to bootstrap the system to the point where it can access the root file system. This includes:

Note that the early userspace serves more than just setting up the root file system. There are tasks that can only be performed before the root file system is mounted, such as fsck and resuming from hibernation.

At the final stage of early userspace, the real root is mounted at /sysroot/ (in case of a systemd-based initramfs) or at /new_root/ (in case of a busybox-based one), and then switched to by using systemctl switch-root when using systemd-based initramfs or switch_root(8) when using busybox-based initramfs. The late userspace starts by executing the init program from the real root file system.

The startup of late userspace is executed by the init process. Arch officially uses systemd which is built on the concept of units and services, but the functionality described here largely overlaps with other init systems.

The init process calls getty once for each virtual terminal (typically six of them). getty initializes each terminal and protects it from unauthorized access. When the username and password are provided, getty checks them against /etc/passwd and /etc/shadow, then calls login(1).

The login program begins a session for the user by setting environment variables and starting the user's shell, based on /etc/passwd. The login program displays the contents of /etc/motd (message of the day) after a successful login, just before it executes the login shell. It is a good place to display your Terms of Service to remind users of your local policies or anything you wish to tell them.

Once the user's shell is started, it will typically run a runtime configuration file, such as bashrc, before presenting a prompt to the user. If the account is configured to start X at login, the runtime configuration file will call startx or xinit. Jump to #Graphical session (Xorg) for the end.

This article or section needs expansion.

Additionally, init can be configured to start a display manager instead of getty on a specific virtual terminal. This requires manually enabling its systemd service file. The display manager then starts a graphical session.

xinit runs the user's xinitrc runtime configuration file, which normally starts a window manager or a desktop environment. When the user is finished and exits, xinit, startx, the shell, and login will terminate in that order, returning to getty or the display manager.

**Examples:**

Example 1 (unknown):
```unknown
/EFI/vendor_name
```

Example 2 (unknown):
```unknown
\EFI\BOOT\BOOTx64.EFI
```

Example 3 (unknown):
```unknown
BOOTIA32.EFI
```

Example 4 (unknown):
```unknown
esp/EFI/Linux/
```

---

## tmpfs

**URL:** https://wiki.archlinux.org/title/Ramdisk

**Contents:**
- Usage
- Examples
- Disable automatic mount
- Troubleshooting
  - Opening symlinks in tmpfs as root fails
- Tips and tricks
  - Allocate more memory to accommodate profiles in /run/user/xxxx
- See also

tmpfs is a temporary filesystem that resides in memory and/or swap partition(s). Mounting directories as tmpfs can be an effective way of speeding up accesses to their files, or to ensure that their contents are automatically cleared upon reboot.

Some directories where tmpfs(5) is commonly used are /tmp, /var/lock and /var/run. Do not use it on /var/tmp, because that directory is meant for temporary files that are preserved across reboots.

Arch uses a tmpfs /run directory, with /var/run and /var/lock simply existing as symlinks for compatibility. It is also used for /tmp by the default systemd setup and does not require an entry in fstab unless a specific configuration is needed.

glibc 2.2 and above expects tmpfs to be mounted at /dev/shm for POSIX shared memory. Mounting tmpfs at /dev/shm is handled automatically by systemd and manual configuration in fstab is not necessary.

Generally, tasks and programs that run frequent read/write operations can benefit from using a tmpfs directory. Some applications can even receive a substantial gain by offloading some (or all) of their data onto the shared memory. For example, relocating the Firefox profile into RAM shows a significant improvement in performance.

By default, a tmpfs partition has its maximum size set to half of the available RAM, however it is possible to overrule this value. To explicitly set a maximum size, in this example to override the default /tmp mount, use the size mount option:

To specify a more secure mounting, specify the following mount option:

See the tmpfs(5) man page and Security#File systems for more information.

Reboot for the changes to take effect. Note that although it may be tempting to simply run mount -a to make the changes effective immediately, this will make any files currently residing in these directories inaccessible (this is especially problematic for running programs with lockfiles, for example). However, if all of them are empty, it should be safe to run mount -a instead of rebooting (or mount them individually).

After applying changes, verify that they took effect by looking at /proc/mounts and using findmnt:

The tmpfs can also be temporarily resized without the need to reboot, for example when a large compile job needs to run soon. In this case, run:

Or resize based on RAM:

Under systemd, /tmp is automatically mounted as a tmpfs, if it is not already a dedicated mountpoint (either tmpfs or on-disk) in /etc/fstab. To disable the automatic mount, mask the tmp.mount systemd unit.

Files will no longer be stored in a tmpfs, but on the block device instead. The /tmp contents will now be preserved between reboots (they are still cleaned up after 10 days though), which might not be the desired behavior. To regain the previous behavior and clean the /tmp directory automatically when restarting, consider using tmpfiles.d(5):

Considering /tmp is using tmpfs, change the current directory to /tmp, then create a file and create a symlink to that file in the same /tmp directory. Permission denied errors are to be expected when attempting to read the symlink due to /tmp having the sticky bit set.

This behavior can be controlled via /proc/sys/fs/protected_symlinks or simply via sysctl: sysctl -w fs.protected_symlinks=0. See Sysctl#Configuration to make this permanent.

The standard way of controlling the size of tmpfs in /run/user/ is the RuntimeDirectorySize directive in /etc/systemd/logind.conf (see logind.conf(5) for more). By default, 10% of physical memory is used but one can increase it safely. Remember that tmpfs only consumes what is actually used; the number specified here is just a maximum allowed.

**Examples:**

Example 1 (unknown):
```unknown
tmpfs   /tmp         tmpfs   rw,nodev,nosuid,size=2G          0  0
```

Example 2 (unknown):
```unknown
tmpfs   /www/cache    tmpfs  rw,size=1G,nr_inodes=5k,noexec,nodev,nosuid,uid=user,gid=group,mode=1700 0 0
```

Example 3 (unknown):
```unknown
/proc/mounts
```

Example 4 (unknown):
```unknown
$ findmnt /tmp
```

---

## Category:Version control system

**URL:** https://wiki.archlinux.org/title/Version_Control_System

See also Wikipedia:Version control and List of applications/Utilities#Version control systems.

---

## Users and groups

**URL:** https://wiki.archlinux.org/title/Root_user

**Contents:**
- Overview
- Permissions and ownership
- Shadow
- File list
- User management
  - Example adding a user
    - Changing user defaults
  - Example adding a system user
  - Change a user's login name or home directory
  - Other examples of user management

Users and groups are used on GNU/Linux for access control—that is, to control access to the system's files, directories, and peripherals. Linux offers relatively simple/coarse access control mechanisms by default. For more advanced options, see ACL, Capabilities and PAM#Configuration How-Tos.

A user is anyone who uses a computer. In this case, we are describing the names which represent those users. It may be Mary or Bill, and they may use the names Dragonlady or Pirate in place of their real name. All that matters is that the computer has a name for each account it creates, and it is this name by which a person gains access to use the computer. Some system services also run using restricted or privileged user accounts.

Managing users is done for the purpose of security by limiting access in certain specific ways. The superuser (root) has complete access to the operating system and its configuration; it is intended for administrative use only. Unprivileged users can use several programs for controlled privilege elevation.

Any individual may have more than one account as long as they use a different name for each account they create. Further, there are some reserved names which may not be used such as "root".

Users may be grouped together into a "group", and users may be added to an existing group to utilize the privileged access it grants.

From In UNIX Everything is a File:

From Extending UNIX File Abstraction for General-Purpose Networking:

Every file on a GNU/Linux system is owned by a user and a group. In addition, there are three types of access permissions: read, write, and execute. Different access permissions can be applied to a file's owning user, owning group, and others (those without ownership). One can determine a file's owners and permissions by viewing the long listing format of the ls command:

The first column displays the file's permissions (for example, the file initramfs-linux.img has permissions -rw-r--r--). The third and fourth columns display the file's owning user and group, respectively. In this example, all files are owned by the root user and the root group.

In this example, the sf_Shared directory is owned by the root user and the vboxsf group. It is also possible to determine a file's owners and permissions using the stat command:

Access permissions are displayed in three groups of characters, representing the permissions of the owning user, owning group, and others, respectively. For example, the characters -rw-r--r-- indicate that the file's owner has read and write permission, but not execute (rw-), whilst users belonging to the owning group and other users have only read permission (r-- and r--). Meanwhile, the characters drwxrwx--- indicate that the file's owner and users belonging to the owning group all have read, write, and execute permissions (rwx and rwx), whilst other users are denied access (---). The first character represents the file's type.

List files owned by a user or group with the find utility:

A file's owning user and group can be changed with the chown (change owner) command. A file's access permissions can be changed with the chmod (change mode) command.

See chown(1), chmod(1), and Linux file permissions for additional detail.

The user, group and password management tools on Arch Linux come from the shadow package, which is a dependency of the base meta package.

To list users currently logged on the system, the who command can be used. To list all existing user accounts including their properties stored in the user database, run passwd -Sa as root. See passwd(1) for the description of the output format.

To add a new user, use the useradd command:

If an initial login group is specified by name or number, it must refer to an already existing group. If not specified, the behaviour of useradd will depend on the USERGROUPS_ENAB variable contained in /etc/login.defs. The default behaviour (USERGROUPS_ENAB yes) is to create a group with the same name as the username.

When the login shell is intended to be non-functional, for example when the user account is created for a specific service, /usr/bin/nologin may be specified in place of a regular shell to politely refuse a login (see nologin(8)).

See useradd(8) for other supported options.

To add a new user named archie, creating its home directory and otherwise using all the defaults in terms of groups, directory names, shell used and various other parameters:

Although it is not required to protect the newly created user archie with a password, it is highly recommended to do so:

The above useradd command will also automatically create a group called archie and makes this the default group for the user archie. Making each user have their own group (with the group name same as the user name) is the preferred way to add users.

You could also make the default group something else using the -g option, but note that, in multi-user systems, using a single default group (e.g. users) for every user is not recommended. The reason is that typically, the method for facilitating shared write access for specific groups of users is setting user umask value to 002, which means that the default group will by default always have write access to any file you create. See also User Private Groups. If a user must be a member of a specific group specify that group as a supplementary group when creating the user.

In the recommended scenario, where the default group has the same name as the user name, all files are by default writeable only for the user who created them. To allow write access to a specific group, shared files/directories can be made writeable by default for everyone in this group and the owning group can be automatically fixed to the group which owns the parent directory by setting the setgid bit on this directory:

Otherwise the file creator's default group (usually the same as the user name) is used.

If a GID change is required temporarily you can also use the newgrp command to change the user's default GID to another GID at runtime. For example, after executing newgrp groupname files created by the user will be associated with the groupname GID, without requiring a re-login. To change back to the default GID, execute newgrp without a groupname.

The default values used for creating new accounts are set in /etc/default/useradd and can be displayed using useradd --defaults. For example, to change the default shell globally, set SHELL=/usr/bin/shell. A different shell can also be specified individually with the -s/--shell option. Use chsh -l to list valid login shells.

Files can also be specified to be added to newly created user home directories in /etc/skel. This is useful for minimalist window managers where config files need manual configuration to reach DE-familiar behavior. For example, to set up default shortcuts for all newly created users:

See also: Display manager#Run ~/.xinitrc as a session to add xinitrc as an option to all users on the display manager.

System users can be used to run processes/daemons under a different user, protecting (e.g. with chown) files and/or directories and more examples of computer hardening.

With the following command a system user without shell access and without a home directory is created (optionally append the -U parameter to create a group with the same name as the user, and add the user to this group):

If the system user requires a specific user and group ID, specify them with the -u/--uid and -g/--gid options when creating the user:

To change a user's home directory:

The -m option also automatically creates the new directory and moves the content there.

Make sure there is no trailing / on /my/old/home.

To change a user's login name:

Changing a username is safe and easy when done properly, just use the usermod command. If the user is associated to a group with the same name, you can rename this with the groupmod command.

Alternatively, the /etc/passwd file can be edited directly, see #User database for an introduction to its format.

Also keep in mind the following notes:

To enter user information for the GECOS comment (e.g. the full user name), type:

(this way chfn runs in interactive mode).

Alternatively the GECOS comment can be set more liberally with:

To mark a user's password as expired, requiring them to create a new password the first time they log in, type:

User accounts may be deleted with the userdel command:

The -r option specifies that the user's home directory and mail spool should also be deleted.

To change the user's login shell:

Local user information is stored in the plain-text /etc/passwd file: each of its lines represents a user account, and has seven fields delimited by colons.

Broken down, this means: user archie, whose password is in /etc/shadow, whose UID is 1001 and whose primary group is 1003. Archie is their full name and there is a comment associated to their account; their home directory is /home/archie and they are using Bash.

The pwck command can be used to verify the integrity of the user database. It can sort the user list by UID at the same time, which can be helpful for comparison:

Instead of running pwck/grpck manually, the systemd timer shadow.timer, which is part of, and is enabled by, installation of the shadow package, will start shadow.service daily. shadow.service will run pwck(8) and grpck(8) to verify the integrity of both password and group files.

If discrepancies are reported, group can be edited with the vigr(8) command and users with vipw(8). This provides an extra margin of protection in that these commands lock the databases for editing. Note that the default text editor is vi, but an alternative editor will be used if the EDITOR environment variable is set, then that editor will be used instead.

As packages adopt the change-sysusers-to-fully-locked-system-accounts, package created system users generated in the past will not inherit new package defaults for the increased security of locked/expired status. These users need to be modified manually for this change. user-analysis.sh does just that.

In addition, the script also identifies orphaned users (those created by a package no longer on the system) and can automatically delete them.

/etc/group is the file that defines the groups on the system (see group(5) for details). There is also its companion gshadow which is rarely used. Its details are at gshadow(5).

Display group membership with the groups command:

If user is omitted, the current user's group names are displayed.

The id command provides additional detail, such as the user's UID and associated GIDs:

To list all groups on the system:

Create new groups with the groupadd command:

Add users to a group with the gpasswd command (see FS#58262 regarding errors):

Alternatively, add a user to additional groups with usermod (replace additional_groups with a comma-separated list):

Modify an existing group with the groupmod command, e.g. to rename the old_group group to new_group:

To delete existing groups:

To remove users from a group:

The grpck command can be used to verify the integrity of the system's group files.

This section explains the purpose of the essential groups from the filesystem package. There are many other groups, which will be created with correct GID when the relevant package is installed. See the main page for the software for details.

Non-root workstation/desktop users often need to be added to some of following groups to allow access to hardware peripherals and facilitate system administration:

The following groups are used for system purposes, an assignment to users is only required for dedicated purposes:

Before arch migrated to systemd, users had to be manually added to these groups in order to be able to access the corresponding devices. This way has been deprecated in favour of udev marking the devices with a uaccess tag and logind assigning the permissions to users dynamically via ACLs according to which session is currently active. Note that the session must not be broken for this to work (see General troubleshooting#Session permissions to check it).

There are some notable exceptions which require adding a user to some of these groups: for example if you want to allow users to access the device even when they are not logged in. However, note that adding users to the groups can even cause some functionality to break (for example, the audio group will break fast user switching and allows applications to block software mixing).

Now solely for direct access to tapes if no custom udev rules is involved.[5][6][7].

Also required for manipulating some devices via udisks/udisksctl.

The following groups are currently not used for any purpose:

This article or section is a candidate for merging with #Shadow.

The factual accuracy of this article or section is disputed.

getent(1) can be used to read a particular record.

As warned in #User database, using specific utilities such as passwd and chfn, is a better way to change the databases. Nevertheless, there are times when editing them directly is looked after. For those times, vipw, vigr are provided. It is strongly recommended to use these tailored editors over using a general text editor as they lock the databases against concurrent editing. They also help prevent invalid entries and/or syntax errors. Note that Arch Linux prefers usage of specific tools, such as chage, for modifying the shadow database over using vipw -s and vigr -s from util-linux. See also FS#31414.

lslogins(1) and lastlog2(1) are some of the tools for viewing utmp related files.

**Examples:**

Example 1 (unknown):
```unknown
cat /dev/audio > myfile
```

Example 2 (unknown):
```unknown
cat myfile > /dev/audio
```

Example 3 (unknown):
```unknown
$ ls -l /boot/
```

Example 4 (unknown):
```unknown
total 13740
drwxr-xr-x 2 root root    4096 Jan 12 00:33 grub
-rw-r--r-- 1 root root 8570335 Jan 12 00:33 initramfs-linux-fallback.img
-rw-r--r-- 1 root root 1821573 Jan 12 00:31 initramfs-linux.img
-rw-r--r-- 1 root root 1457315 Jan  8 08:19 System.map26
-rw-r--r-- 1 root root 2209920 Jan  8 08:19 vmlinuz-linux
```

---

## Arch boot process

**URL:** https://wiki.archlinux.org/title/Boot_manager

**Contents:**
- Firmware types
  - UEFI
  - BIOS
- System initialization
  - UEFI
    - Multibooting
  - BIOS
- Boot loader
  - Feature comparison
- Kernel

In order to boot Arch Linux, a Linux-capable boot loader must be set up. The boot loader is responsible for loading the kernel and initial ramdisk before initiating the boot process. The procedure is quite different for BIOS and UEFI systems.

The firmware is the very first program that is executed once the system is switched on.

The Unified Extensible Firmware Interface has support for reading both the partition table as well as file systems. UEFI does not launch any boot code from the Master Boot Record (MBR) whether it exists or not, instead booting relies on boot entries in the NVRAM.

The UEFI specification mandates support for the FAT12, FAT16, and FAT32 file systems (see UEFI specification version 2.11, section 13.3.1.1), but any conformant vendor can optionally add support for additional file systems; for example, HFS+ or APFS in some Apple's firmwares. UEFI implementations also support ISO 9660 for optical discs.

UEFI launches EFI applications, e.g. boot loaders, boot managers, UEFI shell, etc. These applications are usually stored as files in the EFI system partition. Each vendor can store its files in the EFI system partition under the /EFI/vendor_name directory. The applications can be launched by adding a boot entry to the NVRAM or from the UEFI shell.

The UEFI specification has support for legacy BIOS booting with its Compatibility Support Module (CSM). If CSM is enabled in the UEFI, the UEFI will generate CSM boot entries for all drives. If a CSM boot entry is chosen to be booted from, the UEFI's CSM will attempt to boot from the drive's MBR bootstrap code.

A BIOS or Basic Input-Output System is in most cases stored in a flash memory in the motherboard itself and independent of the system storage. Originally created for the IBM PC to handle hardware initialization and the boot process, it has been replaced progressively since 2010 by UEFI which does not suffer from the same technical limitations.

System switched on, the power-on self-test (POST) is executed. See also Modern CPUs have a backstage cast by Hugo Landau.

If Secure Boot is enabled, the boot process will verify authenticity of the EFI binary by signature.

Since each OS or vendor can maintain its own files within the EFI system partition without affecting the other, multi-booting using UEFI is just a matter of launching a different EFI application corresponding to the particular operating system's boot loader. This removes the need for relying on the chain loading mechanisms of one boot loader to load another OS.

See also Dual boot with Windows.

A boot loader is a piece of software started by the firmware—UEFI or BIOS. It is responsible for loading the kernel with the wanted kernel parameters and any external initramfs images.

A boot manager presents a menu of boot options, or provides some other way to control the boot process—i.e. it just runs other EFI executables.

In the case of UEFI, the kernel itself can be directly launched by the UEFI using the EFI boot stub. A separate boot loader or a boot manager can still be used for the purpose of editing kernel parameters before booting.

Systems with 32-bit IA32 UEFI require a boot loader that supports mixed mode booting.

Since almost no boot loader supports such stacked block devices and since file systems can introduce new features which may not yet be supported by any boot loader (e.g. archlinux/packaging/packages/grub#7, FS#79857, FS#59047, FS#58137, FS#51879, FS#46856, FS#38750, FS#21733 and fscrypt encrypted directories), using a separate /boot partition with a universally supported file system, such as FAT32, is oftentimes more feasible.

See also Wikipedia:Comparison of boot loaders.

The boot loader boots the vmlinux image containing the kernel.

The kernel functions on a low level (kernelspace) interacting between the hardware of the machine and the programs. The kernel initially performs hardware enumeration and initialization before continuing to userspace. See Wikipedia:Kernel (operating system) and Wikipedia:Linux kernel for a detailed explanation.

An initramfs (initial RAM file system) image is a cpio archive providing the necessary files for early userspace (see below) to successfully start the late userspace. This predominantly means all kernel modules, user space tools, associated libraries, supporting files like udev rules, etc. required to locate, access and mount the root file system. With the concept of initramfs it is possible to handle even more complex setups, like e.g. booting from an external drive, stacked devices (logical volumes, software RAIDs, compression, encryption) or running a tiny SSH server in early userspace for remote unlocking or maintenance tasks of the root file system.

The majority of modules will be loaded during later stages of the init process by udev after having switched root to the root file system.

The process is as follows:

Initramfs images are Arch Linux' preferred method for setting up the early userspace and can be generated with mkinitcpio, dracut or booster.

Since 6.13.8 officially supported kernels have Btrfs and Ext4 drivers built-in [4].

This makes it possible for the kernel to use a root partition with these file systems directly and load the rest of external modules needed from there. Although, there are some quirks to keep in mind:

Another thing you really need initramfs for is early microcode loading. But it is not necessary to build full image for that, Arch provides microcode in separate initramfs files, which could be used independently.

If no initramfs image is provided, the kernel always contains still an empty image to start from [8]. So there should be no issues with root partition pinning.

The early userspace stage, a.k.a. the initramfs stage, takes place in rootfs consisting of the files provided by the #initramfs. Early userspace starts by the kernel executing the /init binary as PID 1.

The function of early userspace is configurable, but its main purpose is to bootstrap the system to the point where it can access the root file system. This includes:

Note that the early userspace serves more than just setting up the root file system. There are tasks that can only be performed before the root file system is mounted, such as fsck and resuming from hibernation.

At the final stage of early userspace, the real root is mounted at /sysroot/ (in case of a systemd-based initramfs) or at /new_root/ (in case of a busybox-based one), and then switched to by using systemctl switch-root when using systemd-based initramfs or switch_root(8) when using busybox-based initramfs. The late userspace starts by executing the init program from the real root file system.

The startup of late userspace is executed by the init process. Arch officially uses systemd which is built on the concept of units and services, but the functionality described here largely overlaps with other init systems.

The init process calls getty once for each virtual terminal (typically six of them). getty initializes each terminal and protects it from unauthorized access. When the username and password are provided, getty checks them against /etc/passwd and /etc/shadow, then calls login(1).

The login program begins a session for the user by setting environment variables and starting the user's shell, based on /etc/passwd. The login program displays the contents of /etc/motd (message of the day) after a successful login, just before it executes the login shell. It is a good place to display your Terms of Service to remind users of your local policies or anything you wish to tell them.

Once the user's shell is started, it will typically run a runtime configuration file, such as bashrc, before presenting a prompt to the user. If the account is configured to start X at login, the runtime configuration file will call startx or xinit. Jump to #Graphical session (Xorg) for the end.

This article or section needs expansion.

Additionally, init can be configured to start a display manager instead of getty on a specific virtual terminal. This requires manually enabling its systemd service file. The display manager then starts a graphical session.

xinit runs the user's xinitrc runtime configuration file, which normally starts a window manager or a desktop environment. When the user is finished and exits, xinit, startx, the shell, and login will terminate in that order, returning to getty or the display manager.

**Examples:**

Example 1 (unknown):
```unknown
/EFI/vendor_name
```

Example 2 (unknown):
```unknown
\EFI\BOOT\BOOTx64.EFI
```

Example 3 (unknown):
```unknown
BOOTIA32.EFI
```

Example 4 (unknown):
```unknown
esp/EFI/Linux/
```

---

## fsck

**URL:** https://wiki.archlinux.org/title/Fsck

**Contents:**
- Boot time checking
  - Mechanism
  - Forcing the check
  - Automatically answer yes to all repair questions
- Tips and tricks
  - Attempt to repair damaged blocks
  - Repair damaged blocks interactively
  - Changing the check frequency
  - fstab options
- Troubleshooting

fsck stands for "file system check" and it is used to check and optionally repair one or more Linux file systems. Normally, the fsck program will try to handle file systems on different physical disk drives in parallel to reduce the total amount of time needed to check all of the file systems (see fsck(8)).

The Arch Linux boot process conveniently takes care of the fsck procedure for you and will check all relevant partitions on your drive(s) automatically on every boot. Hence, there is usually no need to resort to the command-line.

There are two players involved:

The first option is the recommended default, and what you will end up with if you follow the Installation guide. If you want to go with option 2 instead, you should remove the fsck hook from mkinitcpio.conf and use ro on the kernel command-line. The kernel parameter fsck.mode=skip can be used to make sure fsck is disabled entirely for both options.

If you use the base mkinitcpio hook, you can force fsck at boot time by passing fsck.mode=force as a kernel parameter. This will check every file system you have on the machine.

Alternatively, systemd provides systemd-fsck@.service(8), which checks all configured file systems, which were not checked in the initramfs. However, checking the root file system this way causes a delay in the boot process, because the file system has to be remounted.

This article or section needs expansion.

The boot time fsck checks might end up saying "UNEXPECTED INCONSISTENCY; RUN fsck MANUALLY."

This happens when you need to apply some changes to fix the file system which are not considered completely safe, and thus require fsck to be run manually.

You can set fsck to automatically apply all suggested change (i.e. answer yes to all questions) by setting the fsck.repair kernel command line option to yes. (Other possible values are no and preen.) Check the documentation systemd-fsck@.service(8) for the meaning of these options.

To automatically repair damaged portions of an ext2/ext3/ext4 or FAT file system, run:

This is useful for when files on the boot partition have changed, and the journal failed to properly update. In this case, unmount the boot partition, and run the following code to repair damaged portions:

By default, fsck checks a file system every 30 boots (counted individually for each partition). To change the frequency of checking, run:

In this example, 20 is the number of boots between two checks.

Note that 1 would make it scan at every boot, while 0 would stop scanning altogether.

If you wish to see the frequency number and the current mount count for a specific partition, use:

fstab is a system configuration file and is used to tell the Linux kernel which partitions (file systems) to mount and where on the file system tree.

A typical /etc/fstab entry may look like this:

The 6th column (in bold) is the fsck option.

There are times (due to power failure) in which an ext(3/4) file system can corrupt beyond normal repair. Normally, there will be a prompt from fsck indicating that it cannot find an external journal. In this case, run the following commands:

Unmount the partition based on its directory

Write a new journal to the partition

Run an fsck to repair the partition

**Examples:**

Example 1 (unknown):
```unknown
mkinitcpio.conf
```

Example 2 (unknown):
```unknown
fsck.mode=skip
```

Example 3 (unknown):
```unknown
fsck.mode=force
```

Example 4 (unknown):
```unknown
"UNEXPECTED INCONSISTENCY; RUN fsck MANUALLY."
```

---

## File systems

**URL:** https://wiki.archlinux.org/title/Umount

**Contents:**
- Types of file systems
  - Journaling
  - FUSE-based file systems
  - Stackable file systems
  - Read-only file systems
  - Clustered file systems
  - Shared-disk file system
- Identify existing file systems
- Create a file system
- Mount a file system

Individual drive partitions can be set up using one of the many different available file systems. Each has its own advantages, disadvantages, and unique idiosyncrasies. A brief overview of supported file systems follows; the links are to Wikipedia pages that provide much more information.

See filesystems(5) for a general overview and Wikipedia:Comparison of file systems for a detailed feature comparison. File systems already loaded by the kernel or built-in are listed in /proc/filesystems, while all the installed modules can be seen with ls /lib/modules/$(uname -r)/kernel/fs.

xfs.html xfs-delayed-logging-design.html xfs-self-describing-metadata.html

The ext3/4, HFS+, JFS, NTFS, ReiserFS, and XFS file systems use journaling. Journaling provides fault-resilience by logging changes before they are committed to the file system. In the event of a system crash or power failure, such file systems are faster to bring back online and less likely to become corrupted. The logging takes place in a dedicated area of the file system.

ext3/4 offer data-mode journaling, which can optionally log data in addition to the metadata. Data-mode journaling comes with a speed penalty, because it does two write operations: first to the journal and then to the disk. Therefore, data-mode journaling is not enabled by default. The trade-off between system speed and data safety should be considered when choosing the file system type and features.

In the same vein, Reiser4 offers configurable "transaction models": a special model called wandering logs, which eliminates the need to write to the disk twice; write-anywhere, a pure copy-on-write approach; and a combined approach called hybrid which heuristically alternates between the two.

File systems based on copy-on-write (also known as write-anywhere), such as Reiser4, Btrfs, Bcachefs and ZFS, by design operate on full atomicity and also provide checksums for both metadata and inline data (operations entirely occur, or they entirely do not, and in properly functioning hardware data does not corrupt due to operations half-occurring). Therefore, these file systems are by design much less prone to data loss than other file systems and have no need to use traditional journal to protect metadata, because they are never updated in-place. Although Btrfs still has a journal-like log tree, it is only used to speed-up fdatasync/fsync.

FAT, exFAT, ext2, and HFS provide neither journaling nor atomicity, They are for temporary or legacy use and not recommended for use when reliable storage is needed.

To identify existing file systems, you can use lsblk:

An existing file system, if present, will be shown in the FSTYPE column. If mounted, it will appear in the MOUNTPOINT column.

File systems are usually created on a partition, inside logical containers such as LVM, RAID and dm-crypt, or on a regular file (see Wikipedia:Loop device). This section describes the partition case.

Before continuing, identify the device where the file system will be created and whether or not it is mounted. For example:

Mounted file systems must be unmounted before proceeding. In the above example an existing file system is on /dev/sda2 and is mounted at /mnt. It would be unmounted with:

To find just mounted file systems, see #List mounted file systems.

To create a new file system, use mkfs(8). See #Types of file systems for the exact type, as well as userspace utilities you may wish to install for a particular file system.

For example, to create a new file system of type ext4 (common for Linux data partitions) on /dev/sda1, run:

The new file system can now be mounted to a directory of choice.

To manually mount a file system located on a device (e.g., a partition) to a directory, use mount(8). This example mounts /dev/sda1 to /mnt.

This attaches the file system on /dev/sda1 at the directory /mnt, making the contents of the file system visible. Any data that existed at /mnt before this action is made invisible until the device is unmounted.

fstab contains information on how devices should be automatically mounted if present. See the fstab article for more information on how to modify this behavior.

If a device is specified in /etc/fstab and only the device or mount point is given on the command line, that information will be used in mounting. For example, if /etc/fstab contains a line indicating that /dev/sda1 should be mounted to /mnt, then the following will automatically mount the device to that location:

mount contains several options, many of which depend on the file system specified. The options can be changed, either by:

See these related articles and the article of the file system of interest for more information.

To list all mounted file systems, use findmnt(8):

findmnt takes a variety of arguments which can filter the output and show additional information. For example, it can take a device or mount point as an argument to show only information on what is specified:

findmnt gathers information from /etc/fstab, /etc/mtab, and /proc/self/mounts.

To unmount a file system use umount(8). Either the device containing the file system (e.g., /dev/sda1) or the mount point (e.g., /mnt) can be specified:

Unmount the file system and run fsck on the problematic volume.

**Examples:**

Example 1 (unknown):
```unknown
/proc/filesystems
```

Example 2 (unknown):
```unknown
ls /lib/modules/$(uname -r)/kernel/fs
```

Example 3 (unknown):
```unknown
NAME   FSTYPE LABEL     UUID                                 MOUNTPOINT
sdb
└─sdb1 vfat   Transcend 4A3C-A9E9
```

Example 4 (unknown):
```unknown
NAME   FSTYPE   LABEL       UUID                                 MOUNTPOINT
sda
├─sda1                      C4DA-2C4D
├─sda2 ext4                 5b1564b2-2e2c-452c-bcfa-d1f572ae99f2 /mnt
└─sda3                      56adc99b-a61e-46af-aab7-a6d07e504652
```

---

## System time

**URL:** https://wiki.archlinux.org/title/System_time

**Contents:**
- Time standard
- Hardware clock
  - Read hardware clock
  - Set hardware clock from system clock
  - Automatic syncing
- System clock
  - Read clock
  - Set system clock
- Multiple systems
  - UTC in Microsoft Windows

This article or section needs expansion.

In an operating system, the time (clock) is determined by three parts: time value, whether it is local time or UTC or something else, time zone, and Daylight Saving Time (DST) if applicable. This article explains what they are and how to read/set them. Two clocks are present on systems: a hardware clock and a system clock which are also detailed in this article.

Standard behavior of most operating systems is:

There are two time standards: localtime and Coordinated Universal Time (UTC). The localtime standard is dependent on the current time zone, while UTC is the global time standard and is independent of time zone values. Though conceptually different, UTC is also known as GMT (Greenwich Mean Time).

The standard used by the hardware clock (CMOS clock, the BIOS time) is set by the operating system. By default, Windows uses localtime, macOS uses UTC, other UNIX and UNIX-like systems vary. An OS that uses the UTC standard will generally consider the hardware clock as UTC and make an adjustment to it to set the OS time at boot according to the time zone.

The hardware clock (a.k.a. the Real Time Clock (RTC) or CMOS clock) stores the values of: Year, Month, Day, Hour, Minute, and Seconds. A UEFI firmware has the additional ability to store the timezone, and whether DST is used.

The following sets the hardware clock from the system clock. Additionally it updates /etc/adjtime or creates it if not present. See hwclock(8) § The Adjtime File for more information on this file as well as the #Time skew section.

By default, Arch Linux kernels have a feature enabled where the hardware clock is synchronized to the system clock every 11 minutes. You can see if this is enabled on your kernel as follows:

The first synchronization happens at boot time. What this means is that if your hardware clock is extremely out of date (for example, a CMOS battery failure has reset the clock to the year 2000) then for the first 11 minutes after boot anything which requires a reasonably accurate time will give an error - including SSL, uses the Online Certificate Status Protocol (OCSP). A web browser running on your computer typically sends the hardware clock time in its requests to websites, and a time which is too far out will result in the browser refusing to connect because of an OCSP error.

The system clock (a.k.a. the software clock) keeps track of: time, time zone, and DST if applicable. It is calculated by the Linux kernel as the number of seconds since midnight January 1st 1970, UTC. The initial value of the system clock is calculated from the hardware clock, dependent on the contents of /etc/adjtime. After boot-up has completed, the system clock runs independently of the hardware clock. The Linux kernel keeps track of the system clock by counting timer interrupts.

To check the current system clock time (presented both in local time and UTC) as well as the RTC (hardware clock):

To set the local time of the system clock directly:

sets the time to May 26th, year 2014, 11:13 and 54 seconds.

If multiple operating systems are installed on a machine, they will all derive the current time from the same hardware clock: it is recommended to set it to UTC to avoid conflicts across systems. Otherwise, if the hardware clock is set to localtime, more than one operating system may adjust it after a DST change for example, thus resulting in an over-correction; problems may also arise when traveling between different time zones and using one of the operating systems to reset the system/hardware clock.

The hardware clock can be queried and set with the timedatectl command. You can see the current hardware clock time standard of the Arch system using:

To change the hardware clock time standard to localtime, use:

To revert to the hardware clock being in UTC, type:

These generate /etc/adjtime automatically and update the RTC accordingly; no further configuration is required.

During kernel startup, at the point when the RTC driver is loaded, the system clock may be set from the hardware clock. Whether this occurs depends on the hardware platform, the version of the kernel and kernel build options. If this does occur, at this point in the boot sequence, the hardware clock time is assumed to be UTC and the value of /sys/class/rtc/rtcN/hctosys (N=0,1,2,..) will be set to 1.

Later, the system clock is set again from the hardware clock by systemd, dependent on values in /etc/adjtime. Hence, having the hardware clock using localtime may cause some unexpected behavior during the boot sequence; e.g system time going backwards, which is always a bad idea (there is a lot more to it). Since systemd version 216, when the RTC is configured to the local time (rather than UTC) systemd will never synchronize back to it, as this might confuse Windows at a later boot. And systemd will no longer inform the kernel about the current timezone. This hence means FAT timestamps will be always considered UTC[1].

To dual boot with Windows, it is recommended to configure Windows to use UTC, rather than Linux to use localtime. (Windows by default uses localtime [2].)

It can be done by a simple registry fix: Open regedit and add a DWORD value with hexadecimal value 1 to the registry HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Control\TimeZoneInformation\RealTimeIsUniversal

You can do this from an Administrator Command Prompt running:

Alternatively, create a *.reg file (on the desktop) with the following content and double-click it to import it into registry:

Should Windows ask to update the clock due to DST changes, let it. It will leave the clock in UTC as expected, only correcting the displayed time.

The #Hardware clock and #System clock time may need to be updated after setting this value.

If you are having issues with the offset of the time, try reinstalling tzdata and then setting your time zone again:

Many Linux distributions have the hardware clock set to be interpreted as in "localtime" if Windows was detected on any disk during their installation. This is apparently done deliberately to allow new users to try out Linux on their Windows computers without editing the registry.

For changing this behavior, see above.

If you use an NTP client (see #Time synchronization below) that keeps track of RTC drift on any system, you should disable time synchronization on all but one system. Otherwise the NTP clients would be unaware of each other's adjustment and make grossly incorrect estimates of the RTC drift.

For Windows, go to the Date and time settings and uncheck the time sync option. You can also run w32tm /unregister as an administrator to unregister the time-sync service: Active Directory machines are known to ignore the synchronization settings and perform a synchronization anyways to prevent replay attacks. The Windows clock synchronization routine is quite inaccurate to start with, requiring even extra work to reach one-second accuracy, so disabling it should not be much of a loss.

To check the current zone defined for the system:

To list available zones:

To set your time zone:

This will create an /etc/localtime symlink that points to a zoneinfo file under /usr/share/zoneinfo/. In case you choose to create the link manually (for example during chroot where timedatectl will not work), keep in mind that it must be a symbolic link, as specified in localtime(5) § DESCRIPTION:

See timedatectl(1) and localtime(5) for details.

To set the timezone automatically based on the IP address location, one can use a geolocation API to retrieve the timezone, for example curl https://ipapi.co/timezone, and pass the output to timedatectl set-timezone for automatic setting. Some geo-IP APIs that provide free or partly free services are listed below:

See NetworkManager#Automatically set the timezone.

Every clock has a value that differs from real time (the best representation of which being International Atomic Time); no clock is perfect. A quartz-based electronic clock keeps imperfect time, but maintains a consistent inaccuracy. This base 'inaccuracy' is known as 'time skew' or 'time drift'.

When the hardware clock is set with hwclock, a new drift value is calculated in seconds per day. The drift value is calculated by using the difference between the new value set and the hardware clock value just before the set, taking into account the value of the previous drift value and the last time the hardware clock was set. The new drift value and the time when the clock was set is written to the file /etc/adjtime overwriting the previous values. The hardware clock can therefore be adjusted for drift when the command hwclock --adjust is run; this also occurs on shutdown but only if the hwclock daemon is enabled, hence for Arch systems which use systemd, this does not happen.

If the hardware clock keeps losing or gaining time in large increments, it is possible that an invalid drift has been recorded (but only applicable, if the hwclock daemon is running). This can happen if you have set the hardware clock time incorrectly or your time standard is not synchronized with a Windows or macOS install. The drift value can be removed by first removing the file /etc/adjtime, then setting the correct hardware clock and system clock time. You should then check if your time standard is correct.

The software clock is very accurate but like most clocks is not perfectly accurate and will drift as well. Though rarely, the system clock can lose accuracy if the kernel skips interrupts. There are some tools to improve software clock accuracy:

The Network Time Protocol (NTP) is a protocol for synchronizing the clocks of computer systems over packet-switched, variable-latency data networks.

For proper NTP support, as defined by the RFC, a client must be able to merge time from multiple servers, compensate for delay, and keep track of drift on the system (software) clock. The following are implementations of NTP available for Arch Linux:

Anything that does less than a proper NTP node is considered Simple Network Time Protocol (SNTP). A basic SNTP client may simply fetch the time from a single server and set it immediately, without keeping track of long-term drifts. SNTP provides lower accuracy, but takes less resources. The accuracy is usually good enough for desktop users and embedded workloads, but unacceptable for NTP servers. The following implement SNTP:

For some use cases it may be useful to change the time settings without touching the global system values. For example to test applications relying on the time during development or adjusting the system time zone when logging into a server remotely from another zone.

To make an application "see" a different date/time than the system one, you can use the faketime(1) utility (from libfaketime).

If instead you want an application to "see" a different time zone than the system one, set the TZ environment variable, for example:

This is different than just setting the time, as for example it allows to test the behavior of a program with positive or negative UTC offset values, or the effects of DST changes when developing on systems in a non-DST time zone.

Another use case is having different time zones set for different users of the same system: this can be accomplished by setting the TZ variable in the shell's configuration file, see Environment variables#Defining variables.

alarm-fake-hwclock designed especially for system without battery backed up RTC, it includes a systemd service which on shutdown saves the current time and on startup restores the saved time, thus avoiding strange time travel errors.

Install fake-hwclock-gitAUR, start/enable the service fake-hwclock.service.

Virtual machine guests may obtain time from the host machine using the PTP (Precision Time Protocol) /dev/ptp0 interface. The interface is more accurate compared to using NTP over IP between the host and guest.

chrony and ntpd can each use the virtual-PTP device to sync the time between guest and host, by configuring the device as if it is a real PTP reference clock.

This might be caused by a number of reasons. For example, if your hardware clock is running on local time, but timedatectl is set to assume it is in UTC, the result would be that your timezone's offset to UTC effectively gets applied twice, resulting in wrong values for your local time and UTC.

To force your clock to the correct time, and to also write the correct UTC to your hardware clock, follow these steps:

**Examples:**

Example 1 (unknown):
```unknown
# hwclock --show
```

Example 2 (unknown):
```unknown
/etc/adjtime
```

Example 3 (unknown):
```unknown
# hwclock --systohc
```

Example 4 (unknown):
```unknown
$ zgrep CMOS /proc/config.gz
```

---

## PipeWire

**URL:** https://wiki.archlinux.org/title/PipeWire

**Contents:**
- Installation
  - Session manager
    - WirePlumber
    - PipeWire Media Session
  - GUI
- Configuration
- Usage
  - Audio
    - ALSA clients
    - PulseAudio clients

PipeWire is a new low-level multimedia framework. It aims to offer capture and playback for both audio and video with minimal latency and support for PulseAudio, JACK, ALSA and GStreamer-based applications.

The daemon based on the framework can be configured to be both an audio server (with PulseAudio and JACK features) and a video capture server.

PipeWire also supports containers like Flatpak and does not rely on the audio and video user groups. Instead, it uses a Polkit-like security model, asking Flatpak or Wayland for permission to record screen or audio.

Install the pipewire package from the official repositories. There is also lib32-pipewire for multilib support.

Pipewire uses systemd/User for management of the server and automatic socket activation.

Optionally, install pipewire-docs to review the documentation.

Pipewire can work as drop-in replacement for other audio servers. See #Audio for details.

Like JACK, PipeWire implements no connection logic internally. The burden of watching for new streams and connecting them to the appropriate output device or application is left to an external component known as a session manager.

WirePlumber is the recommended session manager. It is based on a modular design, with Lua plugins that implement the actual management functionality.

The stock configuration files are stored in /usr/share/wireplumber. The recommended way to customize Wireplumber is adding snippets overriding specific settings in /etc/wireplumber or ~/.config/wireplumber. [1].

WirePlumber changed its configuration format in version 0.5 from .lua to .conf. See https://pipewire.pages.freedesktop.org/wireplumber/daemon/configuration/migration.html#config-migration for migration instructions.

pipewire-media-session is deprecated and no longer recommended. It was mostly implemented for testing and as an example for building new session managers.

The PipeWire package provides an initial set of configuration files in /usr/share/pipewire. You should not edit these files directly, as package updates will overwrite your changes. To configure PipeWire, you can copy files from /usr/share/pipewire to the alternate system-wide location /etc/pipewire, or to the user location ~/.config/pipewire. An equally named file in a directory with a higher precedence makes the analogous files ignored.

PipeWire brings a custom Pro Audio (do not confuse with pro audio) profile in addition to the PulseAudio profiles, selectable through pavucontrol.

PipeWire can be used as an audio server, similar to PulseAudio and JACK. It aims to replace both PulseAudio and JACK, by providing a PulseAudio-compatible server implementation and ABI-compatible libraries for JACK clients. See the blog post PipeWire Late Summer Update 2020 for more information.

First, install pipewire-audio. Depending on the type of audio clients, you may also need to take some extra steps. You may need to install additional firmware for your audio device, see Advanced Linux Sound Architecture#Firmware.

Install pipewire-alsa (and remove pulseaudio-alsa if it was installed) to route all applications using the ALSA API through PipeWire.

Install pipewire-pulse. It will replace pulseaudio and pulseaudio-bluetooth. Reboot, re-login or stop pulseaudio.service and start the pipewire-pulse.service user unit to see the effect.

Normally, no further action is needed as the user service pipewire-pulse.socket should be enabled automatically by the package. To check if the replacement is working, run the following command for the Server Name and default input/output:

pactl(1) is provided by PulseAudio client library package (libpulse), which is installed with pipewire-pulse as a dependency.

To adjust output channel volume, the sink needs to be specified using pactl get-sink-volume {sink} using the value of Default Sink: (above) or Name: (below), default sink device (@DEFAULT_SINK@), or Sink # (e.g. 1 below):

Hint: if audio is playing, grep(1) for RUNNING as other devices will be SUSPENDED.

The balance ratio is calculated automatically. To set the overall volume of the default device use:

To set individual channels, provide each channel volume separately:

Source inputs are handled similarly. For further configuration (e.g. regarding modules) see the official upstream Wiki about Migration from PulseAudio and Pipewire-Pulse Configuration.

Install pipewire-jack for JACK support. There is also lib32-pipewire-jack for multilib support.

pw-jack(1) may be used to start JACK clients, but it is technically not required, as it only serves as a wrapper around the PIPEWIRE_REMOTE, PIPEWIRE_DEBUG and PIPEWIRE_LATENCY environment variables.

It is possible to request a custom buffer size by setting a quotient of buffersize/samplerate (which equals the block latency in seconds):

PipeWire handles Bluetooth audio devices if the pipewire-audio package is installed.

WirePlumber has profile auto-switching enabled by default. It can automatically switch between HSP/HFP and A2DP profiles whenever an input stream is detected. You can disable it with the following command:

pipewire-media-session has it disabled by default. You can set bluez5.autoswitch-profile property to true to enable it:

qpwgraph can be used to visualize and create connections, and also save and load patch sets.

For non-GUI needs, the following are bash scripts to save wiresets, load wiresets, and dewire all connections. For saving and loading, use a command-line parameter for the filename.

PipeWire supports sharing audio over the network using several mechanisms, including:

The PipeWire wiki includes an overview and comparison page for the different network protocols.

The Pulse Tunnel method is described below. The PipeWire PulseAudio implementation supports network streaming. An easy way to share audio between computers on the network is to use the Avahi daemon for discovery. To enable this functionality, install the pipewire-zeroconf package.

Make sure that the avahi-daemon.service is running (and UDP port 5353 is open if using a firewall) on all computers that will be sharing audio.

To share the local audio devices load the appropriate modules on the host (make sure to use the local IP address):

Then load the discovery module on the clients:

It is also possible to load the modules automatically by creating a dedicated configuration file:

It is possible to stream audio to a device that is posing as an AirPlay Receiver. To enable this functionality, load the RAOP Discover module:

It is also possible to load this module automatically by creating a dedicated configuration file:

Some speakers' AirPlay implementations (like Sonos AirPlay 2 speakers) may require opening up ports 6001 and 6002 for incoming UDP traffic on your source device.

PipeWire can also run as a JACK client on top of the native JACK daemon if desired.

See JACK and PipeWire (PipeWire wiki) and JACK Bridge (PipeWire wiki) for more information and additional configuration (like available channels for example).

To use it install the pipewire-jack-client and start JACK. Pipewire should be bridged automatically.

It can manually be loaded (as explained by pactl(1)) like a PulseAudio module: pactl load-module module-jackdbus-detect before starting jack.

It is possible to have a PipeWire server (or multiple, for each user) output to ALSA via ALSA dmix devices. This allows you to use ALSA as the primary audio output system while being able to use non-ALSA devices such as Bluetooth headphones.

Suppose you have two cards, PCH and HDMI:

and your PCMs look like:

and suppose your ALSA configuration looks something like this:

In this particular example, the dmix devices would be dmix:PCH,0 and dmix:HDMI,9.

First of all, stop WirePlumber from monitoring and adding hardware ALSA devices by disabling the monitor.alsa feature:

Now, configure PipeWire to use dmix devices. The default configuration file (/usr/share/pipewire/pipewire.conf) contains a commented out example which you can use as a basis.

Add your own element to the context.objects array:

As a user (non-root), check out the output of wpctl status, and set the default input(source) and output(sink) devices to your liking with wpctl set-default ID. ID is the number before sink/source names.

Now, you can fully test your configuration.

Some hardware audio devices, like snd_hda_intel, function differently depending on which profile the device is running in. In the case of snd_hda_intel, there are separate profiles for HDMI and analog output.

Switching to HDMI with WirePlumber:

Switching to analog with WirePlumber:

Sometimes it is useful to let other users connect to your PipeWire instance. For example, if you login into a different user's account using Xephyr and want the audio you play in the Xephyr session to come out the speakers which are managed by the outer user.

One method to do this is to configure the outer user's pipewire-pulse config to listen for localhost tcp connections.

Create a file like under the outer user's home directory:

Then set the environment variable PULSE_SERVER=tcp:127.0.0.1:4713 in the inner user's session. For example, exporting it before you start Xephyr as the inner user. More information and alternative setups can be found on this forum thread.

Most applications used to rely on X11 for capturing the desktop (or individual applications), for example when using WebRTC in web browsers (e.g. on Google Meet). On Wayland, the screen sharing mechanism is handled through the XDG Desktop Portal and PipeWire, which enables sharing content under Wayland with fine-grained access controls.

Firefox (84+) and Chromium (110+) support this method by default, while on older versions of Chromium (73+), one needs to enable WebRTC PipeWire support by setting the corresponding (experimental) flag at the URL chrome://flags/#enable-webrtc-pipewire-capturer or via CLI argument --enable-features=WebRTCPipeWireCapturer.

obs-studio (27+) supports this method by using the new PipeWire capture source.

This article or section needs expansion.

Although the software is not yet production-ready, it is safe to play around with. Most applications that rely on GStreamer to handle e.g. video streams should work out-of-the-box using the PipeWire GStreamer plugin, see GStreamer#PipeWire. Applications like e.g. cheese are therefore already able to share video input using it.

Using pipewire-v4l2, it should also be possible to use the pw-v4l2 script to preload a library (/lib/pipewire-0.3/v4l2/libpw-v4l2.so) that intercepts v4l2 calls and routes video through pipewire.

Pipewire has an internal module called filter-chain that can create nodes to process audio input and output. See /usr/share/pipewire/filter-chain/ for examples including equalization, virtual surround sound, LADSPA plugins and channel mixing.

You can install many LADSPA plugins from the official repositories and use them in Pipewire filter chains. To list plugin labels and available controls provided by a specific file use analyseplugin from the ladspa package:

Copy /usr/share/pipewire/filter-chain/sink-eq6.conf to /etc/pipewire/pipewire.conf.d/ (or ~/.config/pipewire/pipewire.conf.d/).

Then edit sink-eq6.conf to incorporate the desired parameters. For headphones, these can be obtained from Oratory1990's database or, if not listed there, the AutoEQ project.

If you require a pre-amp, modify eq_band_1 to apply a bq_highshelf filter at frequency 0Hz with a negative gain (gains from -120 to +20dB supported):

For more than 6 bands, add more entries to the nodes list and corresponding links connecting one filter ":Out" to the next filter ":In", for instance to increase to 11 bands (preamp + 10):

Restart Pipewire, select "Equalizer Sink" as your default sound output device; this should then apply to all applications.

EasyEffects (former PulseEffects) is a GTK utility which provides a large array of audio effects and filters to individual application output streams and microphone input streams. Notable effects include an input/output equalizer, output loudness equalization and bass enhancement, input de-esser and noise reduction plug-in. See the GitHub page for a full list of effects.

In order to use EasyEffects, install easyeffects. See Community Presets for a collection of preset configurations. See AutoEq for collection of algorithmically generated EQ presets for headphones.

NoiseTorch is an alternative way for noise suppression, packaged with noisetorchAUR. There also exists noisetorch-gitAUR.

After starting it the module can be loaded for the selected microphone. It is possible to adjust the voice activation threshold, which should be set to the highest level, not filtering out any actual voice.

You can start audio processing with systemd automatically, see [3]. Note that the noisetorch binary path is different if installed from AUR.

Install the noise-suppression-for-voice package.

Then simply follow the instructions given on GitHub.

JamesDSP for Linux (available as jamesdspAUR) provides open-source sound effects for PipeWire and PulseAudio. It uses its own effects engine and without depending on LADSPA, Calf, etc. JamesDSP was initially published as an audio effects processor for Android devices.

If you want to choose between the full list of available LADSPA, LV2 and VST plugins, you can apply them using carla with pipewire-jack.

Start Carla and go to Settings > Configure Carla > Engine. Make sure Audio driver is set to JACK and choose a process mode depending on your needs. You can also choose the process mode by running Carla with a specific command, for example carla-rack for the Continuous Rack mode.

You can connect application outputs to Carla manually, but if you want to pass multiple applications through Carla, it might be more convenient to create a single virtual device between applications and Carla and optionally use it as a default device. At the begin, create a new null sink named default_null_sink.

Restart PipeWire to apply changes.

Alternatively, you can create a temporary virtual device with pw-cli(1) or, if pipewire-pulse is installed, with pactl(1). See the PipeWire wiki for details.

In the Rack tab, add whichever plugin you want. Make sure they are stereo type. You can change their order. In the Continuous Rack process mode, the one on top of the list will be the first to receive the audio stream, just like in EasyEffects. Afterwards go to the Patchbay tab and connect the default_null_sink L/R monitors to Carla inputs, then Carla outputs to the playbacks of your desired device (speakers, earphones, HDMI, etc). Save the configuration to a local file, for example ~/Documents/carla_sink_effects.carxp. Carla will automatically restore the connections after opening this file.

You can test the effects while a multimedia application is reproducing audio, i.e. watching a video on a website through Firefox. There are two methods to do it. The first one, inside Carla Patchbay tab, disconnecting all Firefox connections and linking its L/R outputs to default_null_sink playbacks. The second through pavucontrol, locating Firefox audio stream and redirecting it to default_null_sink (this should remember the connection to automatically redirect the application to the same sink on the next instance).

To run Carla with the Continuous Rack process mode and load the saved file at startup, create a systemd user service:

Then enable the jack-carla-rack.service user unit.

Note that if you set the default_null_sink as the default device in system settings, all applications will be redirected to it and the volume keys will change its level, not the one on the speakers. If you want to control volume speakers, leave them as the default in system settings and redirect your desired application to default_null_sink inside pavucontrol (Pipewire compatibility layer will remember the connection on the next instance of the same application).

This article or section is a candidate for merging with PipeWire/Troubleshooting.

PipeWire's alsa-monitor module uses alsa-card-profiles to detect devices by default. If this is not working for you, try to turn off api.alsa.use-acp, or optionally turn on api.alsa.use-ucm in wireplumber:

With pipewire-media-session:

Then, restart WirePlumber and check available devices:

This article or section is out of date.

An alternative solution suggested in this PipeWire issue is to add the microphone manually. First of all, make sure the microphone is detected by ALSA.

Choose your microphone from the list, and to further test the microphone, run the following commands.

If the microphone is working with arecord, but not detected by PipeWire, try to add a config file to manually add this device.

And then restart PipeWire to reload the config.

To automatically switch to newly connected devices, create this file:

Then restart the pipewire-pulse.service with systemctl --user and check that module-switch-on-connect is loaded.

As of 2020-12-07, if there is no sound after connecting a Bluetooth device, you might need to switch the default sink and/or move a sink input to the correct sink. Use pactl list sinks to list the available sinks and pactl set-default-sink to switch the default sink to the Bluetooth device. This can be automated via udev using a script similar to this one.

See this Reddit thread for a discussion of the issue. According to author of the script, the headset profile (HSP) might still have problems.

The factual accuracy of this article or section is disputed.

The best tool to verify the condition of this issue is to use mpv on a file that is expected to work with installed codecs:

This recipie applies if some or all of the above tests produce sound and the same test with pipewire option does not produce sound:

Gnome desktop speaker test and web browser 'youtube' produce valid sound outcomes.

Switching inputs, muting, unmuting, changing volume in Gnome does not resolve the issue.

Sink status reported by pactl list sinks as 'SUSPENDED' is of no concern, because status properly changes when running video through a web browser.

Use of pactl info does not point to any obvious issues.

Inspection of relevant systemd unit logs does not point to any obvious issues.

It seems that a path from pipewire to hardware got muted or changed somehow. The original author does not know how to identify and point out at the issue using command line tooling.

Install the pavucontrol package. Run pavucontrol, select the appropriate source in the Configuration tab, select it again in the Output device tab and then use Mute button to mute and unmute the source while mpv --ao=pipewire test.mp4 video is running.

In another case, removing ~/.local/state/wireplumber/ and rebooting solved the same problem.

After replacing PulseAudio with Pipewire, sound may work fine, but after a reboot, the volume becomes intolerably low.

Open alsamixer, use F6 to select the proper soundcard, and make sure the ALSA volumes are at 100%. alsactl should maintain this setting after reboot.

Install realtime-privileges and add your own user to the realtime group.

Alternatively, increasing memlock from 64kB to 128kB seems enough to fix this. If you are running pipewire-pulse under systemd/User, add:

to /etc/security/limits.d/username.conf

By default PipeWire sets a fixed global sample rate of 48kHz. If you need to change it, you can set a new default (although it isn't recommended):

This, however, isn't recommended as this will affect latencies as the quantum values aren't re-calculated automatically. You will have to change these yourself if you want to preserve the same ratio. To quote the documentation:

Keep in mind that the rates of the streams will remain the same. All that PipeWire will be doing here is resampling to meet your new rate. So the 48kHz streams that would've been left unaltered will now be resampled.

If you have gear that can handle different sample rates, it is instead recommended to leave the defaults and follow as indicated here: #Changing the allowed sample rate(s)

PipeWire can also change dynamically the output sample rates supported by your DAC. The sample rate follows the sample rate of the audio stream being played.

for example, [ 44100 88200 176400 48000 96000 192000 ].

Say the default sample rate is 48000, which is the default. Normally if a stream outputs audio at 44100Hz (i.e. a music player playing a song at CD quality), Pipewire will resample the stream to 48kHz. However, if you have 44100 in this list, Pipewire will instead do the following. Pipewire will see that the stream is at 44100Hz, then check if 44100 is in the allowed rates and that the receiving output device (i.e. DAC) supports the rate. In such case, the DAC will play the song losslessly. If the rate isn't in the list or if the DAC doesn't support the rate, PipeWire will simply fall back to resampling the stream to the default sample rate.

According to the developer: "PipeWire allows up to 16 different sample rates and will switch when possible". That means, with configuration above, no resampling is done when supported. Since PipeWire 0.3.61 up to 32 different sample rates can be configured.

Consult your hardware manual for supported values of your DAC. Supported rates by the kernel driver codec are listed with the following command.

If your DAC does not report codec information, you can try to obtain supported rates like this:

To check which output sample rate is being used for a card run:

In pcm0p or pcm0c c is short for "capture" and p is for "playback".

also shows currently used sample rate for each card and audio stream.

Lossless output (no resampling) is easy to configure with PipeWire. All you should have to do is set the following, which is described here: #Changing the allowed sample rate(s). Read that section for further context. These are the industry standard rates that you'll encounter, being the CD quality family (44100Hz, 88200Hz, 176400Hz) and DVD quality family (48kHz, 96kHz, 192kHz). Most audio streams will be in one of these rates. As long as your player of choice is the main stream, and your DAC supports the rate, PipeWire will use it. Ensure that your player is the only stream playing or resampling may occur as everything else is resampled to match the sample rate of the main graph.

If you used PulseAudio with resample-method = speex-float-10 or soxr-vhq, then you might consider setting resample.quality to 10 or the maximum 14:

Do not forget to restart the pipewire.service and pipewire-pulse.socket user units (never forget pipewire-pulse.socket if you want your configuration changes to be applied).

There is a very little quality difference between 10 and 14, but the CPU load difference is 2-3x. And the latency difference between 4, 10, 14 is yet to be investigated by anybody. resample.quality = 14 on 44100→48000 Hz on Ryzen 2600 causes pipewire or pipewire-pulse processes to cause 4.0% one CPU core load.

You can compare resamplers here: https://src.infinitewave.ca/ (do not pay attention to anything above 18 KHz and over 120 dB). speex is listed as "Xiph.org Speex".

PipeWire uses its own resampling algorithm called Spa. Like with SoX's sox, Speex's speexenc, PipeWire includes its standalone version: spa-resample. Usage:

It is probably somehow possible to use other resamplers by creating your own sink. Or just use a plugin in your music player (e.g., Qmmp has SoX plugin).

Check ~/.config/pipewire/media-session.d/default-profile if there is any entry with default profile "off" and remove it. If that does not help, remove all files from ~/.config/pipewire/media-session.d/ and restart the pipewire.service user unit.

It means applications are unable to connect to the PipeWire-Pulse service check if the pipewire-pulse.service user unit is running.

If that does not fix it, run strace -f -o /tmp/pipe.txt pactl info and pastebin /tmp/pipe.txt while seeking help on IRC (#pipewire on OFTC) or the mailing-lists.

In case Bluetooth playback stutters, check the unit status of the pipewire.service user unit for errors similar as below:

If they appear, check the currently selected codec using pactl list sinks and try changing it by setting bluez5.codec to one of sbc aac ldac aptx aptx_hd. You can also try mSBC support (fixes mic on Sony 1000XM3, i.e. Headphones WH-1000XM3 and Earbuds WF-1000XM3), and the SBC-XQ codec.

With pipewire-media-session:

Restart PipeWire by restarting the pipewire.service user unit for the changes to take effect.

This is caused by node suspension when inactive.

With wireplumber, create a new file to overwrite the default configuration:

Restart pipewire.service and wireplumber.service to apply changes.

Instead of disabling suspension entirely, you can also change the timeout value to the desired number of seconds of delay before source suspension.

Some devices implement their own detection of silence and suspension. For them disabling node suspention alone won't work. It's possible to work around them by adding a small amount of noise, making it so the output never goes fully silent:

It may be necessary to play with dither.noise and dither.method parameters to make it so the noise is sufficiently silent and simultaneously loud enough to prevent detection of silence. See PipeWire documentation.

With pipewire-media-session:

Disable this by editing /etc/pipewire/media-session.d/*-monitor.conf depending on where the delay occurs and changing property session.suspend-timeout-seconds to 0 to disable or experiment with other values and see what works.

Alternatively you can comment out the line suspend-node in /etc/pipewire/media-session.d/media-session.conf.

Restart both pipewire.service and pipewire-pulse.service to apply these changes, or alternatively reboot.

This problem can typically be diagnosed by reading the journal of the pipewire-pulse.service user unit and finding lines similar to:

According to the official PipeWire troubleshooting guide, to solve this problem for wireplumber:

With pipewire-media-session:

If you experience audio stuttering because of kernel page locking or late scheduling see Gaming#Tweaking kernel parameters for response time consistency.

If the sound is missing or otherwise garbled after waking the machine up from sleep, it might help to reinitialize ALSA:

Changing sample rates or formats might help reduce latency with some DACs such as Schiit Hel 2.[4]

For pipewire-media-session:

Copy the default configuration file /usr/share/pipewire/media-session.d/alsa-monitor.conf into /etc/pipewire/media-session.d/ (or ~/.config/pipewire/media-session.d/). Then append a new rule-block similar to the following one:

alsa_output.name-of-node node can be obtained using pw-top.

Your DAC might support a different format or sample rate. You can check what your DAC supports by querying ALSA:

First get the card number of your DAC:

So in this example it would be card 3. Get all supported sample rates and formats:

In this case S16_LE, S24_3LE, S32_LE are the supported formats and 44100, 48000, 88200, 96000, 176400, 192000, 352800, 384000 are the supported sample rates across all formats.

Some USB DACs will have no sound output until a certain level of volume is reached [5]. Typically, this is around 15% to 30%, which may result in an uncomfortably loud initial volume and the inability to maintain a low volume. The solution is to ignore hardware mixer volume control by setting api.alsa.soft-mixer for the device to true.

To achieve this with wireplumber, use:

Refer to WirePlumber#Obtain_interface_name_for_rules_matching to find the correct value to replace "alsa_card.name-of-device".

Alternatively, you may specify "~alsa_card.*" to apply the rules to all your audio devices.

Then, restart pipewire, e.g. by running systemctl --user restart pipewire. Set your master volume in alsamixer, then save the settings by running alsactl store as root. You should now be able to use your volume mixer as normal.

If RTKit error: org.freedesktop.DBus.Error.AccessDenied shows up in the status of the pipewire.service user unit, then the priority of the pipewire daemon was not changed to realtime. See [6] for this issue.

Create a copy of /usr/share/alsa-card-profile/mixer/profile-sets/default.conf so that changes persist across updates. Here we define a profile joining the two default mappings for Analog and HDMI.

Then configure your session manager to use the new card-profile for matching devices. Identifying information can be found using pw-dump or wpctl.

For pipewire-media-session:

This might cause by having the min.quantum too low, try setting it to more than 700. You can make an override for Discord specifically by appending the following rule to the pulse.rules section of pipewire-pulse.conf.

Some games that use an old version of the FMOD audio engine, like Pillars of Eternity, invoke pulseaudio --check and crash if the PulseAudio binary is not present. A workaround is to symlink /bin/pulseaudio to /bin/true.[7]

Note that if you wish to reinstall PulseAudio, you need to remove the symlink.

If auto-switching is not working it may be an issue with WirePlumber state. As suggested by this comment you can delete WirePlumber's local state and restart the daemon to see if that helps:

Then restart the wireplumber.service user unit.

Due to a bug from 2011 in rtkit, suspend events cause PipeWire's realtime priority to be revoked and not restored. To disable the protection which causes this, edit rtkit-daemon.service:

Then restart the rtkit-daemon.service unit and pipewire.service user unit, along with the media session service.

Set up mDNS hostname resolution using either Avahi or systemd-resolved.

PipeWire clients (including the desktop environment) may rely on the XDG_RUNTIME_DIR environment variable to connect to the PipeWire daemon. [8] If you experience no sound devices immediately after login, it may be because this variable has manually been set to the wrong path.

Although this be resolved by manually restarting PipeWire, other issues can still occur such as being unable to screen share in Chromium (with pipewire context failed). XDG_RUNTIME_DIR is automatically set by pam_systemd(8), so you should remove any instances of it being set in your initialization files.

If you use SDDM or LightDM and notice that your audio volume level is not properly restored after logging in, mask PipeWire for the display manager's user, since WirePlumber running under the display manager can interfere with your user's WirePlumber session.

Replace user with sddm for SDDM or lightdm for LightDM.

For more details, see this Debian Wiki article.

From PipeWire's perspective, one must have the module x11.bell loaded. This shall be the configuration default (see also in config files mentioned above). Check if you have package pipewire-x11-bell installed. Also, your window manager might influence the terminal bell, e.g., for xfwm, check in the xfwm-terminal settings that "Audible bell" is activated. Now, restart pipewire service:

You can try if the terminal bell works with:

When PipeWire is started using socket activation, some PipeWire-native applications may attempt to play audio before WirePlumber configures the nodes, resulting in an error, for example:

As a workaround, you can enable the pipewire.service user unit.

If you are sure that you have xdg-desktop-portal installed as well as either xdg-desktop-portal-gtk or xdg-desktop-portal-kde, check the running state of the daemons.

In OBS, if everything is working, you should see this in stdout:

For multi-monitor setups the slurp package will allow to capture of all the screens.

**Examples:**

Example 1 (unknown):
```unknown
/usr/share/wireplumber
```

Example 2 (unknown):
```unknown
/etc/wireplumber
```

Example 3 (unknown):
```unknown
~/.config/wireplumber
```

Example 4 (unknown):
```unknown
/usr/share/pipewire
```

---

## System maintenance

**URL:** https://wiki.archlinux.org/title/Partial_upgrade

**Contents:**
- Check for errors
  - Failed systemd services
  - Log files
- Backup
  - Configuration files
  - List of installed packages
  - Pacman database
  - Encryption metadata
  - System and user data
- Upgrading the system

Regular system maintenance is necessary for the proper functioning of Arch over a period of time. Timely maintenance is a practice many users get accustomed to.

Check if any systemd services have failed:

See systemd#Using units for more information.

Look for errors in the log files located in /var/log/, as well as messages logged in the systemd journal:

See Xorg#Troubleshooting for information on where and how Xorg logs errors.

Having backups of important data is a necessary measure to take, since human and machine processing errors are very likely to generate corruption as time passes, and also the physical media where the data is stored is inevitably destined to fail.

See Synchronization and backup programs for many alternative applications that may better suit your case. See Category:System recovery for other articles of interest.

It is highly encouraged to automate backups and test the recovery process to ensure everything works as intended. For automation see System backup#Automation.

Before editing any configuration files, create a backup so that you can revert to a working version in case of problems. Editors like vim and emacs can do this automatically. On a larger scale, consider using a configuration manager.

For dotfiles (configuration files in the home directory), see dotfiles#Tracking dotfiles directly with Git.

Maintain a list of all installed packages so that if a complete re-installation is inevitable, it is easier to re-create the original environment.

See pacman/Tips and tricks#List of installed packages for details.

See pacman/Tips and tricks#Back up the pacman database.

See Data-at-rest encryption#Backup for disk encryption scenarios.

It is recommended to perform full system upgrades regularly via pacman#Upgrading packages, to enjoy both the latest bug fixes and security updates, and also to avoid having to deal with too many package upgrades that require manual intervention at once. When requesting support from the community, it will usually be assumed that the system is up to date.

Make sure to have the Arch install media or another Linux "live" CD/USB available so you can easily rescue your system if there is a problem after updating. If you are running Arch in a production environment, or cannot afford downtime for any reason, test changes to configuration files, as well as updates to software packages, on a non-critical duplicate system first. Then, if no problems arise, roll out the changes to the production system.

If the system has packages from the AUR, carefully upgrade all of them.

pacman is a powerful package management tool, but it does not attempt to handle all corner cases. Users must be vigilant and take responsibility for maintaining their own system.

Before upgrading, users are expected to visit the Arch Linux home page to check the latest news, or alternatively subscribe to the RSS feed or the arch-announce mailing list. When updates require out-of-the-ordinary user intervention (more than what can be handled simply by following the instructions given by pacman), an appropriate news post will be made.

Before upgrading fundamental software (such as the kernel, xorg, systemd, or glibc) to a new version, look over the appropriate forum to see if there have been any reported problems.

Users must equally be aware that upgrading packages can raise unexpected problems that could need immediate intervention; therefore, it is discouraged to upgrade a stable system shortly before it is required for carrying out an important task. Instead, wait to upgrade until there is enough time available to resolve any post-upgrade issues.

Avoid doing partial upgrades. In other words, never run pacman -Sy; instead, always use pacman -Syu.

Generally avoid using the --overwrite option with pacman. The --overwrite option takes an argument containing a glob. When used, pacman will bypass file conflict checks for files that match the glob. In a properly maintained system, it should only be used when explicitly recommended by the Arch Developers. See the #Read before upgrading the system section.

Avoid using the -d option with pacman. pacman -Rdd package skips dependency checks during package removal. As a result, a package providing a critical dependency could be removed, resulting in a broken system.

Arch Linux is a rolling release distribution. That means when new library versions are pushed to the repositories, the Developers and Package Maintainers rebuild all the packages in the repositories that need to be rebuilt against the libraries. For example, if two packages depend on the same library, upgrading only one package might also upgrade the library (as a dependency), which might then break the other package which depends on an older version of the library.

That is why partial upgrades are not supported. Do not use:

When refreshing the package database, always do a full upgrade with pacman -Syu.

Be very careful when using IgnorePkg and IgnoreGroup for the same reason. If the system has locally built packages (such as AUR packages), users will need to rebuild them when their dependencies receive a soname bump.

If a partial upgrade scenario has been created, and binaries are broken because they cannot find the libraries they are linked against, do not "fix" the problem simply by symlinking. Libraries receive soname bumps when they are not backwards compatible. A simple pacman -Syu to a properly synced mirror will fix the problem as long as pacman is not broken.

When upgrading the system, be sure to pay attention to the alert notices provided by pacman. If any additional actions are required by the user, be sure to take care of them right away. If a pacman alert is confusing, search the forums or check the latest news on the Arch Linux homepage (see #Read before upgrading the system) for more detailed instructions.

When pacman is invoked, .pacnew and .pacsave files can be created. Pacman provides notice when this happens and users must deal with these files promptly. Users are referred to the pacman/Pacnew and Pacsave wiki page for detailed instructions.

Also, think about other configuration files you may have copied or created. If a package had an example configuration that you copied to your home directory, check to see if a new one has been created.

Upgrades are typically not applied to existing processes. You must restart processes to fully apply the upgrade.

The archlinux-contrib package provides a script called checkservices which runs pacdiff to merge .pacnew files then checks for processes running with outdated libraries and prompts the user if they want them to be restarted.

The kernel is particularly difficult to patch without a reboot. A reboot is always the most secure option, but if this is very inconvenient kernel live patching can be used to apply upgrades without a reboot.

If a package update is expected/known to cause problems, packagers will ensure that pacman displays an appropriate message when the package is updated. If experiencing trouble after an update, double-check pacman's output by looking at /var/log/pacman.log.

At this point, only after ensuring there is no information available through pacman, there is no relevant news on https://archlinux.org/, and there are no forum posts regarding the update, consider seeking help on the forum or over IRC. Downgrading the offending package to revert broken updates should be considered as a last resort.

After upgrading you may now have packages that are no longer needed or that are no longer in the official repositories.

Use pacman -Qtd to check for packages that were installed as a dependency but now, no other packages depend on them. If an orphaned package is still needed, it is recommended to change the installation reason to explicit. Otherwise, if the package is no longer needed, it can be removed. See pacman/Tips and tricks#Removing unused packages (orphans) for details.

Additionally, some packages may no longer be in the remote repositories, but they still may be on your local system. To list all foreign packages use pacman -Qm. Note that this list will include packages that have been installed manually (e.g., from the AUR). To exclude packages that are (still) available on the AUR, use the script from BBS#288205 or try the ancient-packagesAUR tool.

Pacman does a much better job than you at keeping track of files. If you install things manually you will, sooner or later, forget what you did, forget where you installed to, install conflicting software, install to the wrong locations, etc.

To clean up improperly installed files, see pacman/Tips and tricks#Identify files not owned by any package.

Always try open source drivers before resorting to proprietary drivers. Most of the time, open source drivers are more stable and reliable than proprietary drivers. Open source driver bugs are fixed more easily and quickly. While proprietary drivers can offer more features and capabilities, this can come at the cost of stability. To avoid this dilemma, try to choose hardware components known to have mature open source driver support with full features. Information about hardware with open source Linux drivers is available at linux-drivers.org.

Use precaution when using packages from the AUR or an unofficial user repository. Most are supplied by regular users and thus may not have the same standards as those in the official repositories. Always check PKGBUILDs for sanity and signs of mistake or malicious code before building and/or installing the package.

To simplify maintenance, limit the amount of unofficial packages used. Make periodic checks on which are in actual use, and remove (or replace with their official counterparts) any others. See pacman/Tips and tricks#Maintenance for useful commands. Following system upgrade, use rebuild-detector to identify any unofficial packages that may need to be rebuilt.

Update pacman's mirrorlist, as the quality of mirrors can vary over time, and some might go offline or their download rate might degrade.

See mirrors for details.

Programs that help with this can be found in List of applications/Utilities#Disk cleaning.

When looking for files to remove, it is important to find the files that take up the most disk space. Programs that help with this can be found in List of applications/Utilities#Disk usage display.

Remove unwanted .pkg files from /var/cache/pacman/pkg/ to free up disk space.

See pacman#Cleaning the package cache for more information.

Remove unused packages from the system to free up disk space and simplify maintenance.

See #Check for orphans and dropped packages.

Old configuration files may conflict with newer software versions, or corrupt over time. Remove unneeded configurations periodically, particularly in your home directory and ~/.config. For similar reasons, be careful when sharing home directories between installations.

Look for the following directories:

See XDG Base Directory support for more information about these directories.

To keep the home directory clean from temporary files created at the wrong place, it is a good idea to manage a list of unwanted files and remove them regularly, for example with rmshit.py.

rmlint-gitAUR can be used to find and optionally remove duplicate files, empty files, recursive empty directories and broken symlinks.

Old, broken symbolic links might be sitting around your system; you should remove them. Examples on achieving this can be found here and here. However, you should not blindly delete all broken symbolic links, as some of them serve a purpose [1].

To quickly list all the broken symlinks of permanent files on your system, use:

Then inspect and remove unnecessary entries from this list.

The following tips are generally not required, but certain users may find them useful.

Arch's rolling releases can be a boon for users who want to try the latest features and get upstream updates as soon as possible, but they can also make system maintenance more difficult. To simplify maintenance and improve stability, try to avoid cutting edge software and install only mature and proven software. Such packages are less likely to receive difficult upgrades such as major configuration changes or feature removals. Prefer software that has a strong and active development community, as well as a high number of competent users, in order to simplify support in the event of a problem.

Avoid any use of the testing repository, even individual packages from testing. These packages are experimental and not suitable for a stable system. Similarly, avoid packages which are built directly from upstream development sources. These are usually found in the AUR, with names including things like: "dev", "devel", "svn", "cvs", "git", etc.

The linux-lts package is an alternative Arch kernel package, and is available in the core repository. This particular kernel version has long-term support (LTS) from upstream, including security and bug fixes. It is useful if you use out-of-tree kernel modules and want to ensure their compatibility or if you want a fallback kernel in case a new kernel version causes problems.

To make it available as a boot option, you will need to update your boot loader's configuration file to use the LTS kernel and ram disk: vmlinuz-linux-lts and initramfs-linux-lts.img.

**Examples:**

Example 1 (unknown):
```unknown
$ systemctl --failed
```

Example 2 (unknown):
```unknown
# journalctl -b
```

Example 3 (unknown):
```unknown
pacman -Syu
```

Example 4 (unknown):
```unknown
--overwrite
```

---

## Xorg/Keyboard configuration

**URL:** https://wiki.archlinux.org/title/Keyboard_configuration_in_Xorg

**Contents:**
- Viewing keyboard settings
  - Third party utilities
- Setting keyboard layout
  - Using X configuration files
    - Using localectl
  - Using setxkbmap
- Frequently used XKB options
  - Switching between keyboard layouts
    - Switch languages using Alt Shift
  - Terminating Xorg with Ctrl+Alt+Backspace

This article describes the basics of Xorg keyboard configuration. For advanced topics such as keyboard layout modification or additional key mappings, see X keyboard extension or Extra keyboard keys respectively.

The Xorg server uses the X keyboard extension (XKB) to define keyboard layouts. Optionally, xmodmap can be used to access the internal keymap table directly, although this is not recommended for complex tasks. Also systemd's localectl can be used to define the keyboard layout for both the Xorg server and the virtual console.

You can use the following command to see the actual XKB settings:

There are some "unofficial" utilities which allow to print specific information about the currently used keyboard layout.

Keyboard layout in Xorg can be set in multiple ways. Here is an explanation of used options:

The layout name is usually a 2-letter country code. To see a full list of keyboard models, layouts, variants and options, along with a short description, open /usr/share/X11/xkb/rules/base.lst. Alternatively, you may use one of the following commands to see a list without a description:

Examples in the following subsections will have the same effect, they will set pc104 model, cz as primary layout, us as secondary layout, dvorak variant for us layout and the Super+Space combination for switching between layouts. See xkeyboard-config(7) for more detailed information.

The syntax of X configuration files is explained in Xorg#Configuration. This method creates system-wide configuration which is persistent across reboots.

For convenience, the tool localectl may be used instead of manually editing X configuration files. It will save the configuration in /etc/X11/xorg.conf.d/00-keyboard.conf, this file should not be manually edited, because localectl will overwrite the changes on next start.

The usage is as follows:

To set a model, variant or options, all preceding fields need to be specified, but the preceding fields can be skipped by passing an empty string with "". Unless the --no-convert option is passed, the specified keymap is also converted to the closest matching console keymap and applied to the console configuration in vconsole.conf. See localectl(1) for more information.

To create a /etc/X11/xorg.conf.d/00-keyboard.conf like the above:

This article or section needs expansion.

setxkbmap sets the keyboard layout for the current X session only, but can be made persistent in xinitrc or xprofile. This overrides system-wide configuration specified following #Using X configuration files.

The usage is as follows (see setxkbmap(1)):

To change just the layout (-layout is the default flag):

For multiple customizations:

To be able to easily switch keyboard layouts, first specify multiple layouts between which you want to switch (the first one is the default). Then specify a key (or key combination), which will be used for switching. For example, to switch between a US and a Swedish layout using the CapsLock key, use us,se as an argument of XkbLayout and grp:caps_toggle as an argument of XkbOptions. The number of XkbLayouts should match that of the XkbVariants — if you want to switch solely between different variants, then duplicate the layout accordingly (e.g. de,de).

The list of available layouts (and variants) can be found in xkeyboard-config(7) § LAYOUTS. The key combinations available for layout switching are listed in xkeyboard-config(7) § Switching to another layout.

Note that the grp:alts_toggle option is unreliable and unlikely to be fixed; prefer other combinations.

To set Alt+Shift as a layout shortcut, use grp:alt_shift_toggle in XkbOptions.

However, there is a known issue with XKB that causes other shortcuts of the type Alt+Shift+any_key to break. Moreover, XKB may set the right Alt to be AltGr by default in some keyboard layouts, making RAlt+RShift not working for layout switching.

As a workaround, sxhkd may be used to switch layouts by adding the following to sxhkdrc:

Note that for some reason, Alt must be pressed before Shift to be detected by sxhkd.

By default, the key combination Ctrl+Alt+Backspace is disabled. You can enable it by passing terminate:ctrl_alt_bksp to XkbOptions. This can also be done by binding a key to Terminate_Server in xmodmap (which undoes any existing XkbOptions setting). In order for either method to work, one also needs to have DontZap set to "off" in ServerFlags: since 2004 [2] this is the default.

To swap Caps Lock with Left Control key, add ctrl:swapcaps to XkbOptions. Run the following command to see similar options along with their descriptions:

Mouse keys, not to be confused with the keys of the mouse, is disabled by default and has to be manually enabled by passing keypad:pointerkeys to XkbOptions. This will make the Shift+NumLock shortcut toggle mouse keys.

See also X keyboard extension#Mouse control for advanced configuration.

The AltGr (Alternate Graphic) key can be used to access additional characters and symbols on a keyboard. It functions as a modifier key similar to Shift but provides access to a third level of key mappings. Note that mapping levels work as follows:

2nd level characters are usually printed on keyboard keys and are easy to find. On the other hand, to check the characters on additional levels, you can use xmodmap -pk or look up your keyboard mapping on /usr/share/X11/xkb/symbols.

Though typically not on traditional keyboards, a Compose key can be configured to an existent key.

The Compose key begins a keypress sequence that involves (usually two) additional keypresses. Usage is typically either for entering characters in a language that the keyboard was not designed for, or for other less-used characters that are not covered with the AltGr modifier. For example, pressing Compose ' e produces é, or Compose - - - will produce an "em dash": —.

Though a few more eccentric keyboards feature a Compose key, its availability is usually through substituting an already existing key to it. For example, to make the Menu key a Compose key use the Desktop environment configuration, or pass compose:menu to XkbOptions (or setxkbmap: setxkbmap -option compose:menu). Allowed key substitutions are defined in /usr/share/X11/xkb/rules/base.lst:

If the desired mapping is not found in that file, an alternative is to use xmodmap to map the desired key to the Multi_key keysym, which acts as a compose key by default (note that xmodmap settings are reset by setxkbmap).

The default combinations for the compose keys depend on the locale configured for the session and are stored in /usr/share/X11/locale/used_locale/Compose, where used_locale is for example en_US.UTF-8.

You can define your own compose key combinations by copying the default file to ~/.XCompose and editing it. Alternatively, create an empty ~/.XCompose and include the default one using include "%L", for example:

The compose key (denoted as <Multi_key> in the ~/.XCompose file) works with any of the thousands of valid Unicode characters, including those outside the Basic Multilingual Plane. Take a look at the Compose(5) man page, it explains the format of the XCompose files.

However, GTK does not use XIM by default and therefore does not follow ~/.XCompose keys. This can be fixed by forcing GTK to use XIM by configure the graphical environment variables GTK_IM_MODULE=xim and/or XMODIFIERS="@im=none".

Most European keyboards have a Euro sign (€) printed on on the 5 key. For example, to access it with Alt+5, use the lv3:ralt_switch and eurosign:5 options.

The Rupee sign (₹) can be used the same way with rupeesign:4.

Those who prefer typing capital letters with the Caps Lock key may experience a short delay when Caps Lock state is switched, resulting in two or more capital letters (e.g. THe, ARch LInux). This occurs because Caps Lock is enabled immediately once the Caps Lock key is pressed, but is only disabled upon release of the second key-press. This behaviour stems from typewriters where a Caps Lock function was achieved by physically locking the shifted typebars in place, and the release of a shift key-press was the action that caused the release of the lock.

Some more popular operating systems have removed this behaviour, either voluntarily (as it can be confusing to some) or by mistake, however this is a question of preference. Bug reports have been filed on the Xserver bug tracker, as there is currently no easy way to switch to the behaviour reflected by those other operating systems. For anyone who would like to follow up the issue, bug reports and latest working progress can be found at [3] and [4].

First, export your keyboard configurations to a file:

In the file xkbmap, locate the Caps Lock section which begins with key <CAPS>:

and replace whole section with the following code:

Save and reload keyboard configurations:

Consider making it a service launching after X starts, since reloaded configurations do not survive a system reboot.

To assign an additional one-click function to a modifier key, you can use xcape. For example it is possible to have CapsLock work as Escape when pressed alone, and as Control when used with another key. First set the Control swapping using setxkbmap as mentioned earlier, and xcape to set the Escape association:

You can set multiple associations separated with a semicolon, e.g.: Caps_Lock=Escape;Shift_L=Escape.

If you hold a key for longer than the timeout value (default 500 ms), xcape will not generate a key event.

The typematic delay indicates the amount of time (typically in milliseconds) a key needs to be pressed and held in order for the repeating process to begin. After the repeating process has been triggered, the character will be repeated with a certain frequency (usually given in Hz) specified by the typematic rate. Note that these settings are configured separately for Xorg and for the virtual console.

The tool xset, provided by xorg-xset, can be used to set the typematic delay and rate for an active X server, though certain actions during runtime may cause the X server to reset these changes and revert instead to its seat defaults.

For example to set a typematic delay to 200ms and a typematic rate to 30Hz, use the following command:

Issuing the command without specifying the delay and rate will reset the typematic values to their respective defaults; a delay of 660ms and a rate of 25Hz:

xautocfgAUR can apply repeat rate settings for newly connected devices automatically. It watches for X11 events and applies repeat rate configuration to newly connected keyboards.

Adjust the configuration:

If graphical-session.target is started by your window manager or desktop environment, enable the systemd/User xautocfg.service. Alternatively, launch xautocfg manually.

To persist the configuration system-wide, change the seat defaults with an Xorg configuration file as described in #Using X configuration files, and add a AutoRepeat section entry: [5]

The parameters for AutoRepeat are delay and interval in milliseconds. If you like 25 Hz rate of xset, the corresponding interval 1000 / 25 = 40 milliseconds.

Another method of persisting the configuration is to pass the desired settings to the X server on its startup using the following options:

See Xserver(1) for a full list of X server options and refer to your display manager for information about how to pass these options.

To manually modify or create a layout, some low-level knowledge is needed.

To physical keys, numbers assigned by kernel. They are listed in /usr/include/linux/input-event-codes.h, provided linux-api-headers—a dependency of linux-headers—is installed, like this:

These values are used by libevdev. For input remap utilities that rely on libevdev, key names are often these entries in lowercase, without the prefix KEY_. Xorg's "keycode" value is the one in the file input-event-codes.h + 8. (For example KEY_Q is 16. The X's keycode of the key printed "Q" 24.)

In XKB however, i.e. for X and Wayland, the key names look like <AD11> or <TLDE>. Most of them are easy to guess; for example if you have an Italian keyboard, just look at /usr/share/X11/xkb/symbols/it. It has the lines:

When it is not sufficient, look at files in /usr/share/X11/xkb/keycodes, in particular the file evdev. These lines define the keycode for the key name.

Finally, what are possible symbols that can be assigned to keys, like notsign or brokenbar in the above example? They are defined in /usr/include/X11/keysymdef.h

Some symbols with the prefix XF86 is listed in the separate file /usr/include/X11/XF86keysym.h.

This key is called XF86MonBrightnessUp in X. (Again rip off XK_, but in such cases the middle of the name.)

**Examples:**

Example 1 (unknown):
```unknown
$ setxkbmap -print -verbose 10
```

Example 2 (unknown):
```unknown
Setting verbose level to 10
locale is C
Applied rules from evdev:
model:      evdev
layout:     us
options:    terminate:ctrl_alt_bksp
Trying to build keymap using the following components:
keycodes:   evdev+aliases(qwerty)
types:      complete
compat:     complete
symbols:    pc+us+inet(evdev)+terminate(ctrl_alt_bksp)
geometry:   pc(pc104)
xkb_keymap {
        xkb_keycodes  { include "evdev+aliases(qwerty)" };
        xkb_types     { include "complete"      };
        xkb_compat    { include "complete"      };
        xkb_symbols   { include "pc+us+inet(evdev)+terminate(ctrl_alt_bksp)"    };
        xkb_geometry  { include "pc(pc104)"     };
};
```

Example 3 (unknown):
```unknown
$ xkb-switch
```

Example 4 (unknown):
```unknown
$ xkblayout-state print "%s"
```

---

## Unified Extensible Firmware Interface

**URL:** https://wiki.archlinux.org/title/UEFI_shell

**Contents:**
- UEFI firmware bitness
  - Checking the firmware bitness
    - From Linux
    - From macOS
    - From Microsoft Windows
- UEFI variables
  - UEFI variables support in Linux kernel
  - Requirements for UEFI variable support
    - Mount efivarfs
  - Userspace tools

The Unified Extensible Firmware Interface (UEFI) is an interface between operating systems and firmware. It provides a standard environment for booting an operating system and running pre-boot applications.

It is distinct from the MBR boot code method that was used by legacy BIOS systems. See Arch boot process for their differences and the boot process using UEFI. To set up UEFI boot loaders, see Arch boot process#Boot loader.

Under UEFI, every program whether it is an operating system loader or a utility (e.g. a memory testing or recovery tool), should be an EFI application corresponding to the UEFI firmware bitness/architecture.

The vast majority of x86_64 systems, including recent Apple Macs, use x64 (64-bit) UEFI firmware. The only known devices that use IA32 (32-bit) UEFI are older (pre 2008) Apple Macs, Intel Atom System-on-Chip systems (as on 2 November 2013)[1] and some older Intel server boards that are known to operate on Intel EFI 1.10 firmware.

An x64 UEFI firmware does not include support for launching 32-bit EFI applications (unlike x86_64 Linux and Windows versions which include such support). Therefore the EFI application must be compiled for that specific firmware processor bitness/architecture.

The firmware bitness can be checked from a booted operating system.

On distributions running Linux kernel 4.0 or newer, the UEFI firmware bitness can be found via the sysfs interface. Run:

It will return 64 for a 64-bit (x64) UEFI or 32 for a 32-bit (IA32) UEFI. If the file does not exist, you have not booted in UEFI mode.

Pre-2008 Macs mostly have IA32 EFI firmware while >=2008 Macs have mostly x64 EFI. All Macs capable of running Mac OS X Snow Leopard 64-bit Kernel have x64 EFI 1.x firmware.

To find out the arch of the EFI firmware in a Mac, type the following into the Mac OS X terminal:

If the command returns EFI32, it is IA32 (32-bit) EFI firmware. If it returns EFI64, it is x64 EFI firmware. Most of the Macs do not have UEFI 2.x firmware as Apple's EFI implementation is not fully compliant with UEFI 2.x specification.

64-bit versions of Windows do not support booting on a 32-bit UEFI. So, if you have a 32-bit version of Windows booted in UEFI mode, you have a 32-bit UEFI.

To check the bitness run msinfo32.exe. In the System Summary section look at the values of "System Type" and "BIOS mode".

For 64-bit Windows on a 64-bit UEFI, it will be System Type: x64-based PC and BIOS mode: UEFI. For 32-bit Windows on a 32-bit UEFI—System Type: x86-based PC and BIOS mode: UEFI. If the "BIOS mode" is not UEFI, Windows is not booted in UEFI mode.

UEFI defines variables through which an operating system can interact with the firmware. UEFI boot variables are used by the boot loader and used by the operating system only for early system start-up. UEFI runtime variables allow an operating system to manage certain settings of the firmware like the UEFI boot manager or managing the keys for UEFI Secure Boot protocol etc. You can get the list using:

Linux kernel exposes UEFI variables data to userspace via efivarfs (EFI VARiable FileSystem) interface (CONFIG_EFIVAR_FS) - mounted using efivarfs kernel module at /sys/firmware/efi/efivars - it has no maximum per-variable size limitation and supports UEFI Secure Boot variables. Introduced in kernel 3.8.

If UEFI Variables support does not work even after the above conditions are satisfied, try the below workarounds:

If efivarfs is not automatically mounted at /sys/firmware/efi/efivars by systemd during boot, you need to manually mount it to expose UEFI variables to userspace tools like efibootmgr:

See efivarfs.html for kernel documentation.

There are few tools that can access/modify the UEFI variables, namely

You will have to install the efibootmgr package.

To add a new boot option using efibootmgr, you need to know three things:

For example, if you want to add a boot option for /efi/EFI/refind/refind_x64.efi where /efi is the mount point of the ESP, run

In this example, findmnt(8) indicates that the ESP is on disk /dev/sda and has partition number 1. The path to the EFI application relative to the root of the ESP is /EFI/refind/refind_x64.efi. So you would create the boot entry as follows:

Get an overview of all boot entries and the boot order:

To set the boot order:

Where XXXX is the number that appears in the previous output of efibootmgr command.

Delete an unwanted entry:

See efibootmgr(8) or efibootmgr README for more info.

Access to the UEFI can potentially cause harm beyond the running operating system level. There are dangerous UEFI exploits like LogoFAIL which allows a malicious actor to take full control over the machine. Even hardware-level bricking is possible in some cases of poor UEFI implementation [2].

So, as the UEFI variables access is not required for daily system usage, you may want to disable it, to avoid potential security breaches or accidental harm.

Possible solutions are:

The UEFI Shell is a shell/terminal for the firmware which allows launching EFI applications which include UEFI boot loaders. Apart from that, the shell can also be used to obtain various other information about the system or the firmware like memory map (memmap), modifying boot manager variables (bcfg), running partitioning programs (diskpart), loading UEFI drivers, editing text files (edit), hexedit etc.

You can obtain a BSD licensed UEFI Shell from the TianoCore EDK2 project:

Shell v2 works best in UEFI 2.3+ systems and is recommended over Shell v1 in those systems. Shell v1 should work in all UEFI systems irrespective of the spec. version the firmware follows. More information at ShellPkg and the EDK2 mailing list thread—Inclusion of UEFI shell in Linux distro iso.

Few Asus and other AMI Aptio x64 UEFI firmware based motherboards (from Sandy Bridge onwards) provide an option called Launch EFI Shell from filesystem device. For those motherboards, copy the x64 UEFI Shell to the root of your EFI system partition, named as shellx64.efi.

Systems with Phoenix SecureCore Tiano UEFI firmware is known to have embedded UEFI Shell which can be launched using either F6, F11 or F12 key.

UEFI Shell commands usually support -b option which makes output pause after each page. Run help -b to list available internal commands. Available commands are either built into the shell or discrete EFI applications.

For more info see Intel Scripting Guide 2008[dead link 2023-07-30—HTTP 404] and Intel "Course" 2011[dead link 2023-07-30—HTTP 404].

bcfg modifies the UEFI NVRAM entries which allows the user to change the boot entries or driver options. This command is described in detail in page 96 (Section 5.3) of the UEFI Shell Specification 2.2 document.

To dump a list of current boot entries:

To add a boot menu entry for rEFInd (for example) as 4th (numbering starts from zero) option in the boot menu:

where FS0: is the mapping corresponding to the EFI system partition and FS0:\EFI\refind\refind_x64.efi is the file to be launched.

To add an entry to boot directly into your system without a boot loader, see EFI boot stub#bcfg.

To remove the 4th boot option:

To move the boot option #3 to #0 (i.e. 1st or the default entry in the UEFI Boot menu):

map displays a list of device mappings i.e. the names of available file systems (FS0) and storage devices (blk0).

Before running file system commands such as cd or ls, you need to change the shell to the appropriate file system by typing its name:

edit provides a basic text editor with an interface similar to nano, but slightly less functional. It handles UTF-8 encoding and takes care or LF vs CRLF line endings.

For example, to edit rEFInd's refind.conf in the EFI system partition (FS0: in the firmware),

Press Ctrl+e for help.

This article or section needs expansion.

UEFI drivers are pieces of software that support some functionality. For example, access to NTFS formatted partitions is usually not possible from a UEFI shell. The efifs package has drivers that support reading many more file systems from within an EFI shell. A usage example is to copy such driver to a partition that can be accessed from an UEFI shell. Then, from the UEFI shell, issuing commands such as:

After the map command has been executed, the user should be able to access NTFS formatted partitions from within a UEFI shell.

Most of the 32-bit EFI Macs and some 64-bit EFI Macs refuse to boot from a UEFI(X64)+BIOS bootable CD/DVD. If one wishes to proceed with the installation using optical media, it might be necessary to remove UEFI support first.

Extract the ISO skipping the UEFI-specific directories:

Then rebuild the ISO, excluding the UEFI optical media booting support, using xorriso(1) from libisoburn. Be sure to set the correct volume label, e.g. ARCH_202103; it can be acquired using file(1) on the original ISO.

Burn archlinux-version-x86_64-noUEFI.iso to optical media and proceed with installation normally.

OVMF is a TianoCore project to enable UEFI support for Virtual Machines. OVMF contains a sample UEFI firmware and a separate non-volatile variable store for QEMU.

You can install edk2-ovmf from the extra repository.

It is advised to make a local copy of the non-volatile variable store for your virtual machine:

To use the OVMF firmware and this variable store, add following to your QEMU command:

DUET was a TianoCore project that enabled chainloading a full UEFI environment from a BIOS system, in a way similar to BIOS operating system booting. This method is being discussed extensively. Pre-build DUET images can be downloaded from one of the repos[dead link 2023-04-07—404 Page Not Found]. Read specific instructions[dead link 2023-04-07—404 Page Not Found] for setting up DUET. However, as of November 2018, the DUET code has been removed from TianoCore git repository.

You can also try Clover which provides modified DUET images that may contain some system specific fixes and is more frequently updated compared to the gitlab repos.

To boot back into Arch Linux when you are stuck with Windows, reach Advanced startup in Windows by the Windows PowerShell command shutdown /r /o, or via Settings > Update & Security > Recovery > Advanced startup and select Restart now. When you have reached the Advanced startup menu, choose Use a device, which actually contains your UEFI boot options (not limited to USB or CD, but can also boot operating system in hard drive), and choose "Arch Linux".

On some laptops, like Lenovo XiaoXin 15are 2020, using keys like F2 or F12 does not do anything. This can possibly be fixed by returning laptops to OEM to repair mainboard information, but sometimes this is not possible or not desired. There are however other means to enter firmware setup:

If any userspace tool is unable to modify UEFI variable data, check for existence of /sys/firmware/efi/efivars/dump-* files. If they exist, delete them, reboot and retry again. If the above step does not fix the issue, try booting with efi_no_storage_paranoia kernel parameter to disable kernel UEFI variable storage space check that may prevent writing/modification of UEFI variables.

Some kernel and efibootmgr version combinations might refuse to create new boot entries. This could be due to lack of free space in the NVRAM. You can try the solution at #Userspace tools are unable to modify UEFI variable data.

You can also try to downgrade your efibootmgr install to version 0.11.0. This version works with Linux version 4.0.6. See the bug discussion FS#34641, in particular the closing comment, for more information.

If you dual boot with Windows and your motherboard just boots Windows immediately instead of your chosen EFI application, there are several possible causes and workarounds.

This issue can occur due to KMS issue. Try disabling KMS while booting the USB.

Some firmware do not support custom boot entries. They will instead only boot from hardcoded boot entries.

A typical workaround is to not rely on boot entries in the NVRAM and install the boot loader to one of the common fallback paths on the EFI system partition.

The following sections describe the fallback paths.

The UEFI specification defines default file paths for EFI binaries for booting from removable media. The relevant ones are:

While the specification defines these for removable drives only, most firmware support booting these from any drive.

See the appropriate boot loader article on how to install or migrate the boot loader to the default/fallback boot path.

On certain UEFI motherboards like some boards with an Intel Z77 chipset, adding entries with efibootmgr or bcfg from the UEFI Shell will not work because they do not show up on the boot menu list after being added to NVRAM.

This issue is caused because the motherboards can only load Microsoft Windows. To solve this you have to place the .efi file in the location that Windows uses.

Copy the BOOTx64.EFI file from the Arch Linux installation medium (FSO:) to the Microsoft directory your ESP partition on your hard drive (FS1:). Do this by booting into EFI shell and typing:

After reboot, any entries added to NVRAM should show up in the boot menu.

This is a recurring problem with Acer laptops, which occurs if .efi files have not been manually authorized. See Laptop/Acer#Firmware Setup became inaccessible after Linux installation.

efibootmgr can fail to detect EDD 3.0 and as a result create unusable boot entries in NVRAM. See efibootmgr issue 86 for the details.

To work around this, when creating boot entries manually, add the -e 3 option to the efibootmgr command. E.g.

To fix boot loader installers, like grub-install and refind-install, create a wrapper script /usr/local/bin/efibootmgr and make it executable:

Some firmware will remove boot entries referencing drives that are not present during boot. This could be an issue when frequently detaching/attaching drives or when booting from a removable drive.

The solution is to install the boot loader to the default/fallback boot path.

Some motherboards may remove boot entries due to lack of free space in the NVRAM instead of giving an error at creation. To prevent this from occurring, reduce the amount of boot entries being added by minimizing your entry creation process, as well as reducing the amount of automatic drive boot entries by the Compatibility Support Module (CSM) by disabling it from your UEFI settings. See BBS#1608838.

Another reason why boot entries might have been removed is the fact that UEFI specification allows OEMs to do "NVRAM maintenance" during boot process. Those manufacturers do it simply: they just look up for EFI applications in predefined, hardcoded paths on the device. If they fail to find any, they conclude there is no operating system on the device and wipe all boot entries from NVRAM associated with it, because they assume the NVRAM contains some corrupted or outdated data. If you do not plan to install Windows and still want to load the Linux kernel directly from the firmware, one possible workaround is to create an empty file esp/EFI/BOOT/BOOTx64.EFI:

And restore the deleted boot entry. Now after reboot the motherboard will see the "Fake OS" and should not wipe other boot entries from NVRAM. You can change the fake operating system loader with an actual EFI application if you want, of course, as long as you keep the standard fallback name.

This article or section is a candidate for merging with Lenovo.

This article or section needs expansion.

On recent Lenovo ThinkPad laptops (e.g. T16 Gen 2 AMD models), users report that custom UEFI boot entries (created with efibootmgr or bootctl) are automatically deleted at each boot, with only Windows Boot Manager and Lenovo’s own entries (PXE, Recovery, Diagnostics) restored.

This is caused by the BIOS option "Restart / OS Optimized Defaults", which resets the UEFI boot variables at each reboot to defaults optimized for Windows.

Solution: Disable "OS Optimized Defaults" in the BIOS/UEFI setup. After doing so, manually created boot entries persist correctly, allowing systemd-boot or other custom boot managers to work as intended.

**Examples:**

Example 1 (unknown):
```unknown
$ cat /sys/firmware/efi/fw_platform_size
```

Example 2 (unknown):
```unknown
$ ioreg -l -p IODeviceTree | grep firmware-abi
```

Example 3 (unknown):
```unknown
msinfo32.exe
```

Example 4 (unknown):
```unknown
System Type: x64-based PC
```

---

## D-Bus

**URL:** https://wiki.archlinux.org/title/Dbus

**Contents:**
- Implementations
  - dbus-broker
  - Reference implementation
- Tips and tricks
  - Override dbus service
- Debugging
- See also

This article or section needs expansion.

D-Bus is a message bus system that provides an easy way for inter-process communication. It consists of a daemon, which can be run both system-wide and for each user session, and a set of libraries to allow applications to use D-Bus.

dbus is pulled and installed as a dependency of systemd and user session bus is started automatically for each user.

Arch provides two D-Bus message broker implementations. Initially, user will be asked to choose a desired dbus-units provider during installation of systemd package. Only one implementation can be installed at a time.

dbus-broker is currently the default implementation for Arch [1] [2]. It is a drop-in replacement for the reference implementation, which aims "to provide high performance and reliability, while keeping compatibility to the D-Bus reference implementation".

Select dbus-broker-units when asked for dbus-units provider, or install it explicitly.

The reference implementation is still officially supported by Arch.

Select dbus-daemon-units when asked for dbus-units provider, or install it explicitly.

This is useful when specifying a particular service among other services providing the same well-known bus name. See KeePass#Autostart and KDE Wallet#Automatic D-Bus activation for example.

D-Bus services can be masked by setting Exec=/bin/false in service files of $XDG_DATA_HOME/dbus-1/services. For example, to mask gvfsd,

If the service is already launched, the override will not work. The existing service's process must be killed, or launched earlier.

You can also use busctl(1) from systemd.

**Examples:**

Example 1 (unknown):
```unknown
systemctl mask
```

Example 2 (unknown):
```unknown
/etc/dbus-1/services
```

Example 3 (unknown):
```unknown
Exec=/bin/false
```

Example 4 (unknown):
```unknown
$XDG_DATA_HOME/dbus-1/services
```

---

## Stunnel

**URL:** https://wiki.archlinux.org/title/Stunnel

**Contents:**
- Installation
- Configuration
  - Byte order mark (BOM)
  - Authentication
- See also

stunnel (“Secure Tunnel”) is a

Can tunnel only TCP packets. Its FAQ has some work around for UDP. WireGuard also has UDP capabilities.

Authentication can also be used by the server to allow access only to approved clients.

Install the stunnel package.

Depending on your usage, you might also edit the provided systemd units to better handle dependencies. In order for the stunnel to start up automatically at system boot you must enable it.

The main configuration file is read from /etc/stunnel/stunnel.conf. It is an ini-style file. It is composed from a global section, followed by one, or more, service sections.

A client is one to accept non TLS encrypted data. Stunnel will encrypt its data with TLS and connect to the stunnel server. The stunnel server accepts TLS encrypted data and decrypts it. It then connects to where the data should be sent to.

The default debug value is notice, which is very verbose. After verifying correct operation, it is worth explicitly setting lower value in the configuration file.

For better security, it is advised to explicitly set an appropriate uid and gid, other then root, for the global section and the per service sections. The configuration tokens setuid and setgid are available for this purpose.

The configuration file should have a UTF-8 byte order mark (BOM), at the beginning of the file. A BOM is the unicode character U+FEFF. Its UTF-8 representation is the (hexadecimal) byte sequence 0xEF, 0xBB, 0xBF. Creating a file with these bytes at its beginning can be done by

To test if those bytes appear, one can use

Note that when printing the file to the screen, such as with cat, or when editing the file with a text editor, the BOM bytes are usually not displayed. They should be there, though. Which is why you might want to verify that they are still there after editing is completed with the above od, or similar, command.

At least one of the client and the server, and optionally both, should be authenticated. Either a pre shared secret, or a key and certificate pair, can be used for authentication. A pre shared secret has to be transferred to all involved machines a priory by other means, such as SCP and SFTP. When such transfer is acceptable, pre shared key is the fastest method. Its speed might help mitigating attacks. A simple configuration for a single server with a single client that are using a pre shared secret is:

where /etc/stunnel/psk.txt could be created on one machine by

and copied to the other machine by secure means before starting stunnel. The permissions for each psk.txt file should be set appropriately, neither world-readable nor world-writable. The psk string from the sed command is just a random name for the sake of the example. Do read stunnel(8).

**Examples:**

Example 1 (unknown):
```unknown
/etc/stunnel/stunnel.conf
```

Example 2 (unknown):
```unknown
/etc/stunnel/stunnel.conf-sample
```

Example 3 (unknown):
```unknown
/etc/stunnel/stunnel.conf
```

Example 4 (unknown):
```unknown
debug = err
```

---

## Network Time Protocol daemon

**URL:** https://wiki.archlinux.org/title/Ntpd

**Contents:**
- Installation
- Configuration
  - Connection to NTP servers
  - Leap seconds file
  - NTP server mode
- Usage
  - Start ntpd at boot
  - Synchronize time once per boot
- Tips and tricks
  - Start ntpd on network connection

Network Time Protocol is the most common method to synchronize the software clock of a GNU/Linux system with internet time servers. It is designed to mitigate the effects of variable network latency and can usually maintain time to within tens of milliseconds over the public Internet. The accuracy on local area networks is even better, up to one millisecond.

The NTP Project provides a reference implementation of the protocol called simply NTP. This article further describes how to set up and run the NTP daemon, both as a client and as a server.

See System time#Time synchronization for other NTP implementations.

Install the ntp package. By default, ntpd works in client mode without further configuration. You can skip to #Usage if you want to use the Arch Linux default configuration file for it. For server configuration, see #NTP server mode.

The main daemon is ntpd, which is configured in /etc/ntp.conf. Refer to ntp.conf(5) for detail.

NTP servers are classified in a hierarchical system with many levels called strata: the devices which are considered independent time sources are classified as stratum 0 sources; the servers directly connected to stratum 0 devices are classified as stratum 1 sources; servers connected to stratum 1 sources are then classified as stratum 2 sources and so on.

It has to be understood that a server's stratum cannot be taken as an indication of its accuracy or reliability. Typically, stratum 2 servers are used for general synchronization purposes: if you do not already know the servers you are going to connect to, you should choose a server pool close to your location from the pool.ntp.org servers (alternative link).

Since ntp version 4.2.7.p465-2, Arch Linux uses its own default vendor pool of NTP servers provided by the NTP Pool Project (see FS#41700). Modify those to suit your needs, e.g. if you want to use your country's servers with an option:

The iburst option is recommended, and sends a burst of packets only if it cannot obtain a connection with the first attempt. The burst option always does this, even on the first attempt, and should never be used without explicit permission and may result in blacklisting.

In order for the system to be able to provide the International Atomic Time to an application that requests it, the list of leap seconds must be loaded. The list is part of the tzdata package and can be loaded by adding the following line to the NTP configuration file:

If setting up an NTP server, check that you have orphan mode enabled, so that, in case it loses internet access, it will continue serving time to the network; enable orphan mode using the tos configuration parameter (you can set up to stratum 15) so that it will never be used unless internet access is lost:

Next, define the rules that will allow clients to connect to your service (localhost is considered a client too) using the restrict command; you should already have a line like this in your file:

This restricts everyone from modifying anything and prevents everyone from querying the status of your time server: nomodify prevents reconfiguring ntpd (with ntpq or ntpdc), and noquery is important to prevent dumping status data from ntpd (also with ntpq or ntpdc).

You can also add other options:

If you want to change any of these, see the full docs for the "restrict" option in ntp.conf(5), the detailed ntp instructions and #Usage.

Following this line, you need to tell ntpd what to allow through into your server; the following line is enough if you are not configuring an NTP server:

If you want to force DNS resolution to the IPv6 namespace, write -6 before the IP address or host name (-4 forces IPv4 instead), for example:

Lastly, specify the drift file (which keeps track of your clock's time deviation) and optionally the log file location:

A very basic configuration file will look like this:

The package has a default client-mode configuration and its own user and group to drop root privileges after starting. If you start it from the console, you should always do so with the -u option:

The -u option is employed by the two included systemd services. These services also use the -g option, which disables a threshold (so-called panic-gate). Hence, they will synchonize time even in case the ntp-server's time exceeds the threshold deviation from the system clock.

Both services are tied to the system's resolver, and will start synchronizing when an active network connection is detected.

Enable the daemon with ntpd.service. See also #Running in a chroot.

Use ntpq to see the list of configured peers and status of synchronization:

The delay, offset and jitter columns should be non-zero. The servers ntpd is synchronizing with are prefixed by an asterisk. It can take several minutes before ntpd selects a server to synchronize with; try checking after 17 minutes (1024 seconds).

Alternatively, enable ntpdate.service to synchronize time once (option -q) and non-forking (option -n) per boot, instead of running the daemon in the background. This method is discouraged on servers, and in general on machines that run without rebooting for more than a few days.

If the synchronized time should be written to the hardware clock as well, configure the provided unit as described in systemd#Editing provided units before starting it:

ntpd can be started by your network manager, so that the daemon only runs when the computer is online.

Append the following lines to your netctl profile:

The ntpd daemon can be brought up/down along with a network connection through the use of NetworkManager's dispatcher scripts. The networkmanager-dispatcher-ntpdAUR package installs one, pre-configured to start and stop the ntpd service with a connection.

KDE can use NTP (ntp must be installed) by right clicking the clock and selecting Adjust date/time. However, this requires the ntp daemon to be disabled before configuring KDE to use NTP. [2]

Most of the articles online about configuring ntpd to receive time from a GPS suggest to use the SHM (shared memory) method. However, at least since ntpd version 4.2.8, a much better method is available. It connects directly to gpsd, so gpsd needs to be installed.

Add these lines to your /etc/ntp.conf:

This will work as long as you have gpsd working. It connects to gpsd via the local socket and queries the "gpsd_json" object that is returned.

To test the setup, first ensure that gpsd is working by running:

Then wait a few minutes and run ntpq -p. This will show if ntpd is talking to gpsd:

Create a new directory /etc/systemd/system/ntpd.service.d/ if it does not exist and a file named customexec.conf inside with the following content:

Then, edit /etc/ntp.conf to change the driftfile path such that it is relative to the chroot directory, rather than to the real system root. Change:

Create a suitable chroot environment so that getaddrinfo() will work by creating pertinent directories and files (as root):

and by bind-mounting the aformentioned files:

Finally, restart ntpd daemon again. Once it restarted you can verify that the daemon process is chrooted by checking where /proc/{PID}/root symlinks to:

should now link to /var/lib/ntp instead of /.

It is relatively difficult to be sure that your driftfile configuration is actually working without waiting a while, as ntpd does not read or write it very often. If you get it wrong, it will log an error; if you get it right, it will update the timestamp. If you do not see any errors about it after a full day of running, and the timestamp is updated, you should be confident of success.

You can limit sockets ntpd is listening to using the interface option:

**Examples:**

Example 1 (unknown):
```unknown
/etc/ntp.conf
```

Example 2 (unknown):
```unknown
/etc/ntp.conf
```

Example 3 (unknown):
```unknown
server 0.fr.pool.ntp.org iburst
server 1.fr.pool.ntp.org iburst
server 2.fr.pool.ntp.org iburst
server 3.fr.pool.ntp.org iburst
```

Example 4 (unknown):
```unknown
leapfile /usr/share/zoneinfo/leap-seconds.list
```

---

## Kernel module

**URL:** https://wiki.archlinux.org/title/Module_parameter

**Contents:**
- Obtaining information
- Automatic module loading
  - Early module loading
  - systemd
- Manual module handling
- Setting module options
  - Using modprobe
  - Using modprobe.d
  - Using kernel command line
- Aliasing

Kernel modules are pieces of code that can be loaded and unloaded into the kernel upon demand. They extend the functionality of the kernel without the need to reboot the system.

To create a kernel module, you can read The Linux Kernel Module Programming Guide. A module can be configured as built-in or loadable. To dynamically load or remove a module, it has to be configured as a loadable module in the kernel configuration (the line related to the module will therefore display the letter M).

To rebuild a kernel module automatically when a new kernel is installed, see Dynamic Kernel Module Support (DKMS).

Usually modules depend on the kernel release and are stored in the /usr/lib/modules/kernel_release/ directory.

To show what kernel modules are currently loaded:

To show information about a module:

To list the options that are set for a loaded module use systool(1) from sysfsutils:

To display the comprehensive configuration of all the modules:

To display the configuration of a particular module:

List the dependencies of a module (or alias), including the module itself:

Today, all necessary modules loading is handled automatically by udev, so if you do not need to use any out-of-tree kernel modules, there is no need to put modules that should be loaded at boot in any configuration file. However, there are cases where you might want to load an extra module during the boot process, or blacklist another one for your computer to function properly.

Early module loading depends on the initramfs generator used:

Kernel modules can be explicitly listed in files under /etc/modules-load.d/ for systemd to load them during boot. Each configuration file is named in the style of /etc/modules-load.d/program.conf. Configuration files simply contain a list of kernel modules names to load, separated by newlines. Empty lines and lines whose first non-whitespace character is # or ; are ignored.

See modules-load.d(5) for more details.

Kernel modules are handled by tools provided by the kmod package, which is installed as a dependency of a kernel package. You can use these tools manually. To load a module:

To load a module by a file name—i.e. one that is not installed in the /usr/lib/modules/kernel_release/ directory—use any of:

To unload—remove—a module, use any of:

To pass a parameter to a kernel module, you can pass them manually with modprobe or assure certain parameters are always applied using a modprobe configuration file or by using the kernel command line. If the module is built into the kernel, the kernel command line must be used and other methods will not work.

The basic way to pass parameters to a module is using the modprobe command. Parameters are specified on command line using simple key=value assignments:

Configuration files in the /etc/modprobe.d/ directory can be used to pass module settings to udev, which will use modprobe to manage the loading of the modules during system boot. Files in this directory can have any name, given that they end with the .conf extension. The file name matters, see modprobe.d(5) § CONFIGURATION DIRECTORIES AND PRECEDENCE. To show the effective configuration:

Multiple module parameters are separated by spaces, in turn a parameter can receive a list of values which is separated by commas:

You can also pass options to the module using the kernel command line. This is the only working option for modules built into the kernel. For all common boot loaders, the following syntax is correct:

Simply add this to the appropriate line in your boot loader configuration, as described in Kernel parameters#Boot loader configuration.

Aliases are alternate names for a module. For example: alias my-mod really_long_modulename means you can use modprobe my-mod instead of modprobe really_long_modulename. You can also use shell-style wildcards, so alias my-mod* really_long_modulename means that modprobe my-mod-something has the same effect. Create an alias:

Aliases can be internal—contained in the module itself. Internal aliases are usually used for #Automatic module loading when it is needed by an application, e.g. when the kernel detects a new device. To see the module internal aliases:

To see both configured and internal aliases:

Blacklisting, in the context of kernel modules, is a mechanism to prevent the kernel module from loading. This could be useful if, for example, the associated hardware is not needed, or if loading that module causes problems: for instance there may be two kernel modules that try to control the same piece of hardware, and loading them together would result in a conflict.

Some modules are loaded as part of the initramfs. mkinitcpio -M will print out all automatically detected modules: to prevent the initramfs from loading some of those modules, blacklist them in a .conf file under /etc/modprobe.d and it shall be added in by the modconf hook during image generation. Running mkinitcpio -v will list all modules pulled in by the various hooks (e.g. filesystems hook, block hook, etc.). Remember to add that .conf file to the FILES array in /etc/mkinitcpio.conf if you do not have the modconf hook in your HOOKS array (e.g. you have deviated from the default configuration), and once you have blacklisted the modules regenerate the initramfs, and reboot afterwards.

Disable an alias by overriding. For example, to prevent Bluetooth module autoloading (assuming a module named off does not exist):

To disable all internal aliases for a given module use the blacklist keyword. For example, to prevent the pcspkr module from loading on boot to avoid sounds through the PC speaker:

There is a workaround for the behaviour described in the #alias and #blacklist notes. The install configuration command instructs modprobe to run a custom command instead of inserting the module in the kernel as normal, so you can simulate the successful module loading with:

You can force the module to always fail loading with /bin/false: this will effectively prevent the module—and any other that depends on it—from loading by any means, and a log error message may be produced.

You can also blacklist modules from the boot loader boot entry configuration.

Simply add module_blacklist=module_name_1,module_name_2,module_name_3 to your kernel command line, as described in Kernel parameters#Boot loader configuration.

Another use case for a command line option is to disable hardware-specific components of a module without disabling the module entirely. For example, disabling a microphone while retaining other sound out options. See BBS#303475 for a few examples.

In case a specific module does not load and the boot log (accessible by running journalctl -b as root) says that the module is blacklisted, but the directory /etc/modprobe.d/ does not show a corresponding entry, check another modprobe source directory at /usr/lib/modprobe.d/ for blacklisting entries.

A module will not be loaded if the "vermagic" string contained within the kernel module does not match the value of the currently running kernel. If it is known that the module is compatible with the current running kernel the "vermagic" check can be ignored with modprobe --force-vermagic.

**Examples:**

Example 1 (unknown):
```unknown
/usr/lib/modules/kernel_release/
```

Example 2 (unknown):
```unknown
uname --kernel-release
```

Example 3 (unknown):
```unknown
/etc/modprobe.d/
```

Example 4 (unknown):
```unknown
$ modinfo module_name
```

---

## systemd/Journal

**URL:** https://wiki.archlinux.org/title/Journalctl

**Contents:**
- Priority level
- Facility
- Filtering output
- Tips and tricks
  - Journal size limit
    - Per unit size limit by a journal namespace
  - Clean journal files manually
  - Journald in conjunction with syslog
  - Forward journald to /dev/tty12
  - Specify a different journal to view

systemd has its own logging system called the journal; running a separate logging daemon is not required. To read the log, use journalctl(1).

In Arch Linux, the directory /var/log/journal/ is a part of the systemd package, and the journal (when Storage= is set to auto in /etc/systemd/journald.conf) will write to /var/log/journal/. If that directory is deleted, systemd will not recreate it automatically and instead will write its logs to /run/log/journal/ in a non-persistent way. However, the directory will be recreated if Storage=persistent is added to journald.conf and systemd-journald.service is restarted (or the system is rebooted).

Systemd journal classifies messages by Priority level and Facility. Logging classification corresponds to classic Syslog protocol (RFC 5424).

A syslog severity code (in systemd called priority) is used to mark the importance of a message RFC 5424 6.2.1.

These rules are recommendations, and the priority level of a given error is at the application developer's discretion. It is always possible that the error will be at a higher or lower level than expected.

A syslog facility code is used to specify the type of program that is logging the message RFC 5424 6.2.1.

Useful facilities to watch: 0, 1, 3, 4, 9, 10, 15.

journalctl allows for the filtering of output by specific fields. If there are many messages to display, or if the filtering of large time spans has to be done, the output of this command can be extensively delayed.

See journalctl(1), systemd.journal-fields(7), or Lennart Poettering's blog post for details.

If the journal is persistent (non-volatile), its size limit is set to a default value of 10% of the size of the underlying file system but capped at 4 GiB. For example, with /var/log/journal/ located on a 20 GiB partition, journal data may take up to 2 GiB. On a 50 GiB partition, it would max at 4 GiB. To confirm current limits on your system review systemd-journald unit logs:

The maximum size of the persistent journal can be controlled by uncommenting and changing the following:

It is also possible to use the drop-in snippets configuration override mechanism rather than editing the global configuration file. In this case, place the overrides under the [Journal] header:

Restart the systemd-journald.service after changing this setting to apply the new limit.

See journald.conf(5) for more info.

Edit the unit file for the service you wish to configure (for example sshd) and add LogNamespace=ssh in the [Service] section.

Then create /etc/systemd/journald@ssh.conf by copying /etc/systemd/journald.conf. After that, edit journald@ssh.conf and adjust SystemMaxUse to your liking.

Restarting the service should automatically start the new journal service systemd-journald@ssh.service. The logs from the namespaced service can be viewed with journalctl --namespace ssh.

See systemd-journald.service(8) § JOURNAL NAMESPACES for details about journal namespaces.

Journal files can be globally removed from /var/log/journal/ using e.g. rm, or can be trimmed according to various criteria using journalctl. For example:

Journal files must have been rotated out and made inactive before they can be trimmed by vacuum commands. Rotation of journal files can be done by running journalctl --rotate. The --rotate argument can also be provided alongside one or more vacuum criteria arguments to perform rotation and then trim files in a single command.

See journalctl(1) for more info.

Compatibility with a classic, non-journald aware syslog implementation can be provided by letting systemd forward all messages via the socket /run/systemd/journal/syslog. To make the syslog daemon work with the journal, it has to bind to this socket instead of /dev/log (official announcement).

The default journald.conf for forwarding to the socket is ForwardToSyslog=no to avoid system overhead, because rsyslog or syslog-ng pull the messages from the journal by itself.

See Syslog-ng#Overview and Syslog-ng#syslog-ng and systemd journal, or rsyslog respectively, for details on configuration.

Create a drop-in directory /etc/systemd/journald.conf.d and create a fw-tty12.conf file in it:

Then restart systemd-journald.service.

There may be a need to check the logs of another system that is dead in the water, like booting from a live system to recover a production system. In such case, one can mount the disk in e.g. /mnt, and specify the journal path via -D/--directory, like so:

By default, a regular user only has access to their own per-user journal. To grant read access for the system journal as a regular user, you can add that user to the systemd-journal user group. Members of the adm and wheel groups are also given read access.

See journalctl(1) § DESCRIPTION and Users and groups#User groups for more information.

Desktop notifications can help you quickly notice error messages, improving awareness compared to manually checking logs or not noticing them at all.

The journalctl-desktop-notificationAUR package provides an automatic desktop notification for each error message logged by any process. For more details and configuration options, visit the GitLab project page.

**Examples:**

Example 1 (unknown):
```unknown
/var/log/journal/
```

Example 2 (unknown):
```unknown
/etc/systemd/journald.conf
```

Example 3 (unknown):
```unknown
/var/log/journal/
```

Example 4 (unknown):
```unknown
/run/log/journal/
```

---

## Power management

**URL:** https://wiki.archlinux.org/title/Allow_users_to_shutdown

**Contents:**
- Userspace tools
  - Console
  - Graphical
- ACPI events
  - Power managers
  - xss-lock
- Power saving
  - Print power settings
  - Processors with Intel Hardware P-state support
  - Audio

Power management is a feature that turns off the power or switches system components to a low-power state when inactive.

In Arch Linux, power management consists of two main parts:

These tools allow you to change a lot of settings without the need to edit config files by hand. Only run one of these tools to avoid possible conflicts as they all work more or less similarly. Have a look at the power management category to get an overview on what power management options exist in Arch Linux.

These are the more popular scripts and tools designed to help power saving:

systemd handles some power-related ACPI events, whose actions can be configured in /etc/systemd/logind.conf or /etc/systemd/logind.conf.d/*.conf — see logind.conf(5). On systems with no dedicated power manager, this may replace the acpid daemon which is usually used to react to these ACPI events.

The specified action for each event can be one of ignore, poweroff, reboot, halt, suspend, hibernate, hybrid-sleep, suspend-then-hibernate, lock or kexec. In case of hibernation and suspension, they must be properly set up. If an event is not configured, systemd will use a default action.

To apply changes, reload systemd-logind.service.

Some desktop environments include power managers which inhibit (temporarily turn off) some or all of the systemd ACPI settings. If such a power manager is running, then the actions for ACPI events can be configured in the power manager alone. Changes to /etc/systemd/logind.conf or /etc/systemd/logind.conf.d/*.conf need be made only if you wish to configure behaviour for a particular event that is not inhibited by the power manager.

Note that if the power manager does not inhibit systemd for the appropriate events you can end up with a situation where systemd suspends your system and then when the system is woken up the other power manager suspends it again. The power managers of GNOME, MATE, Plasma and Xfce issue the necessary inhibited commands. If the inhibited commands are not being issued, such as when using acpid or others to handle ACPI events, set the Handle options to ignore. See also systemd-inhibit(1).

xss-lock subscribes to the systemd-events suspend, hibernate, lock-session, and unlock-session with appropriate actions (run locker and wait for user to unlock or kill locker). xss-lock also reacts to DPMS events and runs or kills the locker in response.

Autostarting the following for example:

This section is a reference for creating custom scripts and power saving settings such as by udev rules. Make sure that the settings are not managed by some other utility to avoid conflicts.

Almost all of the features listed here are worth using whether or not the computer is on AC or battery power. Most have negligible performance impact and are just not enabled by default because of commonly broken hardware/drivers. Reducing power usage means reducing heat, which can even lead to higher performance on a modern Intel or AMD CPU, thanks to dynamic overclocking.

This script prints power settings and a variety of other properties for USB and PCI devices. Note that root permissions are needed to see all settings.

This article or section is a candidate for merging with CPU frequency scaling.

The available energy preferences of an Intel Hardware P-state (HWP) supported processor are default, performance, balance_performance, balance_power, power.

This can be validated by running

To conserve more energy, you can edit the configuration by creating the following file:

See the x86_energy_perf_policy(8) man page for more details on energy-performance policy in Intel processors. Also see systemd-tmpfiles(8) and tmpfiles.d(5) man pages for temporary files/directories details.

Whether power saving is turned on by default depends on a given driver, e.g. it is on for HD Audio. Identify the module in use, then run

and look for a kernel module parameter (like power_save) that adjusts or disables power-saving feature.

To disable Bluetooth completely, blacklist the btusb and bluetooth modules.

Alternatively, create the following udev rules:

To turn off Bluetooth only temporarily, use rfkill(8):

If you will not use integrated web camera then blacklist the uvcvideo module.

This section uses configurations in /etc/sysctl.d/, which is "a drop-in directory for kernel sysctl parameters." See The New Configuration Files and more specifically sysctl.d(5) for more information.

This article or section needs expansion.

The NMI watchdog is a debugging feature to catch hardware hangs that cause a kernel panic. On some systems it can generate a lot of interrupts, causing a noticeable increase in power usage. To list these interrupts per CPU core since last boot, you can use:

To turn the hardlockup detector off, use:

or add nmi_watchdog=0 to the kernel line.

Alternatively add nowatchdog to the kernel line to disable both hard and soft lockup detectors. See [3]

Increasing the virtual memory dirty writeback time helps to aggregate disk I/O together, thus reducing spanned disk writes, and increasing power saving. To set the value to 60 seconds (default is 5 seconds):

To do the same for journal commits on supported filesystems (e.g. ext4, btrfs...), use commit=60 as an option in fstab.

Note that this value is modified as a side effect of the Laptop Mode setting below. See also sysctl#Virtual memory for other parameters affecting I/O performance and power saving.

See the kernel documentation on the laptop mode "knob" - "A sensible value for the knob is 5 seconds".

Wake-on-LAN can be a useful feature, but if you are not making use of it then it is simply draining extra power waiting for a magic packet while in suspend. You can adapt the Wake-on-LAN#udev rule to disable the feature for all ethernet interfaces. To enable powersaving with iw on all wireless interfaces:

The name of the configuration file is important. With the use of persistent device names in systemd, the above network rule, named lexicographically after 80-net-setup-link.rules, is applied after the device is renamed with a persistent name e.g. wlan0 renamed wlp3s0. Be aware that the RUN command is executed after all rules have been processed and must anyway use the persistent name, available in $name for the matched device.

Additional power saving functions of Intel wireless cards with iwlwifi driver can be enabled by passing the correct parameters to the kernel module. Making them persistent can be achieved by adding the lines below to the /etc/modprobe.d/iwlwifi.conf file:

This option will probably increase your median latency:

On kernels < 5.4 you can use this option, but it will probably decrease your maximum throughput:

Depending on your wireless card one of these two options will apply.

You can check which one is relevant by checking which of these modules is running using

Keep in mind that these power saving options are experimental and can cause an unstable system.

If using iwd, power-saving can be disabled for all Wi-Fi devices with the following config file:

You can also replace * with a specific driver name, see iwd.config(5) § SETTINGS.

If using NetworkManager, power-saving can be disabled globally for every connection with a config file, for example:

At boot, the BIOS enables or disables ASPM based on hardware support. To check for support:

Fetch available ASPM policies and the current system default using the following:

ASPM might be disabled for the following reasons [4]:

If you believe that your hardware has support for ASPM despite the above, it can be force-enabled for the kernel to handle with the pcie_aspm=force kernel parameter.

As long as ASPM is supported and enabled, it is possible to select a desired policy for the current session. For example, switch to powersupersave for the current session by doing the following:

To configure a specific ASPM state to enable upon system boot (using powersupersave as an example), add pcie_aspm.policy=powersupersave as a kernel parameter.

The rule above powers down unused devices.

Some devices will not wake up again. To allow runtime power management only for devices that are known to work, use simple matching against vendor and device IDs (use lspci -nn to get these values):

Alternatively, to blacklist devices that are not working with PCI runtime power management and enable it for all other devices:

The Linux kernel can automatically suspend USB devices when they are not in use. This can sometimes save quite a bit of power, however some USB devices are not compatible with USB power saving and start to misbehave (common for USB mice/keyboards). udev rules based on whitelist or blacklist filtering can help to mitigate the problem.

The example is enabling autosuspend for all USB devices except for keyboards and mice:

To allow autosuspend only for devices that are known to work, use simple matching against vendor and product IDs (use lsusb to get these values):

Alternatively, to blacklist devices that are not working with USB autosuspend and enable it for all other devices:

The default autosuspend idle delay time is controlled by the autosuspend parameter of the usbcore built-in kernel module. To set the delay to 5 seconds instead of the default 2 seconds, add the following kernel parameter for your boot loader.

Similarly to power/control, the delay time can be fine-tuned per device by setting the power/autosuspend attribute. This means, alternatively, autosuspend can be disabled by setting power/autosuspend to -1 (i.e., never autosuspend):

See the Linux kernel documentation for more information on USB power management.

The current setting can be read from or written to /sys/class/scsi_host/host*/link_power_management_policy as follows:

You can configure link_power_management_policy settings persistently by adding a udev rules file, for example:

See hdparm#Power management configuration for drive parameters that can be set.

Power saving is not effective when too many programs are frequently writing to the disk. Tracking all programs, and how and when they write to disk is the way to limit disk usage. Use iotop to see which programs use the disk frequently. See Improving performance#Storage devices for other tips.

Small adjustments such as setting the noatime option can also help. If enough RAM is available, consider disabling or limiting swappiness as it has the possibility to limit a good number of disk writes.

For Seagate drives with PowerChoice technology, tricks setting APM via hdparm will not work due to the EPC (Extended Power Conditions) feature. Rather than setting APM, you can install openseachestAUR and fully disable EPC like so (replace X with actual drive letter):

Last invocation will give the following summary:

Zeroes in the first column confirm that parking and spindown were disabled successfully

This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.

This article or section is a candidate for merging with Laptop#Power management.

Since systemd users can suspend and hibernate through systemctl suspend or systemctl hibernate and handle acpi events with /etc/systemd/logind.conf, it might be interesting to remove pm-utils and acpid. There is just one thing systemd cannot do (as of systemd-204): power management depending on whether the system is running on AC or battery. To fill this gap, you can create a single udev rule that runs a script when the AC adapter is plugged and unplugged:

Examples of powersave scripts:

The above udev rule should work as expected, but if your power settings are not updated after a suspend or hibernate cycle, you should add a script in /usr/lib/systemd/system-sleep/ with the following contents:

Do not forget to make it executable!

This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.

The suspend, poweroff and hibernate button presses and lid close events are handled by logind as described in #ACPI events.

If you are using polkit, users with non-remote session can issue power-related commands as long as the session is not broken.

To check if your session is active:

The user can then use systemctl commands in the command line, or add them to menus:

Other commands can be used as well, including systemctl suspend and systemctl hibernate. See the System Commands section in systemctl(1).

Install sudo, and configure it to give the user root privileges. The user will then be able to use the sudo systemctl commands (e.g. sudo systemctl poweroff, sudo systemctl reboot, sudo systemctl suspend and sudo systemctl hibernate). See the System Commands section in systemctl(1)

If users should only be allowed to use shutdown commands, but not have other privileges, add the following to the end of /etc/sudoers using the visudo command as root. Substitute user for your username and hostname for the machine's hostname.

Now your user can shutdown with sudo systemctl poweroff, and reboot with sudo systemctl reboot. Users wishing to power down a system can also use sudo systemctl halt. Use the NOPASSWD: tag only if you do not want to be prompted for your password.

**Examples:**

Example 1 (unknown):
```unknown
/etc/systemd/logind.conf
```

Example 2 (unknown):
```unknown
/etc/systemd/logind.conf.d/*.conf
```

Example 3 (unknown):
```unknown
hybrid-sleep
```

Example 4 (unknown):
```unknown
suspend-then-hibernate
```

---

## GRUB Legacy

**URL:** https://wiki.archlinux.org/title/GRUB_Legacy

**Contents:**
- Supported file systems
- Installation
- Configuration
  - Finding GRUB's root
  - Dual booting with Windows
  - Dual booting with GNU/Linux
  - chainloader and configfile
  - Dual booting with GNU/Linux (GRUB2)
- Boot loader installation
  - Manual recovery of GRUB libs

GRUB Legacy is a multiboot boot loader previously maintained by the GNU Project. It was derived from GRUB, the GRand Unified Bootloader, which was originally designed and implemented by Erich Stefan Boleyn.

Briefly, the boot loader is the first software program that runs when a computer starts. It is responsible for loading and transferring control to the Linux kernel. The kernel, in turn, initializes the rest of the operating system.

GRUB legacy bundles its own support for multiple file systems, notably FAT32, ext2, ReiserFS or XFS (although v4 only).

GRUB Legacy can be installed from the grub-legacyAUR package.

Additionally, GRUB must be installed to the boot sector of a drive or partition to serve as a boot loader. This is covered in #Boot loader installation.

The configuration file is located at /boot/grub/menu.lst. Edit this file to suit your needs.

An example configuration (with /boot on a separate partition) is provided with the package.

GRUB must be told where its files reside on the system, since multiple instances may exist (i.e., in multi-boot environments). GRUB files always reside under /boot, which may be on a dedicated partition.

If you are unaware of the location of /boot, use the GRUB shell find command to locate the GRUB files. Enter the GRUB shell as root by:

The following example is for systems without a separate /boot partition, wherein /boot is merely a directory under /:

The following example is for systems with a separate /boot partition:

GRUB will find the file, and output the location of the stage1 file. For example:

This value should be entered on the root line in your configuration file. Type quit to exit the shell.

This article or section is out of date.

Add the following to the end of your /boot/grub/menu.lst (assuming that your Windows partition is on the first partition of the first drive):

If Windows is located on another hard disk, the map command must be used. This will make your Windows install think it is actually on the first drive. Assuming that your Windows partition is on the first partition of the second drive:

This can be done the same way that an Arch Linux install is defined. For example:

To facilitate system maintenance, the chainloader or configfile command should be used to boot another Linux distribution that provides an "automagic" GRUB configuration mechanism (e.g. Debian, Ubuntu, openSUSE). This allows the distribution to manage its own menu.lst and boot options.

For example, GRUB is to be installed to the MBR and some other boot loader (be it GRUB or LILO) is already installed to the boot sector of (hd0,2).

One can simply include in menu.lst:

Or, if the boot loader on (hd0,2) is GRUB:

The chainloader command can also be used to load the MBR of a second drive:

If the other Linux distribution uses GRUB2 (e.g. Ubuntu 9.10+), and you installed its boot loader to its root partition, you can add an entry like this one to your /boot/grub/menu.lst:

Selecting this entry at boot will load the other distribution's GRUB2 menu assuming that the distribution is installed on /dev/sda3.

The *stage* files are expected to be in /boot/grub, which may not be the case if the boot loader was not installed during system installation or if the partition/filesystem was damaged, accidentally deleted, etc.

Manually copy the GRUB libs like so:

GRUB may be installed from a separate medium (e.g. a LiveCD), or directly from a running Arch install. GRUB is seldom required to be reinstalled and installation is not necessary when:

Installation is necessary when:

Before continuing, a few notes:

First, enter the GRUB shell:

Use the root command with the output from the find command (see #Finding GRUB's root) to instruct GRUB which partition contains stage1 (and therefore, /boot):

The following example installs GRUB to the MBR of the first drive:

The following example installs GRUB to the first partition of the first drive:

After running setup, enter quit to exit the shell. If you chrooted, exit your chroot and unmount partitions. Now reboot to test.

Use the grub-install command followed by the location to install the boot loader. For example to install GRUB to the MBR of the first drive:

GRUB will indicate whether it successfully installs. If it does not, you will have to use the GRUB shell method.

One can use the resolution given in the menu.lst, but you might want to use your LCD wide-screen at its full native resolution. Here is what you can do to achieve this:

On Wikipedia, there is a list of extended framebuffer resolutions (i.e. beyond the ones in the VBE standard).

If the desired resolution does not work with the codes obtained from the table, it usually is because the graphics card manufacturers are free to choose any number they wish, as this is not part of the VBE 3 standard. These codes may change from one card to the other (possibly even for the same manufacturer).

Instead of using that table, use one of the tools mentioned below to get the correct code:

This is an easy way to find the resolution code using only GRUB itself.

On the kernel line, specify that the kernel should ask you which mode to use.

Now reboot. GRUB will now present a list of suitable codes to use and the option to scan for even more.

You can pick the code you would like to use (do not forget it, it is needed for the next step) and boot using it.

Now replace ask in the kernel line with the correct one you have picked.

e.g. the kernel line for [369] 1680x1050x32 would be:

Example output of hwinfo:

If you alter (or plan to alter) partition sizes from time to time, you might want to consider defining your drive/partitions by a label. You can label ext2, ext3, ext4 partitions by:

The label name can be up to 16 characters long but cannot have spaces for GRUB to understand it. Then define it in your menu.lst:

The UUID (Universally Unique IDentifier) of a partition may be discovered with blkid or ls -l /dev/disk/by-uuid. It is defined in menu.lst with either:

At the boot loader, select an entry and edit it (e key). Append the following parameters to the kernel options:

This will start in single-user mode (init 1), i.e. you will end up to a root prompt without being asked for password. This may be useful for recovery features, like resetting the root password. However, this is a huge security flaw if you have not set any #Password protection for grub.

You can enable password protection in the GRUB configuration file for operating systems you wish to have protected. Boot loader password protection may be desired if your BIOS lacks such functionality and you need the extra security.

First, choose a password you can remember and then encrypt it:

Then add your password to the beginning of the GRUB configuration file at /boot/grub/menu.lst (the password must be at the beginning of the configuration file for GRUB to be able to recognize it):

Then for each operating system you wish to protect, add the lock command:

It is always possible to reset your BIOS settings by setting the appropriate jumper on the motherboard (see your motherboard's manual, as it is specific to every model). So in case other have access to the hardware, there is basically no way to prevent boot breakthroughs.

If you realize that you often need to switch to some other non-default OS (e.g. Windows) having to reboot and wait for the GRUB menu to appear is tedious. GRUB offers a way to record your OS choice when restarting instead of waiting for the menu, by designating a temporary new default which will be reset as soon as it has been used.

Supposing a simple menu.lst setup like this:

Arch is the default (0). We want to restart in to Windows. Change default 0 to default saved -- this will record the current default in a default file in the GRUB directory whenever the savedefault command is used. Now add the line savedefault 0 to the bottom of the Windows entry. Whenever Windows is booted, it will reset the default to Arch, thus making changing the default to Windows temporary.

Now all that is needed is a way to easily change the default manually. This can be accomplished using the command grub-set-default. So, to reboot into Windows, enter the following commands:

For ease of use, you might to wish to setup sudo and add /sbin/grub-set-default amongst the commands the user is allowed to issue without supplying a password.

If the LILO package is installed on your system, uninstall it. As some tasks (e.g. kernel compilation using make all) will make a LILO call, and LILO will then be installed over GRUB. LILO may have been included in your base system, depending on your installer media version and whether you selected/deselected it during the package selection stage.

First, format a floppy disk:

Install GRUB to the disk:

Copy your menu.lst file to the disk:

Now unmount your floppy:

Now you should be able to restart your computer with the disk in the drive and it should boot to GRUB. Make sure that your floppy disk is set to have higher priority than your hard drive when booting in your BIOS first, of course.

See also: Super GRUB Disk.

The hiddenmenu option can be used in order to hide the menu by default. That way no menu is displayed and the default option is going to be automatically selected after the timeout passes. Still, you are able to press Esc and the menu shows up. To use it, just add to your /boot/grub/menu.lst:

The first check to do is to unplug any external drive. Seems obvious, but sometimes we get tired ;)

If your partition table gets messed up, an unpleasant "GRUB error 17" message might be the only thing that greets you on your next reboot. There are a number of reasons why the partition table could get messed up. Commonly, users who manipulate their partitions with GParted -- particularly logical drives -- can cause the order of the partitions to change. For example, you delete /dev/sda6 and resize /dev/sda7, then finally re-create what used to be /dev/sda6 only now it appears at the bottom of the list, /dev/sda9 for example. Although the physical order of the partitions/logical drives has not changed, the order in which they are recognized has changed.

Fixing the partition table is easy. Boot from your Arch CD/DVD/USB, login as root and fix the partition table:

Once in disk, enter e[x]tra/expert mode, [f]ix the partition order, then [w]rite the table and exit.

You can verify that the partition table was indeed fixed by issuing an fdisk -l. Now you just need to fix GRUB. See #Boot loader installation.

Basically you need to tell GRUB the correct location of your /boot then re-write GRUB to the MBR on the disk.

See [1] for a more in-depth summary of this section.

If you see this error message while trying to set up GRUB, and you are not using a fresh partition table, it is worth checking it.

This will show you the partition table for /dev/sda. So check here, whether the "Id" values of your partitions are correct. The "System" column will show you the description of the "Id" values.

If your boot partition is marked as being "HPFS/NTFS", for example, then you have to change it to "Linux". To do this, go to fdisk,

change a partition's system id with t, select your partition number and type in the new system id (Linux = 83). You can also list all available system ids by typing L instead of a system id.

If you have changed a partitions system id, you should [v]erify your partition table and then [w]rite it.

Now try to set up GRUB again.

See also the forum post reporting this problem.

If you accidentally install GRUB to a Windows partition, GRUB will write some information to the boot sector of the partition, erasing the reference to the Windows boot loader. (This is true for NTLDR the boot loader for Windows XP and earlier, unsure about later versions).

To fix this you will need to use the Windows Recovery Console for your Windows release. Because many computer manufacturers do not include this with their product (many choose to use a recovery partition) Microsoft has made them available for download. If you use XP, look at this page to be able to turn the floppy disks to a Recovery CD. Boot the Recovery CD (or enable Windows Recovery mode) and run fixboot to repair the partition boot sector. After this, you will have to install GRUB again---this time, to the MBR, not to the Windows partition---to boot Linux.

Once you have selected an entry in the boot menu, you can edit it by pressing key e. Use tab-completion if you need to discover devices then Esc to exit. Then you can try to boot by pressing b.

If an error is raised mentioning /boot/grub/device.map during installation or boot, run:

to force GRUB to recheck the device map, even if it already exists. This may be necessary after resizing partitions or adding/removing drives.

If you have opened a sub-menu with the list of all operating systems configured in GRUB, selected one, and upon restart, you still booted your default OS, then you might want to check if you have the line:

in /boot/grub/menu.lst.

I had trouble installing GRUB while installing Arch Linux in an virtual KVM machine using a virtio device for hard drive. To install GRUB, I figured out the following: Enter a virtual console by typing Ctrl+Alt+F2 or any other F-key for a free virtual console. This assumes that your root file system is mounted in the folder /mnt and the boot file system is either mounted or stored in the folder /mnt/boot.

1. Assure that all needed GRUB files is present in your boot directory (assuming it is mounted in /mnt/boot folder), by issuing the command:

2. If the /mnt/boot/grub folder already contains all the needed files, jump to step 3. Otherwise, do the following commands (replacing /mnt, your_kernel and your_initramfs with the real paths and file names). You should also have the menu.lst file written to this folder:

3. Start the GRUB shell with the following command:

4. Enter the following commands. Replace /dev/vda, and (hd0,0) with the correct device and partition corresponding to your setup.

5. If GRUB reports no error messages, then you probably are done. You also need to add appropriate modules to the ramdisk. For more information, please refer to QEMU#Preparing an Arch Linux guest.

**Examples:**

Example 1 (unknown):
```unknown
/boot/grub/menu.lst
```

Example 2 (unknown):
```unknown
grub> find /boot/grub/stage1
```

Example 3 (unknown):
```unknown
grub> find /grub/stage1
```

Example 4 (unknown):
```unknown
grub> find /grub/stage1
```

---

## Xorg

**URL:** https://wiki.archlinux.org/title/X_Window_System

**Contents:**
- Installation
  - Driver installation
    - AMD
- Running
- Configuration
  - Using .conf files
  - Using xorg.conf
- Input devices
  - Input identification
  - Mouse acceleration

X.Org Server — commonly referred to as simply X — is the X.Org Foundation implementation of the X Window System (X11) display server, and it is the most popular display server among Linux users. Its ubiquity has led to making it an ever-present requisite for GUI applications, resulting in massive adoption from most distributions.

For the alternative and successor, see Wayland.

Xorg can be installed with the xorg-server package.

Additionally, some packages from the xorg-apps group are necessary for certain configuration tasks. They are pointed out in the relevant sections.

Finally, an xorg group is also available, which includes Xorg server packages, packages from the xorg-apps group and fonts.

This article or section is a candidate for moving to Graphics processing unit.

The Linux kernel includes open-source video drivers and support for hardware accelerated framebuffers. However, userland support is required for OpenGL, Vulkan and 2D acceleration in X11.

First, identify the graphics card (the Subsystem output shows the specific model):

Then, install an appropriate driver. You can search the package database for a complete list of open-source Device Dependent X (DDX) drivers:

However, hardware-specific DDX is considered legacy nowadays. There is a generic modesetting(4) DDX driver in xorg-server, which uses kernel mode setting and works well on modern hardware. The modesetting DDX driver uses Glamor[1] for 2D acceleration, which requires OpenGL.

If you want to install another DDX driver, note that Xorg searches for installed DDX drivers automatically:

In order for video acceleration to work, and often to expose all the modes that the GPU can set, a proper video driver is required:

Other DDX drivers can be found in the xorg-drivers group.

Xorg should run smoothly without closed source drivers, which are typically needed only for advanced features such as fast 3D-accelerated rendering for games. The exceptions to this rule are recent GPUs (especially NVIDIA GPUs) not supported by open source drivers.

For a translation of model names (e.g. Radeon RX 6800) to GPU architectures (e.g. RDNA 2), see Wikipedia:List of AMD graphics processing units#Features overview.

The Xorg(1) command is usually not run directly. Instead, the X server is started with either a display manager or xinit.

Xorg uses a configuration file called xorg.conf and files ending in the suffix .conf for its initial setup: the complete list of the folders where these files are searched can be found in xorg.conf(5), together with a detailed explanation of all the available options.

The /etc/X11/xorg.conf.d/ directory stores host-specific configuration. You are free to add configuration files there, but they must have a .conf suffix: the files are read in ASCII order, and by convention their names start with XX- (two digits and a hyphen, so that for example 10 is read before 20). These files are parsed by the X server upon startup and are treated like part of the traditional xorg.conf configuration file. Note that on conflicting configuration, the file read last will be processed. For this reason, the most generic configuration files should be ordered first by name. The configuration entries in the xorg.conf file are processed at the end.

For option examples to set, see Fedora:Input device configuration#xorg.conf.d.

Xorg can also be configured via /etc/X11/xorg.conf or /etc/xorg.conf. You can also generate a skeleton for xorg.conf with:

This should create a xorg.conf.new file in /root/ that you can copy over to /etc/X11/xorg.conf.

Alternatively, your proprietary video card drivers may come with a tool to automatically configure Xorg: see the article of your video driver, NVIDIA or AMDGPU PRO, for more details.

For input devices the X server defaults to the libinput driver (xf86-input-libinput), but xf86-input-evdev and related drivers are available as alternative.[2]

Udev, which is provided as a systemd dependency, will detect hardware and both drivers will act as hotplugging input driver for almost all devices, as defined in the default configuration files 10-quirks.conf and 40-libinput.conf in the /usr/share/X11/xorg.conf.d/ directory.

After starting X server, the log file will show which driver hotplugged for the individual devices (note the most recent log file name may vary):

If both do not support a particular device, install the needed driver from the xorg-drivers group. The same applies, if you want to use another driver.

To influence hotplugging, see #Configuration.

For specific instructions, see also the libinput article, the following pages below, or Fedora:Input device configuration for more examples.

See Keyboard input#Identifying keycodes in Xorg.

See Mouse acceleration.

See libinput or Synaptics.

See Keyboard configuration in Xorg.

For a headless configuration, the xf86-video-dummy driver is necessary; install it and create a configuration file, such as the following:

See main article Multihead for general information.

You must define the correct driver to use and put the bus ID of your graphic cards (in decimal notation).

To get your bus IDs (in hexadecimal):

The bus IDs here are 0:2:0 and 1:0:0.

By default, Xorg always sets DPI to 96 since 2009-01-30. A change was made with version 21.1 to provide proper DPI auto-detection, but reverted.

The DPI of the X server can be set with the -dpi command line option.

Having the correct DPI is helpful where fine detail is required (like font rendering). Previously, manufacturers tried to create a standard for 96 DPI (a 10.3" diagonal monitor would be 800x600, a 13.2" monitor 1024x768). These days, screen DPIs vary and may not be equal horizontally and vertically. For example, a 19" widescreen LCD at 1440x900 may have a DPI of 89x87.

To see if your display size and DPI are correct:

Check that the dimensions match your display size.

If you have specifications on the physical size of the screen, they can be entered in the Xorg configuration file so that the proper DPI is calculated (adjust identifier to your xrandr output):

If you only want to enter the specification of your monitor without creating a full xorg.conf, create a new configuration file. For example (/etc/X11/xorg.conf.d/90-monitor.conf):

If you do not have specifications for physical screen width and height (most specifications these days only list by diagonal size), you can use the monitor's native resolution (or aspect ratio) and diagonal length to calculate the horizontal and vertical physical dimensions. Using the Pythagorean theorem on a 13.3" diagonal length screen with a 1280x800 native resolution (or 16:10 aspect ratio):

This will give the pixel diagonal length, and with this value you can discover the physical horizontal and vertical lengths (and convert them to millimeters):

For RandR compliant drivers (for example the open source ATI driver), you can set it by:

To make it permanent, see Autostarting#On Xorg startup.

You can manually set the DPI by adding the option under the Device or Screen section:

GTK very often overrides the server's DPI via the optional X resource Xft.dpi. To find out whether this is happening to you, check with:

With GTK library versions since 3.16, when this variable is not otherwise explicitly set, GTK sets it to 96. To have GTK apps obey the server DPI you may need to explicitly set Xft.dpi to the same value as the server. The Xft.dpi resource is the method by which some desktop environments optionally force DPI to a particular value in personal settings. Among these are KDE and TDE.

DPMS is a technology that allows power saving behaviour of monitors when the computer is not in use. This will allow you to have your monitors automatically go into standby after a predefined period of time.

The Composite extension for X causes an entire sub-tree of the window hierarchy to be rendered to an off-screen buffer. Applications can then take the contents of that buffer and do whatever they like. The off-screen buffer can be automatically merged into the parent window, or merged by external programs called compositing managers. For more information, see Wikipedia:Compositing window manager.

Some window managers (e.g. Compiz, Enlightenment, KWin, marco, metacity, muffin, mutter, Xfwm) do compositing on their own. For other window managers, a standalone composite manager can be used.

This section lists utilities for automating keyboard / mouse input and window operations (like moving, resizing or raising).

See also Clipboard#Tools and an overview of X automation tools.

This article or section is out of date.

To run a nested session of another desktop environment:

This will launch a Window Maker session in a 1024 by 768 window within your current X session.

This needs the package xorg-server-xnest to be installed.

A more modern way of doing a nested X session is with Xephyr.

See xinit#Starting applications without a window manager.

See main article: OpenSSH#X11 forwarding.

With the help of xinput you can temporarily disable or enable input sources. This might be useful, for example, on systems that have more than one mouse, such as the ThinkPads and you would rather use just one to avoid unwanted mouse clicks.

Install the xorg-xinput package.

Find the name or ID of the device you want to disable:

For example in a Lenovo ThinkPad T500, the output looks like this:

Disable the device with xinput --disable device, where device is the device ID or name of the device you want to disable. In this example we will disable the Synaptics Touchpad, with the ID 10:

To re-enable the device, just issue the opposite command:

Alternatively using the device name, the command to disable the touchpad would be:

You can disable a particular input source using a configuration snippet:

device is an arbitrary name, and driver_name is the name of the input driver, e.g. libinput. device_name is what is actually used to match the proper device. For alternate methods of targeting the correct device, such as libinput's MatchIsTouchscreen, consult your input driver's documentation. Though this example uses libinput, this is a driver-agnostic method which simply prevents the device from being propagated to the driver.

Run script on hotkey:

Dependencies: xorg-xprop, xdotool

See also #Killing an application visually.

To block tty access when in an X add the following to xorg.conf:

This can be used to help restrict command line access on a system accessible to non-trusted users.

To prevent a user from killing X when it is running add the following to xorg.conf:

When an application is misbehaving or stuck, instead of using kill or killall from a terminal and having to find the process ID or name, xorg-xkill allows to click on said application to close its connection to the X server. Many existing applications do indeed abort when their connection to the X server is closed, but some can choose to continue.

Xorg may run with standard user privileges instead of root (so-called "rootless" Xorg). This is a significant security improvement over running as root. Note that some popular display managers do not support rootless Xorg (e.g. LightDM or XDM).

You can verify which user Xorg is running as with ps -o user= -C Xorg.

See also Xorg.wrap(1), systemd-logind(8), Systemd/User#Xorg as a systemd user service, Fedora:Changes/XorgWithoutRootRights and FS#41257.

To configure rootless Xorg using xinitrc:

Note that executing startx directly without exec leaves the shell open in the case of a xorg crash. Since some lock screens are executed inside xorg, this can lead to full access to the executing user.

GDM will run Xorg without root privileges by default when kernel mode setting is used.

When Xorg is run in rootless mode, Xorg logs are saved to ~/.local/share/xorg/Xorg.log. However, the stdout and stderr output from the Xorg session is not redirected to this log. To re-enable redirection, start Xorg with the -keeptty flag and redirect the stdout and stderr output to a file:

Alternatively, copy /etc/X11/xinit/xserverrc to ~/.xserverrc, and append -keeptty. See [5].

As explained above, there are circumstances in which rootless Xorg is defaulted to. If this is the case for your configuration, and you have a need to run Xorg as root, you can configure Xorg.wrap(1) to require root:

Wayback is an X11 compatibility layer which allows for running full X11 desktop environments (and window managers) using Wayland components. It's available from the AUR as wayback-x11AUR package.

If a problem occurs, view the log stored in either /var/log/ or, for the rootless X default since v1.16, in ~/.local/share/xorg/. GDM users should check the systemd journal. [6]

The logfiles are of the form Xorg.n.log with n being the display number. For a single user machine with default configuration the applicable log is frequently Xorg.0.log, but otherwise it may vary. To make sure to pick the right file it may help to look at the timestamp of the X server session start and from which console it was started. For example:

X creates configuration and temporary files in current user's home directory. Make sure there is free disk space available on the partition your home directory resides in. Unfortunately, X server does not provide any more obvious information about lack of disk space in this case.

If you use a Matrox card and DRI stopped working after upgrading to Xorg, try adding the line:

to the Device section that references the video card in xorg.conf.

X fails to start with the following log messages:

To correct, uninstall the xf86-video-fbdev package.

Error message: unable to load font `(null)'.

Some programs only work with bitmap fonts. Two major packages with bitmap fonts are available, xorg-fonts-75dpi and xorg-fonts-100dpi. You do not need both; one should be enough. To find out which one would be better in your case, try xdpyinfo from xorg-xdpyinfo, like this:

and use what is closer to the shown value.

If Xorg is set to boot up automatically and for some reason you need to prevent it from starting up before the login/display manager appears (if the system is wrongly configured and Xorg does not recognize your mouse or keyboard input, for instance), you can accomplish this task with two methods.

Depending on setup, you will need to do one or more of these steps:

If you are getting Client is not authorized to connect to server, try adding the line:

to /etc/pam.d/su and /etc/pam.d/su-l. pam_xauth will then properly set environment variables and handle xauth keys.

If the filesystem (specifically /tmp) is full, startx will fail. The log file will contain:

Make some free space on the relevant filesystem and X will start.

Your color depth is set wrong. It may need to be 24 instead of 16, for example.

If X terminates with error message SocketCreateListener() failed, you may need to delete socket files in /tmp/.X11-unix. This may happen if you have previously run Xorg as root (e.g. to generate an xorg.conf).

That error means that only the current user has access to the X server. The solution is to give access to root:

That line can also be used to give access to X to a different user than root.

**Examples:**

Example 1 (unknown):
```unknown
$ lspci -v -nn -d ::03xx
```

Example 2 (unknown):
```unknown
$ pacman -Ss xf86-video
```

Example 3 (unknown):
```unknown
/usr/share/X11/xorg.conf.d/
```

Example 4 (unknown):
```unknown
/etc/X11/xorg.conf.d/
```

---

## Deepin Desktop Environment

**URL:** https://wiki.archlinux.org/title/Deepin_Desktop_Environment

**Contents:**
- Installation
- Starting
  - Via a display manager
  - Via xinit
- Known issues
- Configuration
  - Networking
  - Customize touchpad gesture behavior
  - Changing default deepin sounds
  - Changing system language

The Deepin Desktop Environment (DDE) is the desktop environment of the deepin Linux distribution. It is designed by the Wuhan Deepin Technology Co.,Ltd. deepin is a Linux distribution devoted to providing a beautiful, easy to use, safe and reliable system for global users. deepin is an open source GNU/Linux operating system, based on Linux kernel and mainly on desktop applications, supporting laptops, desktops, and all-in-ones. The DDE is comprised of the Desktop Environment, deepin Window Manager, Control Center, Launcher and Dock.

Install deepin and deepin-kwin for the basic components for a minimal desktop interface.

Optionally, also install deepin-extra for some extra applications for a more complete desktop environment.

LightDM is the default display manager for DDE, and it will installed as dependency. Simply enable lightdm.service to use it.

To use Deepin via xinit, you will need to add the following to your .xinitrc file.

NetworkManager is integrated in DDE network administration and is installed together. Enable NetworkManager.service to use it.

Deepin does not officially support customizing the gesture behaviors, but it is possible to manually change this by editing the configuration file /usr/share/dde-daemon/gesture.json.

For instance, if you want to disable tapping gesture activity, set its action to none:

To apply the changes, reboot your system or log off and log in again.

While this is not officially supported, it is possible to change or even remove the default sounds that are used by Deepin (ex. login sound). Simply replace the sounds in the directory:

Note: If you simply want to disable the sound effects entirely, it can be done from Deepin's system settings (sound section).

The environment variable LANG of Deepin can be affected by ~/.dde_env, and it has the highest precedence at the moment, the /etc/locale.conf and $XDG_HOME/.config/locale.conf will be ignored if this file exists:

Because of the way the NVIDIA driver stores its FBOs [2], it happens that after resuming from standby the background suddenly disappears, leaving only a white screen with possibly some color noise on it. The bug appears to be fixed in GNOME upstream, but the Deepin desktop environment still has it.

A possible workaround would be restarting the window manager every time the computer resumes from suspension. A way to do that would be to create the following systemd service

That executes the following script

Once those two files are created in the correct directories, make the script executable and start/enable resume@user

NetworkManager sets the MAC address generated randomly. This was already enabled by default, to disable it add the following lines to the NetworkManager configuration file.

Start/enable bluetooth.service. This service is not enabled by default.

When home directory is shared in dual boot, ~/.config/kglobalaccels can lead to strange shortcut behavior and we can try renaming this file to another name.

Any bugs related to Arch packaging should be reported in the bug tracker.

Any upstream related bugs should be reported here. All the Deepin developers will see the bug reports and solve them as soon as possible.

**Examples:**

Example 1 (unknown):
```unknown
lightdm.service
```

Example 2 (unknown):
```unknown
exec startdde
```

Example 3 (unknown):
```unknown
NetworkManager.service
```

Example 4 (unknown):
```unknown
/usr/share/dde-daemon/gesture.json
```

---

## hpfall

**URL:** https://wiki.archlinux.org/title/Hpfall

**Contents:**
- Installation
- Configuration
- Testing
  - Testing Shock protection

hpfall is a simple daemon providing HDD shock protection for HP laptops supporting the feature officially called "HP Mobile Data Protection System 3D" or "HP 3D DriveGuard".

Install hpfall-gitAUR.

You need to set your hard drive in the configuration file:

Start and enable hpfall.service

After rebooting with new kernel, check if hp_accel was initialized correctly through the journal.

Find your HDD's unload_heads file:

Go to its directory and run:

Lift your laptop into free space and simulate a free fall while holding it firmly with your hands (~10 cm should be enough). If the disk protection works, you should hear your HDD making "click" sound and see one of your laptop's LEDs flashing. The watch value's background should also permanently turn black.

**Examples:**

Example 1 (unknown):
```unknown
/etc/conf.d/hpfall
```

Example 2 (unknown):
```unknown
#
# Parameters to be passed to hpfall
#
DEVICE=/dev/sdX
```

Example 3 (unknown):
```unknown
hpfall.service
```

Example 4 (unknown):
```unknown
# find /sys -name unload_heads
```

---

## File systems

**URL:** https://wiki.archlinux.org/title/File_systems

**Contents:**
- Types of file systems
  - Journaling
  - FUSE-based file systems
  - Stackable file systems
  - Read-only file systems
  - Clustered file systems
  - Shared-disk file system
- Identify existing file systems
- Create a file system
- Mount a file system

Individual drive partitions can be set up using one of the many different available file systems. Each has its own advantages, disadvantages, and unique idiosyncrasies. A brief overview of supported file systems follows; the links are to Wikipedia pages that provide much more information.

See filesystems(5) for a general overview and Wikipedia:Comparison of file systems for a detailed feature comparison. File systems already loaded by the kernel or built-in are listed in /proc/filesystems, while all the installed modules can be seen with ls /lib/modules/$(uname -r)/kernel/fs.

xfs.html xfs-delayed-logging-design.html xfs-self-describing-metadata.html

The ext3/4, HFS+, JFS, NTFS, ReiserFS, and XFS file systems use journaling. Journaling provides fault-resilience by logging changes before they are committed to the file system. In the event of a system crash or power failure, such file systems are faster to bring back online and less likely to become corrupted. The logging takes place in a dedicated area of the file system.

ext3/4 offer data-mode journaling, which can optionally log data in addition to the metadata. Data-mode journaling comes with a speed penalty, because it does two write operations: first to the journal and then to the disk. Therefore, data-mode journaling is not enabled by default. The trade-off between system speed and data safety should be considered when choosing the file system type and features.

In the same vein, Reiser4 offers configurable "transaction models": a special model called wandering logs, which eliminates the need to write to the disk twice; write-anywhere, a pure copy-on-write approach; and a combined approach called hybrid which heuristically alternates between the two.

File systems based on copy-on-write (also known as write-anywhere), such as Reiser4, Btrfs, Bcachefs and ZFS, by design operate on full atomicity and also provide checksums for both metadata and inline data (operations entirely occur, or they entirely do not, and in properly functioning hardware data does not corrupt due to operations half-occurring). Therefore, these file systems are by design much less prone to data loss than other file systems and have no need to use traditional journal to protect metadata, because they are never updated in-place. Although Btrfs still has a journal-like log tree, it is only used to speed-up fdatasync/fsync.

FAT, exFAT, ext2, and HFS provide neither journaling nor atomicity, They are for temporary or legacy use and not recommended for use when reliable storage is needed.

To identify existing file systems, you can use lsblk:

An existing file system, if present, will be shown in the FSTYPE column. If mounted, it will appear in the MOUNTPOINT column.

File systems are usually created on a partition, inside logical containers such as LVM, RAID and dm-crypt, or on a regular file (see Wikipedia:Loop device). This section describes the partition case.

Before continuing, identify the device where the file system will be created and whether or not it is mounted. For example:

Mounted file systems must be unmounted before proceeding. In the above example an existing file system is on /dev/sda2 and is mounted at /mnt. It would be unmounted with:

To find just mounted file systems, see #List mounted file systems.

To create a new file system, use mkfs(8). See #Types of file systems for the exact type, as well as userspace utilities you may wish to install for a particular file system.

For example, to create a new file system of type ext4 (common for Linux data partitions) on /dev/sda1, run:

The new file system can now be mounted to a directory of choice.

To manually mount a file system located on a device (e.g., a partition) to a directory, use mount(8). This example mounts /dev/sda1 to /mnt.

This attaches the file system on /dev/sda1 at the directory /mnt, making the contents of the file system visible. Any data that existed at /mnt before this action is made invisible until the device is unmounted.

fstab contains information on how devices should be automatically mounted if present. See the fstab article for more information on how to modify this behavior.

If a device is specified in /etc/fstab and only the device or mount point is given on the command line, that information will be used in mounting. For example, if /etc/fstab contains a line indicating that /dev/sda1 should be mounted to /mnt, then the following will automatically mount the device to that location:

mount contains several options, many of which depend on the file system specified. The options can be changed, either by:

See these related articles and the article of the file system of interest for more information.

To list all mounted file systems, use findmnt(8):

findmnt takes a variety of arguments which can filter the output and show additional information. For example, it can take a device or mount point as an argument to show only information on what is specified:

findmnt gathers information from /etc/fstab, /etc/mtab, and /proc/self/mounts.

To unmount a file system use umount(8). Either the device containing the file system (e.g., /dev/sda1) or the mount point (e.g., /mnt) can be specified:

Unmount the file system and run fsck on the problematic volume.

**Examples:**

Example 1 (unknown):
```unknown
/proc/filesystems
```

Example 2 (unknown):
```unknown
ls /lib/modules/$(uname -r)/kernel/fs
```

Example 3 (unknown):
```unknown
NAME   FSTYPE LABEL     UUID                                 MOUNTPOINT
sdb
└─sdb1 vfat   Transcend 4A3C-A9E9
```

Example 4 (unknown):
```unknown
NAME   FSTYPE   LABEL       UUID                                 MOUNTPOINT
sda
├─sda1                      C4DA-2C4D
├─sda2 ext4                 5b1564b2-2e2c-452c-bcfa-d1f572ae99f2 /mnt
└─sda3                      56adc99b-a61e-46af-aab7-a6d07e504652
```

---

## Bash

**URL:** https://wiki.archlinux.org/title/Bash

**Contents:**
- Invocation
  - Configuration files
  - Shell and environment variables
- Command line
  - Tab completion
    - Single-tab
    - Common programs and options
    - Customize per-command
  - History
    - History completion

Bash (Bourne-Again SHell) is a command-line shell/programming language by the GNU Project. Its name alludes to its predecessor, the long-deprecated Bourne shell. Bash can be run on most Unix-like operating systems, including GNU/Linux.

Bash is the default command-line shell on Arch Linux.

Bash behaviour can be altered depending on how it is invoked. Some descriptions of different modes follow.

If Bash is spawned by login in a TTY, by an SSH daemon, or similar means, it is considered a login shell. This mode can also be engaged using the -l/--login command line option.

Bash is considered an interactive shell when its standard input, output and error are connected to a terminal (for example, when run in a terminal emulator), and it is not started with the -c option or non-option arguments (for example, bash script). All interactive shells source /etc/bash.bashrc and ~/.bashrc, while interactive login shells also source /etc/profile and ~/.bash_profile.

Bash will attempt to execute a set of startup files depending on how it was invoked. See the Bash Startup Files section of the GNU Bash manual for a complete description.

The behavior of Bash and programs run by it can be influenced by a number of environment variables. Environment variables are used to store useful values such as command search directories, or which browser to use. When a new shell or script is launched it inherits its parent's variables, thus starting with an internal set of shell variables[1].

These shell variables in Bash can be exported in order to become environment variables:

Environment variables are conventionally placed in ~/.profile or /etc/profile so that other Bourne-compatible shells can use them.

See Environment variables for more general information.

Bash command line is managed by the separate library called Readline. Readline provides emacs and vi styles of shortcuts for interacting with the command line, i.e. moving back and forth on the word basis, deleting words etc. It is also Readline's responsibility to manage history of input commands. Last, but not least, it allows you to create macros.

Tab completion is the option to auto-complete typed commands by pressing Tab (enabled by default).

It may require up to three tab-presses to show all possible completions for a command. To reduce the needed number of tab-presses, see Readline#Faster completion.

By default, Bash only tab-completes commands, filenames, and variables. The package bash-completion extends this by adding more specialized tab completions for common commands and their options, which can be enabled by sourcing /usr/share/bash-completion/bash_completion (which has been already sourced in Arch's /etc/bash.bashrc). With bash-completion, normal completions (such as ls file.* Tab Tab) will behave differently; however, they can be re-enabled with compopt -o bashdefault program (see [2] and [3] for more detail).

By default, Bash only tab-completes file names following a command. You can change it to complete command names using complete -c:

or complete command names and file names with -cf:

See bash(1) § Programmable Completion for more completion options.

You can bind the up and down arrow keys to search through Bash's history (see: Readline#History and Readline Init File Syntax):

or to affect all readline programs:

The HISTCONTROL variable can prevent certain commands from being logged to the history.

To stop logging of consecutive identical commands:

To remove all but the last identical command:

To avoid saving commands that start with a space:

To avoid saving consecutive identical commands, and commands that start with a space:

To remove all but the last identical command, and commands that start with a space:

See bash(1) § HISTCONTROL for details.

To disable the bash history only temporarily:

The commands entered now are not logged to the $HISTFILE.

For example, now you can hash passwords with printf secret | sha256sum, or hide GPG usage like gpg -eaF secret-pubkey.asc and your secret is not written to disk.

To disable all bash history:

... and just to make sure, destroy your old histfile forever:

Zsh can invoke the manual for the command preceding the cursor by pressing Alt+h. A similar behaviour is obtained in Bash using this Readline bind:

This assumes are you using the (default) Emacs editing mode.

atuin replaces your existing shell history with an SQLite database, and records additional context for your commands. Additionally, it provides optional and fully encrypted synchronization of your history between machines, via an Atuin server.

Enable bash history timestamps (export HISTTIMEFORMAT="%F %T ") before syncing. Atuin works well with tools like blesh-gitAUR and cmd-wrapped to provide an enhanced terminal experience across machines.

alias is a command, which enables a replacement of a word with another string. It is often used for abbreviating a system command, or for adding default arguments to a regularly used command.

Personal aliases can be stored in ~/.bashrc or any separate file sourced from ~/.bashrc. System-wide aliases (which affect all users) belong in /etc/bash.bashrc. See [4] for example aliases.

For functions, see Bash/Functions.

See Bash/Prompt customization.

ble.sh (Bash Line Editor), packed as blesh-gitAUR, is a command line editor written in pure Bash, which is an alternative to GNU Readline. It has many enhanced features like syntax highlighting, autosuggestions, menu-completion, abbreviations, Vim editing mode, and hook functions. Other interesting features include status line, history share, right prompt, transient prompt, and xterm title.

After installing it, source it in an interactive session.

Configurations are explained in depth in the ~/.blerc file and at the wiki. The stable bleshAUR package is also available.

pkgfile includes a "command not found" hook that will automatically search the official repositories, when entering an unrecognized command.

You need to source the hook to enable it, for example:

Then attempting to run an unavailable command will show the following info:

You can disable the Ctrl+z feature (pauses/closes your application) by wrapping your command like this:

Now, when you accidentally press Ctrl+z in adomAUR instead of Shift+z, nothing will happen because Ctrl+z will be ignored.

To clear the screen after logging out on a virtual terminal:

Bash can automatically prepend cd when entering just a path in the shell. For example:

But after adding one line into .bashrc file:

autojump-gitAUR is a python script which allows navigating the file system by searching for strings in a database with the user's most-visited paths.

zoxide is an alternative which has additional features and performance improvements compared to the original autojump and can serve as a drop-in replacement for autojump.

For the current session, to disallow existing regular files to be overwritten by redirection of shell output:

This is identical to set -C.

To make the changes persistent for your user:

To manually overwrite a file while noclobber is set:

pushd and popd can be used to push or pop directories to a stack while switching to them. This can be useful for "replaying" your navigation history.

See bash(1) § DIRSTACK.

When resizing a terminal emulator, Bash may not receive the resize signal. This will cause typed text to not wrap correctly and overlap the prompt. The checkwinsize shell option checks the window size after each command and, if necessary, updates the values of LINES and COLUMNS.

If you have set the ignoreeof option and you find that repeatedly hitting ctrl-d causes the shell to exit, it is because this option only allows 10 consecutive invocations of this keybinding (or 10 consecutive EOF characters, to be precise), before exiting the shell.

To allow higher values, you have to use the IGNOREEOF variable.

The package shellcheck analyzes bash (and other shell) scripts, prints possible errors, and suggests better coding.

There is also the web site shellcheck.net of the same purpose, based on this program.

**Examples:**

Example 1 (unknown):
```unknown
bash script
```

Example 2 (unknown):
```unknown
/etc/bash.bashrc
```

Example 3 (unknown):
```unknown
/etc/profile
```

Example 4 (unknown):
```unknown
~/.bash_profile
```

---

## Cloudflared

**URL:** https://wiki.archlinux.org/title/Cloudflared

**Contents:**
- Installation
- Usage
- Checking
- Endpoints
- See also

Cloudflared may be used to run a local DNS over HTTPS server (DoH), i.e., a stub resolver.

Install the cloudflared package.

Run cloudflared proxy-dns to run a DNS over HTTPS proxy server.

Use the --address and --port options to specify the address and port cloudflared listens to. They default to localhost and 53 respectively. For a list of available command line options, see here.

You can create a systemd service file, for example:

After starting the service, you can test that it works by using drill(1) (provided by the ldns package):

Use 1.1.1.1/help to check if browser is using Cloudflare DoH.

By default cloudflared uses https://1.1.1.1/dns-query and https://1.0.0.1/dns-query, i.e. Cloudflare's DNS over HTTPS servers, as upstream endpoint URLs.

You can specify different upstream endpoint URLs with the --upstream option.

**Examples:**

Example 1 (unknown):
```unknown
cloudflared proxy-dns
```

Example 2 (unknown):
```unknown
/etc/systemd/system/cloudflared.service
```

Example 3 (unknown):
```unknown
[Unit]
Description=DNS over HTTPS proxy client
Wants=network-online.target nss-lookup.target
Before=nss-lookup.target

[Service]
AmbientCapabilities=CAP_NET_BIND_SERVICE
CapabilityBoundingSet=CAP_NET_BIND_SERVICE
DynamicUser=yes
ExecStart=/usr/bin/cloudflared proxy-dns --port 54

[Install]
WantedBy=multi-user.target
```

Example 4 (unknown):
```unknown
$ drill archlinux.org @127.0.0.1 -p 54
```

---

## Linux console/Keyboard configuration

**URL:** https://wiki.archlinux.org/title/Console_keymap

**Contents:**
- Viewing keyboard settings
- Keymaps
  - Listing keymaps
  - Loadkeys
  - Persistent configuration
  - Creating a custom keymap
    - Adding directives
    - Other examples
    - Saving changes
- Adjusting typematic delay and rate

Keyboard mappings, console fonts and console maps are provided by the kbd package (a dependency of systemd), which also provides low-level tools for managing the text console. In addition, systemd provides the localectl(1) tool, which can control both the system locale and keyboard layout settings for both the console and Xorg.

Use localectl status to view the current keyboard configuration.

The keymap files are stored in the /usr/share/kbd/keymaps/ directory tree. A keymap file fully describes the keyboard layout, possibly with symbols for different languages and layout switching is simulated via AltGr_Lock keysym usage.

The include statement can be used to share common parts of keymap files. Where to look for an include file is described in the source code only.

For more details see keymaps(5).

The naming conventions of console keymaps are somewhat arbitrary, but usually they are based on:

For a list of all the available keymaps, use the command:

To search for a keymap, use the following command, replacing search_term with the code for your language, country, or layout:

Alternatively, using find:

It is possible to set a keymap just for the current session. This is useful for testing different keymaps, solving problems etc. The loadkeys tool is used for this purpose:

See loadkeys(1) for details. The same tool is used internally by systemd-vconsole-setup(8) when loading the keymap configured in /etc/vconsole.conf.

A persistent keymap can be set in /etc/vconsole.conf, which is read by systemd on start-up. The KEYMAP variable is used for specifying the keymap. If the variable is empty or not set, the us keymap is used as default value. See vconsole.conf(5) for all options. For example:

For convenience, localectl may be used to set the console keymap. It will change the KEYMAP variable in /etc/vconsole.conf and also set the keymap for the current session:

The --no-convert option can be used to prevent localectl from automatically changing the Xorg keymap to the nearest match. See localectl(1) for more information.

If required, the keymap from /etc/vconsole.conf can be loaded during early userspace by the keymap mkinitcpio hook.

When using the console, you can use hotkeys to print a specific character. Moreover we can also print a sequence of characters and some escape sequences. Thus, if we print the sequence of characters constituting a command and afterwards an escape character for a new line, that command will be executed.

One method of doing this is editing the keymap file. However, since it will be rewritten anytime the package it belongs to is updated, editing this file is discouraged. It is better to integrate the existing keymap with a personal keymap. The loadkeys utility can do this.

First, create a keymap file. This keymap file can be anywhere, but one method is to mimic the directory hierarchy in /usr/local: create the /usr/local/share/kbd/keymaps directory, then edit /usr/local/share/kbd/keymaps/personal.map.

As a side note, it is worth noting that such a personal keymap is useful also to redefine the behaviour of keys already treated by the default keymap: when loaded with loadkeys, the directives in the default keymap will be replaced when they conflict with the new directives and conserved otherwise. This way, only changes to the keymap must be specified in the personal keymap.

Two kinds of directives are required in this personal keymap. First of all, the keycode directives, which matches the format seen in the default keymaps. These directives associate a keycode with a keysym. Keysyms represent keyboard actions. The actions available include outputting character codes or character sequences, switching consoles or keymaps, booting the machine, and many other actions. The full currently active keymap can be obtained with

Most keysyms are intuitive. For example, to set key 112 to output an 'e', the directive will be:

To set key 112 to output a euro symbol, the directive will be:

Some keysym are not immediately connected to a keyboard actions. In particular, the keysyms prefixed by a capital F and one to three digits (F1-F246) constituting a number greater than 30 are always free. This is useful directing a hotkey to output a sequence of characters and other actions:

Then, F70 can be bound to output a specific string:

When key 112 is pressed, it will output the contents of F70. In order to execute a printed command in a terminal, a newline escape character must be appended to the end of the command string. For example, to enter a system into hibernation, the following keymap is added:

In order to make use of the personal keymap, it must be loaded with loadkeys:

However this keymap is only active for the current session. In order to load the keymap at boot, specify the full path to the file in the KEYMAP variable in /etc/vconsole.conf. The file does not have to be gzipped as the official keymaps provided by kbd.

The typematic delay indicates the amount of time (typically in milliseconds) a key needs to be pressed and held in order for the repeating process to begin. After the repeating process has been triggered, the character will be repeated with a certain frequency (usually given in Hz) specified by the typematic rate. These values can be changed using the kbdrate command. Note that these settings are configured separately for the console and for Xorg.

For example to set a typematic delay to 200ms and a typematic rate to 30Hz, use the following command:

Issuing the command without specifying the delay and rate will reset the typematic values to their respective defaults; a delay of 250ms and a rate of 11Hz:

A systemd service can be used to set the keyboard rate. For example:

Then start/enable the kbdrate.service systemd service.

Layout switching can only be simulated by establishing a different layout on one of the higher layers (typically the 3rd, AltGr).

For a list of layouts that likely support this, run

You can test this by passing main and augmenting layout to loadkeys, e.g.

If it worked, the 3rd level shift (AltGr) will allow you to access the second layout, the AltGr_Lock (often Alt+Shift, you will have to inspect the keymap file with zless) will provide an effective toggle.

For a permanent configuration, set the KEYMAP_TOGGLE next to the KEYMAP in /etc/vconsole.conf.

**Examples:**

Example 1 (unknown):
```unknown
localectl status
```

Example 2 (unknown):
```unknown
/usr/share/kbd/keymaps/
```

Example 3 (unknown):
```unknown
$ localectl list-keymaps
```

Example 4 (unknown):
```unknown
search_term
```

---

## Multiboot USB drive

**URL:** https://wiki.archlinux.org/title/Multiboot_USB_drive

**Contents:**
- Using GRUB and loopback devices
  - Preparation
  - Installing GRUB
    - Simple installation
    - Hybrid UEFI GPT + BIOS GPT/MBR boot
  - Configuring GRUB
    - Using a template
    - Manual configuration
  - Boot entries
    - Arch Linux monthly release

A multiboot USB flash drive allows booting multiple ISO files from a single device. The ISO files can be copied to the device and booted directly without unpacking them first. There are multiple methods available, but they may not work for all ISO images.

This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.

This article or section needs expansion.

Create at least one partition and a filesystem supported by GRUB on the USB drive. See Partitioning and File systems#Create a file system. Choose the size based on the total size of the ISO files that you want to store on the drive, and plan for extra space for the boot loader.

Mount the filesystem located on the USB drive:

Create the directory /boot:

Install GRUB on the USB drive:

In case you want to boot ISOs in UEFI mode, you have to install grub for the UEFI target:

For UEFI, the partition has to be the first one in an MBR partition table and formatted with FAT32.

This article or section is being considered for removal.

This configuration is useful for creating a universal USB key, bootable everywhere. First of all you must create a GPT partition table on your device. You need at least 3 partitions:

Next you must create a hybrid MBR partition table. Without it, a BIOS MBR based system will not boot. It will not find the partitions it expects to find.

Hybrid MBR partition table creation example using gdisk:

Do not forget to format the partitions:

You can now install GRUB to support both EFI + GPT and BIOS + GPT/MBR. The GRUB configuration (--boot-directory) can be kept in the same place.

First, you need to mount the EFI system partition and the data partition of your USB drive.

An example of this would be as follows:

Then, you can install GRUB for UEFI with:

In most cases EFI_MOUNTPOINT will correspond to the /mnt/efi directory on your mounted USB disk. DATA_MOUNTPOINT is where your data partition is mounted. In this example it would be the mount point of sdX3, /mnt.

As an additional fallback, you can also install GRUB on your MBR-bootable data partition:

There are some git projects which provide some pre-existing GRUB configuration files, and a nice generic grub.cfg which can be used to load the other boot entries on demand, showing them only if the specified ISO files - or folders containing them - are present on the drive.

Multiboot USB: https://github.com/hackerncoder/multibootusb

GLIM (GRUB2 Live ISO Multiboot): https://github.com/thias/glim

For the purpose of multiboot USB drive it is easier to edit grub.cfg by hand instead of generating it. Alternatively, make the following changes in /etc/grub.d/40_custom or /mnt/boot/grub/custom.cfg and generate /mnt/boot/grub/grub.cfg using grub-mkconfig.

As it is recommend to use a persistent name instead of /dev/sdxY to identify the partition on the USB drive where the image files are located, define a variable for convenience to hold the value. If the ISO images are on the same partition as GRUB, use the following to read the UUID at boot time:

Or specify the UUID explicitly:

Alternatively, use the device label instead of UUID:

The necessary UUID or label can be found using lsblk -f. Do not use the same label as the Arch ISO for the USB device, otherwise the boot process will fail.

To complete the configuration, a boot entry for each ISO image has to be added below this header, see the next section for examples.

It is assumed that the ISO images are stored in the /boot-isos directory on the same filesystem where GRUB is installed. Otherwise it would be necessary to prefix the path to ISO file with device identification when using the loopback command, for example loopback loop (hd1,2)$iso_path. As this identification of devices is not persistent, it is not used in the examples in this section.

One can use persistent block device naming like so. Replace the UUID according to your ISO filesystem UUID.

The ISO provides loopback.cfg.

MemTest86+ is included in the monthly ISO.

See Archboot Homepage.

Using the memdisk module, the ISO image is loaded into memory, and its boot loader is loaded. Make sure that the system that will boot this USB drive has sufficient amount of memory for the image file and running operating system.

Make sure that the USB drive is properly partitioned and that there is a partition with file system supported by Syslinux, for example fat32 or ext4. Then install Syslinux to this partition, see Syslinux#BIOS systems.

The memdisk module was not installed during Syslinux installation, it has to be installed manually. Mount the partition where Syslinux is installed to /mnt/ and copy the memdisk module to the same directory where Syslinux is installed:

After copying the ISO files on the USB drive, edit the Syslinux configuration file and create menu entries for the ISO images. The basic entry looks like this:

See memdisk on Syslinux wiki for more configuration options.

**Examples:**

Example 1 (unknown):
```unknown
# mount /dev/sdXY /mnt
```

Example 2 (unknown):
```unknown
# mkdir /mnt/boot
```

Example 3 (unknown):
```unknown
# grub-install --target=i386-pc --recheck --boot-directory=/mnt/boot /dev/sdX
```

Example 4 (unknown):
```unknown
# grub-install --target=x86_64-efi --removable --boot-directory=/mnt/boot --efi-directory=/mnt
```

---

## XDG Base Directory

**URL:** https://wiki.archlinux.org/title/XDG_Base_Directory_support

**Contents:**
- Specification
  - User directories
  - System directories
- Support
  - Contributing
  - Supported
  - Partial
  - Hardcoded
- Tools
- Libraries

This article summarizes the XDG Base Directory specification in #Specification and tracks software support in #Support.

Please read the full specification. This section will attempt to break down the essence of what it tries to achieve.

Only XDG_RUNTIME_DIR is set by default through pam_systemd(8). It is up to the user to explicitly define the other variables according to the specification. Changing it might cause issues with pipewire and screen sharing on chromium.

See Environment variables#Globally for information on defining variables.

This article or section is out of date.

This article or section needs expansion.

This section exists to catalog the growing set of software using the XDG Base Directory Specification introduced in 2003. This is here to demonstrate the viability of this specification by listing commonly found dotfiles and their support status. For those not currently supporting the Base Directory Specification, workarounds will be demonstrated to emulate it instead.

The workarounds will be limited to anything not involving patching the source, executing code stored in environment variables or compile-time options. The rationale for this is that configurations should be portable across systems and having compile-time options prevent that.

Hopefully this will provide a source of information about exactly what certain kinds of dotfiles are and where they come from.

When contributing make sure to use the correct section.

Nothing should require code evaluation, patches or compile-time options to gain support and anything which does must be deemed hardcoded. Additionally, if the process is error prone or difficult, it should also be classified as hardcoded.

If present ~/.actrc will be merged with the XDG path config.

Location overview by Google does not mention XDG - paths could be hardcoded instead of using the proper variable, though that is unlikely as Intellij IDEA, which Android Studio is based on, implements it properly as well

The BITWARDENCLI_APPDATA_DIR environment variable takes precedence.

Currently contains a single data.json file with all the vault data, so it ought to belong in XDG_DATA_HOME

XDG_CONFIG_HOME/byobu

Legacy path takes precedence if present, or if XDG_CONFIG_HOME is not set.

If the legacy path ~/.calcurse is present, it will take precedence.

XDG_CACHE_HOME/clangd

Project specific configuration can be specified in proj/.clangd. Configuration is combined when this is sensible. In case of conflicts, user config has the highest precedence, then inner project, then outer project.

If the legacy path is present, it will take precedence.

See Ctags Option files.

libcups added XDG support in v3 (still in beta). The version in the official repositories is still hardcoded to ~/.cups.

Legacy paths have precedence over XDG paths. Emacs will never create XDG_CONFIG_HOME/emacs/. Workaround for 26.3 or older: It's possible to set HOME, but it has unexpected side effects.

legacy path can be used with FreeCAD --keep-deprecated-paths

The old method of export STACK_ROOT="$XDG_DATA_HOME"/stack still works and takes priority [55][dead link 2024-07-30—HTTP 404].

XDG_CONFIG_HOME/latexmk/latexmkrc

XDG_CONFIG_HOME/lesskey

XDG_STATE_HOME/lesshst or XDG_DATA_HOME/lesshst

If the legacy path ~/.luarocks is present, it will take precedence.

1b99570 0b71156 ce401d7

mkdir -p "$XDG_DATA_HOME"/newsboat "$XDG_CONFIG_HOME"/newsboat

XDG_CONFIG_HOME/utop/utoprc

github.com/osc/pull/940

XDG_CONFIG_HOME/osc/oscrc XDG_STATE_HOME/osc/cookiejar

Legacy path takes precedence if it exists

87f1e8f a9020c6 3b22f0f 0a012ae

a0be0cc7 15e1fc92 e9d1be0e

59a8618 87ae830 9ab510a 4c195bc

fd8686e 66d704b 51cff01

mkdir "$XDG_CONFIG_HOME"/scummvm/ "$XDG_DATA_HOME"/scummvm mv ~/.scummvmrc "$XDG_CONFIG_HOME"/scummvm/scummvm.ini mv ~/.scummvm "$XDG_DATA_HOME"/scummvm/saves

See Shellcheck RC Files for more info.

3e4591d bd8c427 f57fc71

For Qt programs, GTK or Qt programs on Wayland, to use cursors in XDG_DATA_HOME/icons, the XCURSOR_PATH environment variable needs to be configured.

See :h xdg-base-dir for more details.

The viminfo file can be set with :set viminfofile=$XDG_STATE_HOME/vim/viminfo

9fc6b37[dead link 2024-07-30—HTTP 404] eaccf70[dead link 2024-07-30—HTTP 404]

[156][dead link 2024-07-30—HTTP 404]

Alternatively, it always respects XMONAD_CACHE_DIR, XMONAD_CONFIG_DIR, and XMONAD_DATA_DIR.

The remote's ~/.ansible/tmp can be moved by setting remote_tmp = ${XDG_CONFIG_HOME}/ansible/tmp in an appropriate ansible.cfg. [170] [171]

HOME="$XDG_DATA_HOME" btcli

or use the fork that has native XDG support: [188]

export CRAWL_DIR="$XDG_DATA_HOME"/crawl/

Despite this, clusterssh will still create ~/.clusterssh/.

Undocumented, though actively used: export DISCORD_USER_DATA_DIR="${XDG_DATA_HOME}"

Source: <discord_system_package_root>/resources/app.asar.

export MIX_XDG="true"

mkdir "$XDG_CONFIG_HOME"/erlang mv ~/.erlang.cookie "$XDG_CONFIG_HOME"/erlang

The environment variable GHCUP_USE_XDG_DIRS can be set to any non-empty value. See [211].

export GR_PREFS_PATH="$XDG_CONFIG_HOME"/gnuradio

GNU Radio Companion: export GRC_PREFS_PATH="$XDG_CONFIG_HOME"/gnuradio/grc.conf

Note that this currently does not work out-of-the-box using systemd user units and socket-based activation, since the socket directory changes based on the hash of $GNUPGHOME. You can get the new socket directory using gpgconf --list-dirs socketdir and have to modify the systemd user units to listen on the correct sockets accordingly. You also have to use the following gpg-agent.service drop-in file (or otherwise pass the GNUPGHOME env var to the agent running in systemd), or you might experience issues with "missing" private keys:

If you use GPG as your SSH agent, set SSH_AUTH_SOCK to the output of gpgconf --list-dirs agent-ssh-socket instead of some hardcoded value.

If GOMODCACHE is not set, it defaults to $GOPATH/pkg/mod (see [217]). GOCACHE is supported and defaults to $XDG_CACHE_HOME/go-build (see [218]).

PASSWORD_STORE_DIR is supported only during initialization.

If Lxappearance is used, ~/.gtkrc-2.0 may keep being created because it is where clicking "Apply" customizations writes to. The path is hardcoded in Lxappearance, but simply being an output file, the settings can be repeatedly moved to the location.

To prevent KDE Plasma from creating this file, disable the "GNOME/GTK Settings Synchronization" background service.

The value of this variable must include the substring __HVER__, which will be replaced at run time with the current MAJOR.MINOR version string.

export JUPYTER_CONFIG_DIR="$XDG_CONFIG_HOME"/jupyter

v5.0.0 <= python-jupyter-core < v6.0.0:

export JUPYTER_PLATFORM_DIRS="1" (see [229])

python-jupyter-core >= v6.0.0: full support (via python-platformdirs) enabled by default

KDE4 uses KDEHOME. It is not recommended to set the variable for newer versions.

to change the m2 repo location used by leiningen look here: Leiningen#m2_repo_location

Make sure XDG_CACHE_HOME is set beforehand to directory user running Xorg has write access to.

Do not use XDG_RUNTIME_DIR as it is available after login. Display managers that launch Xorg (like GDM) will repeatedly fail otherwise.

However, no way to change the location of this configuration file.

, mvn -gs "$XDG_CONFIG_HOME"/maven/settings.xml and set <localRepository> as appropriate in settings.xml

Used to be MATHEMATICA_USERBASE, see Upgrading from Mathematica to Wolfram.

Creates a further .minikube directory in MINIKUBE_HOME for whatever reason.

~/.my.cnf only supported for mysql-server, not mysql-client [251]

~/.mylogin.cnf unsupported

export TERMINFO="$XDG_DATA_HOME"/terminfo, export TERMINFO_DIRS="$XDG_DATA_HOME"/terminfo:/usr/share/terminfo

prefix is unnecessary (and unsupported) if Node.js is installed by nvm.

Both configuration and state data are stored in OPAMROOT, so this solution is not fully compliant.

The local_list option must be given an absolute path.

Currently it hard-codes ~/.local/share.

Log files may still be written to ~/pt/logs regardless of this setting until the .packettracer file is recreated manually.

export PHIVE_HOME="$XDG_DATA_HOME/phive"

It is required to create both directories: mkdir "$XDG_CONFIG_HOME/pg" && mkdir "$XDG_STATE_HOME"

PYTHON_HISTORY: export PYTHON_HISTORY=$XDG_STATE_HOME/python_history PYTHONPYCACHEPREFIX: export PYTHONPYCACHEPREFIX=$XDG_CACHE_HOME/python PYTHONUSERBASE: export PYTHONUSERBASE=$XDG_DATA_HOME/python

For more info see Bundler: bundle config.

Both configuration and data files are stored in SDKMAN_DIR, so this solution does not fully conform to the XDG Base Directory Specification.

mv ~/.ssr "$XDG_CONFIG_HOME"/simplescreenrecorder

export SPACEMACSDIR="$XDG_CONFIG_HOME"/spacemacs, mv ~/.spacemacs "$SPACEMACSDIR"/init.el

Other files need to be configured like Emacs.

tiptop -W "$XDG_CONFIG_HOME"/tiptop

Setting this makes the editor look for the contents of .config/Code - OSS in $VSCODE_PORTABLE/user-data.

You can also run Visual Studio with the --extensions-dir flag, such as code --extensions-dir "$XDG_DATA_HOME/vscode". This is documented and probably will not break as unexpectedly, as it has other use cases.

You can also edit the value of dataFolderName in product.json file to .local/share/codium or the path you want. But this workaround will have to be applies after every update of the pacakge, so you can install vscodium-xdg-dir-patchAUR that does it automatically.

The directory needs to be created manually

mkdir "$XDG_CONFIG_HOME/wakatime"

mkdir -p "$XDG_DATA_HOME"/wineprefixes, export WINEPREFIX="$XDG_DATA_HOME"/wineprefixes/default

App also creates ~/.x3270connect but this is currently unsupported.

Note that LightDM does not allow you to change this variable. If you change it nonetheless, you will not be able to login. Use startx instead or configure LightDM. According to [311] SLiM has ~/.Xauthority hardcoded.

The SDDM Xauthority path can be changed in its own configuration files as shown below. Unfortunately, it is relative to the home directory.

On Wayland, overriding this may cause Xorg programs to fail to connect to the Xwayland server. For example, both kwin and mutter use a randomized name, so it cannot be set to a static value.

Depending on where you have configured your $XDG_CACHE_HOME, you might need to expand the paths yourself.

Unlike most other examples in this table, actual X11 init scripts will vary a lot between installations.

Finally, if you use zsh as a login shell and chose to rely on either of the startup files ~/.zshenv or, ~/.zprofile or, ~/.zlogin to set important environment variables such as ZDOTDIR, to bootstrap, there is no way around having the one file that sets ZDOTDIR be in the default location. For context, an exception is if your wider system configuration does set the ZDOTDIR environment variable before that.

In the above config file, some locations can be customized using options like newsrc-path= and address-book=.

Specify the new directories used by Arduino CLI in arduino-cli.yaml as mentioned in the documentation here. alias arduino-cli='arduino-cli --config-file $XDG_CONFIG_HOME/arduino15/arduino-cli.yaml'

export HISTFILE="$XDG_STATE_HOME"/bash/history

bashrc can be sourced from a different location in /etc/bash.bashrc. Specify --init-file <file> as an alternative to ~/.bashrc for interactive shells.

Generated by the maintenance scripts eval.php and sql.php.

export OLLAMA_MODELS=$XDG_DATA_HOME/ollama/models

Note that this requires root privileges and will change the location of ~/.sbclrc for all users. This can be mitigated by checking for an existing ~/.sbclrc inside the lambda form.

export VIMPERATOR_RUNTIME="$XDG_CONFIG_HOME"/vimperator

The tool xdg-ninjaAUR detects unwanted files/directories in $HOME which can be moved to XDG base directories. See README for examples.

The tool boxxy can be used to wrap applications which do not respect the XDG base directories and redirect any unwanted files.

The tool ephemeral can be used to link chromium/electron caches that normally live in XDG_CONFIG_HOME to locations in XDG_CACHE_HOME.

For directories which cannot be relocated, some desktop environments such as KDE allow you to hide them:

path is the path of the file/directory, relative to the parent directory of .hidden.

**Examples:**

Example 1 (unknown):
```unknown
XDG_RUNTIME_DIR
```

Example 2 (unknown):
```unknown
XDG_CONFIG_HOME
```

Example 3 (unknown):
```unknown
$HOME/.config
```

Example 4 (unknown):
```unknown
XDG_CACHE_HOME
```

---

## XFS

**URL:** https://wiki.archlinux.org/title/XFS

**Contents:**
- Preparation
- Creation
  - Checksumming
  - Free inode btree
  - Reverse mapping btree
  - Big timestamps
    - Upgrading
- Performance
  - Stripe size and width
  - Access time

XFS is a high-performance journaling file system created by Silicon Graphics, Inc. XFS is particularly proficient at parallel IO due to its allocation group based design. This enables extreme scalability of IO threads, filesystem bandwidth, file and filesystem size when spanning multiple storage devices.

For XFS userspace utilities install the xfsprogs package. It contains the tools necessary to manage an XFS file system.

To create a new filesystem on device use:

In general, the default options are optimal for common use.[1][2]

xfsprogs 3.2.0 introduced a new on-disk format (v5) that includes a metadata checksum scheme called Self-Describing Metadata. Based on CRC32, it provides additional protection against metadata corruption (e.g. on unexpected power losses). Checksums are enabled by default when using xfsprogs 3.2.3 or later, but can be disabled (necessary for read-write mounts on older kernels) using the -m crc=0 switch when calling mkfs.xfs(8):

The XFS v5 on-disk format is considered stable for production workloads starting in Linux Kernel 3.15.

Starting in Linux 3.16, XFS has added a btree that tracks free inodes. It is equivalent to the existing inode allocation btree with the exception that the free inode btree tracks inode chunks with at least one free inode. The purpose is to improve lookups for free inode clusters for inode allocation. It improves performance on aged filesystems i.e. months or years down the track when you have added and removed millions of files to/from the filesystem. Using this feature does not impact overall filesystem reliability level or recovery capabilities.

This feature relies on the new v5 on-disk format that has been considered stable for production workloads starting Linux Kernel 3.15. It does not change existing on-disk structures, but adds a new one that must remain consistent with the inode allocation btree; for this reason older kernels will only be able to mount read-only filesystems with the free inode btree feature.

The feature is enabled by default when using xfsprogs 3.2.3 or later. If you need a writable filesystem for older kernels, it can be disable with the finobt=0 switch when formatting an XFS partition. You will need crc=0 together:

or shortly (because finobt depends on crc):

The reverse mapping btree is at its core:

From mkfs.xfs(8) § OPTIONS:

See also [5] and [6] for more information.

This feature is enabled by default for new filesystems as of xfsprogs 6.5.0.

Starting in Linux 5.10, XFS supports using refactored "timestamp and inode encoding functions to handle timestamps as a 64-bit nanosecond counter and bit shifting to increase the effective size. This now allows XFS to run well past the Year 2038 problem to now the Year 2486. Making a new XFS file-system with bigtime enabled allows a timestamp range from December 1901 to July 2486 rather than December 1901 to January 2038." The feature will also allow quota timer expirations from January 1970 to July 2486 rather than January 1970 to February 2106.

Big timestamps are enabled by default for new filesystems as of xfsprogs 5.15.

This article or section needs expansion.

Verify whether an existing filesystem has bigtime enabled with xfs_info(8):

With xfsprogs 5.11 and newer you can upgrade an existing (unmounted) filesystem with xfs_admin(8):

Or with xfs_repair(8):

While there, you may want to enable inobtcount as well (another new default).

(see #Stripe size and width)

Therefore for optimal performance, in most cases you can just follow #Creation.

If this filesystem will be on a striped RAID you can gain significant speed improvements by specifying the stripe size to the mkfs.xfs(8) command.

XFS can sometimes detect the geometry under software RAID, but in case you reshape it or you are using hardware RAID see how to calculate the correct sunit,swidth values for optimal performance.

On some filesystems you can increase performance by adding the noatime mount option to the /etc/fstab file. For XFS filesystems "the default atime behaviour is relatime, which has almost no overhead compared to noatime but still maintains sane atime values. All Linux filesystems use this as the default now (since around 2.6.30), but XFS has used relatime-like behaviour since 2006, so no-one should really need to ever use noatime on XFS for performance reasons."[7]

See Fstab#atime options for more on this topic.

Despite XFS supporting async discard[8] since kernel 4.7[9][10], xfs(5) still recommends "that you use the fstrim application to discard unused blocks rather than the discard mount option because the performance impact of this option is quite severe."

See Solid state drive#Periodic TRIM.

Although the extent-based nature of XFS and the delayed allocation strategy it uses significantly improves the file system's resistance to fragmentation problems, XFS provides a filesystem defragmentation utility (xfs_fsr, short for XFS filesystem reorganizer) that can defragment the files on a mounted and active XFS filesystem. It can be useful to view XFS fragmentation periodically.

xfs_fsr(8) improves the organization of mounted filesystems. The reorganization algorithm operates on one file at a time, compacting or otherwise improving the layout of the file extents (contiguous blocks of file data).

To see how much fragmentation your file system currently has:

To begin defragmentation, use the xfs_fsr(8) command:

The reflink feature, available since kernel version 4.9 and enabled by default since mkfs.xfs version 5.1.0, allows creating fast reflink'ed copies of files as well as deduplication after the fact, in the same way as btrfs:

Reflink copies initially use no additional space:

Until either file is edited, and a copy-on-write takes place. This can be very useful to create snapshots of (large) files.

Existing filesystems can be deduped using tools like duperemove or hardlink(1) from util-linux.

Using an external log (metadata journal) on for instance a SSD may be useful to improve performance [11]. See mkfs.xfs(8) for details about the logdev parameter.

To reserve an external journal with a specified size when you create an XFS file system, specify the -l logdev=device,size=size option to the mkfs.xfs command. If you omit the size parameter, a journal size based on the size of the file system is used. To mount the XFS file system so that it uses the external journal, specify the -o logdev=device option to the mount command.

XFS has a dedicated sysctl variable for setting the writeback interval with a default value of 3000.

XFS can be resized online using xfs_growfs(8):

If -D size is omitted, the filesystem is automatically grown to the largest size possible, i.e. the size of the partition.

xfs_scrub asks the kernel to scrub all metadata objects in the XFS filesystem. Metadata records are scanned for obviously bad values and then cross-referenced against other metadata. The goal is to establish a reasonable confidence about the consistency of the overall filesystem by examining the consistency of individual metadata records against the other metadata in the filesystem. Damaged metadata can be rebuilt from other metadata if there exists redundant data structures which are intact.

Enable/start xfs_scrub_all.timer to periodic check online metadata for all XFS filesystems.

See Checking and Repairing an XFS File System, Which factors influence the memory usage of xfs_repair? and XFS Repair.

Even when being mounted read-only with mount -o ro an XFS file system's log will be replayed if it has not been unmounted cleanly.

There may be situations where a compromised XFS file system on a damaged storage device should be mounted read-only, so that files may be copied off it hopefully without causing further damage, yet it cannot be mounted because it has not been unmounted cleanly and is damaged to such an extent that the log cannot be replayed. Also, consider that replaying the log means writing to the compromised file system, which might be a bad idea in itself.

To mount an XFS file system without writing to it in any way and without replaying the log, use mount -o ro,norecovery.

xfs_undelete-gitAUR can recover (under certain conditions) deleted files on an unmounted or read-only mounted XFS filesystem. See https://github.com/ianka/xfs_undelete for more information.

XFS quota mount options (uquota, gquota, prjquota, etc.) fail during re-mount of the file system. To enable quota for root file system, the mount option must be passed to initramfs as a kernel parameter rootflags=. Subsequently, it should not be listed among mount options in /etc/fstab for the root (/) filesystem.

When running xfs_scrub_all, it will launch xfs_scrub@.service for each mounted XFS file system. The service is run as user nobody, so if nobody can not navigate to the directory, it will fail with the error:

To allow the service to run, change the permissions of the mountpoint so that user nobody has execute permissions.

When using a mkinitcpio-generated systemd based initramfs without the base hook, you will see the following messages in the journal:

This is because fsck.xfs(8) is a shell script and requires /bin/sh to execute. /usr/bin/sh is provided by the base hook, so the solution is to prepend it to the HOOKS array in /etc/mkinitcpio.conf. E.g.:

**Examples:**

Example 1 (unknown):
```unknown
# mkfs.xfs device
```

Example 2 (unknown):
```unknown
meta-data=/dev/device            isize=256    agcount=4, agsize=3277258 blks
         =                       sectsz=512   attr=2
data     =                       bsize=4096   blocks=13109032, imaxpct=25
         =                       sunit=0      swidth=0 blks
naming   =version 2              bsize=4096   ascii-ci=0
log      =internal log           bsize=4096   blocks=6400, version=2
         =                       sectsz=512   sunit=0 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0
```

Example 3 (unknown):
```unknown
# mkfs.xfs -m crc=0 /dev/target_partition
```

Example 4 (unknown):
```unknown
# mkfs.xfs -m crc=0,finobt=0 /dev/target_partition
```

---

## pkgstats

**URL:** https://wiki.archlinux.org/title/Pkgstats

**Contents:**
- Installation
- Usage
- Results and reference

pkgstats sends a list of all installed packages, the architecture and the mirror you are using to the archlinux.de project. This information is anonymous and cannot be used to identify the user, but it will help Arch developers prioritize their efforts (source code). See also the project's Privacy policy.

Install the pkgstats package.

pkgstats is set up to automatically run every week using a systemd timer. Once installed, it will be activated after the next reboot.

If you do not want to wait for a reboot cycle, you can manually start pkgstats.timer.

pkgstats can also be run manually: see pkgstats -h for usage information.

Statistics and documentation are available at https://pkgstats.archlinux.de/ and comparisons can be found at https://pkgstats.archlinux.de/fun .

There is a public JSON API to query statistics: API documentation.

**Examples:**

Example 1 (unknown):
```unknown
pkgstats.timer
```

Example 2 (unknown):
```unknown
pkgstats -h
```

---

## XDG Desktop Portal

**URL:** https://wiki.archlinux.org/title/XDG_Desktop_Portal

**Contents:**
- Installation
- Backends
  - List of backends and interfaces
- Configuration
  - Force desktop environment
- Troubleshooting
  - Portal does not start
  - Using multiple monitors with xdg-desktop-portal-wlr
  - Poor font rendering in GTK applications on KDE Plasma
  - GTK (possibly others) file chooser not working

From the Flatpak documentation:

Portals were designed for use with applications sandboxed through Flatpak, but any application can use portals to provide uniform access to features independent of desktops and toolkits. This is commonly used, for example, to allow screen sharing on Wayland via PipeWire, or to use file open and save dialogs on Firefox that use the same toolkit as your current desktop environment.

Install xdg-desktop-portal and one or more backends. The package includes a systemd/User service that will be automatically started via D-Bus.

When an application sends a request to the portal, it is handled by xdg-desktop-portal, which then forwards it to a backend implementation. This allows implementations to provide suitable user interfaces that fit into the user's desktop environments, and access environment-specific APIs for requests like opening a URI or recording the screen. Multiple backends can be installed and used at the same time. For example, a Sway setup may use xdg-desktop-portal-wlr for screen sharing support and xdg-desktop-portal-gtk as a fallback for all other interfaces that xdg-desktop-portal-wlr does not implement.

Portal backend definitions are located in /usr/share/xdg-desktop-portal/portals/*.portal. Each portal backend file contains a list of interfaces that it can handle, and the desktop environments which it supports.

The following table lists all backends available and their support for certain common interfaces.

The following packages provide a specific backend only, and not specific to a desktop environment:

When a request is made, xdg-desktop-portal will use the /usr/share/xdg-desktop-portal/DE-portals.conf file, where DE is based on the XDG_CURRENT_DESKTOP environment variable. These files are provided by the desktop environments themselves and determine which backends should be used when a specific environment is running.

If you want to override the desktop environment defaults, or your desktop environment does not provide a default configuration, you may create a portal configuration file at $XDG_CONFIG_HOME/xdg-desktop-portal/portals.conf to determine which backends you want to use, either generally or for each individual interface. If you use multiple desktop environments, you may also create multiple $XDG_CONFIG_HOME/xdg-desktop-portal/DE-portals.conf files for each environment.

For example, if your desktop environment does not have a portal backend, and you want to use xdg-desktop-portal-gtk as a generic fallback but also use the LXQt file picker through xdg-desktop-portal-lxqt, you can use the following configuration:

See portals.conf(5) for more information.

In some cases, such as when you have a standalone window manager, you might want to make xdg-desktop-portal to think you are using a specific desktop environment. This can be achieved by setting the XDG_CURRENT_DESKTOP environment variable for the xdg-desktop-portal.service user unit using a drop-in snippet. For example, to use the backend associated with KDE:

For xdg-desktop-portal-wlr and xdg-desktop-portal-hyprland to work, the XDG_CURRENT_DESKTOP and WAYLAND_DISPLAY environment variables have to be set in the systemd user session.

XDG_CURRENT_DESKTOP has to be set to the name of your compositor, e.g. XDG_CURRENT_DESKTOP=sway. WAYLAND_DISPLAY is set automatically by the compositor.

Check whether these variables are set with systemctl --user show-environment. If they are not set, import these environment variables into the systemd user session and dbus by running the following commands before launching the compositor (e.g., include them in the compositor's configuration file).

See [1] and [2] for more details.

xdg-desktop-portal-wlr requires an external chooser to select the shared monitor. By default, it looks for slurp, wofi and bemenu in this order. When using slurp, after a request for screen sharing you will be presented with a crosshair cursor and you will need to click the screen you want to share. When using wofi or bemenu, you will be presented with a menu of available displays to share. If no choosers are available, xdg-desktop-portal-wlr will fallback to the first monitor found. For more information, see xdg-desktop-portal-wlr(5) § SCREENCAST OPTIONS.

Some GTK apps require xdg-desktop-portal-gtk on Plasma in order to render font correctly. Install it and then run:

If the application runs on X via Xwayland (easiest way to check is to run xeyes and see whether they follow the mouse over the application in question), then xdg-desktop-portal-gtk will show up on demand, but after you select the file nothing will happen. In that case adding DISPLAY=:0 to the xdg-desktop-portal-gtk environment might help. To do this, you can either follow the instructions above about import-environment or just edit the systemd user unit file of xdg-desktop-portal-gtk. Alternatively, you can force the application under Wayland (e.g. if it is using Electron).

**Examples:**

Example 1 (unknown):
```unknown
xdg-desktop-portal
```

Example 2 (unknown):
```unknown
/usr/share/xdg-desktop-portal/portals/*.portal
```

Example 3 (unknown):
```unknown
xdg-desktop-portal
```

Example 4 (unknown):
```unknown
/usr/share/xdg-desktop-portal/DE-portals.conf
```

---

## D-Bus

**URL:** https://wiki.archlinux.org/title/D-Bus

**Contents:**
- Implementations
  - dbus-broker
  - Reference implementation
- Tips and tricks
  - Override dbus service
- Debugging
- See also

This article or section needs expansion.

D-Bus is a message bus system that provides an easy way for inter-process communication. It consists of a daemon, which can be run both system-wide and for each user session, and a set of libraries to allow applications to use D-Bus.

dbus is pulled and installed as a dependency of systemd and user session bus is started automatically for each user.

Arch provides two D-Bus message broker implementations. Initially, user will be asked to choose a desired dbus-units provider during installation of systemd package. Only one implementation can be installed at a time.

dbus-broker is currently the default implementation for Arch [1] [2]. It is a drop-in replacement for the reference implementation, which aims "to provide high performance and reliability, while keeping compatibility to the D-Bus reference implementation".

Select dbus-broker-units when asked for dbus-units provider, or install it explicitly.

The reference implementation is still officially supported by Arch.

Select dbus-daemon-units when asked for dbus-units provider, or install it explicitly.

This is useful when specifying a particular service among other services providing the same well-known bus name. See KeePass#Autostart and KDE Wallet#Automatic D-Bus activation for example.

D-Bus services can be masked by setting Exec=/bin/false in service files of $XDG_DATA_HOME/dbus-1/services. For example, to mask gvfsd,

If the service is already launched, the override will not work. The existing service's process must be killed, or launched earlier.

You can also use busctl(1) from systemd.

**Examples:**

Example 1 (unknown):
```unknown
systemctl mask
```

Example 2 (unknown):
```unknown
/etc/dbus-1/services
```

Example 3 (unknown):
```unknown
Exec=/bin/false
```

Example 4 (unknown):
```unknown
$XDG_DATA_HOME/dbus-1/services
```

---

## systemd-timesyncd

**URL:** https://wiki.archlinux.org/title/Systemd-timesyncd

**Contents:**
- Configuration
- Usage
  - Enable and start
  - Check service
  - Check verbose
  - View non default config
  - View log
- See also

From the systemd mailing list:

When starting, systemd-timesyncd will read the configuration file from /etc/systemd/timesyncd.conf, which looks like this:

To add time servers or change the provided ones, uncomment the relevant line and list their host name or IP separated by a space. Alternatively, you can use a configuration snippet in /etc/systemd/timesyncd.conf.d/*.conf, see timesyncd.conf(5).

For example, you can use any servers provided by the NTP pool project or use the default Arch ones (also provided by the NTP pool project):

To verify your configuration:

Further to the daemon configuration, NTP servers may also be provided via a systemd-networkd .network file with a NTP= option or, dynamically, via a DHCP server (when the UseNTP option is enabled in the [DHCPv4] or [DHCPv6] section).

The NTP server to be used will be determined using the following rules:

To enable and start it, simply run:

Alternatively (e.g. when running in chroot), start/enable systemd-timesyncd.service.

The synchronization process might be noticeably slow. This is expected, one should wait a while before determining there is a problem. To check the service status, use:

To see verbose service information, use:

To see the non default configuration options set and files from which those options are being derived, use:

To view the last 24 hours of logged events, use:

**Examples:**

Example 1 (unknown):
```unknown
/etc/systemd/timesyncd.conf
```

Example 2 (unknown):
```unknown
/etc/systemd/timesyncd.conf
```

Example 3 (unknown):
```unknown
[Time]
#NTP=
#FallbackNTP=0.arch.pool.ntp.org 1.arch.pool.ntp.org 2.arch.pool.ntp.org 3.arch.pool.ntp.org
#...
```

Example 4 (unknown):
```unknown
/etc/systemd/timesyncd.conf.d/*.conf
```

---

## Arch boot process

**URL:** https://wiki.archlinux.org/title/Boot_loader

**Contents:**
- Firmware types
  - UEFI
  - BIOS
- System initialization
  - UEFI
    - Multibooting
  - BIOS
- Boot loader
  - Feature comparison
- Kernel

In order to boot Arch Linux, a Linux-capable boot loader must be set up. The boot loader is responsible for loading the kernel and initial ramdisk before initiating the boot process. The procedure is quite different for BIOS and UEFI systems.

The firmware is the very first program that is executed once the system is switched on.

The Unified Extensible Firmware Interface has support for reading both the partition table as well as file systems. UEFI does not launch any boot code from the Master Boot Record (MBR) whether it exists or not, instead booting relies on boot entries in the NVRAM.

The UEFI specification mandates support for the FAT12, FAT16, and FAT32 file systems (see UEFI specification version 2.11, section 13.3.1.1), but any conformant vendor can optionally add support for additional file systems; for example, HFS+ or APFS in some Apple's firmwares. UEFI implementations also support ISO 9660 for optical discs.

UEFI launches EFI applications, e.g. boot loaders, boot managers, UEFI shell, etc. These applications are usually stored as files in the EFI system partition. Each vendor can store its files in the EFI system partition under the /EFI/vendor_name directory. The applications can be launched by adding a boot entry to the NVRAM or from the UEFI shell.

The UEFI specification has support for legacy BIOS booting with its Compatibility Support Module (CSM). If CSM is enabled in the UEFI, the UEFI will generate CSM boot entries for all drives. If a CSM boot entry is chosen to be booted from, the UEFI's CSM will attempt to boot from the drive's MBR bootstrap code.

A BIOS or Basic Input-Output System is in most cases stored in a flash memory in the motherboard itself and independent of the system storage. Originally created for the IBM PC to handle hardware initialization and the boot process, it has been replaced progressively since 2010 by UEFI which does not suffer from the same technical limitations.

System switched on, the power-on self-test (POST) is executed. See also Modern CPUs have a backstage cast by Hugo Landau.

If Secure Boot is enabled, the boot process will verify authenticity of the EFI binary by signature.

Since each OS or vendor can maintain its own files within the EFI system partition without affecting the other, multi-booting using UEFI is just a matter of launching a different EFI application corresponding to the particular operating system's boot loader. This removes the need for relying on the chain loading mechanisms of one boot loader to load another OS.

See also Dual boot with Windows.

A boot loader is a piece of software started by the firmware—UEFI or BIOS. It is responsible for loading the kernel with the wanted kernel parameters and any external initramfs images.

A boot manager presents a menu of boot options, or provides some other way to control the boot process—i.e. it just runs other EFI executables.

In the case of UEFI, the kernel itself can be directly launched by the UEFI using the EFI boot stub. A separate boot loader or a boot manager can still be used for the purpose of editing kernel parameters before booting.

Systems with 32-bit IA32 UEFI require a boot loader that supports mixed mode booting.

Since almost no boot loader supports such stacked block devices and since file systems can introduce new features which may not yet be supported by any boot loader (e.g. archlinux/packaging/packages/grub#7, FS#79857, FS#59047, FS#58137, FS#51879, FS#46856, FS#38750, FS#21733 and fscrypt encrypted directories), using a separate /boot partition with a universally supported file system, such as FAT32, is oftentimes more feasible.

See also Wikipedia:Comparison of boot loaders.

The boot loader boots the vmlinux image containing the kernel.

The kernel functions on a low level (kernelspace) interacting between the hardware of the machine and the programs. The kernel initially performs hardware enumeration and initialization before continuing to userspace. See Wikipedia:Kernel (operating system) and Wikipedia:Linux kernel for a detailed explanation.

An initramfs (initial RAM file system) image is a cpio archive providing the necessary files for early userspace (see below) to successfully start the late userspace. This predominantly means all kernel modules, user space tools, associated libraries, supporting files like udev rules, etc. required to locate, access and mount the root file system. With the concept of initramfs it is possible to handle even more complex setups, like e.g. booting from an external drive, stacked devices (logical volumes, software RAIDs, compression, encryption) or running a tiny SSH server in early userspace for remote unlocking or maintenance tasks of the root file system.

The majority of modules will be loaded during later stages of the init process by udev after having switched root to the root file system.

The process is as follows:

Initramfs images are Arch Linux' preferred method for setting up the early userspace and can be generated with mkinitcpio, dracut or booster.

Since 6.13.8 officially supported kernels have Btrfs and Ext4 drivers built-in [4].

This makes it possible for the kernel to use a root partition with these file systems directly and load the rest of external modules needed from there. Although, there are some quirks to keep in mind:

Another thing you really need initramfs for is early microcode loading. But it is not necessary to build full image for that, Arch provides microcode in separate initramfs files, which could be used independently.

If no initramfs image is provided, the kernel always contains still an empty image to start from [8]. So there should be no issues with root partition pinning.

The early userspace stage, a.k.a. the initramfs stage, takes place in rootfs consisting of the files provided by the #initramfs. Early userspace starts by the kernel executing the /init binary as PID 1.

The function of early userspace is configurable, but its main purpose is to bootstrap the system to the point where it can access the root file system. This includes:

Note that the early userspace serves more than just setting up the root file system. There are tasks that can only be performed before the root file system is mounted, such as fsck and resuming from hibernation.

At the final stage of early userspace, the real root is mounted at /sysroot/ (in case of a systemd-based initramfs) or at /new_root/ (in case of a busybox-based one), and then switched to by using systemctl switch-root when using systemd-based initramfs or switch_root(8) when using busybox-based initramfs. The late userspace starts by executing the init program from the real root file system.

The startup of late userspace is executed by the init process. Arch officially uses systemd which is built on the concept of units and services, but the functionality described here largely overlaps with other init systems.

The init process calls getty once for each virtual terminal (typically six of them). getty initializes each terminal and protects it from unauthorized access. When the username and password are provided, getty checks them against /etc/passwd and /etc/shadow, then calls login(1).

The login program begins a session for the user by setting environment variables and starting the user's shell, based on /etc/passwd. The login program displays the contents of /etc/motd (message of the day) after a successful login, just before it executes the login shell. It is a good place to display your Terms of Service to remind users of your local policies or anything you wish to tell them.

Once the user's shell is started, it will typically run a runtime configuration file, such as bashrc, before presenting a prompt to the user. If the account is configured to start X at login, the runtime configuration file will call startx or xinit. Jump to #Graphical session (Xorg) for the end.

This article or section needs expansion.

Additionally, init can be configured to start a display manager instead of getty on a specific virtual terminal. This requires manually enabling its systemd service file. The display manager then starts a graphical session.

xinit runs the user's xinitrc runtime configuration file, which normally starts a window manager or a desktop environment. When the user is finished and exits, xinit, startx, the shell, and login will terminate in that order, returning to getty or the display manager.

**Examples:**

Example 1 (unknown):
```unknown
/EFI/vendor_name
```

Example 2 (unknown):
```unknown
\EFI\BOOT\BOOTx64.EFI
```

Example 3 (unknown):
```unknown
BOOTIA32.EFI
```

Example 4 (unknown):
```unknown
esp/EFI/Linux/
```

---

## Domain name resolution

**URL:** https://wiki.archlinux.org/title/DNS_resolver

**Contents:**
- Name Service Switch
  - Resolve a domain name using NSS
- Glibc resolver
  - Overwriting of /etc/resolv.conf
    - Alternative using nmcli
  - Limit lookup time
  - Hostname lookup delayed with IPv6
  - Local domain names
- Lookup utilities
- Resolver performance

In general, a domain name represents an IP address and is associated to it in the Domain Name System (DNS). This article explains how to configure domain name resolution and resolve domain names.

This article or section needs expansion.

The Name Service Switch (NSS) facility is part of the GNU C Library (glibc) and backs the getaddrinfo(3) API, used to resolve domain names. NSS allows system databases to be provided by separate services, whose search order can be configured by the administrator in nsswitch.conf(5). The database responsible for domain name resolution is the hosts database, for which glibc offers the following services:

systemd provides three NSS services for hostname resolution:

NSS databases can be queried with getent(1). A domain name can be resolved through NSS using:

The glibc resolver reads /etc/resolv.conf for every resolution to determine the nameservers and options to use.

resolv.conf(5) lists nameservers together with some configuration options. Nameservers listed first are tried first, up to three nameservers may be listed. Lines starting with a number sign (#) are ignored.

Network managers tend to overwrite /etc/resolv.conf, for specifics see the corresponding section:

To prevent programs from overwriting /etc/resolv.conf, it is also possible to write-protect it by setting the immutable file attribute:

This article or section is a candidate for merging with NetworkManager#/etc/resolv.conf.

If you use NetworkManager, nmcli(1) can be used to set persistent options for /etc/resolv.conf. Change "Wired" to the name of your connection. Example:

For more options have a look at the man pages of nmcli(1), nm-settings-nmcli(5) and resolv.conf(5).

If you are confronted with a very long hostname lookup (may it be in pacman or while browsing), it often helps to define a small timeout after which an alternative nameserver is used. To do so, put the following in /etc/resolv.conf.

If you experience a 5 second delay when resolving hostnames it might be due to a DNS-server/Firewall misbehaving and only giving one reply to a parallel A and AAAA request.[1] You can fix that by setting the following option in /etc/resolv.conf:

To be able to use the hostname of local machine names without the fully qualified domain name, add a line to /etc/resolv.conf with the local domain such as:

That way you can refer to local hosts such as mainmachine1.example.org as simply mainmachine1 when using the ssh command, but the drill command still requires the fully qualified domain names in order to perform lookups.

To query specific DNS servers and DNS/DNSSEC records you can use dedicated DNS lookup utilities or those shipped with DNS servers. These tools implement DNS themselves and do not use NSS.

Some DNS server packages ship with DNS lookup utilities that can be used without running the DNS server:

The Glibc resolver does not cache queries. To implement local caching, use systemd-resolved or set up a local caching DNS server and use it as the name server by setting 127.0.0.1 and ::1 as the name servers in /etc/resolv.conf or in /etc/resolvconf.conf if using openresolv.

This article or section needs expansion.

The DNS protocol (Do53) is unencrypted and does not account for confidentiality, integrity or authentication, so if you use an untrusted network or a malicious ISP, your DNS queries can be eavesdropped and the responses manipulated. Furthermore, DNS servers can conduct DNS hijacking.

You need to trust your DNS server to treat your queries confidentially. DNS servers are provided by ISPs and third-parties. Alternatively you can run your own recursive name server (a.k.a. recursive resolver, a.k.a. DNS recursor), which however takes more effort. If you use a DHCP client in untrusted networks, be sure to set static name servers to avoid using and being subject to arbitrary DNS servers, or alternatively, use a VPN to connect to a secure network and use its DNS servers. To secure your communication with a remote DNS server you can use an encrypted protocol, provided that both the upstream server and your local resolver support the protocol. Common encrypted DNS protocols are:

To verify that responses are actually from authoritative name servers, you can validate DNSSEC, provided that both the upstream server(s) and your local resolver support it.

Although one may use an encrypted DNS resolver, a TLS connection still leaks the domain names in the Server Name Indication (SNI) when requesting the domain certificate. This leak can be checked using the Wireshark filter tls.handshake.extensions_server_name_len > 0, or using the following tshark command:

A proposed solution is to use the Encrypted Client Hello (ECH), a TLS 1.3 protocol extension.

Be aware that some client software, such as major web browsers[2][3], are starting to implement DNS over HTTPS. While the encryption of queries may often be seen as a bonus, it also means the software sidetracks queries around the system resolver configuration.[4]

Firefox provides configuration options to enable or disable DNS over HTTPS and select a DNS server. Mozilla has setup a Trusted Recursive Resolver (TRR) programme with transparency information on their default providers. It is notable that Firefox supports and automatically enables the Encrypted Client Hello (ECH) for TRR providers, see Firefox/Privacy#Encrypted Client Hello.

Chromium will examine the user's system resolver and enable DNS over HTTPS if the system resolver addresses are known to also provide DNS over HTTPS. See this blog post for more information and how DNS over HTTPS can be disabled.

Mozilla has proposed universally disabling application-level DNS if the system resolver cannot resolve the domain use-application-dns.net. Currently, this is only implemented in Firefox.

Oblivious DNS over HTTPS (ODoH)—RFC 9230—is a system which addresses a number of DNS privacy concerns. See Cloudflare's article for more information. It added DNS over HTTPS to the academic Oblivious DNS design. See the Improving the privacy of DNS and DoH with oblivion article for a discussion of the differences.

This article or section needs expansion.

Communication between recursive resolvers and root servers is not encrypted and the root server operators are against implementing it. For encrypted communication with authoritative servers there is the experimental RFC 9539 which allows the opportunistic use of DNS over TLS and DNS over QUIC.

There are various third-party DNS services. Wikipedia has a list of "notable" public DNS service operators while the curl project's wiki has a more extensive list of publicly available DNS over HTTPS servers (a lot of which also support DNS over TLS). The systemd package configures fallback DNS for systemd-resolved when no DNS servers are configured (manually or via DHCP/RA).

You can use dnsperftest to test the performance of the most popular DNS resolvers from your location. dnsperf.com provides global benchmarks between providers.

Some DNS services also provide dedicated software:

DNS servers can be authoritative and recursive. If they are neither, they are called stub resolvers and simply forward all queries to another recursive name server. Stub resolvers are typically used to introduce DNS caching on the local host or network. Note that the same can also be achieved with a fully-fledged name server. This section compares the available DNS servers, for a more detailed comparison, refer to Wikipedia:Comparison of DNS server software.

It is possible to use specific DNS resolvers when querying specific domain names. This is particularly useful when connecting to a VPN, so that queries to the VPN network are resolved by the VPN's DNS, while queries to the internet will still be resolved by your standard DNS resolver. It can also be used on local networks.

To implement it, you need to use a local resolver because glibc does not support it.

In a dynamic environment (laptops and to some extents desktops), you need to configure your resolver based on the network(s) you are connected to. The best way to do that is to use openresolv because it supports multiple subscribers. Some network managers support it, either through openresolv, or by configuring the resolver directly. NetworkManager supports conditional forwarding without openresolv.

**Examples:**

Example 1 (unknown):
```unknown
/etc/resolv.conf
```

Example 2 (unknown):
```unknown
$ getent ahosts domain_name
```

Example 3 (unknown):
```unknown
/etc/resolv.conf
```

Example 4 (unknown):
```unknown
/etc/resolv.conf
```

---

## Glusterfs

**URL:** https://wiki.archlinux.org/title/Glusterfs

**Contents:**
- Installation
- Configuration
- Automount gluster volume on boot
- See also

Glusterfs is a scalable network filesystem.

Install the package glusterfs.

Glusterfs can be setup to run in many different configurations depending operating needs, including distributed and replicated. For the example below, a two node replicated server is being created, with nodes gluster1 and gluster2 each have two disks, one containing the OS (sda), the other to be shared by glusterfs (sdb). Unless stated, all setup is carried on gluster1:

To mount a gluster volume on boot, systemd needs to wait for both the network, and the glusterd service to be started. You can specify the following fstab options to do this:

**Examples:**

Example 1 (unknown):
```unknown
glusterd.service
```

Example 2 (unknown):
```unknown
# gluster peer probe gluster2
```

Example 3 (unknown):
```unknown
/dev/sdXY /export/sdXY xfs defaults 0 0
```

Example 4 (unknown):
```unknown
# mkdir -p /export/sdXY/brick
```

---

## systemd

**URL:** https://wiki.archlinux.org/title/Edit

**Contents:**
- Basic systemctl usage
  - Using units
  - Power management
    - Soft reboot
- Writing unit files
  - Handling dependencies
  - Service types
  - Editing provided units
    - Replacement unit files
    - Drop-in files

From the project web page:

Historically, what systemd calls "service" was named daemon: any program that runs as a "background" process (without a terminal or user interface), commonly waiting for events to occur and offering services. A good example is a web server that waits for a request to deliver a page, or an ssh server waiting for someone trying to log in. While these are full featured applications, there are daemons whose work is not that visible. Daemons are for tasks like writing messages into a log file (e.g. syslog, metalog) or keeping your system time accurate (e.g. ntpd). For more information see daemon(7).

The main command used to introspect and control systemd is systemctl. Some of its uses are examining the system state and managing the system and services. See systemctl(1) for more details.

Units commonly include, but are not limited to, services (.service), mount points (.mount), devices (.device) and sockets (.socket).

When using systemctl, you generally have to specify the complete name of the unit file, including its suffix, for example sshd.socket. There are however a few short forms when specifying the unit in the following systemctl commands:

See systemd.unit(5) for details.

The commands in the below table operate on system units since --system is the implied default for systemctl. To instead operate on user units (for the calling user), use systemctl --user without root privileges. See also systemd/User#Basic setup to enable/disable user units for all users.

polkit is necessary for power management as an unprivileged user. If you are in a local systemd-logind user session and no other session is active, the following commands will work without root privileges. If not (for example, because another user is logged into a tty), systemd will automatically ask you for the root password.

Soft reboot is a special kind of a userspace-only reboot operation that does not involve the kernel. It is implemented by systemd-soft-reboot.service(8) and can be invoked through systemctl soft-reboot. As with kexec, it skips firmware re-initialization, but additionally the system does not go through kernel initialization and initramfs, and unlocked dm-crypt devices remain attached.

When /run/nextroot/ contains a valid root file system hierarchy (e.g. is the root mount of another distribution or another snapshot), soft-reboot would switch the system root into it, allowing for switching to another installation without losing states managed by kernel, e.g. networking.

The syntax of systemd's unit files (systemd.unit(5)) is inspired by XDG Desktop Entry Specification .desktop files, which are in turn inspired by Microsoft Windows .ini files. Unit files are loaded from multiple locations (to see the full list, run systemctl show --property=UnitPath), but the main ones are (listed from lowest to highest precedence):

Look at the units installed by your packages for examples, as well as systemd.service(5) § EXAMPLES.

systemd-analyze(1) can help verifying the work. See the systemd-analyze verify FILE... section of that page.

With systemd, dependencies can be resolved by designing the unit files correctly. The most typical case is when unit A requires unit B to be running before A is started. In that case add Requires=B and After=B to the [Unit] section of A. If the dependency is optional, add Wants=B and After=B instead. Note that Wants= and Requires= do not imply After=, meaning that if After= is not specified, the two units will be started in parallel.

Dependencies are typically placed on services and not on #Targets. For example, network.target is pulled in by whatever service configures your network interfaces, therefore ordering your custom unit after it is sufficient since network.target is started anyway.

There are several different start-up types to consider when writing a custom service file. This is set with the Type= parameter in the [Service] section:

See the systemd.service(5) § OPTIONS man page for a more detailed explanation of the Type values.

This article or section needs expansion.

To avoid conflicts with pacman, unit files provided by packages should not be directly edited. There are two safe ways to modify a unit without touching the original file: create a new unit file which overrides the original unit or create drop-in snippets which are applied on top of the original unit. For both methods, you must reload the unit afterwards to apply your changes. This can be done either by editing the unit with systemctl edit (which reloads the unit automatically) or by reloading all units with:

To replace the unit file /usr/lib/systemd/system/unit, create the file /etc/systemd/system/unit and reenable the unit to update the symlinks.

This opens /etc/systemd/system/unit in your editor (copying the installed version if it does not exist yet) and automatically reloads it when you finish editing.

To create drop-in files for the unit file /usr/lib/systemd/system/unit, create the directory /etc/systemd/system/unit.d/ and place .conf files there to override or add new options. systemd will parse and apply these files on top of the original unit.

The easiest way to do this is to run:

This opens the file /etc/systemd/system/unit.d/drop_in_name.conf in your text editor (creating it if necessary) and automatically reloads the unit when you are done editing. Omitting --drop-in= option will result in systemd using the default file name override.conf .

To revert any changes to a unit made using systemctl edit do:

For example, if you simply want to add an additional dependency to a unit, you may create the following file:

As another example, in order to replace the ExecStart directive, create the following file:

Note how ExecStart must be cleared before being re-assigned [2]. The same holds for every item that can be specified multiple times, e.g. OnCalendar for timers.

One more example to automatically restart a service:

For services that send logs directly to journald or syslog, you can control their verbosity by setting a numeric value between 0 and 6 for the LogLevelMax= parameter in the [Service] section using the methods described above. For example:

The standard log levels are identical to those used to filter the journal. Setting a lower number excludes all higher and less important log messages from your journal.

If a service is echoing stdout and/or stderr output, by default this will end up in the journal as well. This behavior can be suppressed by setting StandardOutput=null and/or StandardError=null in the [Service] section. Other values than null can be used to further tweak this behavior. See systemd.exec(5) § LOGGING_AND_STANDARD_INPUT/OUTPUT.

systemd uses targets to group units together via dependencies and as standardized synchronization points. They serve a similar purpose as runlevels but act a little differently. Each target is named instead of numbered and is intended to serve a specific purpose with the possibility of having multiple ones active at the same time. Some targets are implemented by inheriting all of the services of another target and adding additional services to it. There are systemd targets that mimic the common SystemVinit runlevels.

The following should be used under systemd instead of running runlevel:

The runlevels that held a defined meaning under sysvinit (i.e., 0, 1, 3, 5, and 6); have a 1:1 mapping with a specific systemd target. Unfortunately, there is no good way to do the same for the user-defined runlevels like 2 and 4. If you make use of those it is suggested that you make a new named systemd target as /etc/systemd/system/your target that takes one of the existing runlevels as a base (you can look at /usr/lib/systemd/system/graphical.target as an example), make a directory /etc/systemd/system/your target.wants, and then symlink the additional services from /usr/lib/systemd/system/ that you wish to enable.

In systemd, targets are exposed via target units. You can change them like this:

This will only change the current target, and has no effect on the next boot. This is equivalent to commands such as telinit 3 or telinit 5 in Sysvinit.

The standard target is default.target, which is a symlink to graphical.target. This roughly corresponds to the old runlevel 5.

To verify the current target with systemctl:

To change the default target to boot into, change the default.target symlink. With systemctl:

Alternatively, append one of the following kernel parameters to your boot loader:

systemd chooses the default.target according to the following order:

Some (not exhaustive) components of systemd are:

systemd is in charge of mounting the partitions and filesystems specified in /etc/fstab. The systemd-fstab-generator(8) translates all the entries in /etc/fstab into systemd units; this is performed at boot time and whenever the configuration of the system manager is reloaded.

systemd extends the usual fstab capabilities and offers additional mount options. These affect the dependencies of the mount unit. They can, for example, ensure that a mount is performed only once the network is up or only once another partition is mounted. The full list of specific systemd mount options, typically prefixed with x-systemd., is detailed in systemd.mount(5) § FSTAB.

An example of these mount options is automounting, which means mounting only when the resource is required rather than automatically at boot time. This is provided in fstab#Automount with systemd.

On UEFI-booted systems, GPT partitions such as root, home, swap, etc. can be mounted automatically following the Discoverable Partitions Specification. These partitions can thus be omitted from fstab, and if the root partition is automounted, then root= can be omitted from the kernel command line. See systemd-gpt-auto-generator(8).

The prerequisites are:

udev will create symlinks to the discovered partitions which can be used to reference the partitions and volumes in configuration files.

For /var automounting to work, the PARTUUID must match the SHA256 HMAC hash of the partition type UUID keyed by the machine ID. The required PARTUUID can be obtained using:

The primary role of systemd-sysvcompat (required by base) is to provide the traditional linux init binary. For systemd-controlled systems, init is just a symbolic link to its systemd executable.

In addition, it provides four convenience shortcuts that SysVinit users might be used to. The convenience shortcuts are halt(8), poweroff(8), reboot(8) and shutdown(8). Each one of those four commands is a symbolic link to systemctl, and is governed by systemd behavior. Therefore, the discussion at #Power management applies.

systemd-based systems can give up those System V compatibility methods by using the init= boot parameter (see, for example, /bin/init is in systemd-sysvcompat ?) and systemd native systemctl command arguments.

systemd-tmpfiles creates, deletes and cleans up volatile and temporary files and directories. It reads configuration files in /etc/tmpfiles.d/ and /usr/lib/tmpfiles.d/ to discover which actions to perform. Configuration files in the former directory take precedence over those in the latter directory.

Configuration files are usually provided together with service files, and they are named in the style of /usr/lib/tmpfiles.d/program.conf. For example, the Samba daemon expects the directory /run/samba to exist and to have the correct permissions. Therefore, the samba package ships with this configuration:

Configuration files may also be used to write values into certain files on boot. For example, if you used /etc/rc.local to disable wakeup from USB devices with echo USBE > /proc/acpi/wakeup, you may use the following tmpfile instead:

It is possible to write multiple lines to the same file, either with \n in the argument or using the w+ type on multiple lines (including the first one) for appending:

See the systemd-tmpfiles(8) and tmpfiles.d(5) man pages for details.

The factual accuracy of this article or section is disputed.

Configuration files provided by packages should not be directly edited to avoid conflicts with pacman updates. For this, many (but not all) systemd packages provide a way to modify the configuration, but without touching the original file by creation of drop-in snippets. Check the package manual to see if drop-in configuration files are supported.

To create a drop-in configuration file for the unit file /etc/systemd/unit.conf, create the directory /etc/systemd/unit.conf.d/ and place .conf files there to override or add new options. systemd will parse and apply these files on top of the original unit.

Check the overall configuration:

The applied drop-in snippets file(s) and content will be listed at the end. Restart the service for the changes to take effect.

Some packages provide a .socket unit. For example, cups provides a cups.socket unit[3]. If cups.socket is enabled (and cups.service is left disabled), systemd will not start CUPS immediately; it will just listen to the appropriate sockets. Then, whenever a program attempts to connect to one of these CUPS sockets, systemd will start cups.service and transparently hand over control of these ports to the CUPS process.

To delay a service until after the network is up, include the following dependencies in the .service file:

The network wait service of the network manager in use must also be enabled so that network-online.target properly reflects the network status.

For more detailed explanations, see the discussion in the Network configuration synchronization points.

If a service needs to perform DNS queries, it should additionally be ordered after nss-lookup.target:

See systemd.special(7) § Special Passive System Units.

For nss-lookup.target to have any effect it needs a service that pulls it in via Wants=nss-lookup.target and orders itself before it with Before=nss-lookup.target. Typically this is done by local DNS resolvers.

Check which active service, if any, is pulling in nss-lookup.target with:

This article or section needs expansion.

By default, Arch Linux does not make use of systemd presets, and does not enable most services on installation.

If you wish for systemd presets to be honored when packages are installed, you will need to create a a pacman hook in order to run systemctl preset or systemctl preset-all whenever a new systemd unit is installed. Something like

Arch Linux ships with /usr/lib/systemd/system-preset/99-default.preset containing disable *. This causes systemctl preset to disable all units by default, such that when a new package is installed, the user must manually enable the unit.

To enable units by default instead, simply create a symlink from /etc/systemd/system-preset/99-default.preset to /dev/null or replace with a file containing enable * in order to override the configuration file. This will cause systemctl preset to enable all units that get installed—regardless of unit type—unless specified in another file in one systemctl preset's configuration directories. User units are not affected. It is also possible to configure higher priority files with more specific rules on what should be enabled. See systemd.preset(5) for more information.

See systemd/Sandboxing.

In order to notify about service failures, an OnFailure= directive needs to be added to the according service file, for example by using a drop-in configuration file. Adding this directive to every service unit can be achieved with a top-level drop-in configuration file. For details about top-level drop-ins, see systemd.unit(5).

Create a top-level drop-in for services:

This adds OnFailure=failure-notification@%n.service to every service file. If some_service_unit fails, failure-notification@some_service_unit.service will be started to handle the notification delivery (or whatever task it is configured to perform).

Create the failure-notification@.service template unit:

You can create the failure-notification.sh script and define what to do or how to notify. Examples include sending e-mail, showing desktop notifications, using gotify, XMPP, etc. The %i will be the name of the failed service unit and will be passed as an argument to the script.

In order to prevent a recursion for starting instances of failure-notification@.service again and again if the start fails, create an empty drop-in configuration file with the same name as the top-level drop-in (the empty service-level drop-in configuration file takes precedence over the top-level drop-in and overrides the latter one):

You can set up systemd to send an e-mail when a unit fails. Cron sends mail to MAILTO if the job outputs to stdout or stderr, but many jobs are setup to only output on error. First you need two files: an executable for sending the mail and a .service for starting the executable. For this example, the executable is just a shell script using sendmail, which is in packages that provide smtp-forwarder.

Whatever executable you use, it should probably take at least two arguments as this shell script does: the address to send to and the unit file to get the status of. The .service we create will pass these arguments:

Where user is the user being emailed and address is that user's email address. Although the recipient is hard-coded, the unit file to report on is passed as an instance parameter, so this one service can send email for many other units. At this point you can start status_email_user@dbus.service to verify that you can receive the emails.

Then simply edit the service you want emails for and add OnFailure=status_email_user@%n.service to the [Unit] section. %n passes the unit's name to the template.

See udisks#Automatically turn off an external HDD at shutdown.

To find the systemd services which failed to start:

To find out why they failed, examine their log output. See systemd/Journal#Filtering output for details.

systemd has several options for diagnosing problems with the boot process. See boot debugging for more general instructions and options to capture boot messages before systemd takes over the boot process. Also see systemd debugging documentation.

If some systemd service misbehaves or you want to get more information about what is happening, set the SYSTEMD_LOG_LEVEL environment variable to debug. For example, to run the systemd-networkd daemon in debug mode:

Add a drop-in file for the service adding the two lines:

Or equivalently, set the environment variable manually:

then restart systemd-networkd and watch the journal for the service with the -f/--follow option.

If the shutdown process takes a very long time (or seems to freeze), most likely a service not exiting is to blame. systemd waits some time for each service to exit before trying to kill it. To find out whether you are affected, see Shutdown completes eventually in the systemd documentation.

A common problem is a stalled shutdown or suspend process. To verify whether that is the case, you could run either of these commands and check the outputs

The solution to this would be to cancel these jobs by running

and then trying shutdown or reboot again.

If running journalctl -u foounit as root does not show any output for a short lived service, look at the PID instead. For example, if systemd-modules-load.service fails, and systemctl status systemd-modules-load shows that it ran as PID 123, then you might be able to see output in the journal for that PID, i.e. by running journalctl -b _PID=123 as root. Metadata fields for the journal such as _SYSTEMD_UNIT and _COMM are collected asynchronously and rely on the /proc directory for the process existing. Fixing this requires fixing the kernel to provide this data via a socket connection, similar to SCM_CREDENTIALS. In short, it is a bug. Keep in mind that immediately failed services might not print anything to the journal as per design of systemd.

The factual accuracy of this article or section is disputed.

After using systemd-analyze a number of users have noticed that their boot time has increased significantly in comparison with what it used to be. After using systemd-analyze blame NetworkManager is being reported as taking an unusually large amount of time to start.

The problem for some users has been due to /var/log/journal becoming too large. This may have other impacts on performance, such as for systemctl status or journalctl. As such the solution is to remove every file within the folder (ideally making a backup of it somewhere, at least temporarily) and then setting a journal file size limit as described in systemd/Journal#Journal size limit.

Starting with systemd 219, /usr/lib/tmpfiles.d/systemd.conf specifies ACL attributes for directories under /var/log/journal and, therefore, requires ACL support to be enabled for the filesystem the journal resides on.

See Access Control Lists#Enable ACL for instructions on how to enable ACL on the filesystem that houses /var/log/journal.

You may want to disable emergency mode on a remote machine, for example, a virtual machine hosted at Azure or Google Cloud. It is because if emergency mode is triggered, the machine will be blocked from connecting to network.

To disable it, mask emergency.service and emergency.target.

You may be trying to start or enable a user unit as a system unit. systemd.unit(5) indicates, which units reside where. By default systemctl operates on system services.

See systemd/User for more details.

Some bootloaders only set the LoaderDevicePartUUID variable when it is empty. Consequently, even if the UUID of the EFI partition changes, the bootloader will not update LoaderDevicePartUUID. By deleting the EFI variable with the commands below, the bootloader will then repopulate it with the new UUID.

**Examples:**

Example 1 (unknown):
```unknown
-H user@host
```

Example 2 (unknown):
```unknown
sshd.socket
```

Example 3 (unknown):
```unknown
netctl.service
```

Example 4 (unknown):
```unknown
dev-sda2.device
```

---

## mkinitcpio

**URL:** https://wiki.archlinux.org/title/Mkinitcpio

**Contents:**
- Installation
- Image creation and activation
  - Automated generation
  - Manual generation
  - Customized generation
  - Unified kernel images
- Configuration
  - MODULES
  - BINARIES and FILES
  - HOOKS

mkinitcpio is a Bash script used to create initramfs images. See Arch boot process#initramfs for a general introduction.

It is important to note that there are two distinct approaches how the various tasks during initial ramdisk phase are performed:

The concrete variant is determined by the absence or presence of the systemd hook in the HOOKS array of /etc/mkinitcpio.conf. See #Common hooks for more details.

mkinitcpio has been developed by the Arch Linux developers and from community contributions. See the public Git repository.

Install the mkinitcpio package, which is a dependency of the linux package, so most users will already have it installed.

Advanced users may wish to install the latest development version of mkinitcpio from Git with the mkinitcpio-gitAUR package.

Every time a kernel is installed or upgraded, a pacman hook automatically generates a .preset file saved in /etc/mkinitcpio.d/. For example linux.preset for the official stable linux kernel package. A preset is simply a list of information required to create initial ramdisk images, instead of manually specifying the various parameters and the location of the output files. By default, it contains the instructions to create two images:

After creating the preset, the pacman hook calls the mkinitcpio script which generates the two images, using the information provided in the preset.

To run the script manually, refer to the mkinitcpio(8) manual page for instructions. In particular, to (re-)generate an initramfs image based on the preset provided by a kernel package, use the -p/--preset option followed by the preset to utilize. For example, for the linux package, use the command:

To (re-)generate initramfs images based on all existing presets, use the -P/--allpresets switch. This is typically used to regenerate all the initramfs images after a change of the global #Configuration:

Users may create any number of initramfs images with a variety of different configurations. The desired image must be specified in the respective boot loader configuration file.

Users can generate an image using an alternative configuration file. For example, the following will generate an initial ramdisk image according to the directions in /etc/mkinitcpio-custom.conf and save it as /boot/initramfs-custom.img.

If generating an image for a kernel other than the one currently running, add the kernel release version to the command line. The installed kernel releases can be found in /usr/lib/modules/, the syntax is consistent with the output of the command uname -r for each kernel.

mkinitcpio can create unified kernel images (UKIs) either by itself or via systemd-ukify. If systemd-ukify is absent or explicitly disabled using --no-ukify, the UKI will be assembled by mkinitcpio itself. Advanced features of ukify will not be available then.

See unified kernel image for details about UKI generation.

The primary configuration file for mkinitcpio is /etc/mkinitcpio.conf. Drop-in configuration files are also supported, e.g. /etc/mkinitcpio.conf.d/myhooks.conf (they aren't taken into account if mkinitcpio is called with -c option and/or use a preset containing ALL_config). Additionally, preset definitions are provided by kernel packages in the /etc/mkinitcpio.d directory (e.g. /etc/mkinitcpio.d/linux.preset).

Users can modify seven variables within the configuration file, see mkinitcpio.conf(5) § VARIABLES for more details:

The MODULES array is used to specify modules to load before anything else is done.

Modules suffixed with a ? will not throw errors if they are not found. This might be useful for custom kernels that compile in modules which are listed explicitly in a hook or configuration file.

These options allow users to add files to the image. Both BINARIES and FILES are added before hooks are run, and may be used to override files used or provided by a hook. BINARIES are auto-located within a standard PATH and are dependency-parsed, meaning any required libraries will also be added. FILES are added as-is. For example:

Note that as both BINARIES and FILES are Bash arrays, multiple entries can be added delimited with spaces.

The HOOKS array is the most important setting in the file. Hooks are small scripts which describe what will be added to the image. For some hooks, they will also contain a runtime component which provides additional behavior, such as starting a daemon, or assembling a stacked block device. Hooks are referred to by their name, and executed in the order they exist in the HOOKS array of the configuration file.

The default HOOKS setting should be sufficient for most simple, single disk setups. For root devices which are stacked or multi-block devices such as LVM, RAID, or dm-crypt, see the respective wiki pages for further necessary configuration.

Build hooks are found in /usr/lib/initcpio/install/, custom build hooks can be placed in /etc/initcpio/install/. These files are sourced by the bash shell during runtime of mkinitcpio and should contain two functions: build and help. The build function describes the modules, files, and binaries which will be added to the image. An API, documented by mkinitcpio(8), serves to facilitate the addition of these items. The help function outputs a description of what the hook accomplishes.

For a list of all available hooks:

Use mkinitcpio's -H/--hookhelp option to output help for a specific hook, for example:

Runtime hooks are found in /usr/lib/initcpio/hooks/, custom runtime hooks can be placed in /etc/initcpio/hooks/. For any runtime hook, there should always be a build hook of the same name, which calls add_runscript to add the runtime hook to the image. These files are sourced by the busybox ash shell during early userspace. With the exception of cleanup hooks, they will always be run in the order listed in the HOOKS setting. Runtime hooks may contain several functions:

run_earlyhook: Functions of this name will be run once the API file systems have been mounted and the kernel command line has been parsed. This is generally where additional daemons, such as udev, which are needed for the early boot process are started from.

run_hook: Functions of this name are run shortly after the early hooks. This is the most common hook point, and operations such as assembly of stacked block devices should take place here.

run_latehook: Functions of this name are run after the root device has been mounted. This should be used, sparingly, for further setup of the root device, or for mounting other file systems, such as /usr.

run_cleanuphook: Functions of this name are run as late as possible, and in the reverse order of how they are listed in the HOOKS array in the configuration file. These hooks should be used for any last minute cleanup, such as shutting down any daemons started by an early hook.

Post hooks are executables or shell scripts located in /usr/lib/initcpio/post/ (hooks provided by packages) and /etc/initcpio/post/ (custom hooks). These files are executed after an image is (re)generated in order to perform additional tasks like signing.

To each executable the following arguments are passed in this order:

Additionally, the following environment variables are set—KERNELVERSION the full kernel version, KERNELDESTINATION the default location where the kernel should be located on order to be booted.

A table of common hooks and how they affect image creation and runtime follows. Note that this table is not complete, as packages can provide custom hooks.

This article or section needs expansion.

Optional when using the systemd hook as it only provides a busybox recovery shell. In addition to enabling base, you'll need to temporarily add SYSTEMD_SULOGIN_FORCE=1 to your kernel parameters to use the shell.

If the autodetect hook runs before this hook, it will only add early microcode update files for the processor of the system the image is built on.

The use of this hook replaces the now deprecated --microcode flag, and the microcode option in the preset files. This also allows you to drop the microcode initrd lines from your boot configuration as they are now packed together with the main initramfs image. || class="archwiki-table-cell" data-sort-value=5 |–

For sd-encrypt see dm-crypt/System configuration#Using systemd-cryptsetup-generator.

The use of this hook requires the rw parameter to be set on the kernel command line (discussion). See fsck#Boot time checking for more details. || class="archwiki-table-cell" data-sort-value=5 |–

The kernel supports several formats for compression of the initramfs: gzip, bzip2, lzma (xz), xz, lzo (lzop), lz4 and zstd. By default mkinitcpio uses zstd compression for kernel version 5.9 and newer and gzip for kernel versions older than 5.9.

The provided mkinitcpio.conf has the various COMPRESSION options commented out. Uncomment one if you wish to switch to another compression method and make sure you have the corresponding compression utility installed. If none is specified, the default method is used. If you wish to create an uncompressed image, specify COMPRESSION=cat in the configuration file or use -z cat on the command line.

These are additional flags passed to the program specified by COMPRESSION, such as:

This option can be left empty; mkinitcpio will ensure that any supported compression method has the necessary flags to produce a working image.

With the default zstd compression, to save space for custom kernels (especially with a dual boot setup when using the EFI system partition as /boot), the --long option is very effective. However, systems with limited RAM may not be able to decompress initramfs using this option. The -v option may also be desired to see details during the initramfs generation. For example:

Highest, but slowest, compression can be achieved by using xz with the -9e compression level and also decompressing the loadable kernel modules and firmware:

MODULES_DECOMPRESS controls whether the kernel module and firmware files are decompressed during initramfs creation. The default value is no.

Arch compresses its kernel modules and linux-firmware with zstd at level 19. When using a higher compression than that for the initramfs, setting MODULES_DECOMPRESS="yes" will allow to reduce the initramfs size even further. This comes at the expense of increased RAM and CPU usage at early boot which negatively affects systems with limited RAM or weak CPUs since the kernel will spend more time to decompress the whole initramfs image than it would take to decompress the individual modules and firmware upon loading them.

This article or section needs expansion.

Runtime configuration options can be passed to init and certain hooks via the kernel command line. Kernel command-line parameters are often supplied by the boot loader. The options discussed below can be appended to the kernel command line to alter default behavior. See Kernel parameters and Arch boot process for more information.

See Boot debugging and mkinitcpio(8) for other parameters.

See RAID#Configure mkinitcpio.

net requires the mkinitcpio-nfs-utils package.

Comprehensive and up-to-date information can be found in the official kernel documentation.

This parameter tells the kernel how to configure IP addresses of devices and also how to set up the IP routing table. It can take up to nine arguments separated by colons: ip=<client-ip>:<server-ip>:<gw-ip>:<netmask>:<hostname>:<device>:<autoconf>:<dns0-ip>:<dns1-ip>:<ntp0-ip>.

If this parameter is missing from the kernel command line, all fields are assumed to be empty, and the defaults mentioned in the kernel documentation apply. In general this means that the kernel tries to configure everything using autoconfiguration.

The <autoconf> parameter can appear alone as the value to the ip parameter (without all the : characters before). If the value is ip=off or ip=none, no autoconfiguration will take place, otherwise autoconfiguration will take place. The most common way to use this is ip=dhcp.

For parameters explanation, see the kernel documentation.

If you have multiple network cards, this parameter can include the MAC address of the interface you are booting from. This is often useful as interface numbering may change, or in conjunction with pxelinux IPAPPEND 2 or IPAPPEND 3 option. If not given, eth0 will be used.

If the nfsroot parameter is NOT given on the command line, the default /tftpboot/%s will be used.

Run mkinitcpio -H net for parameter explanation.

If your root device is on LVM, see Install Arch Linux on LVM#Adding mkinitcpio hooks.

If using an encrypted root see dm-crypt/System configuration#mkinitcpio for detailed information on which hooks to include.

If you keep /usr as a separate partition, you must adhere to the following requirements:

The generation of fallback images can be disabled:

If you are curious about what is inside the initramfs image, you can extract it and poke at the files inside of it.

The initramfs image is an SVR4 CPIO archive, generated via the find and bsdcpio commands, optionally compressed with a compression scheme understood by the kernel. For more information on the compression schemes, see #COMPRESSION.

mkinitcpio includes a utility called lsinitcpio(1) which will list and/or extract the contents of initramfs images.

You can list the files in the image with:

And to extract them all in the current directory:

You can also get a more human-friendly listing of the important parts in the image:

Invoke the build_image function of the /usr/bin/mkinitcpio script with parameters

It can be done by creating a new script with the contents of the build_image function and running it with the above parameters. This will compress the contents present in the current directory in a file named outfile.

The test used by mkinitcpio to determine if /dev is mounted is to see if /dev/fd/ is there. If everything else looks fine, it can be "created" manually by:

(Obviously, /proc must be mounted as well. mkinitcpio requires that anyway, and that is the next thing it will check.)

When initramfs are being rebuilt after a kernel update, you might get warnings:

If these messages appear when generating a default initramfs image, then, as the warning says, installing additional firmware may be required. Most common firmware files can be acquired by installing the linux-firmware package. For other packages providing firmware see the table below or try searching for the module name in the official repositories or AUR.

Otherwise, if the messages only appear when generating the fallback initramfs image you have the following options:

For unavailable firmware, you can suppress the warnings by creating dummy files, e.g.:

On some motherboards (mostly ancient ones, but also a few new ones), the i8042 controller cannot be automatically detected. It is rare, but some people will surely be without keyboard. You can detect this situation in advance. If you have a PS/2 port and get i8042: PNP: No PS/2 controller found. Probing ports directly message, add atkbd to the MODULES array.[2]

With an improper initial ram-disk a system often is unbootable. So follow a system rescue procedure like below:

mkinitcpio's autodetect hook filters unneeded kernel modules in the primary initramfs scanning /sys and the modules loaded at the time it is run. If you transfer your /boot directory to another machine and the boot sequence fails during early userspace, it may be because the new hardware is not detected due to missing kernel modules. Note that USB 2.0 and 3.0 need different kernel modules.

To fix, first try choosing the fallback image from your boot loader, as it is not filtered by autodetect. Once booted, run mkinitcpio on the new machine to rebuild the primary image with the correct modules. If the fallback image fails, try booting into an Arch Linux live CD/USB, chroot into the installation, and run mkinitcpio on the new machine. As a last resort, try manually adding modules to the initramfs.

The systemd hook disables the root account. To enable the emergency shell, temporarily add SYSTEMD_SULOGIN_FORCE=1 to the kernel parameters.

Alternatively, you can use initcpio-hook-shadowcopyAUR, by installing it and adding the shadowcopy hook after systemd in /etc/mkinitcpio.conf, and regenerating initramfs with mkinitcpio -P. More documentation is available in its GitHub repo.

**Examples:**

Example 1 (unknown):
```unknown
/etc/crypttab.initramfs
```

Example 2 (unknown):
```unknown
/etc/mkinitcpio.conf
```

Example 3 (unknown):
```unknown
/etc/mkinitcpio.d/
```

Example 4 (unknown):
```unknown
linux.preset
```

---

## chroot

**URL:** https://wiki.archlinux.org/title/Change_root

**Contents:**
- Reasoning
- Requirements
- Prepare new root location
- Usage
  - Using arch-chroot
    - Enter a chroot
    - Exit a chroot
    - Run a single command and exit
  - Running on Btrfs
  - Using chroot

A chroot is an operation that changes the apparent root directory for the current running process and their children. A program that is run in such a modified environment cannot access files and commands outside that environmental directory tree. This modified environment is called a chroot jail.

Changing root is commonly done for performing system maintenance on systems where booting and/or logging in is no longer possible. Common examples are:

See also Wikipedia:Chroot#Limitations.

In order to use chroot, you need another Linux installation or installation media (of any distribution), with:

The chroot target should be a directory which contains a file system hierarchy.

In the installation guide, this directory would be /mnt. For an existing installation, you need to mount existing partitions into /mnt yourself:

Run lsblk and note the partition layout of your installation. It will be usually something like /dev/sdXY or if you have an NVMe drive /dev/nvme0nXpY.

Mount the file system:

If you have an EFI system partition and need to make changes in it (e.g. updating the vmlinuz or initramfs images):

If you have any discrete partitions, mount them too.

In the following examples, /path/to/new/root is the directory where the new root resides (e.g. /mnt).

There are two main options for using chroot, described below.

The bash script arch-chroot(8) is part of the arch-install-scripts package. arch-chroot wraps the chroot(1) command while ensuring that important functionality is available, e.g. mounting /dev, /proc and other API filesystems, or exposing /etc/resolv.conf to the chroot.

Run arch-chroot with the new root directory as first argument:

You can now do most of the operations available from your existing installation. Some tasks which needs D-Bus will not work as noted in #Usage.

To exit the chroot, use:

To run a command from the chroot and exit again, append the command to the end of the line:

For example, to run mkinitcpio -p linux for a chroot located at /mnt/arch, do:

On a Btrfs root file system with subvolumes, you have to make sure that all subvolumes are properly mounted as specified in fstab before entering chroot.

An example with the Btrfs default setup from archinstall:

If you run chroot directly, below steps are needed before actual chroot.

First, mount the temporary API filesystems:

If you are running a UEFI system, you will also need access to EFI variables. Otherwise, when installing GRUB, you will receive a message similar to: UEFI variables not supported on this machine:

Next, in order to use an internet connection in the chroot environment, copy over the DNS details:

Finally, to change root into /path/to/new/root using a bash shell:

After chrooting, it may be necessary to load the local Bash configuration:

When finished with the chroot, you can exit it via:

Then unmount the temporary file systems:

If you have an X server running on your system, you can start graphical applications from the chroot environment.

To allow the chroot environment to connect to an X server, open a virtual terminal inside the X server (i.e. inside the desktop of the user that is currently logged in), then run the xhost command, which gives permission to anyone to connect to the user's X server (see also Xhost):

Then, to direct the applications to the X server from chroot, set the DISPLAY environment variable inside the chroot to match the DISPLAY variable of the user that owns the X server. So for example, run:

as the user that owns the X server to see the value of DISPLAY. If the value is ":0" (for example), then run the following in the chroot environment:

Chroot requires root privileges, which may not be desirable or possible for the user to obtain in certain situations. There are, however, various ways to simulate chroot-like behavior using alternative implementations.

PRoot may be used to change the apparent root directory and use mount --bind without root privileges. This is useful for confining applications to a single directory or running programs built for a different CPU architecture, but it has limitations due to the fact that all files are owned by the user on the host system. PRoot provides a --root-id argument that can be used as a workaround for some of these limitations in a similar (albeit more limited) manner to fakeroot.

fakechroot is a library shim which intercepts the chroot call and fakes the results. It can be used in conjunction with fakeroot to simulate a chroot as a regular user.

unshare(1), part of util-linux, can be used to create a new kernel namespace. This works with the usual chroot command. For example:

systemd-detect-virt --chroot detect whether invoked in a chroot environment. See systemd-detect-virt(1) for detection of other virtualized environments. For a broader discussion, and usage of traditional tools, see how-do-i-tell-im-running-in-a-chroot.

Upon executing arch-chroot /path/to/new/root, a warning is issued:

See arch-chroot(8) for an explanation and an example of using bind mounting to make the chroot directory a mountpoint.

**Examples:**

Example 1 (unknown):
```unknown
# swapon /dev/sdxY
```

Example 2 (unknown):
```unknown
/dev/nvme0nXpY
```

Example 3 (unknown):
```unknown
# mount /dev/sdXY /mnt
```

Example 4 (unknown):
```unknown
# mount /dev/sdXZ /mnt/esp
```

---

## Core utilities

**URL:** https://wiki.archlinux.org/title/Core_utilities

**Contents:**
- Essentials
  - Preventing data loss
- Nonessentials
- Alternatives
  - cat alternatives
  - cd alternatives
  - date alternatives
  - cp alternatives
  - ls alternatives
  - find alternatives

Core utilities are the basic, fundamental tools of a GNU/Linux system. This article provides an incomplete overview of them, links their documentation and describes useful alternatives. The scope of this article includes, but is not limited to, the GNU Core Utilities. Most core utilities are traditional Unix tools and many were standardized by POSIX but have been developed further to provide more features.

Most command-line interfaces are documented in man pages, utilities by the GNU Project are documented primarily in Info manuals, some shells provide a help command for shell builtin commands. Additionally most utilities print their usage when run with the --help flag.

The following table lists some important utilities which Arch Linux users should be familiar with. See also intro(1).

rm, mv, cp and shell redirections happily delete or overwrite files without asking. rm, mv, and cp all support the -i flag to prompt the user before every removal / overwrite. Some users like to enable the -i flag by default using aliases. Relying upon these shell options can be dangerous, because you get used to them, resulting in potential data loss when you use another system or user that does not have them. The best way to prevent data loss is to create backups.

This table lists core utilities that often come in handy.

The moreutils package provides useful tools like sponge(1) that are missing from the GNU coreutils.

Alternative core utilities are provided by the following packages:

See also Bash#Auto "cd" when entering just a path and Zsh#Remembering recent directories.

This article or section is a candidate for moving to List of applications/Other.

Using rsync#As cp/mv alternative allows you to resume a failed transfer, to show the transfer status, to skip already existing files and to make sure of the destination files integrity using checksums.

For graphical file searchers, see List of applications/Utilities#File searching.

While diffutils does not provide a word-wise diff, several other programs do:

See also List of applications/Utilities#Comparison, diff, merge.

These tools aim to replace grep for code search. They do recursive search by default, skip binary files and respect .gitignore.

This article or section needs expansion.

See also: dd and ddrescue

This subsection lists dd implementations whose interface and default behaviour is mostly compliant with the POSIX specification of dd(1p).

The GNU implementation of dd found in coreutils also conforms to POSIX. This subsection lists its forks.

This subsection lists dd alternatives that do not conform to POSIX (in terms of the JCL-resembling command-line syntax and default behaviour).

This subsection lists forks of bufferAUR, a general-purpose I/O buffering utility similar to dd but has a dynamic-sized buffer. It supports blockwise I/O and can be used when dumping from/to an LTO-tape to avoid shoe shining.

See also List of applications/Utilities#Disk usage display.

Many common packages already install most popular POSIX utilities as dependencies, but the posix metapackage can be installed to ensure all of them being always present.

Beside mandatory utilities, there are also metapackages for some of the optional categories:

Some commands (arch, kill, etc.) are missing from coreutils or taken from other packages. To complete them for compatibility, install uutils-coreutils and do:

**Examples:**

Example 1 (unknown):
```unknown
--color-words
```

Example 2 (unknown):
```unknown
# ln -sf /usr/bin/uu-coreutils /usr/local/bin/arch
# echo -e "#compdef arch=uu-arch\n_uu-arch" > /usr/local/share/zsh/site-functions/_arch
# echo "complete -c arch -w uu-arch" > /usr/local/share/fish/vendor_completions.d/arch.fish
```

---

## Limine

**URL:** https://wiki.archlinux.org/title/Limine

**Contents:**
- Supported file systems
- Installation
- Deploying the boot loader
  - UEFI systems
  - BIOS systems with MBR
  - BIOS systems with GPT
  - UEFI + BIOS bootable drives
- Configuration
  - Memtest86+
  - Windows entry (UEFI)

Limine is an advanced, portable, multiprotocol boot loader originally developed as the reference implementation for the Limine boot protocol, but also supporting the ability to boot Linux as well as to chainload other boot loaders.

Limine supports FAT12, FAT16, FAT32 and ISO9660. The list of supported file systems is intentionally limited per Limine's design philosophy.

Install the limine package.

Follow the instructions in #Deploying the boot loader and #Configuration.

Deploying Limine on UEFI systems involves copying the /usr/share/limine/BOOTX64.EFI file to the EFI system partition, and to make the UEFI BIOS aware of it.

Limine does not add an entry for the boot loader in the NVRAM automatically. Use efibootmgr to setup an entry for Limine.

To do so, one can do the following:

Deploying Limine on BIOS systems involves copying the /usr/share/limine/limine-bios.sys file, which contains stage 3 code that Limine needs to boot, to either the root, a /boot, a /limine, or a /boot/limine directory of any partition on the disk onto which Limine will be deployed, as long as the filesystem is supported. This usually means having to use a FAT partition for /boot, and copying the limine-bios.sys file to /boot/limine.

Then stage 1 and 2 need to be deployed to the disk:

/dev/sdX is the disk (not a partition) where Limine is to be installed. For example /dev/sda or /dev/nvme0n1. This has to be the disk hosting the /boot partition. See Device file#Block device names for a description of the block device naming scheme.

To deploy Limine for BIOS booting from a GPT partitioned disk you need to specify a GPT partition in which to store the stage 2 boot loader (in addition to a FAT32 partition mounted on either /boot, /limine or /boot/limine as explained in previous section). The partition must be at least 32 KiB in size, must not be formatted with a file system or mounted.

Create a mebibyte partition (+1M with fdisk or gdisk) on the disk with no file system and with partition type GUID 21686148-6449-6E6F-744E-656564454649.

Then deploy stage 1 and 2 to the disk, optionally specifying the 1-based number of the BIOS boot partition:

If the partition number is left out, limine bios-install will try to detect it automatically.

Like the MBR case, you must copy limine-bios.sys file (and your configuration limine.conf to either the root, a /boot, a /limine, or a /boot/limine directory of any partition on the disk other than the one used to deploy the stage 2 bootloader before calling limine bios-install.

As long as a drive is MBR formatted, and it contains an EFI system partition (which can be the same as the /boot partition used for BIOS systems), it is possible to follow both the BIOS and UEFI deployment procedures in order to create a drive capable of booting on both legacy BIOS as well as UEFI systems. This is useful, for example, for installing an operating system on a USB flash drive which is to be used on multiple systems which may or may not support UEFI, or to ease moving hard drives across systems.

limine does not ship a default configuration file, it is therefore necessary to create one. This file is necessary to teach Limine which operating systems are available for boot. The configuration file has a lot of options as Limine allows for a fair degree of customisation. A detailed documentation of the configuration file, its format, and its options can be found here.

The configuration file needs to reside on either the root, a /boot, a /limine, or a /boot/limine directory of a partition on the drive on which Limine is deployed, as long as the file system of said partition is supported. For UEFI systems, it may also reside on esp/EFI/BOOT or on esp/EFI/limine (the recommended location). The configuration file has to be named limine.conf.

Here follows a simple example configuration that contains 1 boot menu entry that describes a typical Arch Linux kernel and initramfs:

xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx is the root file system's UUID.

In case the /boot partition, where the kernel and initramfs are, and the partition of the limine.conf file do not match (such as, for example, on UEFI systems with an extra /boot partition which is not the same as the ESP, and limine.conf is placed on the ESP), it may be necessary to replace boot():/ in the configuration file with uuid(xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx):/, where xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx represents the PARTUUID of the /boot FAT partition.

To prevent this you should first embed the b2sum checksum of every resource file in the configuration. Limine file paths have an optional field containing the b2sum checksum of the file. This field can be specified by appending the # character and then the 128 characters checksum:

To protect the config file you should embed its b2sum in the EFI executable with limine enroll-config command.

Add one of the following entries to the configuration file.

For UEFI, install memtest86+-efi and add:

For BIOS, install memtest86+ and add:

Add the following to the configuration:

Alternatively replace boot():/ with uuid(xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx):/, where xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx is the PARTUUID of the ESP, if limine.conf is not on the ESP.

While not mandatory, it may be useful to set up a pacman hook to deploy Limine whenever it is upgraded.

To automate kernel integration (initramfs or UKI) with Limine, install:

Both tools include pacman hooks for automatically handling kernel entries.

Some non-compliant UEFI motherboards (e.g., certain MSI boards) have non-standard or broken EFI implementations. They may not work with efibootmgr or kernel-based UEFI detection. To skip UEFI detection and registration and set Limine as the fallback bootloader at the standard EFI path esp/EFI/BOOT/BOOTX64.EFI, run:

After that, open BIOS setup and change boot order to make the standard EFI path the default boot option.

(Optional) Copy /etc/limine-entry-tool.conf to /etc/default/limine if not present.

Edit /etc/default/limine:

For more configuration options, refer to limine-entry-tool README

If you do not use an initramfs tool or use a different one like booster, limine-entry-toolAUR does not provide initramfs generation and does not include automatic pacman hooks for kernel management. You are free to create custom scripts using the limine-entry-tool command with your desired options to manage (add, update, and remove) boot entries.

limine-snapper-syncAUR tool provides integration between Snapper and the Limine boot loader. It is useful for:

For dracut, OverlayFS works out of the box in limine-dracut-supportAUR.

For mkinitcpio, manually add the btrfs-overlayfs or sd-btrfs-overlayfs hook after the filesystems hook. Both are provided by limine-mkinitcpio-hookAUR. With systemd hooks, use sd-btrfs-overlayfs, as btrfs-overlayfs is incompatible.

The directory path /usr/lib/modules/* must be in the same root subvolume as /, because a selected snapshot requires its matching kernel modules from this path during boot. Otherwise, the snapshot will fail to boot.

Use limine-dracut-support or limine-mkinitcpio-hook (see #Boot entry automation) to automatically update kernel boot entries in esp/limine.conf whenever kernels are installed, updated, or removed. This allows you to skip the first configuration step below.

1. Configure esp/limine.conf to include either the //Snapshots or /Snapshots keyword for auto-generated snapshot entries.

2. (Optional) Copy any configurations from /etc/limine-snapper-sync.conf to /etc/default/limine if they are not already present.

Edit /etc/default/limine, which will override /etc/limine-snapper-sync.conf:

3. Run the command to check if it succeeds or shows an error message:

This error can be safely ignored if you do not use them. Alternatively, to prevent this error, edit /etc/limine-snapper-sync.conf to remove two lines:

4. If everything works, then enable limine-snapper-sync.service to automatically synchronize boot entries with the Snapper snapshot list.

5. (Optional) Install snap-pac. It triggers Snapper to create snapshots during system updates, which limine-snapper-sync then synchronizes to generate related snapshot entries in Limine.

For further details and additional configuration options, refer to limine-snapper-sync README.

Having problems? Check out the troubleshooting guide in the README

**Examples:**

Example 1 (unknown):
```unknown
BOOTX64.EFI
```

Example 2 (unknown):
```unknown
BOOTIA32.EFI
```

Example 3 (unknown):
```unknown
/usr/share/limine/BOOTX64.EFI
```

Example 4 (unknown):
```unknown
# mkdir -p esp/EFI/limine
# cp /usr/share/limine/BOOTX64.EFI esp/EFI/limine/
```

---

## System time

**URL:** https://wiki.archlinux.org/title/Time_zone

**Contents:**
- Time standard
- Hardware clock
  - Read hardware clock
  - Set hardware clock from system clock
  - Automatic syncing
- System clock
  - Read clock
  - Set system clock
- Multiple systems
  - UTC in Microsoft Windows

This article or section needs expansion.

In an operating system, the time (clock) is determined by three parts: time value, whether it is local time or UTC or something else, time zone, and Daylight Saving Time (DST) if applicable. This article explains what they are and how to read/set them. Two clocks are present on systems: a hardware clock and a system clock which are also detailed in this article.

Standard behavior of most operating systems is:

There are two time standards: localtime and Coordinated Universal Time (UTC). The localtime standard is dependent on the current time zone, while UTC is the global time standard and is independent of time zone values. Though conceptually different, UTC is also known as GMT (Greenwich Mean Time).

The standard used by the hardware clock (CMOS clock, the BIOS time) is set by the operating system. By default, Windows uses localtime, macOS uses UTC, other UNIX and UNIX-like systems vary. An OS that uses the UTC standard will generally consider the hardware clock as UTC and make an adjustment to it to set the OS time at boot according to the time zone.

The hardware clock (a.k.a. the Real Time Clock (RTC) or CMOS clock) stores the values of: Year, Month, Day, Hour, Minute, and Seconds. A UEFI firmware has the additional ability to store the timezone, and whether DST is used.

The following sets the hardware clock from the system clock. Additionally it updates /etc/adjtime or creates it if not present. See hwclock(8) § The Adjtime File for more information on this file as well as the #Time skew section.

By default, Arch Linux kernels have a feature enabled where the hardware clock is synchronized to the system clock every 11 minutes. You can see if this is enabled on your kernel as follows:

The first synchronization happens at boot time. What this means is that if your hardware clock is extremely out of date (for example, a CMOS battery failure has reset the clock to the year 2000) then for the first 11 minutes after boot anything which requires a reasonably accurate time will give an error - including SSL, uses the Online Certificate Status Protocol (OCSP). A web browser running on your computer typically sends the hardware clock time in its requests to websites, and a time which is too far out will result in the browser refusing to connect because of an OCSP error.

The system clock (a.k.a. the software clock) keeps track of: time, time zone, and DST if applicable. It is calculated by the Linux kernel as the number of seconds since midnight January 1st 1970, UTC. The initial value of the system clock is calculated from the hardware clock, dependent on the contents of /etc/adjtime. After boot-up has completed, the system clock runs independently of the hardware clock. The Linux kernel keeps track of the system clock by counting timer interrupts.

To check the current system clock time (presented both in local time and UTC) as well as the RTC (hardware clock):

To set the local time of the system clock directly:

sets the time to May 26th, year 2014, 11:13 and 54 seconds.

If multiple operating systems are installed on a machine, they will all derive the current time from the same hardware clock: it is recommended to set it to UTC to avoid conflicts across systems. Otherwise, if the hardware clock is set to localtime, more than one operating system may adjust it after a DST change for example, thus resulting in an over-correction; problems may also arise when traveling between different time zones and using one of the operating systems to reset the system/hardware clock.

The hardware clock can be queried and set with the timedatectl command. You can see the current hardware clock time standard of the Arch system using:

To change the hardware clock time standard to localtime, use:

To revert to the hardware clock being in UTC, type:

These generate /etc/adjtime automatically and update the RTC accordingly; no further configuration is required.

During kernel startup, at the point when the RTC driver is loaded, the system clock may be set from the hardware clock. Whether this occurs depends on the hardware platform, the version of the kernel and kernel build options. If this does occur, at this point in the boot sequence, the hardware clock time is assumed to be UTC and the value of /sys/class/rtc/rtcN/hctosys (N=0,1,2,..) will be set to 1.

Later, the system clock is set again from the hardware clock by systemd, dependent on values in /etc/adjtime. Hence, having the hardware clock using localtime may cause some unexpected behavior during the boot sequence; e.g system time going backwards, which is always a bad idea (there is a lot more to it). Since systemd version 216, when the RTC is configured to the local time (rather than UTC) systemd will never synchronize back to it, as this might confuse Windows at a later boot. And systemd will no longer inform the kernel about the current timezone. This hence means FAT timestamps will be always considered UTC[1].

To dual boot with Windows, it is recommended to configure Windows to use UTC, rather than Linux to use localtime. (Windows by default uses localtime [2].)

It can be done by a simple registry fix: Open regedit and add a DWORD value with hexadecimal value 1 to the registry HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Control\TimeZoneInformation\RealTimeIsUniversal

You can do this from an Administrator Command Prompt running:

Alternatively, create a *.reg file (on the desktop) with the following content and double-click it to import it into registry:

Should Windows ask to update the clock due to DST changes, let it. It will leave the clock in UTC as expected, only correcting the displayed time.

The #Hardware clock and #System clock time may need to be updated after setting this value.

If you are having issues with the offset of the time, try reinstalling tzdata and then setting your time zone again:

Many Linux distributions have the hardware clock set to be interpreted as in "localtime" if Windows was detected on any disk during their installation. This is apparently done deliberately to allow new users to try out Linux on their Windows computers without editing the registry.

For changing this behavior, see above.

If you use an NTP client (see #Time synchronization below) that keeps track of RTC drift on any system, you should disable time synchronization on all but one system. Otherwise the NTP clients would be unaware of each other's adjustment and make grossly incorrect estimates of the RTC drift.

For Windows, go to the Date and time settings and uncheck the time sync option. You can also run w32tm /unregister as an administrator to unregister the time-sync service: Active Directory machines are known to ignore the synchronization settings and perform a synchronization anyways to prevent replay attacks. The Windows clock synchronization routine is quite inaccurate to start with, requiring even extra work to reach one-second accuracy, so disabling it should not be much of a loss.

To check the current zone defined for the system:

To list available zones:

To set your time zone:

This will create an /etc/localtime symlink that points to a zoneinfo file under /usr/share/zoneinfo/. In case you choose to create the link manually (for example during chroot where timedatectl will not work), keep in mind that it must be a symbolic link, as specified in localtime(5) § DESCRIPTION:

See timedatectl(1) and localtime(5) for details.

To set the timezone automatically based on the IP address location, one can use a geolocation API to retrieve the timezone, for example curl https://ipapi.co/timezone, and pass the output to timedatectl set-timezone for automatic setting. Some geo-IP APIs that provide free or partly free services are listed below:

See NetworkManager#Automatically set the timezone.

Every clock has a value that differs from real time (the best representation of which being International Atomic Time); no clock is perfect. A quartz-based electronic clock keeps imperfect time, but maintains a consistent inaccuracy. This base 'inaccuracy' is known as 'time skew' or 'time drift'.

When the hardware clock is set with hwclock, a new drift value is calculated in seconds per day. The drift value is calculated by using the difference between the new value set and the hardware clock value just before the set, taking into account the value of the previous drift value and the last time the hardware clock was set. The new drift value and the time when the clock was set is written to the file /etc/adjtime overwriting the previous values. The hardware clock can therefore be adjusted for drift when the command hwclock --adjust is run; this also occurs on shutdown but only if the hwclock daemon is enabled, hence for Arch systems which use systemd, this does not happen.

If the hardware clock keeps losing or gaining time in large increments, it is possible that an invalid drift has been recorded (but only applicable, if the hwclock daemon is running). This can happen if you have set the hardware clock time incorrectly or your time standard is not synchronized with a Windows or macOS install. The drift value can be removed by first removing the file /etc/adjtime, then setting the correct hardware clock and system clock time. You should then check if your time standard is correct.

The software clock is very accurate but like most clocks is not perfectly accurate and will drift as well. Though rarely, the system clock can lose accuracy if the kernel skips interrupts. There are some tools to improve software clock accuracy:

The Network Time Protocol (NTP) is a protocol for synchronizing the clocks of computer systems over packet-switched, variable-latency data networks.

For proper NTP support, as defined by the RFC, a client must be able to merge time from multiple servers, compensate for delay, and keep track of drift on the system (software) clock. The following are implementations of NTP available for Arch Linux:

Anything that does less than a proper NTP node is considered Simple Network Time Protocol (SNTP). A basic SNTP client may simply fetch the time from a single server and set it immediately, without keeping track of long-term drifts. SNTP provides lower accuracy, but takes less resources. The accuracy is usually good enough for desktop users and embedded workloads, but unacceptable for NTP servers. The following implement SNTP:

For some use cases it may be useful to change the time settings without touching the global system values. For example to test applications relying on the time during development or adjusting the system time zone when logging into a server remotely from another zone.

To make an application "see" a different date/time than the system one, you can use the faketime(1) utility (from libfaketime).

If instead you want an application to "see" a different time zone than the system one, set the TZ environment variable, for example:

This is different than just setting the time, as for example it allows to test the behavior of a program with positive or negative UTC offset values, or the effects of DST changes when developing on systems in a non-DST time zone.

Another use case is having different time zones set for different users of the same system: this can be accomplished by setting the TZ variable in the shell's configuration file, see Environment variables#Defining variables.

alarm-fake-hwclock designed especially for system without battery backed up RTC, it includes a systemd service which on shutdown saves the current time and on startup restores the saved time, thus avoiding strange time travel errors.

Install fake-hwclock-gitAUR, start/enable the service fake-hwclock.service.

Virtual machine guests may obtain time from the host machine using the PTP (Precision Time Protocol) /dev/ptp0 interface. The interface is more accurate compared to using NTP over IP between the host and guest.

chrony and ntpd can each use the virtual-PTP device to sync the time between guest and host, by configuring the device as if it is a real PTP reference clock.

This might be caused by a number of reasons. For example, if your hardware clock is running on local time, but timedatectl is set to assume it is in UTC, the result would be that your timezone's offset to UTC effectively gets applied twice, resulting in wrong values for your local time and UTC.

To force your clock to the correct time, and to also write the correct UTC to your hardware clock, follow these steps:

**Examples:**

Example 1 (unknown):
```unknown
# hwclock --show
```

Example 2 (unknown):
```unknown
/etc/adjtime
```

Example 3 (unknown):
```unknown
# hwclock --systohc
```

Example 4 (unknown):
```unknown
$ zgrep CMOS /proc/config.gz
```

---

## System time

**URL:** https://wiki.archlinux.org/title/Time_synchronization

**Contents:**
- Time standard
- Hardware clock
  - Read hardware clock
  - Set hardware clock from system clock
  - Automatic syncing
- System clock
  - Read clock
  - Set system clock
- Multiple systems
  - UTC in Microsoft Windows

This article or section needs expansion.

In an operating system, the time (clock) is determined by three parts: time value, whether it is local time or UTC or something else, time zone, and Daylight Saving Time (DST) if applicable. This article explains what they are and how to read/set them. Two clocks are present on systems: a hardware clock and a system clock which are also detailed in this article.

Standard behavior of most operating systems is:

There are two time standards: localtime and Coordinated Universal Time (UTC). The localtime standard is dependent on the current time zone, while UTC is the global time standard and is independent of time zone values. Though conceptually different, UTC is also known as GMT (Greenwich Mean Time).

The standard used by the hardware clock (CMOS clock, the BIOS time) is set by the operating system. By default, Windows uses localtime, macOS uses UTC, other UNIX and UNIX-like systems vary. An OS that uses the UTC standard will generally consider the hardware clock as UTC and make an adjustment to it to set the OS time at boot according to the time zone.

The hardware clock (a.k.a. the Real Time Clock (RTC) or CMOS clock) stores the values of: Year, Month, Day, Hour, Minute, and Seconds. A UEFI firmware has the additional ability to store the timezone, and whether DST is used.

The following sets the hardware clock from the system clock. Additionally it updates /etc/adjtime or creates it if not present. See hwclock(8) § The Adjtime File for more information on this file as well as the #Time skew section.

By default, Arch Linux kernels have a feature enabled where the hardware clock is synchronized to the system clock every 11 minutes. You can see if this is enabled on your kernel as follows:

The first synchronization happens at boot time. What this means is that if your hardware clock is extremely out of date (for example, a CMOS battery failure has reset the clock to the year 2000) then for the first 11 minutes after boot anything which requires a reasonably accurate time will give an error - including SSL, uses the Online Certificate Status Protocol (OCSP). A web browser running on your computer typically sends the hardware clock time in its requests to websites, and a time which is too far out will result in the browser refusing to connect because of an OCSP error.

The system clock (a.k.a. the software clock) keeps track of: time, time zone, and DST if applicable. It is calculated by the Linux kernel as the number of seconds since midnight January 1st 1970, UTC. The initial value of the system clock is calculated from the hardware clock, dependent on the contents of /etc/adjtime. After boot-up has completed, the system clock runs independently of the hardware clock. The Linux kernel keeps track of the system clock by counting timer interrupts.

To check the current system clock time (presented both in local time and UTC) as well as the RTC (hardware clock):

To set the local time of the system clock directly:

sets the time to May 26th, year 2014, 11:13 and 54 seconds.

If multiple operating systems are installed on a machine, they will all derive the current time from the same hardware clock: it is recommended to set it to UTC to avoid conflicts across systems. Otherwise, if the hardware clock is set to localtime, more than one operating system may adjust it after a DST change for example, thus resulting in an over-correction; problems may also arise when traveling between different time zones and using one of the operating systems to reset the system/hardware clock.

The hardware clock can be queried and set with the timedatectl command. You can see the current hardware clock time standard of the Arch system using:

To change the hardware clock time standard to localtime, use:

To revert to the hardware clock being in UTC, type:

These generate /etc/adjtime automatically and update the RTC accordingly; no further configuration is required.

During kernel startup, at the point when the RTC driver is loaded, the system clock may be set from the hardware clock. Whether this occurs depends on the hardware platform, the version of the kernel and kernel build options. If this does occur, at this point in the boot sequence, the hardware clock time is assumed to be UTC and the value of /sys/class/rtc/rtcN/hctosys (N=0,1,2,..) will be set to 1.

Later, the system clock is set again from the hardware clock by systemd, dependent on values in /etc/adjtime. Hence, having the hardware clock using localtime may cause some unexpected behavior during the boot sequence; e.g system time going backwards, which is always a bad idea (there is a lot more to it). Since systemd version 216, when the RTC is configured to the local time (rather than UTC) systemd will never synchronize back to it, as this might confuse Windows at a later boot. And systemd will no longer inform the kernel about the current timezone. This hence means FAT timestamps will be always considered UTC[1].

To dual boot with Windows, it is recommended to configure Windows to use UTC, rather than Linux to use localtime. (Windows by default uses localtime [2].)

It can be done by a simple registry fix: Open regedit and add a DWORD value with hexadecimal value 1 to the registry HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Control\TimeZoneInformation\RealTimeIsUniversal

You can do this from an Administrator Command Prompt running:

Alternatively, create a *.reg file (on the desktop) with the following content and double-click it to import it into registry:

Should Windows ask to update the clock due to DST changes, let it. It will leave the clock in UTC as expected, only correcting the displayed time.

The #Hardware clock and #System clock time may need to be updated after setting this value.

If you are having issues with the offset of the time, try reinstalling tzdata and then setting your time zone again:

Many Linux distributions have the hardware clock set to be interpreted as in "localtime" if Windows was detected on any disk during their installation. This is apparently done deliberately to allow new users to try out Linux on their Windows computers without editing the registry.

For changing this behavior, see above.

If you use an NTP client (see #Time synchronization below) that keeps track of RTC drift on any system, you should disable time synchronization on all but one system. Otherwise the NTP clients would be unaware of each other's adjustment and make grossly incorrect estimates of the RTC drift.

For Windows, go to the Date and time settings and uncheck the time sync option. You can also run w32tm /unregister as an administrator to unregister the time-sync service: Active Directory machines are known to ignore the synchronization settings and perform a synchronization anyways to prevent replay attacks. The Windows clock synchronization routine is quite inaccurate to start with, requiring even extra work to reach one-second accuracy, so disabling it should not be much of a loss.

To check the current zone defined for the system:

To list available zones:

To set your time zone:

This will create an /etc/localtime symlink that points to a zoneinfo file under /usr/share/zoneinfo/. In case you choose to create the link manually (for example during chroot where timedatectl will not work), keep in mind that it must be a symbolic link, as specified in localtime(5) § DESCRIPTION:

See timedatectl(1) and localtime(5) for details.

To set the timezone automatically based on the IP address location, one can use a geolocation API to retrieve the timezone, for example curl https://ipapi.co/timezone, and pass the output to timedatectl set-timezone for automatic setting. Some geo-IP APIs that provide free or partly free services are listed below:

See NetworkManager#Automatically set the timezone.

Every clock has a value that differs from real time (the best representation of which being International Atomic Time); no clock is perfect. A quartz-based electronic clock keeps imperfect time, but maintains a consistent inaccuracy. This base 'inaccuracy' is known as 'time skew' or 'time drift'.

When the hardware clock is set with hwclock, a new drift value is calculated in seconds per day. The drift value is calculated by using the difference between the new value set and the hardware clock value just before the set, taking into account the value of the previous drift value and the last time the hardware clock was set. The new drift value and the time when the clock was set is written to the file /etc/adjtime overwriting the previous values. The hardware clock can therefore be adjusted for drift when the command hwclock --adjust is run; this also occurs on shutdown but only if the hwclock daemon is enabled, hence for Arch systems which use systemd, this does not happen.

If the hardware clock keeps losing or gaining time in large increments, it is possible that an invalid drift has been recorded (but only applicable, if the hwclock daemon is running). This can happen if you have set the hardware clock time incorrectly or your time standard is not synchronized with a Windows or macOS install. The drift value can be removed by first removing the file /etc/adjtime, then setting the correct hardware clock and system clock time. You should then check if your time standard is correct.

The software clock is very accurate but like most clocks is not perfectly accurate and will drift as well. Though rarely, the system clock can lose accuracy if the kernel skips interrupts. There are some tools to improve software clock accuracy:

The Network Time Protocol (NTP) is a protocol for synchronizing the clocks of computer systems over packet-switched, variable-latency data networks.

For proper NTP support, as defined by the RFC, a client must be able to merge time from multiple servers, compensate for delay, and keep track of drift on the system (software) clock. The following are implementations of NTP available for Arch Linux:

Anything that does less than a proper NTP node is considered Simple Network Time Protocol (SNTP). A basic SNTP client may simply fetch the time from a single server and set it immediately, without keeping track of long-term drifts. SNTP provides lower accuracy, but takes less resources. The accuracy is usually good enough for desktop users and embedded workloads, but unacceptable for NTP servers. The following implement SNTP:

For some use cases it may be useful to change the time settings without touching the global system values. For example to test applications relying on the time during development or adjusting the system time zone when logging into a server remotely from another zone.

To make an application "see" a different date/time than the system one, you can use the faketime(1) utility (from libfaketime).

If instead you want an application to "see" a different time zone than the system one, set the TZ environment variable, for example:

This is different than just setting the time, as for example it allows to test the behavior of a program with positive or negative UTC offset values, or the effects of DST changes when developing on systems in a non-DST time zone.

Another use case is having different time zones set for different users of the same system: this can be accomplished by setting the TZ variable in the shell's configuration file, see Environment variables#Defining variables.

alarm-fake-hwclock designed especially for system without battery backed up RTC, it includes a systemd service which on shutdown saves the current time and on startup restores the saved time, thus avoiding strange time travel errors.

Install fake-hwclock-gitAUR, start/enable the service fake-hwclock.service.

Virtual machine guests may obtain time from the host machine using the PTP (Precision Time Protocol) /dev/ptp0 interface. The interface is more accurate compared to using NTP over IP between the host and guest.

chrony and ntpd can each use the virtual-PTP device to sync the time between guest and host, by configuring the device as if it is a real PTP reference clock.

This might be caused by a number of reasons. For example, if your hardware clock is running on local time, but timedatectl is set to assume it is in UTC, the result would be that your timezone's offset to UTC effectively gets applied twice, resulting in wrong values for your local time and UTC.

To force your clock to the correct time, and to also write the correct UTC to your hardware clock, follow these steps:

**Examples:**

Example 1 (unknown):
```unknown
# hwclock --show
```

Example 2 (unknown):
```unknown
/etc/adjtime
```

Example 3 (unknown):
```unknown
# hwclock --systohc
```

Example 4 (unknown):
```unknown
$ zgrep CMOS /proc/config.gz
```

---

## Xen

**URL:** https://wiki.archlinux.org/title/Xen

**Contents:**
- Installation
  - System requirements
  - Installation of the Xen Hypervisor
    - Building xen
  - Modification of the boot loader
    - UEFI
      - systemd-boot
      - EFI boot stub
    - BIOS
      - GRUB

This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.

The Xen hypervisor is a thin layer of software which emulates a computer architecture allowing multiple operating systems to run simultaneously. The hypervisor is started by the boot loader of the computer it is installed on. Once the hypervisor is loaded, it starts the dom0—short for "domain 0", sometimes called the host or privileged domain—which in our case runs Arch Linux. Once the dom0 has started, one or more domU (short for user domains, sometimes called VMs or guests) can be started and controlled from the dom0. For the domU, Xen supports paravirtualized (PV) domains, hardware virtualized domains (HVM), and paravirtualised domains inside a hardware virtualization wrapper (PVH). See Xen.org for a full overview.

The Xen hypervisor relies on a full install of the base operating system. Before attempting to install the Xen hypervisor, the host machine should have a fully operational and up-to-date install of Arch Linux. This installation can be a minimal install with only the base package and does not require a Desktop environment or even Xorg.

If you are building a new host from scratch, see the Installation guide for instructions on installing Arch Linux.

The Xen hypervisor requires kernel-level support which is included in recent Linux kernels and is built into the linux and linux-lts Arch kernel packages. To run HVM domU, the physical hardware must have either Intel VT-x or AMD-V (SVM) virtualization support. In order to verify this, check for the vmx or svm CPU flags when the Xen hypervisor is not running:

If the above command does not produce output, then hardware virtualization support is unavailable and your hardware is unable to run HVM domU (or you are already running the Xen hypervisor). If you believe the CPU supports one of these features you should access the host system's BIOS configuration menu during the boot process and look if options related to virtualization support have been disabled. If such an option exists and is disabled, then enable it, boot the system and repeat the above command. The Xen hypervisor also supports PCI passthrough where PCI devices can be passed directly to the domU even in the absence of dom0 support for the device. In order to use PCI passthrough, the CPU must support IOMMU/VT-d.

To install the Xen hypervisor, install the xenAUR package. It provides the Xen hypervisor, current xl interface and all configuration and support files, including systemd services. To run most VMs, you will also need to install xen-qemuAUR.

For BIOS support in VMs, install seabios. For UEFI support, install edk2-ovmf. To boot VM-local kernels inside of a PVH VM, install xen-grub-pvhAUR.

It is recommended that xen and its components are built in a clean environment, either in a VM or a chroot. When building Xen, there are environmental variables that can be passed to makepkg.

Pass these arguments to makepkg as variables:

xen-docsAUR will be also built for the man pages and documentation. If you choose to build stubdom support, a xen-stubdom package will be built.

The boot loader must be modified to load a special Xen kernel (xen.gz or in the case of UEFI xen.efi) which is then used to boot the normal kernel. To do this a new boot loader entry is needed.

Xen supports booting from UEFI as specified in Xen EFI systems. It also might be necessary to use efibootmgr to set boot order and other parameters.

First, ensure the xen.efi file is in the EFI system partition along with your kernel and ramdisk files.

Second, Xen requires an ASCII (no UTF-8, UTC-16, etc) configuration file that specifies what kernel should be booted as dom0. This file must be placed in the same EFI system partition as the binary. Xen looks for several configuration files and uses the first one it finds. The order of search starts with the .efi extension of the binary's name replaced by .cfg, then drops trailing name components at ., - and _ until a match is found. Typically, a single file named xen.cfg is used with the system requirements, such as:

Add a new EFI-type loader entry. See systemd-boot#UEFI Shells or other EFI applications for more details. For example:

It is possible to boot an EFI kernel directly from UEFI by using an EFI boot stub.

Drop to the build-in UEFI shell and call the EFI file directly. For example:

Note that a xen.cfg configuration file in the EFI system partition is still required as outlined above. In addition, a different configuration file may be specified with the -cfg=file.cfg parameter. For example:

These additional configuration files must reside in the same directory as the Xen EFI binary and linux stub files.

Xen supports booting from system firmware configured as BIOS.

For GRUB users, install the grub-xen-gitAUR package for booting dom0 as well as building PvGrub2 images for booting user domains.

The file /etc/default/grub can be edited to customize the Xen boot commands. For example, to allocate 512 MiB of RAM to dom0 at boot, modify /etc/default/grub by replacing the line:

More information on GRUB configuration keys for Xen can be found in the GRUB documentation.

After customizing the options, update the boot loader configuration with the following command:

Besides the usual platform targets, the grub-xen-gitAUR package builds GRUB for three additional targets that can be used to boot Xen guests: i386-xen, i386-xen_pvh, and x86_64-xen. To create a boot image from one of these targets, first create a GRUB configuration file. Depending on your preference, this file can either locate and load a GRUB configuration file in the guest or it could manage more of the boot process from dom0. Assuming all that is needed is to locate and load a configuration file in the guest, add the following to a file,

and then create a GRUB/Tips and tricks#GRUB standalone image that will incorporate that file:

Lastly, add that image as value of the kernel in the domU configuration file (for a 64-bit guest in this example):

More examples of configuring GRUB images for GRUB guests can be found in the Xen Project's PvGrub2 documentation.

For Syslinux users, add a stanza like this to your /boot/syslinux/syslinux.cfg:

where X.Y.Z is your xen version and /dev/sdaX is your root partition.

This also requires mboot.c32 (and libcom32.c32) to be in the same directory as syslinux.cfg. If you do not have mboot.c32 in /boot/syslinux, copy it from:

Xen requires that network communications between domU and the dom0 (and beyond) be set up manually. The use of both DHCP and static addressing is possible, and the choice should be determined by the network topology. Complex setups are possible, see the Networking article on the Xen wiki for details and /etc/xen/scripts for scripts for various networking configurations. A basic bridged network, in which a virtual switch is created in dom0 that every domU is attached to, can be set up by creating a network bridge with the expected name xenbr0.

See Network bridge#Creating a bridge for details.

See systemd-networkd#Bridge interface for details.

This article or section is a candidate for merging with Network_bridge#With_NetworkManager.

Gnome's Network Manager can sometime be troublesome. If following the bridge creation section outlined in the bridges section of the wiki are unclear or do not work, then the following steps may work.

Open the Network Settings and disable the interface you wish to use in your bridge (ex enp5s0). Edit the setting to off and uncheck "connect automatically."

Create a new bridge connection profile by clicking on the "+" symbol in the bottom left of the network settings. Optionally, run:

to bring up the window immediately. Once the window opens, select Bridge.

Click "Add" next to the "Bridged Connections" and select the interface you wished to use in your bridge (ex. Ethernet). Select the device mac address that corresponds to the interface you intend to use and save the settings

If your bridge is going to receive an IP address via DHCP, leave the IPv4/IPv6 sections as they are. If DHCP is not running for this particular connection, make sure to give your bridge an IP address. Needless to say, all connections will fail if an IP address is not assigned to the bridge. If you forget to add the IP address when you first create the bridge, it can always be edited later.

You should see a connection that matches the name of the bridge you just created. Highlight and copy the UUID on that connection, and then run (again as root):

A new connection should appear under the network settings. It may take 30 seconds to a minute. To confirm that it is up and running, run:

to show a list of active bridges.

Reboot. If everything works properly after a reboot (ie. bridge starts automatically), then you are all set.

<optional> In your network settings, remove the connection profile on your bridge interface that does NOT connect to the bridge. This just keeps things from being confusing later on.

The Xen dom0 requires the xenstored.service, xenconsoled.service, xendomains.service and xen-init-dom0.service to be started and possibly enabled.

Reboot your dom0 host and ensure that the Xen kernel boots correctly and that all settings survive a reboot. A properly set up dom0 should report the following when you run xl list as root:

Of course, the Mem, VCPUs and Time columns will be different depending on machine configuration and uptime. The important thing is that dom0 is listed.

In addition to the required steps above, see best practices for running Xen which includes information on allocating a fixed amount of memory and how to dedicate (pin) a CPU core for dom0 use. It also may be beneficial to create a xenfs filesystem mount point by including in /etc/fstab

Review Xen Project Best Practices before using Xen.

Xen supports both paravirtualized (PV) and hardware virtualized (HVM) domU. In the following sections the steps for creating HVM and PV domU running Arch Linux are described. In general, the steps for creating an HVM domU are independent of the domU OS and HVM domU support a wide range of operating systems including Microsoft Windows. To use HVM domU the dom0 hardware must have virtualization support. Paravirtualized domU do not require virtualization support, but instead require modifications to the guest operating system making the installation procedure different for each operating system (see the Guest Install page of the Xen wiki for links to instructions). Some operating systems (e.g., Microsoft Windows) cannot be installed as a PV domU. In general, HVM domU often run slower than PV domU since HVMs run on emulated hardware. While there are some common steps involved in setting up PV and HVM domU, the processes are substantially different. In both cases, for each domU, a "hard disk" will need to be created and a configuration file needs to be written. Additionally, for installation each domU will need access to a copy of the installation ISO stored on the dom0 (see the Download Page to obtain the Arch Linux ISO).

Xen supports a number of different types of "hard disks" including Logical Volumes, raw partitions, and image files. To create a sparse file, that will grow to a maximum of 10GiB, called domU.img, use:

If file IO speed is of greater importance than domain portability, using Logical Volumes or raw partitions may be a better choice.

Xen may present any partition / disk available to the host machine to a domain as either a partition or disk. This means that, for example, an LVM partition on the host can appear as a hard drive (and hold multiple partitions) to a domain. Note that making sub-partitons on a partition will make accessing those partitions on the host machine more difficult. See kpartx(8) for information on how to map out partitions within a partition.

Each domU requires a separate configuration file that is used to create the virtual machine. Full details about the configuration files can be found at the Xen Wiki or the xl.cfg(5) man page. Both HVM and PV domU share some components of the configuration file. These include

The name= is the name by which the xl tools manage the domU and needs to be unique across all domU. The disk= includes information about both the installation media (file:) and the partition created for the domU phy. If an image file is being used instead of a physical partition, the phy: needs to be changed to file:. The vif= defines a network controller. The 00:16:3e MAC block is reserved for Xen domains, so the last three digits of the mac= must be randomly filled in (hex values 0-9 and a-f only).

If a domU should be started on boot, create a symlink to the configuration file in /etc/xen/auto and ensure the xendomains service is set up correctly. Some useful commands for managing domU are:

In order to use HVM domU install the mesa, numactl and bluez-libs packages.

A minimal configuration file for a HVM Arch domU is:

Since HVM machines do not have a console, they can only be connected to via a vncviewer. The configuration file allows for unauthenticated remote access of the domU vncserver and is not suitable for unsecured networks. The vncserver will be available on port 590X, where X is the value of vncdisplay, of the dom0. The domU can be created with:

and its status can be checked with

Once the domU is created, connect to it via the vncserver and install Arch Linux as described in the Installation guide.

A minimal configuration file for a PV Arch domU is:

This file needs to tweaked for your specific use. Most importantly, the archisodevice=UUID=YYYY-mm-dd-HH-MM-SS-00 line must be edited to use the creation date and time of the ISO being used.

Before creating the domU, the installation ISO must be loop-mounted. To do this, ensure the directory /mnt exists and is empty, then run the following command (being sure to fill in the correct ISO path):

Once the ISO is mounted, the domU can be created with:

The "-c" option will enter the domU's console when successfully created. Then you can install Arch Linux as described in the Installation guide, but with the following deviations. The block devices listed in the disks line of the cfg file will show up as /dev/xvd*. Use these devices when partitioning the domU. After installation and before the domU is rebooted, the xen-blkfront, xen-fbfront, xen-netfront, xen-kbdfront modules must be added to Mkinitcpio. Without these modules, the domU will not boot correctly. For booting, it is not necessary to install Grub. Xen has a Python-based grub emulator, so all that is needed to boot is a grub.cfg file: (It may be necessary to create the /boot/grub directory)

This file must be edited to match the UUID of the root partition. From within the domU, run the following command:

Replace all instances of __UUID__ with the real UUID of the root partition (the one that mounts as /).:

Shutdown the domU with the poweroff command. The console will be returned to the hypervisor when the domain is fully shut down, and the domain will no longer appear in the xl domains list. Now the ISO file may be unmounted:

The domU cfg file should now be edited. Delete the kernel =, ramdisk =, and extra = lines and replace them with the following line:

Also remove the ISO disk from the disk = line.

The Arch domU is now set up. It may be started with the same line as before:

Either you have not booted into the Xen system, or xen modules listed in xencommons script are not installed.

Check the guest's kernel is located correctly, check the pv-xxx.cfg file for spelling mistakes (like using initrd instead of ramdisk).

If creating HVM fails with:

You have missed to install numactl.

Press ctrl-d until you get back to a prompt, rebuild its initramfs described.

is caused by /etc/udev/rules.d/xend.rules. Xend is deprecated and not used, so it is safe to remove that file.

**Examples:**

Example 1 (unknown):
```unknown
$ grep -E 'vmx|svm' --color -m 1 /proc/cpuinfo
```

Example 2 (unknown):
```unknown
$ build_stubdom=true efi_dir="/boot/EFI" makepkg
```

Example 3 (unknown):
```unknown
[global]
default=xen

[xen]
options=console=vga iommu=force:true,qinval:true,debug:true loglvl=all noreboot=true reboot=no vga=ask ucode=scan
kernel=vmlinuz-linux root=/dev/sdaX rw add_efi_memmap #earlyprintk=xen
ramdisk=initramfs-linux.img
```

Example 4 (unknown):
```unknown
/boot/loader/entries/10-xen.conf
```

---

## GNOME/Evolution

**URL:** https://wiki.archlinux.org/title/GNOME/Evolution

**Contents:**
- Installation
- IMAP setup
- Alternative IMAP setup
  - OfflineIMAP setup
  - Evolution setup for offlineimap's maildir
- Gmail setup
- Gmail calendar
  - Using a WebDAV calendar
  - Using a Google calendar
- Google contacts

Evolution is an application for managing email, calendars, contacts, tasks, notes, and RSS web feeds. It is the default mail client for GNOME and includes support for IMAP, Microsoft Exchange Server, Novell GroupWise, LDAP, WebDAV, CalDAV, and many other services and protocols.

Install the evolution package. Non-GNOME users should also see #Using Evolution outside of GNOME.

See IMAP+ mail account settings in GNOME Help.

An alternative to letting Evolution connect directly to the IMAP server is to sync the IMAP server to your PC. This costs as much hard-disk space as you have mail, though it is possible to limit the folders synced in this manner (see #OfflineIMAP setup). An additional benefit (primary inspiration for this app) is that you have a full copy of your email, including attachments, on your PC for retrieval, even if on the move without an internet connection.

To set this up, you will need to install the offlineimap package. See https://www.offlineimap.org/ for more information.

OfflineIMAP takes its settings from the ~/.offlineimaprc file, which you will need to create. Most users will be able to use the template file below for a standard IMAP server. See OfflineIMAP for more information.

You will likely need to add additional translations[dead link 2023-04-23—HTTP 404] if you are using Gmail.

For remote Mailserver repository:

For Local repository:

Other examples of nametrans configurations (including those for Courier IMAP servers) can be found at https://www.offlineimap.org/doc/nametrans.html.

You may also be interested in running offlineimap in the background.

See Maildir Format Mail Directories account settings[dead link 2024-12-15—HTTP 404] in GNOME Help. Set the Mail Directory path in Edit > Preferences > Mail Accounts > Edit > Receiving Email to the "root" folder if you are using a variant of the ~/.offlineimaprc file from #OfflineIMAP setup. You can also choose to "check for new messages" more frequently in Receiving Options (like every minute instead of every 60 minutes) since this process will only check your local copy and not the server-side copy.

See Access a Gmail IMAP via Evolution[dead link 2024-12-15—HTTP 404] or Access a Gmail POP Account via Evolution[dead link 2024-12-15—HTTP 404] in GNOME Help. You may also be interested in reading Check Gmail through other email platforms (for IMAP) or Read Gmail messages on other email clients using POP in Gmail Help to manually fill in the following fields/checkboxes under Receiving Mail and/or Sending Mail in the Evolution Mail Configuration Assistant:

OAuth2 (Google) should be selected from the drop-down menu under Authentication in Receiving Email.

You can use your Gmail calendar in Evolution with one of two methods (barring GNOME Online Accounts as mentioned in #Gmail setup).

Follow the steps under "Get your calendar (view only)" in Google's Calendar Help to obtain the "secret address in iCal format" for your desired calendar. Then follow the steps under Using a WebDev calendar[dead link 2024-12-15—HTTP 404] in GNOME Help. Use the previously obtained secret address for the address in the URL field.

Follow the steps under Using a Google calendar[dead link 2024-12-15—HTTP 404] in GNOME Help. You may need to grant GNOME Evolution access to your Google Account if prompted.

Similarly with #Gmail calendar, you can also sync your Google contacts in Evolution. See Using a Google addressbook in GNOME Help for more information.

If your email is locally hosted on a Microsoft Exchange Server or cloud hosted on Office 365, you can use IMAP/POP and SMTP to access your email. However, some additional features such as access to Outlook Calendar and contact management are only available if you connect to the Microsoft Exchange Server or Office 365 server using Microsoft's proprietary Exchange ActiveSync (EAS) protocol.

There are two methods by which you can add/manage a Microsoft Exchange account in Evolution, but both will require Evolution EWS.

Install the gnome-online-accounts package if it is not already present. Then select Online Accounts in GNOME Settings and add a new Microsoft Exchange account with the following values:

Your Exchange account should be listed alongside your other online accounts after clicking Connect. Choose what you want to synchronize (by default, all features are enabled).

The factual accuracy of this article or section is disputed.

See https://wiki.gnome.org/Apps/Evolution/EWS/OAuth2; in particular, the introduction and "Configure the account in Evolution" for users of free accounts. In other words, users of free accounts do not need to concern themselves with application/tenant IDs since they cannot use OAuth2.

Access to the GNOME Evolution (EWS) application may not not allowed by your organization. One possible workaround (instead of requesting access from an administrator) is to select Basic instead of OAuth2 (Office365) from the drop-down menu under Authentication in the Receiving Email section of the Evolution Mail Configuration Assistant. Users of free accounts can also select Basic—this is an alternative to "creating an application specific password."

Evolution relies on GNOME Keyring for storing account passwords, so to use Evolution outside of GNOME, see GNOME/Keyring#Using the keyring and make sure a password keyring with the name login exists.

See Spell checking[dead link 2024-12-15—HTTP 404] in GNOME Help. Evolution uses Enchant through gspell, so you can use checkers other than Hunspell to facilitate spell checking.

It is possible to change the advertised ciphers used to secure the connection to the server. Evolution does not provide a switch to change the settings for the used ciphers. However, since Evolution uses GnuTLS, it is possible to change the settings using environment variables.

One way to change these settings is to copy the /usr/share/applications/org.gnome.Evolution.desktop file to ~/.local/share/applications/ and set the appropriate environment variable in the copied .desktop file. For example, make the following changes to avoid using ECC ciphers with NIST/NSA curves:

A different way to achieve this would be with a wrapper script:

The available cipher settings are documented in https://gnutls.org/manual/html_node/Priority-Strings.html.

As default, Evolution offers only a few builtin fonts to be used for writing messages. However, you can set others fonts to be used as the "Default" option when writing HTML messages. That is done by creating a webkit editor plugin on ~/.local/share/evolution/webkit-editor-plugins/body-font.js. Check below an example using Microsoft's Calibri font:

**Examples:**

Example 1 (unknown):
```unknown
~/.offlineimaprc
```

Example 2 (unknown):
```unknown
[general]
accounts = MyAccount
# Set this to the number of accounts you have.
maxsyncaccounts = 1
# You can set ui = TTY.TTYUI for interactive password entry if needed.
# Setting it within this file (see below) is easier.
ui = Noninteractive.Basic

[Account MyAccount]
# Each account should have a local and remote repository
localrepository = MyLocal
remoterepository = MyMailserver
# Specifies how often to do a repeated sync (if running without crond)
autorefresh = 10

[Repository MyLocal]
type = Maildir
localfolders = /home/path/to/your/maildir
# This needs to be specified so offlineimap does not complain during resync
sep = .
nametrans = lambda folder: re.sub('^.', '',
                           re.sub('^$', '.INBOX', folder))

[Repository MyMailserver]
# Example for a gmail account
type = IMAP
remotehost = your.imap.server.com
remoteuser = yourname
remotepass = yourpassword
remoteport = 143
# You need to configure some CA certificates
sslcacertfile = /etc/ssl/certs/ca-certificates.crt
# Translate your INBOX to be the root directory.
# All other directories need a dot before the actual name.
nametrans = lambda folder: re.sub('^.INBOX$', '',
                           re.sub('^', '.', folder))
```

Example 3 (unknown):
```unknown
nametrans = lambda folder: re.sub('^.INBOX$', '',
                           re.sub('^', '.',
                           re.sub('\.', '_2E',
                           re.sub('^\[Gmail\].Drafts$', 'Drafts',
                           re.sub('^\[Gmail\].Sent Mail$', 'Sent', folder)))))
```

Example 4 (unknown):
```unknown
nametrans = lambda folder: re.sub('^Sent$',   '[Gmail].Sent Mail',
                           re.sub('^Drafts$', '[Gmail].Drafts',
                           re.sub('_2E', '.',
                           re.sub('^.', '',
                           re.sub('^$', '.INBOX', folder)))))
```

---

## Tailscale

**URL:** https://wiki.archlinux.org/title/Tailscale

**Contents:**
- Installation
  - Third-party clients
- Usage
- Advanced usage
  - Using a custom Control Server
  - Running as a Docker container
    - As an exit node
- Tips and tricks
  - Using with NetworkManager
  - Magic DNS

Tailscale builds on top of WireGuard and provides OAuth2 (SSO), OpenID, and SAML authentication for peers to build a mesh network. It is crossplatform, has ACL settings and internal DNS.

Install tailscale and reboot your system.

It is also possible to run tailscale as a Docker container. This way, one can run multiple exit nodes on a single machine, each with its own tailnet.

To use tailscale, enable/start tailscaled.service and run the server as follows:

You can authenticate a headless machine by specifying the auth key:

Using a custom control server like headscale is possible.

On headless systems a non-interactive login using a token is possible.

Follow this guide for a general idea of how to run tailscale as a docker container.

In order to be able to use a tailscale instance running as a docker container as an exit node, we need to use a smaller MTU for the container's network. This is due to an MTU-related issue.

If you do not have one already, create a custom network:

Then, use that network for the container instance:

If you use NetworkManager, you may notice connection issues with other devices within your tailnet. This is due to a management conflict that is happening between NetworkManager and tailscaled.

To clear this up, we need NetworkManager to unmanage the tailscale network devices (e.g. tailscale0).

To do so, simply create a config like /etc/NetworkManager/conf.d/99-tailscale.conf with the contents of:

Then restart NetworkManager.service and tailscaled.service.

You may also need to cycle tailscale:

After this, you should be able to ping other devices on your tailnet.

Tailscale expects system with working systemd-resolved to set up DNS server and search suffixes from tailnet configuration, otherwise it tries to overwrite /etc/resolv.conf.

After starting tailscaled.service, it may crash and get stuck on a restart loop. This may be due to the absence of the tun kernel module. To verify that this is indeed the case, view the logs of tailscaled.service via systemd

Or, run the binary as root in your terminal

The following, or similar error may appear in systemctl status or by running the binary:

If so, simply reboot your system and observe if tailscaled.service is active instead of loaded with systemctl status.

**Examples:**

Example 1 (unknown):
```unknown
tailscaled.service
```

Example 2 (unknown):
```unknown
# tailscale up
```

Example 3 (unknown):
```unknown
# tailscale up --authkey=tskey-KEY
```

Example 4 (unknown):
```unknown
/etc/default/tailscaled
```

---

## vnStat

**URL:** https://wiki.archlinux.org/title/VnStat

**Contents:**
- Installation
- Configuration
- Usage

vnStat is a lightweight (command line) network traffic monitor. It monitors selectable interfaces and stores network traffic logs in a database for later analysis.

Install the vnstat package.

Start/enable the vnstat.service daemon.

Pick a preferred network interface and edit the Interface variable in the /etc/vnstat.conf accordingly. To list all interfaces available to vnstat, use vnstat --iflist.

To start monitoring a particular interface that was not referred to in the configuration file when the daemon was started, you must initialize a database first. Each interface needs its own database. The command to initialize one for the eth0 interface is:

Remember to restart the vnstat.service daemon after you have added a new interface.

Query the network traffic:

Viewing live network traffic usage:

To find more options, use:

or to see all options use:

Eye candy presentation of the data can also be achieved by vnstati(1), which is part of the vnstat package.

**Examples:**

Example 1 (unknown):
```unknown
vnstat.service
```

Example 2 (unknown):
```unknown
/etc/vnstat.conf
```

Example 3 (unknown):
```unknown
vnstat --iflist
```

Example 4 (unknown):
```unknown
# vnstat --add -i eth0
```

---

## Monit

**URL:** https://wiki.archlinux.org/title/Monit

**Contents:**
- Installation
- Configuration
  - Configuration syntax
- Configuration examples
  - Mailserver declaration
  - Email notification format
  - CPU, memory and swap utilization
  - Filesystem(s) usage
  - Process monitoring
  - Hard drive health and temperature using scripts

Monit, not to be confused to M/Monit, is an AGPL3.0 licensed system and process monitoring tool. Monit can automatically restart crashed services, display temperatures from standard hardware (through lm_sensors and hard drives from smartmontools for example). Service alerts can be sent based on a wide criteria including a single occurrence or occurrences over a period of time. It can be accessed directly through the command line or ran as a web app using its integrated HTTP(S) server. This allows quick and streamlined snapshot of a given systems status.

Install the monit package and any software for optional testing such as lm_sensors or smartmontools. Once you have completed the configuration, be sure to enable and start monit.service.

Monit keeps a main configuration file as /etc/monitrc. You can choose to edit this file but if you wish to run scripts (such as to get hard drive temperatures or health status) you should uncomment the last directive of include /etc/monit.d/*, save /etc/monitrc and create /etc/monit.d/.

Monit utilizes a configuration syntax that makes it very easy to read; essentially check WHAT followed by if THING condition THEN action format. Any occurrence of if, and, with(in), has, us(ing|e), on(ly), then, for, of in the configuration file is for human readability only and are completely ignored by Monit.

Checks are usually performed in cycles. This is defined at the beginning of the configuration file, for example a 30second poll is defined with:

Checks with 4 cycles would therefore happen every 2 minutes

Create the file /etc/monit.d/scripts/hdtemp.sh as well as the /etc/monit.d/scripts folder if necessary.

In this example, the /etc/monit.d/scripts/hdtemp.sh script assumes your drive path is /dev/sdX where X is filled in by the letter at the end of the check declaration. A similar method is used for the SMART health status in the next example.

Alerts can be set globally, where a given user / email address is alerted for any alert condition; or you can set an alert recipient for each type of check (eg network alerts go to recipient A; process alerts go to recipient B). You can set as many global or subsystem recipients as you like, just make multiple declarations.

Global alerts are set outside of any subsystem checks; for ease of reading they should be set in the same location as the mailserver declaration.

Subsystem alerts are set very similarly to global alerts except they lack the SET flag.

**Examples:**

Example 1 (unknown):
```unknown
monit.service
```

Example 2 (unknown):
```unknown
/etc/monitrc
```

Example 3 (unknown):
```unknown
include /etc/monit.d/*
```

Example 4 (unknown):
```unknown
/etc/monitrc
```

---

## fstab

**URL:** https://wiki.archlinux.org/title/Fstab

**Contents:**
- Usage
- Identifying file systems
  - Kernel name descriptors
  - File system labels
  - File system UUIDs
  - GPT partition labels
  - GPT partition UUIDs
- Tips and tricks
  - Automount with systemd
    - Local partition

The fstab(5) file can be used to define how disk partitions, various other block devices, or remote file systems should be mounted into the file system.

Each file system is described in a separate line. These definitions will be converted into systemd mount units dynamically at boot, and when the configuration of the system manager is reloaded. The default setup will automatically fsck and mount file systems before starting services that need them to be mounted. For example, systemd automatically makes sure that remote file system mounts like NFS or Samba are only started after the network has been set up. Therefore, local and remote file system mounts specified in /etc/fstab should work out-of-the-box. See systemd.mount(5) for details.

The mount command will use fstab, if just one of either directory or device is given, to fill in the value for the other parameter. When doing so, mount options which are listed in fstab will also be used.

This article or section needs expansion.

A simple /etc/fstab, using file system UUIDs:

All specified devices within /etc/fstab will be automatically mounted on startup and when the -a flag is used with mount(8) unless the noauto option is specified. Devices that are listed and not present will result in an error unless the nofail option is used.

See fstab(5) § DESCRIPTION for details.

This article or section needs expansion.

There are different ways to identify file systems that will be mounted in /etc/fstab: kernel name descriptor, file system label and UUID, and GPT partition label and UUID for GPT disks. Kernel name descriptors should not be used, while UUIDs or PARTUUIDs should be preferred over labels. See Persistent block device naming for more explanations. It is recommended to read that article first before continuing with this article.

In this section, we will describe how to mount file systems using all the mount methods available via examples. The output of the commands lsblk -f and blkid used in the following examples are available in the article Persistent block device naming.

Run lsblk -f to list the partitions and prefix the values in the NAME column with /dev/.

Run lsblk -f to list the partitions, and prefix the values in the LABEL column with LABEL= or alternatively run blkid and use the LABEL values without the quotes:

Run lsblk -f to list the partitions, and prefix the values in the UUID column with UUID= or alternatively run blkid and use the UUID values without the quotes:

Run blkid to list the partitions, and use the PARTLABEL values without the quotes:

Run blkid to list the partitions, and use the PARTUUID values without the quotes:

See systemd.mount(5) for all systemd mount options.

In case of a large partition, it may be more efficient to allow services that do not depend on it to start while it is checked by fsck. This can be achieved by adding the following options to the /etc/fstab entry of the partition:

This will fsck and mount the partition only when it is first accessed, and the kernel will buffer all file access to it until it is ready. This method can be relevant if one has, for example, a significantly large /home partition.

The same applies to remote file system mounts. If you want them to be mounted only upon access, you will need to use the x-systemd.automount parameters. In addition, you can use the x-systemd.mount-timeout= option to specify how long systemd should wait for the mount command to finish. Also, the _netdev option ensures systemd understands that the mount is network dependent and order it after the network is online.

If you have secondary encrypted file systems with keyfiles, you can also add the nofail parameter to the corresponding entries in /etc/crypttab and /etc/fstab. systemd will not wait for the cryptsetup service to finish unlocking and mounting the filesystem on boot, but instead may finish mounting this after reaching default.target. This will avoid any boot delay caused by unlocking secondary partitions that are not required immediately after boot. See dm-crypt/System configuration#Non blocking mounting for cryptsetup configuration

Since mount services will by default only wait for 90 seconds for the partition to be available, any delay in making the keyfile available may cause the mount to fail. To avoid this, add the option x-systemd.mount-timeout=0 to fstab in order to make sure that the mount service waits indefinitely for the partition to be unlocked.

You may also specify an idle timeout for a mount with the x-systemd.idle-timeout flag. For example:

This will make systemd unmount the mount after it has been idle for 1 minute.

The factual accuracy of this article or section is disputed.

External devices that are to be mounted when present but ignored if absent may require the nofail option. This prevents errors being reported at boot. For example:

The nofail option is best combined with the x-systemd.device-timeout option. This is because the default device timeout is 90 seconds, so a disconnected external device with only nofail will make your boot take 90 seconds longer, unless you reconfigure the timeout as shown. Make sure not to set the timeout to 0, as this translates to infinite timeout.

Since spaces are used in fstab to delimit fields, if any field (PARTLABEL, LABEL or the mount point) contains spaces, these spaces must be replaced by escape characters \ followed by the 3 digit octal code 040:

Below atime options can impact drive performance.

When using Mutt or other applications that need to know if a file has been read since the last time it was modified, the noatime option should not be used; using the relatime option is acceptable and still provides a performance improvement.

Since kernel 4.0 there is another related option:

Note that the lazytime option works in combination with the aforementioned *atime options, not as an alternative. That is relatime by default, but can be even strictatime with the same or less cost of disk writes as the plain relatime option.

If for some reason the root partition has been improperly mounted read only, remount the root partition with read-write access with the following command:

When using UEFI/GPT, it is possible to omit certain partitions from /etc/fstab by partitioning according to the Discoverable Partitions Specification and have systemd-gpt-auto-generator(8) mount the partitions. See systemd#GPT partition automounting.

To specify custom mount options for these volumes, use a by-designator identifier as the device name:

You can link directories with the bind option:

See mount(8) § Bind mount operation for details.

You can use the genfstab tool to create an fstab file. See genfstab for details.

Here is a list of programs that can be used to modify mount points. They might not have all the features possible for editing fstab, but have all of the most used ones and might make your workflow much easier:

If you want to allow any user to mount the drive, consider adding these mount options to add onto your fstab entries.

For filesystems that do not have file permissions built in such as FAT and exFAT, you can explicitly set the user or group for the entire drive and its files. You can view the ID of a specific user in /etc/passwd. The uid is the third number in the entry, and the group id is the fourth.

For ext4, btrfs, and other filesystems that have permission abilities, other users might not be permitted to see the drive. Be sure to double check the permissions of /path/to/drive/ and modify them for what you need.

Use findmnt --verify --verbose to check for syntax errors and invalid options in fstab.

**Examples:**

Example 1 (unknown):
```unknown
# <device>                                <dir> <type> <options>                                        <dump> <fsck>
UUID=0a3407de-014b-458b-b5c1-848e92a327a3 /     ext4 defaults                                           0      1
UUID=CBB6-24F2                            /boot vfat defaults,nodev,nosuid,noexec,fmask=0177,dmask=0077 0      2
UUID=f9fe0b69-a280-415d-a03a-a32752370dee none  swap defaults                                           0      0
UUID=b411dc99-f0a0-4c87-9e05-184977be8539 /home ext4 defaults                                           0      2
```

Example 2 (unknown):
```unknown
/dev/disk/by-*/*
```

Example 3 (unknown):
```unknown
/dev/mapper/*
```

Example 4 (unknown):
```unknown
# <device <dir> <type> <options>                                          <dump> <fsck>
/dev/sda2 /     ext4   defaults                                           0      1
/dev/sda1 /boot vfat   defaults,nodev,nosuid,noexec,fmask=0177,dmask=0077 0      2
/dev/sda3 /home ext4   defaults                                           0      2
/dev/sda4 none  swap   defaults                                           0      0
```

---

## dm-verity

**URL:** https://wiki.archlinux.org/title/Dm-verity

**Contents:**
- Components
- Preparation
  - Partitioning
  - Possible issues with boot and runtime
    - Pacman
    - NetworkManager
- Setting up verity
  - Configuring the kernel command line
    - Additional recommended options
- Devices other than root

Dm-verity uses a tree of sha256 hashes to verify blocks as they are read from a block device. Consequently, this ensures files have not changed between reboots or during runtime. This is useful for extending trust to the OS by mitigating zero days and unauthorized changes to root, as well as enforcing security policies, encryption and userspace security. Verity devices are regular block devices which can be accessed in /dev/mapper.

dm-verity is part of the device mapper in the Linux kernel and is implemented using systemd.

This article mainly describes setting up a verity-protected read-only root partition.

A dm-verity root setup setup consists of the following:

The unified kernel image and Secure Boot are recommended but not required. Verity is intended to be used as one of the last steps in a boot process that protects the OS and the kernel from changes. It is easily defeated without Secure Boot and unified kernel images.

To enable dm-verity, you must have a working system already installed and configured. See Installation guide for the details.

Typically, it is necessary to have a separate partition or logical volume to store the verity hash data.

The recommended disk layout is similar to this:

/home and /var should be writable filesystems. On a server that just has one purpose this may be optional, since e.g. a wireguard server needs no write access to the disk.

mkfs.erofs(1) offers an attractive alternative to ext4 or squashfs on the root partition. EROFS, like squashfs, does not allow writes by design and has better performance in many cases than comparable filesystems on flash and solid-state media. It uses lz4 compression by default and was designed for Android phones by Huawei, which make extensive use of dm-verity.

The factual accuracy of this article or section is disputed.

Any files that need to be written to during init or changed during runtime must be made writable by some method otherwise the program will not function as expected.

Many programs need write access to etc. You can use a separate /etc partition but this will make all of these configuration files writable. Create a folder /var/etc and move the files that need write access into it than symlink into etc, as with the example with NetworkManager below.

Some programs will expect these folders and files to still exist (even read-only) on the root filesystem for early init. For instance, systemd-journald will break if /etc/machine-id does not exist or is a symlink. Bind mounts can be useful for this.

One way to find out which files will change when the system is running is to enable the dracut-overlayroot module, use the system, and check the files in /run/overlayroot/u to see what you may need to address. Any files in this folder were written to the tmpfs overlaid on top of root. Place the module into /usr/lib/dracut/modules.d/, add overlayroot to the dracut modules list, and overlayroot=1 to your kernel command line and regenerate the initramfs. The module can be found at https://github.com/TylerHelt0/dracut-overlayroot[dead link 2023-05-06—HTTP 404].

Since the root filesystem will be mounted read-only and /var should be mounted read-write in most cases, the path to the pacman database should be changed to /usr/lib/pacman. This will ensure the rootfs always has the correct list of installed packages.

To setup connections with NetworkManager, you need write access to /etc/NetworkManager/system-connections. Move the system-connections folder to /var/etc/NetworkManager/system-connections and symlink to it on the root filesystem.

You will now have the rootfs, the verity hash tree, and the roothash. Alternatively you can save the hashes to a file by replacing the verity-device path and write it to the device later.

To test it you can use veritysetup open root-device root verity-device $(cat roothash.txt). The verity device can be mounted from /dev/mapper/root.

Add the following options to your kernel command line:

If the roothash changes you must also edit the cmdline/rebuild the unified kernel image with the new value. Failure to do so can result in an unbootable system.

The use of dm-verity is not limited to the root device. Other devices that need to be verified at boot can be put into /etc/veritytab and will be assembled by systemd-veritysetup@.service. See veritytab(5) for more information.

Be aware that it is much easier to remount a non-root partition as RW while the system is running. Integrity violations also will not trigger a reboot. Even if has verity enabled, it is trivial for a user with root privileges to disable verity on a non-root partition.

dm-verity does not provide an all-in-one solution but should be used alongside other methods of securing the system when the disk is removed and when the system is fully booted.

It is recommended to enable Secure Boot with custom keys after verity is setup.

Verity protection is useless if a virus or attacker can replace the kernel.efi containing the embedded roothash which would allow any root filesystem to be booted. Signing the kernel image for Secure Boot will prevent the kernel image from being replaced and ensure integrity of the root filesystem as long as the firmware is secure.

sbupdate-gitAUR or sbctl can be used to maintain your unified kernel images and keep your boot loader signed. sbupdate-gitAUR will also handle your kernel command line. sbctl can be used to create Secure Boot keys.

UKIs bundle together at minimum the linux kernel, an initramfs, CPU microcode, and a cmdline. The advantage to using an UKI is that it prevents changes to both the kernel, initramfs and cmdline when the UKI is signed and used with secureboot. If the cmdline section of the UKI is left blank, it can be supplied by a boot loader like systemd-boot. Otherwise it can only be changed by rebuilding and resigning a new UKI.

UKIs can be directly booted by UEFI if kernel efistub is enabled or if shim/preloader is used.

Packaged kernels ship with pre-signed modules. If kernel lockdown or module signature verification is enabled, modules built with DKMS or pre-built out-of-tree kernel modules will refuse to load. One must create a custom kernel to enable signing and loading of out-of-tree modules.

See Signed kernel modules for more information about signed kernel modules.

Although the verity root device will be tamper-resistant, it provides no confidentiality. It could be located on an unencrypted partition if it contains no secret data. If the kernel is protected by Secure Boot, it would be impossible to replace the data in the root or verity devices without replacing the kernel.

The verity root device can be used to unlock other encrypted devices. If done with keyfiles, the verity root should be encrypted. If using a TPM and systemd-cryptenroll to store keys, the verity root could be unencrypted.

The factual accuracy of this article or section is disputed.

A TPM 2.0 can be used to protect encryption keys for the LUKS device containing root. After Secure Boot is enabled, you can use systemd-cryptenroll to bind keys to PCRs. Recommended PCRs are 0,1,5,7. This will stop decryption if the firmware, firmware options, GPT layout or secure boot state is changed, respectively.

The reason for binding on 0,1, and 5 is to ensure attackers cannot replace the motherboard firmware to disable secureboot and consequently disable verity.

You must pass this kernel option:

You may also need to add tpm2 support to your initramfs or include the module if using dracut. See systemd-cryptenroll#Trusted Platform Module for more information.

If you use systemd-boot as your boot loader, it will measure the kernel.efi into PCR 4. This can be used to prevent decryption of root if the kernel image, initramfs, or kernel command line is changed.

During runtime, methods such as OverlayFS, tmpfs, and bind mounts can still be used to get write access on the folders within root. For this reason, it is important to still harden the OS. Apparmor, SELinux and other access control mechanisms are useful for this.

A dm-verity read-only root should not be updated the traditional way with pacman. Verity is intended mostly for embedded devices and others which value code-integrity over the rolling release model. This has the primary benefits of extending trust to the OS and ensuring a device always boots the same way. E.g. an emulator box for normies or a secure web server. dm-verity, combined with other security methods like selinux, eliminate entire classes of zero-days and consequently the need to update is less frequent unless new features are desired. Think about a router running linux: many routers are compromised by malware without the user ever knowing. If the router was verity-protected in a secure way, it would prevent viruses from gaining persistence.

You could use ext4 for / which enables you to mount it rw. You can then do updates, rehash the filesystem, and change the roothash in the cmdline, but it would be better to release incrementally updated images of the filesystems. Disabling verity to do updates on a writable filesystem is as simple as omitting the systemd.verity=1 cmdline option.

A VM can be used to maintain a 'rolling' system and imaged when updates are needed. A chroot could work as well. Setup all the partitioning and boot logic the way it is expected to work on the target system, than reboot the VM into a live media and make images of the partitions. If you image the xboot partition, it can be flashed directly to a partition to update the UKI/kernel/initramfs/cmdline. If paired with an image for the root filesystem this is more or less a complete system update.

Systemd already has the logic to retrieve images from an update server (Or local dir) and flash them to partitions which may or may not already exist. It can also install and remove files into existing partitions.

See systemd-sysupdate(8) and systemd-repart(8).

Another way to handle updates would be to use a system similar to Android's A/B partition system. This entails having two sets of the root and verity partitions. When an update is necessary the active partition could be copied to the inactive one. The inactive partition could than be updated as normal from chroot or with pacman and 'sealed' with dm-verity. On next boot, the inactive partition becomes the active partition.

If using UKI the UKI must be updated with the root and verity partitions. At minimum the kernel cmdline must be updated with new roothash.

If the user wants a system that has optional persistence or can install packages which are reverted at reboot, an overlay can be mounted as root with the verity root as the lower dir. The upper dir could be a persistent block device or a tmpfs. If using A/B, one could remount / as a writable OverlayFS and use normal update methods, than copy the contents of the overlayfs into the inactive partition and rehash verity.

If the user requires temporary persistence (for example, the ability to install packages that are reset at boot), systemd.volatile=overlay can be passed on the kernel command line.

Flatpak can be used to install and update apps within var and home without write access to /. Flatpak would be ideal to solve most user's needs for installing applications and updating them in a verity-protected desktop PC. Flatpak works on /var by default.

The above steps can be automated with the package verity-squash-rootAUR. It will build a squashfs rootfs and sign the roothash with the kernel and the initramfs. On boot, you can decide to boot a persistent system, where changes on the overlayfs are saved, or to boot a volatile system. It also keeps the last rootfs as a backup, so you can decide to boot the last working rootfs.

**Examples:**

Example 1 (unknown):
```unknown
/dev/mapper
```

Example 2 (unknown):
```unknown
roothash.txt
```

Example 3 (unknown):
```unknown
systemd-veritysetup.generator
```

Example 4 (unknown):
```unknown
systemd-veritysetup@.service
```

---

## System maintenance

**URL:** https://wiki.archlinux.org/title/System_maintenance

**Contents:**
- Check for errors
  - Failed systemd services
  - Log files
- Backup
  - Configuration files
  - List of installed packages
  - Pacman database
  - Encryption metadata
  - System and user data
- Upgrading the system

Regular system maintenance is necessary for the proper functioning of Arch over a period of time. Timely maintenance is a practice many users get accustomed to.

Check if any systemd services have failed:

See systemd#Using units for more information.

Look for errors in the log files located in /var/log/, as well as messages logged in the systemd journal:

See Xorg#Troubleshooting for information on where and how Xorg logs errors.

Having backups of important data is a necessary measure to take, since human and machine processing errors are very likely to generate corruption as time passes, and also the physical media where the data is stored is inevitably destined to fail.

See Synchronization and backup programs for many alternative applications that may better suit your case. See Category:System recovery for other articles of interest.

It is highly encouraged to automate backups and test the recovery process to ensure everything works as intended. For automation see System backup#Automation.

Before editing any configuration files, create a backup so that you can revert to a working version in case of problems. Editors like vim and emacs can do this automatically. On a larger scale, consider using a configuration manager.

For dotfiles (configuration files in the home directory), see dotfiles#Tracking dotfiles directly with Git.

Maintain a list of all installed packages so that if a complete re-installation is inevitable, it is easier to re-create the original environment.

See pacman/Tips and tricks#List of installed packages for details.

See pacman/Tips and tricks#Back up the pacman database.

See Data-at-rest encryption#Backup for disk encryption scenarios.

It is recommended to perform full system upgrades regularly via pacman#Upgrading packages, to enjoy both the latest bug fixes and security updates, and also to avoid having to deal with too many package upgrades that require manual intervention at once. When requesting support from the community, it will usually be assumed that the system is up to date.

Make sure to have the Arch install media or another Linux "live" CD/USB available so you can easily rescue your system if there is a problem after updating. If you are running Arch in a production environment, or cannot afford downtime for any reason, test changes to configuration files, as well as updates to software packages, on a non-critical duplicate system first. Then, if no problems arise, roll out the changes to the production system.

If the system has packages from the AUR, carefully upgrade all of them.

pacman is a powerful package management tool, but it does not attempt to handle all corner cases. Users must be vigilant and take responsibility for maintaining their own system.

Before upgrading, users are expected to visit the Arch Linux home page to check the latest news, or alternatively subscribe to the RSS feed or the arch-announce mailing list. When updates require out-of-the-ordinary user intervention (more than what can be handled simply by following the instructions given by pacman), an appropriate news post will be made.

Before upgrading fundamental software (such as the kernel, xorg, systemd, or glibc) to a new version, look over the appropriate forum to see if there have been any reported problems.

Users must equally be aware that upgrading packages can raise unexpected problems that could need immediate intervention; therefore, it is discouraged to upgrade a stable system shortly before it is required for carrying out an important task. Instead, wait to upgrade until there is enough time available to resolve any post-upgrade issues.

Avoid doing partial upgrades. In other words, never run pacman -Sy; instead, always use pacman -Syu.

Generally avoid using the --overwrite option with pacman. The --overwrite option takes an argument containing a glob. When used, pacman will bypass file conflict checks for files that match the glob. In a properly maintained system, it should only be used when explicitly recommended by the Arch Developers. See the #Read before upgrading the system section.

Avoid using the -d option with pacman. pacman -Rdd package skips dependency checks during package removal. As a result, a package providing a critical dependency could be removed, resulting in a broken system.

Arch Linux is a rolling release distribution. That means when new library versions are pushed to the repositories, the Developers and Package Maintainers rebuild all the packages in the repositories that need to be rebuilt against the libraries. For example, if two packages depend on the same library, upgrading only one package might also upgrade the library (as a dependency), which might then break the other package which depends on an older version of the library.

That is why partial upgrades are not supported. Do not use:

When refreshing the package database, always do a full upgrade with pacman -Syu.

Be very careful when using IgnorePkg and IgnoreGroup for the same reason. If the system has locally built packages (such as AUR packages), users will need to rebuild them when their dependencies receive a soname bump.

If a partial upgrade scenario has been created, and binaries are broken because they cannot find the libraries they are linked against, do not "fix" the problem simply by symlinking. Libraries receive soname bumps when they are not backwards compatible. A simple pacman -Syu to a properly synced mirror will fix the problem as long as pacman is not broken.

When upgrading the system, be sure to pay attention to the alert notices provided by pacman. If any additional actions are required by the user, be sure to take care of them right away. If a pacman alert is confusing, search the forums or check the latest news on the Arch Linux homepage (see #Read before upgrading the system) for more detailed instructions.

When pacman is invoked, .pacnew and .pacsave files can be created. Pacman provides notice when this happens and users must deal with these files promptly. Users are referred to the pacman/Pacnew and Pacsave wiki page for detailed instructions.

Also, think about other configuration files you may have copied or created. If a package had an example configuration that you copied to your home directory, check to see if a new one has been created.

Upgrades are typically not applied to existing processes. You must restart processes to fully apply the upgrade.

The archlinux-contrib package provides a script called checkservices which runs pacdiff to merge .pacnew files then checks for processes running with outdated libraries and prompts the user if they want them to be restarted.

The kernel is particularly difficult to patch without a reboot. A reboot is always the most secure option, but if this is very inconvenient kernel live patching can be used to apply upgrades without a reboot.

If a package update is expected/known to cause problems, packagers will ensure that pacman displays an appropriate message when the package is updated. If experiencing trouble after an update, double-check pacman's output by looking at /var/log/pacman.log.

At this point, only after ensuring there is no information available through pacman, there is no relevant news on https://archlinux.org/, and there are no forum posts regarding the update, consider seeking help on the forum or over IRC. Downgrading the offending package to revert broken updates should be considered as a last resort.

After upgrading you may now have packages that are no longer needed or that are no longer in the official repositories.

Use pacman -Qtd to check for packages that were installed as a dependency but now, no other packages depend on them. If an orphaned package is still needed, it is recommended to change the installation reason to explicit. Otherwise, if the package is no longer needed, it can be removed. See pacman/Tips and tricks#Removing unused packages (orphans) for details.

Additionally, some packages may no longer be in the remote repositories, but they still may be on your local system. To list all foreign packages use pacman -Qm. Note that this list will include packages that have been installed manually (e.g., from the AUR). To exclude packages that are (still) available on the AUR, use the script from BBS#288205 or try the ancient-packagesAUR tool.

Pacman does a much better job than you at keeping track of files. If you install things manually you will, sooner or later, forget what you did, forget where you installed to, install conflicting software, install to the wrong locations, etc.

To clean up improperly installed files, see pacman/Tips and tricks#Identify files not owned by any package.

Always try open source drivers before resorting to proprietary drivers. Most of the time, open source drivers are more stable and reliable than proprietary drivers. Open source driver bugs are fixed more easily and quickly. While proprietary drivers can offer more features and capabilities, this can come at the cost of stability. To avoid this dilemma, try to choose hardware components known to have mature open source driver support with full features. Information about hardware with open source Linux drivers is available at linux-drivers.org.

Use precaution when using packages from the AUR or an unofficial user repository. Most are supplied by regular users and thus may not have the same standards as those in the official repositories. Always check PKGBUILDs for sanity and signs of mistake or malicious code before building and/or installing the package.

To simplify maintenance, limit the amount of unofficial packages used. Make periodic checks on which are in actual use, and remove (or replace with their official counterparts) any others. See pacman/Tips and tricks#Maintenance for useful commands. Following system upgrade, use rebuild-detector to identify any unofficial packages that may need to be rebuilt.

Update pacman's mirrorlist, as the quality of mirrors can vary over time, and some might go offline or their download rate might degrade.

See mirrors for details.

Programs that help with this can be found in List of applications/Utilities#Disk cleaning.

When looking for files to remove, it is important to find the files that take up the most disk space. Programs that help with this can be found in List of applications/Utilities#Disk usage display.

Remove unwanted .pkg files from /var/cache/pacman/pkg/ to free up disk space.

See pacman#Cleaning the package cache for more information.

Remove unused packages from the system to free up disk space and simplify maintenance.

See #Check for orphans and dropped packages.

Old configuration files may conflict with newer software versions, or corrupt over time. Remove unneeded configurations periodically, particularly in your home directory and ~/.config. For similar reasons, be careful when sharing home directories between installations.

Look for the following directories:

See XDG Base Directory support for more information about these directories.

To keep the home directory clean from temporary files created at the wrong place, it is a good idea to manage a list of unwanted files and remove them regularly, for example with rmshit.py.

rmlint-gitAUR can be used to find and optionally remove duplicate files, empty files, recursive empty directories and broken symlinks.

Old, broken symbolic links might be sitting around your system; you should remove them. Examples on achieving this can be found here and here. However, you should not blindly delete all broken symbolic links, as some of them serve a purpose [1].

To quickly list all the broken symlinks of permanent files on your system, use:

Then inspect and remove unnecessary entries from this list.

The following tips are generally not required, but certain users may find them useful.

Arch's rolling releases can be a boon for users who want to try the latest features and get upstream updates as soon as possible, but they can also make system maintenance more difficult. To simplify maintenance and improve stability, try to avoid cutting edge software and install only mature and proven software. Such packages are less likely to receive difficult upgrades such as major configuration changes or feature removals. Prefer software that has a strong and active development community, as well as a high number of competent users, in order to simplify support in the event of a problem.

Avoid any use of the testing repository, even individual packages from testing. These packages are experimental and not suitable for a stable system. Similarly, avoid packages which are built directly from upstream development sources. These are usually found in the AUR, with names including things like: "dev", "devel", "svn", "cvs", "git", etc.

The linux-lts package is an alternative Arch kernel package, and is available in the core repository. This particular kernel version has long-term support (LTS) from upstream, including security and bug fixes. It is useful if you use out-of-tree kernel modules and want to ensure their compatibility or if you want a fallback kernel in case a new kernel version causes problems.

To make it available as a boot option, you will need to update your boot loader's configuration file to use the LTS kernel and ram disk: vmlinuz-linux-lts and initramfs-linux-lts.img.

**Examples:**

Example 1 (unknown):
```unknown
$ systemctl --failed
```

Example 2 (unknown):
```unknown
# journalctl -b
```

Example 3 (unknown):
```unknown
pacman -Syu
```

Example 4 (unknown):
```unknown
--overwrite
```

---

## Arch boot process

**URL:** https://wiki.archlinux.org/title/Arch_boot_process

**Contents:**
- Firmware types
  - UEFI
  - BIOS
- System initialization
  - UEFI
    - Multibooting
  - BIOS
- Boot loader
  - Feature comparison
- Kernel

In order to boot Arch Linux, a Linux-capable boot loader must be set up. The boot loader is responsible for loading the kernel and initial ramdisk before initiating the boot process. The procedure is quite different for BIOS and UEFI systems.

The firmware is the very first program that is executed once the system is switched on.

The Unified Extensible Firmware Interface has support for reading both the partition table as well as file systems. UEFI does not launch any boot code from the Master Boot Record (MBR) whether it exists or not, instead booting relies on boot entries in the NVRAM.

The UEFI specification mandates support for the FAT12, FAT16, and FAT32 file systems (see UEFI specification version 2.11, section 13.3.1.1), but any conformant vendor can optionally add support for additional file systems; for example, HFS+ or APFS in some Apple's firmwares. UEFI implementations also support ISO 9660 for optical discs.

UEFI launches EFI applications, e.g. boot loaders, boot managers, UEFI shell, etc. These applications are usually stored as files in the EFI system partition. Each vendor can store its files in the EFI system partition under the /EFI/vendor_name directory. The applications can be launched by adding a boot entry to the NVRAM or from the UEFI shell.

The UEFI specification has support for legacy BIOS booting with its Compatibility Support Module (CSM). If CSM is enabled in the UEFI, the UEFI will generate CSM boot entries for all drives. If a CSM boot entry is chosen to be booted from, the UEFI's CSM will attempt to boot from the drive's MBR bootstrap code.

A BIOS or Basic Input-Output System is in most cases stored in a flash memory in the motherboard itself and independent of the system storage. Originally created for the IBM PC to handle hardware initialization and the boot process, it has been replaced progressively since 2010 by UEFI which does not suffer from the same technical limitations.

System switched on, the power-on self-test (POST) is executed. See also Modern CPUs have a backstage cast by Hugo Landau.

If Secure Boot is enabled, the boot process will verify authenticity of the EFI binary by signature.

Since each OS or vendor can maintain its own files within the EFI system partition without affecting the other, multi-booting using UEFI is just a matter of launching a different EFI application corresponding to the particular operating system's boot loader. This removes the need for relying on the chain loading mechanisms of one boot loader to load another OS.

See also Dual boot with Windows.

A boot loader is a piece of software started by the firmware—UEFI or BIOS. It is responsible for loading the kernel with the wanted kernel parameters and any external initramfs images.

A boot manager presents a menu of boot options, or provides some other way to control the boot process—i.e. it just runs other EFI executables.

In the case of UEFI, the kernel itself can be directly launched by the UEFI using the EFI boot stub. A separate boot loader or a boot manager can still be used for the purpose of editing kernel parameters before booting.

Systems with 32-bit IA32 UEFI require a boot loader that supports mixed mode booting.

Since almost no boot loader supports such stacked block devices and since file systems can introduce new features which may not yet be supported by any boot loader (e.g. archlinux/packaging/packages/grub#7, FS#79857, FS#59047, FS#58137, FS#51879, FS#46856, FS#38750, FS#21733 and fscrypt encrypted directories), using a separate /boot partition with a universally supported file system, such as FAT32, is oftentimes more feasible.

See also Wikipedia:Comparison of boot loaders.

The boot loader boots the vmlinux image containing the kernel.

The kernel functions on a low level (kernelspace) interacting between the hardware of the machine and the programs. The kernel initially performs hardware enumeration and initialization before continuing to userspace. See Wikipedia:Kernel (operating system) and Wikipedia:Linux kernel for a detailed explanation.

An initramfs (initial RAM file system) image is a cpio archive providing the necessary files for early userspace (see below) to successfully start the late userspace. This predominantly means all kernel modules, user space tools, associated libraries, supporting files like udev rules, etc. required to locate, access and mount the root file system. With the concept of initramfs it is possible to handle even more complex setups, like e.g. booting from an external drive, stacked devices (logical volumes, software RAIDs, compression, encryption) or running a tiny SSH server in early userspace for remote unlocking or maintenance tasks of the root file system.

The majority of modules will be loaded during later stages of the init process by udev after having switched root to the root file system.

The process is as follows:

Initramfs images are Arch Linux' preferred method for setting up the early userspace and can be generated with mkinitcpio, dracut or booster.

Since 6.13.8 officially supported kernels have Btrfs and Ext4 drivers built-in [4].

This makes it possible for the kernel to use a root partition with these file systems directly and load the rest of external modules needed from there. Although, there are some quirks to keep in mind:

Another thing you really need initramfs for is early microcode loading. But it is not necessary to build full image for that, Arch provides microcode in separate initramfs files, which could be used independently.

If no initramfs image is provided, the kernel always contains still an empty image to start from [8]. So there should be no issues with root partition pinning.

The early userspace stage, a.k.a. the initramfs stage, takes place in rootfs consisting of the files provided by the #initramfs. Early userspace starts by the kernel executing the /init binary as PID 1.

The function of early userspace is configurable, but its main purpose is to bootstrap the system to the point where it can access the root file system. This includes:

Note that the early userspace serves more than just setting up the root file system. There are tasks that can only be performed before the root file system is mounted, such as fsck and resuming from hibernation.

At the final stage of early userspace, the real root is mounted at /sysroot/ (in case of a systemd-based initramfs) or at /new_root/ (in case of a busybox-based one), and then switched to by using systemctl switch-root when using systemd-based initramfs or switch_root(8) when using busybox-based initramfs. The late userspace starts by executing the init program from the real root file system.

The startup of late userspace is executed by the init process. Arch officially uses systemd which is built on the concept of units and services, but the functionality described here largely overlaps with other init systems.

The init process calls getty once for each virtual terminal (typically six of them). getty initializes each terminal and protects it from unauthorized access. When the username and password are provided, getty checks them against /etc/passwd and /etc/shadow, then calls login(1).

The login program begins a session for the user by setting environment variables and starting the user's shell, based on /etc/passwd. The login program displays the contents of /etc/motd (message of the day) after a successful login, just before it executes the login shell. It is a good place to display your Terms of Service to remind users of your local policies or anything you wish to tell them.

Once the user's shell is started, it will typically run a runtime configuration file, such as bashrc, before presenting a prompt to the user. If the account is configured to start X at login, the runtime configuration file will call startx or xinit. Jump to #Graphical session (Xorg) for the end.

This article or section needs expansion.

Additionally, init can be configured to start a display manager instead of getty on a specific virtual terminal. This requires manually enabling its systemd service file. The display manager then starts a graphical session.

xinit runs the user's xinitrc runtime configuration file, which normally starts a window manager or a desktop environment. When the user is finished and exits, xinit, startx, the shell, and login will terminate in that order, returning to getty or the display manager.

**Examples:**

Example 1 (unknown):
```unknown
/EFI/vendor_name
```

Example 2 (unknown):
```unknown
\EFI\BOOT\BOOTx64.EFI
```

Example 3 (unknown):
```unknown
BOOTIA32.EFI
```

Example 4 (unknown):
```unknown
esp/EFI/Linux/
```

---

## iwd

**URL:** https://wiki.archlinux.org/title/Iwd

**Contents:**
- Installation
- Usage
  - iwctl
    - Connect to a network
    - Connect to a network using WPS/WSC
    - Disconnect from a network
    - Show device and connection information
    - Manage known networks
  - iwgtk
    - Indicator icon

iwd (iNet wireless daemon) is a wireless daemon for Linux written by Intel. The core goal of the project is to optimize resource utilization by not depending on any external libraries and instead utilizing features provided by the Linux Kernel to the maximum extent possible.

iwd can work in standalone mode or in combination with comprehensive network managers like ConnMan, systemd-networkd and NetworkManager.

Install the iwd package.

Optionally, third-party graphical and terminal user interface front-ends can be installed:

The iwd package provides the client program iwctl, the daemon iwd and the Wi-Fi monitoring tool iwmon.

Start/enable iwd.service so it can be controlled through the iwctl command or through your preferred iwd front-end.

To get an interactive prompt do:

The interactive prompt is then displayed with a prefix of [iwd]#.

To list all available commands:

First, if you do not know your wireless device name, list all Wi-Fi devices:

If the device or its corresponding adapter is turned off, turn it on:

Then, to initiate a scan for networks (note that this command will not output anything):

You can then list all available networks:

Finally, to connect to a network:

If network is hidden:

If a passphrase is required (and it is not already stored in one of the profiles that iwd automatically checks), you will be prompted to enter it. Alternatively, you can supply it as a command line argument:

If your network is configured such that you can connect to it by pressing a button (Wikipedia:Wi-Fi Protected Setup), check first that your network device is also capable of using this setup procedure.

Then, provided that your device appeared in the above list,

and push the button on your router. The procedure works also if the button was pushed beforehand, less than 2 minutes earlier.

If your network requires to validate a PIN number to connect that way, check the help command output to see how to provide the right options to the wsc command.

To disconnect from a network:

To display the details of a Wi-Fi device, like MAC address:

To display the connection state, including the connected network of a Wi-Fi device:

To list networks you have connected to previously:

To forget a known network:

Alternatively, iwgtkAUR provides a GUI front-end through which iwd can be controlled.

Running iwgtk without any arguments launches the application window, which can be used to toggle your adapters and devices on/off, change their operating modes, view available networks, connect to available networks, and manage known networks.

To launch iwgtk's indicator (tray) icon daemon, run:

If the indicator icon does not appear, then your system tray most likely lacks support for the StatusNotifierItem API, in which case you need to run a compatibility layer such as snixembed-gitAUR.

The following system trays support StatusNotifierItem, and therefore work out of the box:

The following trays only support XEmbed, and therefore require snixembed-gitAUR:

The most common use case for iwgtk is to start the indicator daemon every time you log into your desktop. If your desktop environment supports the XDG Autostart standard, this should happen automatically due to the iwgtk-indicator.desktop file which is placed in /etc/xdg/autostart/ by the AUR package.

Alternatively, a systemd unit file to start the indicator daemon is provided by the AUR package. If your desktop environment supports systemd's graphical-session.target unit, then iwgtk can be autostarted via systemd by enabling the iwgtk.service user unit.

By default, iwd stores the network configuration in the directory /var/lib/iwd. The configuration file is named as network.type, where network is the network SSID and .type is the network type, either .open, .psk or .8021x. The file is used to store the encrypted PreSharedKey and optionally the cleartext Passphrase and can also be created by the user without invoking iwctl. The file can be used for other configuration pertaining to that network SSID as well. For more settings, see iwd.network(5).

A minimal example file to connect to a WPA-PSK or WPA2-PSK secured network with SSID "spaceship" and passphrase "test1234":

To calculate the pre-shared key from the passphrase, one of these two methods can be used:

For connecting to a EAP-PWD protected enterprise access point you need to create a file called: essid.8021x in the /var/lib/iwd directory with the following content:

If you do not want autoconnect to the AP you can set the option to False and connect manually to the access point via iwctl. The same applies to the password, if you do not want to store it plaintext leave the option out of the file and just connect to the enterprise AP.

Like EAP-PWD, you also need to create a essid.8021x file in the directory. Before you proceed to write the configuration file, this is also a good time to find out which CA certificate your organization uses. This is an example configuration file that uses MSCHAPv2 password authentication:

MsCHAPv2 passwords can also be stored as an encrypted hash. The correct md4 hash can be calculated with:

Insert an EOF after your password by pressing Ctrl+d, do not hit Enter. The resulting hash needs to be stored inside the EAP-PEAP-Phase2-Password-Hash key.

Like EAP-PWD, you also need to create a essid.8021x file in the directory. Before you proceed to write the configuration file, this is also a good time to find out which CA certificate your organization uses. This is an example configuration file that uses PAP password authentication:

EAP-TLS uses x509 client certificates to authenticate you. Like ssh keys, these use public-key cryptography, so the Wi-Fi authentication server never needs to be sent a secret, and you do not need to copy and reuse a password between devices. Usually each device will use a distinct cert, one that can, in theory at least, be revoked without forcing you to change a password or disrupt your other devices.

As with the other enterprise methods you need to know the CA cert your organization uses (cacert.pem), which is used to prove to your device it is connecting to the right place. You also need to have the client certificate, which represents you and will be uploaded on each connection (client-cert.pem), and the private key that goes with it (client-key.pem), which is used to prove you own that client certificate.

You can either provide a path to the required certificate or you can embed them inside your configuration.

When you have collected the credentials, put this in your /var/lib/iwd/essid.8021x file:

This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.

One possible method of connecting to eduroam via iwd is provided here. Create the following file, filling in the necessary values:

The factual accuracy of this article or section is disputed.

If that does not work, eduroam also offers a configuration assistant tool (CAT). If your organisation has a profile within the CAT, getting connected to eduroam can be done by downloading the Linux script and running it using python. If your organisation does not support CAT, you will have to create the configuration file manually using parameters provided to you by the administrators (the below table can be helpful in doing so). It is possible to extract the necessary configuration options from the generated configuration, including the certificate and server domain mask. Additionally, some institutions are upgrading to EAP-TLS, and outsourcing the generation of client-cert.pem to SecureW2, in which case you will need to use their tool as well to generate a client cert.

The following table contains a mapping of iwd configuration options to eduroam CAT install script variables.

where method is the content of EAP-Method and should be either TLS, TTLS or PEAP. Once you have extracted all necessary information and converted them to their iwd configuration equivalent you can put them in a configuration file called essid.8021x as explained in the preceding methods.

More example tests can be found in the test cases of the upstream repository.

Instead of including an absolute path to a PEM file (for certificates and keys), the PEM itself can be included inside the network configuration file.

An embedded PEM can appear anywhere in the settings file using the following format:

where my_ca_cert is any name you can use to identify the certificate inside the configuration file.

Then the embedded certificate can be used anywhere in the settings file a certificate path is required by prefixing the value with embed:

This is not limited to CA certificates either. Client certificates, client keys (encrypted or not), and certificate chains can be included.

For WPA on a wired ethernet connection create a config as above, but place it in the /var/lib/ead directory instead.

Afterwards Start/enable ead.service.

File /etc/iwd/main.conf can be used for main configuration. See iwd.config(5).

Create or edit the file /var/lib/iwd/network.type. Add the following section to it:

By default when iwd is in disconnected state, it periodically scans for available networks. To disable periodic scan (so as to always scan manually), create / edit file /etc/iwd/main.conf and add the following section to it:

Since version 0.19, iwd can assign IP address(es) and set up routes using a built-in DHCP client or with static configuration. It is a good alternative to standalone DHCP clients.

To activate iwd's network configuration feature, create/edit /etc/iwd/main.conf and add the following section to it:

There is also ability to set route metric with RoutePriorityOffset:

Since version 1.10, iwd supports IPv6, but it is disabled by default in versions below 2.0. Since version 2.0, it is enabled by default.

To disable it, add the following to the configuration file:

To enable it in version below 2.0 and higher than 1.10:

This setting is required to be enabled whether you want to use DHCPv6 or static IPv6 configuration. It can also be set on a per-network basis.

Add the following section to /var/lib/iwd/network.type file. For example:

At the moment, iwd supports two DNS managers—systemd-resolved and resolvconf.

Add the following section to /etc/iwd/main.conf for systemd-resolved:

If you want to allow any user to read the status information, but not modify the settings, you can create the following D-Bus configuration file:

This article or section needs expansion.

By default, iwd stores network credentials to the system unencrypted. Since iwd version 1.25, iwd provides experimental support for creating encrypted profiles for systems using systemd.

First, create an encrypted credential. The following example uses systemd-creds and creates an encrypted credential called iwd-secret that is bound to the system's Trusted Platform Module which will be used to create encrypted profiles:

Next, add the LoadCredentialEncrypted option by creating a drop-in file for the iwd service.

Finally, add the SystemdEncrypt option with the value being the named credential to the iwd configuration file, reload the systemd manager, and restart the iwd service.

This can be useful, if you have trouble setting up MSCHAPv2 or TTLS. You can set the following environment variable via a drop-in snippet:

Check the iwd logs afterwards by running journalctl -u iwd.service as root.

On some machines, it is reported that iwd.service has to be restarted to work after boot. See FS#63912 and thread 251432. This probably occurs because iwd starts before wireless network card powers on.

As a workaround, find the unit needed to wait for by systemctl list-units --type=device | grep wlan0 and extend the unit accordingly:

Then reload the systemd manager configuration.

If it does not work, try also

Since version 1.0, iwd disables network interface renaming to predictable network interface names. It installs the following systemd.link(5) configuration file which prevents udev from renaming the interface to a predictable, stable name (e.g. wlp#s#):

As a result the wireless link name wlan# is kept after boot. This resolved a race condition between iwd and udev on interface renaming as explained in iwd udev interface renaming.

If this results in issues try masking it with:

Clients may not receive an IP address via DHCP when connecting to iwd in AP mode. It is therefore necessary to enable network configuration by iwd on managed interfaces:

The mentioned file has to be created if it does not already exist.

Some users experience disconnections with Wi-Fi, re-connecting continuously but stabilizing eventually and managing to connect.

Users report crashes ([1]) of iwd.service in their journal.

The core issue is having multiple conflicting services for managing their network connections. Check that you do not have enabled them at the same time to fix this issue.

To load key files iwd requires the pkcs8_key_parser kernel module. While on boot it gets loaded by systemd-modules-load.service(8) using /usr/lib/modules-load.d/pkcs8.conf, that will not be the case if iwd has just been installed.

If messages such as Error loading client private key /path/to/key show up in the journal when trying to connect to WPA Enterprise networks, manually load the module:

iwd will roam to other known APs if the connection is too bad.

This will show up in the system log as wlan0: deauthenticating from xx:xx:xx:xx:xx:xx by local choice (Reason: 3=DEAUTH_LEAVING)

You can see the connection signal strength with

You can increase the threshold to allow a worse connection. RoamThreshold defaults to -70 and RoamThreshold5G to -76.

Set SendHostname in the network's configuration file, not in /etc/iwd/main.conf.

When using resolvconf as DNS resolution method, it may have trouble writing to /etc/resolv.conf complaining about read-only file system:

To fix this problem, extend the configuration of the iwd.service systemd unit by adding drop-in file:

This will allow the iwd.service system unit to update /etc/resolv.conf. Restart iwd.service to make the change effective.

**Examples:**

Example 1 (unknown):
```unknown
iwd.service
```

Example 2 (unknown):
```unknown
iwctl device wlan0 show
```

Example 3 (unknown):
```unknown
[iwd]# help
```

Example 4 (unknown):
```unknown
[iwd]# device list
```

---

## File systems

**URL:** https://wiki.archlinux.org/title/Mounting

**Contents:**
- Types of file systems
  - Journaling
  - FUSE-based file systems
  - Stackable file systems
  - Read-only file systems
  - Clustered file systems
  - Shared-disk file system
- Identify existing file systems
- Create a file system
- Mount a file system

Individual drive partitions can be set up using one of the many different available file systems. Each has its own advantages, disadvantages, and unique idiosyncrasies. A brief overview of supported file systems follows; the links are to Wikipedia pages that provide much more information.

See filesystems(5) for a general overview and Wikipedia:Comparison of file systems for a detailed feature comparison. File systems already loaded by the kernel or built-in are listed in /proc/filesystems, while all the installed modules can be seen with ls /lib/modules/$(uname -r)/kernel/fs.

xfs.html xfs-delayed-logging-design.html xfs-self-describing-metadata.html

The ext3/4, HFS+, JFS, NTFS, ReiserFS, and XFS file systems use journaling. Journaling provides fault-resilience by logging changes before they are committed to the file system. In the event of a system crash or power failure, such file systems are faster to bring back online and less likely to become corrupted. The logging takes place in a dedicated area of the file system.

ext3/4 offer data-mode journaling, which can optionally log data in addition to the metadata. Data-mode journaling comes with a speed penalty, because it does two write operations: first to the journal and then to the disk. Therefore, data-mode journaling is not enabled by default. The trade-off between system speed and data safety should be considered when choosing the file system type and features.

In the same vein, Reiser4 offers configurable "transaction models": a special model called wandering logs, which eliminates the need to write to the disk twice; write-anywhere, a pure copy-on-write approach; and a combined approach called hybrid which heuristically alternates between the two.

File systems based on copy-on-write (also known as write-anywhere), such as Reiser4, Btrfs, Bcachefs and ZFS, by design operate on full atomicity and also provide checksums for both metadata and inline data (operations entirely occur, or they entirely do not, and in properly functioning hardware data does not corrupt due to operations half-occurring). Therefore, these file systems are by design much less prone to data loss than other file systems and have no need to use traditional journal to protect metadata, because they are never updated in-place. Although Btrfs still has a journal-like log tree, it is only used to speed-up fdatasync/fsync.

FAT, exFAT, ext2, and HFS provide neither journaling nor atomicity, They are for temporary or legacy use and not recommended for use when reliable storage is needed.

To identify existing file systems, you can use lsblk:

An existing file system, if present, will be shown in the FSTYPE column. If mounted, it will appear in the MOUNTPOINT column.

File systems are usually created on a partition, inside logical containers such as LVM, RAID and dm-crypt, or on a regular file (see Wikipedia:Loop device). This section describes the partition case.

Before continuing, identify the device where the file system will be created and whether or not it is mounted. For example:

Mounted file systems must be unmounted before proceeding. In the above example an existing file system is on /dev/sda2 and is mounted at /mnt. It would be unmounted with:

To find just mounted file systems, see #List mounted file systems.

To create a new file system, use mkfs(8). See #Types of file systems for the exact type, as well as userspace utilities you may wish to install for a particular file system.

For example, to create a new file system of type ext4 (common for Linux data partitions) on /dev/sda1, run:

The new file system can now be mounted to a directory of choice.

To manually mount a file system located on a device (e.g., a partition) to a directory, use mount(8). This example mounts /dev/sda1 to /mnt.

This attaches the file system on /dev/sda1 at the directory /mnt, making the contents of the file system visible. Any data that existed at /mnt before this action is made invisible until the device is unmounted.

fstab contains information on how devices should be automatically mounted if present. See the fstab article for more information on how to modify this behavior.

If a device is specified in /etc/fstab and only the device or mount point is given on the command line, that information will be used in mounting. For example, if /etc/fstab contains a line indicating that /dev/sda1 should be mounted to /mnt, then the following will automatically mount the device to that location:

mount contains several options, many of which depend on the file system specified. The options can be changed, either by:

See these related articles and the article of the file system of interest for more information.

To list all mounted file systems, use findmnt(8):

findmnt takes a variety of arguments which can filter the output and show additional information. For example, it can take a device or mount point as an argument to show only information on what is specified:

findmnt gathers information from /etc/fstab, /etc/mtab, and /proc/self/mounts.

To unmount a file system use umount(8). Either the device containing the file system (e.g., /dev/sda1) or the mount point (e.g., /mnt) can be specified:

Unmount the file system and run fsck on the problematic volume.

**Examples:**

Example 1 (unknown):
```unknown
/proc/filesystems
```

Example 2 (unknown):
```unknown
ls /lib/modules/$(uname -r)/kernel/fs
```

Example 3 (unknown):
```unknown
NAME   FSTYPE LABEL     UUID                                 MOUNTPOINT
sdb
└─sdb1 vfat   Transcend 4A3C-A9E9
```

Example 4 (unknown):
```unknown
NAME   FSTYPE   LABEL       UUID                                 MOUNTPOINT
sda
├─sda1                      C4DA-2C4D
├─sda2 ext4                 5b1564b2-2e2c-452c-bcfa-d1f572ae99f2 /mnt
└─sda3                      56adc99b-a61e-46af-aab7-a6d07e504652
```

---

## systemd

**URL:** https://wiki.archlinux.org/title/Systemctl

**Contents:**
- Basic systemctl usage
  - Using units
  - Power management
    - Soft reboot
- Writing unit files
  - Handling dependencies
  - Service types
  - Editing provided units
    - Replacement unit files
    - Drop-in files

From the project web page:

Historically, what systemd calls "service" was named daemon: any program that runs as a "background" process (without a terminal or user interface), commonly waiting for events to occur and offering services. A good example is a web server that waits for a request to deliver a page, or an ssh server waiting for someone trying to log in. While these are full featured applications, there are daemons whose work is not that visible. Daemons are for tasks like writing messages into a log file (e.g. syslog, metalog) or keeping your system time accurate (e.g. ntpd). For more information see daemon(7).

The main command used to introspect and control systemd is systemctl. Some of its uses are examining the system state and managing the system and services. See systemctl(1) for more details.

Units commonly include, but are not limited to, services (.service), mount points (.mount), devices (.device) and sockets (.socket).

When using systemctl, you generally have to specify the complete name of the unit file, including its suffix, for example sshd.socket. There are however a few short forms when specifying the unit in the following systemctl commands:

See systemd.unit(5) for details.

The commands in the below table operate on system units since --system is the implied default for systemctl. To instead operate on user units (for the calling user), use systemctl --user without root privileges. See also systemd/User#Basic setup to enable/disable user units for all users.

polkit is necessary for power management as an unprivileged user. If you are in a local systemd-logind user session and no other session is active, the following commands will work without root privileges. If not (for example, because another user is logged into a tty), systemd will automatically ask you for the root password.

Soft reboot is a special kind of a userspace-only reboot operation that does not involve the kernel. It is implemented by systemd-soft-reboot.service(8) and can be invoked through systemctl soft-reboot. As with kexec, it skips firmware re-initialization, but additionally the system does not go through kernel initialization and initramfs, and unlocked dm-crypt devices remain attached.

When /run/nextroot/ contains a valid root file system hierarchy (e.g. is the root mount of another distribution or another snapshot), soft-reboot would switch the system root into it, allowing for switching to another installation without losing states managed by kernel, e.g. networking.

The syntax of systemd's unit files (systemd.unit(5)) is inspired by XDG Desktop Entry Specification .desktop files, which are in turn inspired by Microsoft Windows .ini files. Unit files are loaded from multiple locations (to see the full list, run systemctl show --property=UnitPath), but the main ones are (listed from lowest to highest precedence):

Look at the units installed by your packages for examples, as well as systemd.service(5) § EXAMPLES.

systemd-analyze(1) can help verifying the work. See the systemd-analyze verify FILE... section of that page.

With systemd, dependencies can be resolved by designing the unit files correctly. The most typical case is when unit A requires unit B to be running before A is started. In that case add Requires=B and After=B to the [Unit] section of A. If the dependency is optional, add Wants=B and After=B instead. Note that Wants= and Requires= do not imply After=, meaning that if After= is not specified, the two units will be started in parallel.

Dependencies are typically placed on services and not on #Targets. For example, network.target is pulled in by whatever service configures your network interfaces, therefore ordering your custom unit after it is sufficient since network.target is started anyway.

There are several different start-up types to consider when writing a custom service file. This is set with the Type= parameter in the [Service] section:

See the systemd.service(5) § OPTIONS man page for a more detailed explanation of the Type values.

This article or section needs expansion.

To avoid conflicts with pacman, unit files provided by packages should not be directly edited. There are two safe ways to modify a unit without touching the original file: create a new unit file which overrides the original unit or create drop-in snippets which are applied on top of the original unit. For both methods, you must reload the unit afterwards to apply your changes. This can be done either by editing the unit with systemctl edit (which reloads the unit automatically) or by reloading all units with:

To replace the unit file /usr/lib/systemd/system/unit, create the file /etc/systemd/system/unit and reenable the unit to update the symlinks.

This opens /etc/systemd/system/unit in your editor (copying the installed version if it does not exist yet) and automatically reloads it when you finish editing.

To create drop-in files for the unit file /usr/lib/systemd/system/unit, create the directory /etc/systemd/system/unit.d/ and place .conf files there to override or add new options. systemd will parse and apply these files on top of the original unit.

The easiest way to do this is to run:

This opens the file /etc/systemd/system/unit.d/drop_in_name.conf in your text editor (creating it if necessary) and automatically reloads the unit when you are done editing. Omitting --drop-in= option will result in systemd using the default file name override.conf .

To revert any changes to a unit made using systemctl edit do:

For example, if you simply want to add an additional dependency to a unit, you may create the following file:

As another example, in order to replace the ExecStart directive, create the following file:

Note how ExecStart must be cleared before being re-assigned [2]. The same holds for every item that can be specified multiple times, e.g. OnCalendar for timers.

One more example to automatically restart a service:

For services that send logs directly to journald or syslog, you can control their verbosity by setting a numeric value between 0 and 6 for the LogLevelMax= parameter in the [Service] section using the methods described above. For example:

The standard log levels are identical to those used to filter the journal. Setting a lower number excludes all higher and less important log messages from your journal.

If a service is echoing stdout and/or stderr output, by default this will end up in the journal as well. This behavior can be suppressed by setting StandardOutput=null and/or StandardError=null in the [Service] section. Other values than null can be used to further tweak this behavior. See systemd.exec(5) § LOGGING_AND_STANDARD_INPUT/OUTPUT.

systemd uses targets to group units together via dependencies and as standardized synchronization points. They serve a similar purpose as runlevels but act a little differently. Each target is named instead of numbered and is intended to serve a specific purpose with the possibility of having multiple ones active at the same time. Some targets are implemented by inheriting all of the services of another target and adding additional services to it. There are systemd targets that mimic the common SystemVinit runlevels.

The following should be used under systemd instead of running runlevel:

The runlevels that held a defined meaning under sysvinit (i.e., 0, 1, 3, 5, and 6); have a 1:1 mapping with a specific systemd target. Unfortunately, there is no good way to do the same for the user-defined runlevels like 2 and 4. If you make use of those it is suggested that you make a new named systemd target as /etc/systemd/system/your target that takes one of the existing runlevels as a base (you can look at /usr/lib/systemd/system/graphical.target as an example), make a directory /etc/systemd/system/your target.wants, and then symlink the additional services from /usr/lib/systemd/system/ that you wish to enable.

In systemd, targets are exposed via target units. You can change them like this:

This will only change the current target, and has no effect on the next boot. This is equivalent to commands such as telinit 3 or telinit 5 in Sysvinit.

The standard target is default.target, which is a symlink to graphical.target. This roughly corresponds to the old runlevel 5.

To verify the current target with systemctl:

To change the default target to boot into, change the default.target symlink. With systemctl:

Alternatively, append one of the following kernel parameters to your boot loader:

systemd chooses the default.target according to the following order:

Some (not exhaustive) components of systemd are:

systemd is in charge of mounting the partitions and filesystems specified in /etc/fstab. The systemd-fstab-generator(8) translates all the entries in /etc/fstab into systemd units; this is performed at boot time and whenever the configuration of the system manager is reloaded.

systemd extends the usual fstab capabilities and offers additional mount options. These affect the dependencies of the mount unit. They can, for example, ensure that a mount is performed only once the network is up or only once another partition is mounted. The full list of specific systemd mount options, typically prefixed with x-systemd., is detailed in systemd.mount(5) § FSTAB.

An example of these mount options is automounting, which means mounting only when the resource is required rather than automatically at boot time. This is provided in fstab#Automount with systemd.

On UEFI-booted systems, GPT partitions such as root, home, swap, etc. can be mounted automatically following the Discoverable Partitions Specification. These partitions can thus be omitted from fstab, and if the root partition is automounted, then root= can be omitted from the kernel command line. See systemd-gpt-auto-generator(8).

The prerequisites are:

udev will create symlinks to the discovered partitions which can be used to reference the partitions and volumes in configuration files.

For /var automounting to work, the PARTUUID must match the SHA256 HMAC hash of the partition type UUID keyed by the machine ID. The required PARTUUID can be obtained using:

The primary role of systemd-sysvcompat (required by base) is to provide the traditional linux init binary. For systemd-controlled systems, init is just a symbolic link to its systemd executable.

In addition, it provides four convenience shortcuts that SysVinit users might be used to. The convenience shortcuts are halt(8), poweroff(8), reboot(8) and shutdown(8). Each one of those four commands is a symbolic link to systemctl, and is governed by systemd behavior. Therefore, the discussion at #Power management applies.

systemd-based systems can give up those System V compatibility methods by using the init= boot parameter (see, for example, /bin/init is in systemd-sysvcompat ?) and systemd native systemctl command arguments.

systemd-tmpfiles creates, deletes and cleans up volatile and temporary files and directories. It reads configuration files in /etc/tmpfiles.d/ and /usr/lib/tmpfiles.d/ to discover which actions to perform. Configuration files in the former directory take precedence over those in the latter directory.

Configuration files are usually provided together with service files, and they are named in the style of /usr/lib/tmpfiles.d/program.conf. For example, the Samba daemon expects the directory /run/samba to exist and to have the correct permissions. Therefore, the samba package ships with this configuration:

Configuration files may also be used to write values into certain files on boot. For example, if you used /etc/rc.local to disable wakeup from USB devices with echo USBE > /proc/acpi/wakeup, you may use the following tmpfile instead:

It is possible to write multiple lines to the same file, either with \n in the argument or using the w+ type on multiple lines (including the first one) for appending:

See the systemd-tmpfiles(8) and tmpfiles.d(5) man pages for details.

The factual accuracy of this article or section is disputed.

Configuration files provided by packages should not be directly edited to avoid conflicts with pacman updates. For this, many (but not all) systemd packages provide a way to modify the configuration, but without touching the original file by creation of drop-in snippets. Check the package manual to see if drop-in configuration files are supported.

To create a drop-in configuration file for the unit file /etc/systemd/unit.conf, create the directory /etc/systemd/unit.conf.d/ and place .conf files there to override or add new options. systemd will parse and apply these files on top of the original unit.

Check the overall configuration:

The applied drop-in snippets file(s) and content will be listed at the end. Restart the service for the changes to take effect.

Some packages provide a .socket unit. For example, cups provides a cups.socket unit[3]. If cups.socket is enabled (and cups.service is left disabled), systemd will not start CUPS immediately; it will just listen to the appropriate sockets. Then, whenever a program attempts to connect to one of these CUPS sockets, systemd will start cups.service and transparently hand over control of these ports to the CUPS process.

To delay a service until after the network is up, include the following dependencies in the .service file:

The network wait service of the network manager in use must also be enabled so that network-online.target properly reflects the network status.

For more detailed explanations, see the discussion in the Network configuration synchronization points.

If a service needs to perform DNS queries, it should additionally be ordered after nss-lookup.target:

See systemd.special(7) § Special Passive System Units.

For nss-lookup.target to have any effect it needs a service that pulls it in via Wants=nss-lookup.target and orders itself before it with Before=nss-lookup.target. Typically this is done by local DNS resolvers.

Check which active service, if any, is pulling in nss-lookup.target with:

This article or section needs expansion.

By default, Arch Linux does not make use of systemd presets, and does not enable most services on installation.

If you wish for systemd presets to be honored when packages are installed, you will need to create a a pacman hook in order to run systemctl preset or systemctl preset-all whenever a new systemd unit is installed. Something like

Arch Linux ships with /usr/lib/systemd/system-preset/99-default.preset containing disable *. This causes systemctl preset to disable all units by default, such that when a new package is installed, the user must manually enable the unit.

To enable units by default instead, simply create a symlink from /etc/systemd/system-preset/99-default.preset to /dev/null or replace with a file containing enable * in order to override the configuration file. This will cause systemctl preset to enable all units that get installed—regardless of unit type—unless specified in another file in one systemctl preset's configuration directories. User units are not affected. It is also possible to configure higher priority files with more specific rules on what should be enabled. See systemd.preset(5) for more information.

See systemd/Sandboxing.

In order to notify about service failures, an OnFailure= directive needs to be added to the according service file, for example by using a drop-in configuration file. Adding this directive to every service unit can be achieved with a top-level drop-in configuration file. For details about top-level drop-ins, see systemd.unit(5).

Create a top-level drop-in for services:

This adds OnFailure=failure-notification@%n.service to every service file. If some_service_unit fails, failure-notification@some_service_unit.service will be started to handle the notification delivery (or whatever task it is configured to perform).

Create the failure-notification@.service template unit:

You can create the failure-notification.sh script and define what to do or how to notify. Examples include sending e-mail, showing desktop notifications, using gotify, XMPP, etc. The %i will be the name of the failed service unit and will be passed as an argument to the script.

In order to prevent a recursion for starting instances of failure-notification@.service again and again if the start fails, create an empty drop-in configuration file with the same name as the top-level drop-in (the empty service-level drop-in configuration file takes precedence over the top-level drop-in and overrides the latter one):

You can set up systemd to send an e-mail when a unit fails. Cron sends mail to MAILTO if the job outputs to stdout or stderr, but many jobs are setup to only output on error. First you need two files: an executable for sending the mail and a .service for starting the executable. For this example, the executable is just a shell script using sendmail, which is in packages that provide smtp-forwarder.

Whatever executable you use, it should probably take at least two arguments as this shell script does: the address to send to and the unit file to get the status of. The .service we create will pass these arguments:

Where user is the user being emailed and address is that user's email address. Although the recipient is hard-coded, the unit file to report on is passed as an instance parameter, so this one service can send email for many other units. At this point you can start status_email_user@dbus.service to verify that you can receive the emails.

Then simply edit the service you want emails for and add OnFailure=status_email_user@%n.service to the [Unit] section. %n passes the unit's name to the template.

See udisks#Automatically turn off an external HDD at shutdown.

To find the systemd services which failed to start:

To find out why they failed, examine their log output. See systemd/Journal#Filtering output for details.

systemd has several options for diagnosing problems with the boot process. See boot debugging for more general instructions and options to capture boot messages before systemd takes over the boot process. Also see systemd debugging documentation.

If some systemd service misbehaves or you want to get more information about what is happening, set the SYSTEMD_LOG_LEVEL environment variable to debug. For example, to run the systemd-networkd daemon in debug mode:

Add a drop-in file for the service adding the two lines:

Or equivalently, set the environment variable manually:

then restart systemd-networkd and watch the journal for the service with the -f/--follow option.

If the shutdown process takes a very long time (or seems to freeze), most likely a service not exiting is to blame. systemd waits some time for each service to exit before trying to kill it. To find out whether you are affected, see Shutdown completes eventually in the systemd documentation.

A common problem is a stalled shutdown or suspend process. To verify whether that is the case, you could run either of these commands and check the outputs

The solution to this would be to cancel these jobs by running

and then trying shutdown or reboot again.

If running journalctl -u foounit as root does not show any output for a short lived service, look at the PID instead. For example, if systemd-modules-load.service fails, and systemctl status systemd-modules-load shows that it ran as PID 123, then you might be able to see output in the journal for that PID, i.e. by running journalctl -b _PID=123 as root. Metadata fields for the journal such as _SYSTEMD_UNIT and _COMM are collected asynchronously and rely on the /proc directory for the process existing. Fixing this requires fixing the kernel to provide this data via a socket connection, similar to SCM_CREDENTIALS. In short, it is a bug. Keep in mind that immediately failed services might not print anything to the journal as per design of systemd.

The factual accuracy of this article or section is disputed.

After using systemd-analyze a number of users have noticed that their boot time has increased significantly in comparison with what it used to be. After using systemd-analyze blame NetworkManager is being reported as taking an unusually large amount of time to start.

The problem for some users has been due to /var/log/journal becoming too large. This may have other impacts on performance, such as for systemctl status or journalctl. As such the solution is to remove every file within the folder (ideally making a backup of it somewhere, at least temporarily) and then setting a journal file size limit as described in systemd/Journal#Journal size limit.

Starting with systemd 219, /usr/lib/tmpfiles.d/systemd.conf specifies ACL attributes for directories under /var/log/journal and, therefore, requires ACL support to be enabled for the filesystem the journal resides on.

See Access Control Lists#Enable ACL for instructions on how to enable ACL on the filesystem that houses /var/log/journal.

You may want to disable emergency mode on a remote machine, for example, a virtual machine hosted at Azure or Google Cloud. It is because if emergency mode is triggered, the machine will be blocked from connecting to network.

To disable it, mask emergency.service and emergency.target.

You may be trying to start or enable a user unit as a system unit. systemd.unit(5) indicates, which units reside where. By default systemctl operates on system services.

See systemd/User for more details.

Some bootloaders only set the LoaderDevicePartUUID variable when it is empty. Consequently, even if the UUID of the EFI partition changes, the bootloader will not update LoaderDevicePartUUID. By deleting the EFI variable with the commands below, the bootloader will then repopulate it with the new UUID.

**Examples:**

Example 1 (unknown):
```unknown
-H user@host
```

Example 2 (unknown):
```unknown
sshd.socket
```

Example 3 (unknown):
```unknown
netctl.service
```

Example 4 (unknown):
```unknown
dev-sda2.device
```

---

## cgroups

**URL:** https://wiki.archlinux.org/title/Control_group

**Contents:**
- Installing
- With systemd
  - Hierarchy
  - Find cgroup of a process
  - cgroup resource usage
  - Custom cgroups
  - As service
    - Service unit file
    - Grouping unit under a slice
  - As root

Control groups (or cgroups as they are commonly known) are a feature provided by the Linux kernel to manage, restrict, and audit groups of processes. Compared to other approaches like the nice(1) command or /etc/security/limits.conf, cgroups are more flexible as they can operate on (sub)sets of processes (possibly with different system users).

Control groups can be accessed with various tools:

For Arch Linux, systemd is the preferred and easiest method of invoking and configuring cgroups as it is a part of the default installation.

Make sure you have one of these packages installed for automated cgroup handling:

Current cgroup hierarchy can be seen with systemctl status or systemd-cgls command.

The cgroup name of a process can be found in /proc/PID/cgroup.

For example, the cgroup of the shell:

The systemd-cgtop command can be used to see the resource usage:

systemd.slice(5) systemd unit files can be used to define a custom cgroup configuration. They must be placed in a systemd directory, such as /etc/systemd/system/. The resource control options that can be assigned are documented in systemd.resource-control(5).

This is an example slice unit that only allows 30% of one CPU to be used:

Remember to do a daemon-reload to pick up any new or changed .slice files.

Resources can be directly specified in service definition or as a drop-in file:

This example limits the service to 1 gigabyte.

Service can be specified what slice to run in:

systemd-run can be used to run a command in a specific slice.

--uid=username option can be used to spawn the command as specific user.

The --shell option can be used to spawn a command shell inside the slice.

Unprivileged users can divide the resources provided to them into new cgroups, if some conditions are met.

Cgroups v2 must be utilized for a non-root user to be allowed managing cgroup resources.

Not all resources can be controlled by user.

For user to control cpu and io resources, the resources need to be delegated. This can be done with a drop-in file.

For example if your user id is 1000:

Reboot and verify that the slice your user session is under has cpu and io controller:

The user slice files can be placed in ~/.config/systemd/user/.

To run the command under certain slice:

You can also run your login shell inside the slice:

cgroups resources can be adjusted at run-time using systemctl set-property command. Option syntax is the same as in systemd.resource-control(5).

For example, cutting off internet access for all user sessions:

One layer lower than management with systemd is the cgroup virtual file system. "libcgroup" provides a library and utilities for making management easier, so we will use them here as well.

The reason for using the lower level is simple: systemd does not provide an interface for every single interface file in cgroups and nor should it be expected for it to provide them at any point in the future. It is completely harmless to read from them for additional insights on a cgroup's resource use.

One cgroup should only have one set of programs writing to it to avoid race conditions, the "single-writer rule". This is not enforced by the kernel, but following this recommendation prevents hard-to-debug issues from happening. To set the boundary at which systemd stops managing child cgroups, see the Delegate= property. Otherwise do not be surprised if system to overwrites what you have set.

One of the powers of cgroups is that you can create "ad-hoc" groups on the fly. You can even grant the privileges to create custom groups to regular users. groupname is the cgroup name:

Now all the tunables in the group groupname are writable by your user:

Cgroups are hierarchical, so you can create as many subgroups as you like. If a normal user wants to make new subgroup called foo:

As previously mentioned, only one thing should write to a cgroup at any point. This does not affect non-write operations including spawning new processes inside a group, moving processes to a group, or reading properties from cgroup files.

libcgroup contains a simple tool for running new processes inside a cgroup. If a normal user wants to run a bash shell under a our previous groupname/foo:

Inside of the shell, we can confirm which cgroup it belongs to with:

This makes use of /proc/$PID/cgroup, a file that exists in every process. Manually writing to the file causes the cgroup to change as well.

To move all 'bash' commands to this group:

Internally (i.e. without cgclassify the kernel provides two ways to move processes between cgroups. These two are equivalent:

A new subdirectory is crated for groupname/foo at its creation, located at /sys/fs/cgroup/groupname/foo. These files can be read and written to change the group's properties. (Again, writing is not recommended unless delegation is done!)

Let us try to see how much memory all the processes in our group is taking up:

To limit the RAM (not swap) usage of all processes, run the following:

To change the CPU priority of this group (the default is 100):

You can find more tunables or statistics by listing the cgroup directory.

If you want your cgroups to be created at boot, you can define them in /etc/cgconfig.conf instead. This causes a service booted at launch to configure your cgroups. See the relevant manual page about the syntax of this file; we will make no instruction on how to use a truly deprecated mechanism.

The following example shows a cgroup that constrains a given command to 2GB of memory.

The following example shows a command restricted to 20% of one CPU core.

Doing large calculations in MATLAB can crash your system, because Matlab does not have any protection against taking all your machine's memory or CPU. The following examples show a cgroup that constrains Matlab to first 6 CPU cores and 5 GB of memory.

Launch Matlab like this (be sure to use the right path):

For commands and configuration files, see relevant man pages, e.g. cgcreate(1) or cgrules.conf(5)

Before our current cgroup v2 there was an earlier version called v1. V1 allowed a lot of additional flexibility including a non-unified hierarchy and thread-granular management. This was, in retrospect, a bad idea (see the rationales for v2):

To avoid further chaos, cgroup v2 has two key design rules on top of the removal of features:

Before systemd v258, the kernel parameters SYSTEMD_CGROUP_ENABLE_LEGACY_FORCE=1 systemd.unified_cgroup_hierarchy=0 could be used to force booting with cgroup-v1 (the first parameter was added in v256 to make it harder to use cgroup-v1). However, this feature has now been removed. It is still worth knowing about because some software like to put systemd.unified_cgroup_hierarchy=0 in your kernel command-line without telling you, causing your entire system to break.

**Examples:**

Example 1 (unknown):
```unknown
/etc/security/limits.conf
```

Example 2 (unknown):
```unknown
/etc/cgrules.conf
```

Example 3 (unknown):
```unknown
cgconfig.service
```

Example 4 (unknown):
```unknown
cgconfig.conf
```

---

## Domain name resolution

**URL:** https://wiki.archlinux.org/title/Domain_name_resolution

**Contents:**
- Name Service Switch
  - Resolve a domain name using NSS
- Glibc resolver
  - Overwriting of /etc/resolv.conf
    - Alternative using nmcli
  - Limit lookup time
  - Hostname lookup delayed with IPv6
  - Local domain names
- Lookup utilities
- Resolver performance

In general, a domain name represents an IP address and is associated to it in the Domain Name System (DNS). This article explains how to configure domain name resolution and resolve domain names.

This article or section needs expansion.

The Name Service Switch (NSS) facility is part of the GNU C Library (glibc) and backs the getaddrinfo(3) API, used to resolve domain names. NSS allows system databases to be provided by separate services, whose search order can be configured by the administrator in nsswitch.conf(5). The database responsible for domain name resolution is the hosts database, for which glibc offers the following services:

systemd provides three NSS services for hostname resolution:

NSS databases can be queried with getent(1). A domain name can be resolved through NSS using:

The glibc resolver reads /etc/resolv.conf for every resolution to determine the nameservers and options to use.

resolv.conf(5) lists nameservers together with some configuration options. Nameservers listed first are tried first, up to three nameservers may be listed. Lines starting with a number sign (#) are ignored.

Network managers tend to overwrite /etc/resolv.conf, for specifics see the corresponding section:

To prevent programs from overwriting /etc/resolv.conf, it is also possible to write-protect it by setting the immutable file attribute:

This article or section is a candidate for merging with NetworkManager#/etc/resolv.conf.

If you use NetworkManager, nmcli(1) can be used to set persistent options for /etc/resolv.conf. Change "Wired" to the name of your connection. Example:

For more options have a look at the man pages of nmcli(1), nm-settings-nmcli(5) and resolv.conf(5).

If you are confronted with a very long hostname lookup (may it be in pacman or while browsing), it often helps to define a small timeout after which an alternative nameserver is used. To do so, put the following in /etc/resolv.conf.

If you experience a 5 second delay when resolving hostnames it might be due to a DNS-server/Firewall misbehaving and only giving one reply to a parallel A and AAAA request.[1] You can fix that by setting the following option in /etc/resolv.conf:

To be able to use the hostname of local machine names without the fully qualified domain name, add a line to /etc/resolv.conf with the local domain such as:

That way you can refer to local hosts such as mainmachine1.example.org as simply mainmachine1 when using the ssh command, but the drill command still requires the fully qualified domain names in order to perform lookups.

To query specific DNS servers and DNS/DNSSEC records you can use dedicated DNS lookup utilities or those shipped with DNS servers. These tools implement DNS themselves and do not use NSS.

Some DNS server packages ship with DNS lookup utilities that can be used without running the DNS server:

The Glibc resolver does not cache queries. To implement local caching, use systemd-resolved or set up a local caching DNS server and use it as the name server by setting 127.0.0.1 and ::1 as the name servers in /etc/resolv.conf or in /etc/resolvconf.conf if using openresolv.

This article or section needs expansion.

The DNS protocol (Do53) is unencrypted and does not account for confidentiality, integrity or authentication, so if you use an untrusted network or a malicious ISP, your DNS queries can be eavesdropped and the responses manipulated. Furthermore, DNS servers can conduct DNS hijacking.

You need to trust your DNS server to treat your queries confidentially. DNS servers are provided by ISPs and third-parties. Alternatively you can run your own recursive name server (a.k.a. recursive resolver, a.k.a. DNS recursor), which however takes more effort. If you use a DHCP client in untrusted networks, be sure to set static name servers to avoid using and being subject to arbitrary DNS servers, or alternatively, use a VPN to connect to a secure network and use its DNS servers. To secure your communication with a remote DNS server you can use an encrypted protocol, provided that both the upstream server and your local resolver support the protocol. Common encrypted DNS protocols are:

To verify that responses are actually from authoritative name servers, you can validate DNSSEC, provided that both the upstream server(s) and your local resolver support it.

Although one may use an encrypted DNS resolver, a TLS connection still leaks the domain names in the Server Name Indication (SNI) when requesting the domain certificate. This leak can be checked using the Wireshark filter tls.handshake.extensions_server_name_len > 0, or using the following tshark command:

A proposed solution is to use the Encrypted Client Hello (ECH), a TLS 1.3 protocol extension.

Be aware that some client software, such as major web browsers[2][3], are starting to implement DNS over HTTPS. While the encryption of queries may often be seen as a bonus, it also means the software sidetracks queries around the system resolver configuration.[4]

Firefox provides configuration options to enable or disable DNS over HTTPS and select a DNS server. Mozilla has setup a Trusted Recursive Resolver (TRR) programme with transparency information on their default providers. It is notable that Firefox supports and automatically enables the Encrypted Client Hello (ECH) for TRR providers, see Firefox/Privacy#Encrypted Client Hello.

Chromium will examine the user's system resolver and enable DNS over HTTPS if the system resolver addresses are known to also provide DNS over HTTPS. See this blog post for more information and how DNS over HTTPS can be disabled.

Mozilla has proposed universally disabling application-level DNS if the system resolver cannot resolve the domain use-application-dns.net. Currently, this is only implemented in Firefox.

Oblivious DNS over HTTPS (ODoH)—RFC 9230—is a system which addresses a number of DNS privacy concerns. See Cloudflare's article for more information. It added DNS over HTTPS to the academic Oblivious DNS design. See the Improving the privacy of DNS and DoH with oblivion article for a discussion of the differences.

This article or section needs expansion.

Communication between recursive resolvers and root servers is not encrypted and the root server operators are against implementing it. For encrypted communication with authoritative servers there is the experimental RFC 9539 which allows the opportunistic use of DNS over TLS and DNS over QUIC.

There are various third-party DNS services. Wikipedia has a list of "notable" public DNS service operators while the curl project's wiki has a more extensive list of publicly available DNS over HTTPS servers (a lot of which also support DNS over TLS). The systemd package configures fallback DNS for systemd-resolved when no DNS servers are configured (manually or via DHCP/RA).

You can use dnsperftest to test the performance of the most popular DNS resolvers from your location. dnsperf.com provides global benchmarks between providers.

Some DNS services also provide dedicated software:

DNS servers can be authoritative and recursive. If they are neither, they are called stub resolvers and simply forward all queries to another recursive name server. Stub resolvers are typically used to introduce DNS caching on the local host or network. Note that the same can also be achieved with a fully-fledged name server. This section compares the available DNS servers, for a more detailed comparison, refer to Wikipedia:Comparison of DNS server software.

It is possible to use specific DNS resolvers when querying specific domain names. This is particularly useful when connecting to a VPN, so that queries to the VPN network are resolved by the VPN's DNS, while queries to the internet will still be resolved by your standard DNS resolver. It can also be used on local networks.

To implement it, you need to use a local resolver because glibc does not support it.

In a dynamic environment (laptops and to some extents desktops), you need to configure your resolver based on the network(s) you are connected to. The best way to do that is to use openresolv because it supports multiple subscribers. Some network managers support it, either through openresolv, or by configuring the resolver directly. NetworkManager supports conditional forwarding without openresolv.

**Examples:**

Example 1 (unknown):
```unknown
/etc/resolv.conf
```

Example 2 (unknown):
```unknown
$ getent ahosts domain_name
```

Example 3 (unknown):
```unknown
/etc/resolv.conf
```

Example 4 (unknown):
```unknown
/etc/resolv.conf
```

---

## Unified Extensible Firmware Interface

**URL:** https://wiki.archlinux.org/title/Unified_Extensible_Firmware_Interface

**Contents:**
- UEFI firmware bitness
  - Checking the firmware bitness
    - From Linux
    - From macOS
    - From Microsoft Windows
- UEFI variables
  - UEFI variables support in Linux kernel
  - Requirements for UEFI variable support
    - Mount efivarfs
  - Userspace tools

The Unified Extensible Firmware Interface (UEFI) is an interface between operating systems and firmware. It provides a standard environment for booting an operating system and running pre-boot applications.

It is distinct from the MBR boot code method that was used by legacy BIOS systems. See Arch boot process for their differences and the boot process using UEFI. To set up UEFI boot loaders, see Arch boot process#Boot loader.

Under UEFI, every program whether it is an operating system loader or a utility (e.g. a memory testing or recovery tool), should be an EFI application corresponding to the UEFI firmware bitness/architecture.

The vast majority of x86_64 systems, including recent Apple Macs, use x64 (64-bit) UEFI firmware. The only known devices that use IA32 (32-bit) UEFI are older (pre 2008) Apple Macs, Intel Atom System-on-Chip systems (as on 2 November 2013)[1] and some older Intel server boards that are known to operate on Intel EFI 1.10 firmware.

An x64 UEFI firmware does not include support for launching 32-bit EFI applications (unlike x86_64 Linux and Windows versions which include such support). Therefore the EFI application must be compiled for that specific firmware processor bitness/architecture.

The firmware bitness can be checked from a booted operating system.

On distributions running Linux kernel 4.0 or newer, the UEFI firmware bitness can be found via the sysfs interface. Run:

It will return 64 for a 64-bit (x64) UEFI or 32 for a 32-bit (IA32) UEFI. If the file does not exist, you have not booted in UEFI mode.

Pre-2008 Macs mostly have IA32 EFI firmware while >=2008 Macs have mostly x64 EFI. All Macs capable of running Mac OS X Snow Leopard 64-bit Kernel have x64 EFI 1.x firmware.

To find out the arch of the EFI firmware in a Mac, type the following into the Mac OS X terminal:

If the command returns EFI32, it is IA32 (32-bit) EFI firmware. If it returns EFI64, it is x64 EFI firmware. Most of the Macs do not have UEFI 2.x firmware as Apple's EFI implementation is not fully compliant with UEFI 2.x specification.

64-bit versions of Windows do not support booting on a 32-bit UEFI. So, if you have a 32-bit version of Windows booted in UEFI mode, you have a 32-bit UEFI.

To check the bitness run msinfo32.exe. In the System Summary section look at the values of "System Type" and "BIOS mode".

For 64-bit Windows on a 64-bit UEFI, it will be System Type: x64-based PC and BIOS mode: UEFI. For 32-bit Windows on a 32-bit UEFI—System Type: x86-based PC and BIOS mode: UEFI. If the "BIOS mode" is not UEFI, Windows is not booted in UEFI mode.

UEFI defines variables through which an operating system can interact with the firmware. UEFI boot variables are used by the boot loader and used by the operating system only for early system start-up. UEFI runtime variables allow an operating system to manage certain settings of the firmware like the UEFI boot manager or managing the keys for UEFI Secure Boot protocol etc. You can get the list using:

Linux kernel exposes UEFI variables data to userspace via efivarfs (EFI VARiable FileSystem) interface (CONFIG_EFIVAR_FS) - mounted using efivarfs kernel module at /sys/firmware/efi/efivars - it has no maximum per-variable size limitation and supports UEFI Secure Boot variables. Introduced in kernel 3.8.

If UEFI Variables support does not work even after the above conditions are satisfied, try the below workarounds:

If efivarfs is not automatically mounted at /sys/firmware/efi/efivars by systemd during boot, you need to manually mount it to expose UEFI variables to userspace tools like efibootmgr:

See efivarfs.html for kernel documentation.

There are few tools that can access/modify the UEFI variables, namely

You will have to install the efibootmgr package.

To add a new boot option using efibootmgr, you need to know three things:

For example, if you want to add a boot option for /efi/EFI/refind/refind_x64.efi where /efi is the mount point of the ESP, run

In this example, findmnt(8) indicates that the ESP is on disk /dev/sda and has partition number 1. The path to the EFI application relative to the root of the ESP is /EFI/refind/refind_x64.efi. So you would create the boot entry as follows:

Get an overview of all boot entries and the boot order:

To set the boot order:

Where XXXX is the number that appears in the previous output of efibootmgr command.

Delete an unwanted entry:

See efibootmgr(8) or efibootmgr README for more info.

Access to the UEFI can potentially cause harm beyond the running operating system level. There are dangerous UEFI exploits like LogoFAIL which allows a malicious actor to take full control over the machine. Even hardware-level bricking is possible in some cases of poor UEFI implementation [2].

So, as the UEFI variables access is not required for daily system usage, you may want to disable it, to avoid potential security breaches or accidental harm.

Possible solutions are:

The UEFI Shell is a shell/terminal for the firmware which allows launching EFI applications which include UEFI boot loaders. Apart from that, the shell can also be used to obtain various other information about the system or the firmware like memory map (memmap), modifying boot manager variables (bcfg), running partitioning programs (diskpart), loading UEFI drivers, editing text files (edit), hexedit etc.

You can obtain a BSD licensed UEFI Shell from the TianoCore EDK2 project:

Shell v2 works best in UEFI 2.3+ systems and is recommended over Shell v1 in those systems. Shell v1 should work in all UEFI systems irrespective of the spec. version the firmware follows. More information at ShellPkg and the EDK2 mailing list thread—Inclusion of UEFI shell in Linux distro iso.

Few Asus and other AMI Aptio x64 UEFI firmware based motherboards (from Sandy Bridge onwards) provide an option called Launch EFI Shell from filesystem device. For those motherboards, copy the x64 UEFI Shell to the root of your EFI system partition, named as shellx64.efi.

Systems with Phoenix SecureCore Tiano UEFI firmware is known to have embedded UEFI Shell which can be launched using either F6, F11 or F12 key.

UEFI Shell commands usually support -b option which makes output pause after each page. Run help -b to list available internal commands. Available commands are either built into the shell or discrete EFI applications.

For more info see Intel Scripting Guide 2008[dead link 2023-07-30—HTTP 404] and Intel "Course" 2011[dead link 2023-07-30—HTTP 404].

bcfg modifies the UEFI NVRAM entries which allows the user to change the boot entries or driver options. This command is described in detail in page 96 (Section 5.3) of the UEFI Shell Specification 2.2 document.

To dump a list of current boot entries:

To add a boot menu entry for rEFInd (for example) as 4th (numbering starts from zero) option in the boot menu:

where FS0: is the mapping corresponding to the EFI system partition and FS0:\EFI\refind\refind_x64.efi is the file to be launched.

To add an entry to boot directly into your system without a boot loader, see EFI boot stub#bcfg.

To remove the 4th boot option:

To move the boot option #3 to #0 (i.e. 1st or the default entry in the UEFI Boot menu):

map displays a list of device mappings i.e. the names of available file systems (FS0) and storage devices (blk0).

Before running file system commands such as cd or ls, you need to change the shell to the appropriate file system by typing its name:

edit provides a basic text editor with an interface similar to nano, but slightly less functional. It handles UTF-8 encoding and takes care or LF vs CRLF line endings.

For example, to edit rEFInd's refind.conf in the EFI system partition (FS0: in the firmware),

Press Ctrl+e for help.

This article or section needs expansion.

UEFI drivers are pieces of software that support some functionality. For example, access to NTFS formatted partitions is usually not possible from a UEFI shell. The efifs package has drivers that support reading many more file systems from within an EFI shell. A usage example is to copy such driver to a partition that can be accessed from an UEFI shell. Then, from the UEFI shell, issuing commands such as:

After the map command has been executed, the user should be able to access NTFS formatted partitions from within a UEFI shell.

Most of the 32-bit EFI Macs and some 64-bit EFI Macs refuse to boot from a UEFI(X64)+BIOS bootable CD/DVD. If one wishes to proceed with the installation using optical media, it might be necessary to remove UEFI support first.

Extract the ISO skipping the UEFI-specific directories:

Then rebuild the ISO, excluding the UEFI optical media booting support, using xorriso(1) from libisoburn. Be sure to set the correct volume label, e.g. ARCH_202103; it can be acquired using file(1) on the original ISO.

Burn archlinux-version-x86_64-noUEFI.iso to optical media and proceed with installation normally.

OVMF is a TianoCore project to enable UEFI support for Virtual Machines. OVMF contains a sample UEFI firmware and a separate non-volatile variable store for QEMU.

You can install edk2-ovmf from the extra repository.

It is advised to make a local copy of the non-volatile variable store for your virtual machine:

To use the OVMF firmware and this variable store, add following to your QEMU command:

DUET was a TianoCore project that enabled chainloading a full UEFI environment from a BIOS system, in a way similar to BIOS operating system booting. This method is being discussed extensively. Pre-build DUET images can be downloaded from one of the repos[dead link 2023-04-07—404 Page Not Found]. Read specific instructions[dead link 2023-04-07—404 Page Not Found] for setting up DUET. However, as of November 2018, the DUET code has been removed from TianoCore git repository.

You can also try Clover which provides modified DUET images that may contain some system specific fixes and is more frequently updated compared to the gitlab repos.

To boot back into Arch Linux when you are stuck with Windows, reach Advanced startup in Windows by the Windows PowerShell command shutdown /r /o, or via Settings > Update & Security > Recovery > Advanced startup and select Restart now. When you have reached the Advanced startup menu, choose Use a device, which actually contains your UEFI boot options (not limited to USB or CD, but can also boot operating system in hard drive), and choose "Arch Linux".

On some laptops, like Lenovo XiaoXin 15are 2020, using keys like F2 or F12 does not do anything. This can possibly be fixed by returning laptops to OEM to repair mainboard information, but sometimes this is not possible or not desired. There are however other means to enter firmware setup:

If any userspace tool is unable to modify UEFI variable data, check for existence of /sys/firmware/efi/efivars/dump-* files. If they exist, delete them, reboot and retry again. If the above step does not fix the issue, try booting with efi_no_storage_paranoia kernel parameter to disable kernel UEFI variable storage space check that may prevent writing/modification of UEFI variables.

Some kernel and efibootmgr version combinations might refuse to create new boot entries. This could be due to lack of free space in the NVRAM. You can try the solution at #Userspace tools are unable to modify UEFI variable data.

You can also try to downgrade your efibootmgr install to version 0.11.0. This version works with Linux version 4.0.6. See the bug discussion FS#34641, in particular the closing comment, for more information.

If you dual boot with Windows and your motherboard just boots Windows immediately instead of your chosen EFI application, there are several possible causes and workarounds.

This issue can occur due to KMS issue. Try disabling KMS while booting the USB.

Some firmware do not support custom boot entries. They will instead only boot from hardcoded boot entries.

A typical workaround is to not rely on boot entries in the NVRAM and install the boot loader to one of the common fallback paths on the EFI system partition.

The following sections describe the fallback paths.

The UEFI specification defines default file paths for EFI binaries for booting from removable media. The relevant ones are:

While the specification defines these for removable drives only, most firmware support booting these from any drive.

See the appropriate boot loader article on how to install or migrate the boot loader to the default/fallback boot path.

On certain UEFI motherboards like some boards with an Intel Z77 chipset, adding entries with efibootmgr or bcfg from the UEFI Shell will not work because they do not show up on the boot menu list after being added to NVRAM.

This issue is caused because the motherboards can only load Microsoft Windows. To solve this you have to place the .efi file in the location that Windows uses.

Copy the BOOTx64.EFI file from the Arch Linux installation medium (FSO:) to the Microsoft directory your ESP partition on your hard drive (FS1:). Do this by booting into EFI shell and typing:

After reboot, any entries added to NVRAM should show up in the boot menu.

This is a recurring problem with Acer laptops, which occurs if .efi files have not been manually authorized. See Laptop/Acer#Firmware Setup became inaccessible after Linux installation.

efibootmgr can fail to detect EDD 3.0 and as a result create unusable boot entries in NVRAM. See efibootmgr issue 86 for the details.

To work around this, when creating boot entries manually, add the -e 3 option to the efibootmgr command. E.g.

To fix boot loader installers, like grub-install and refind-install, create a wrapper script /usr/local/bin/efibootmgr and make it executable:

Some firmware will remove boot entries referencing drives that are not present during boot. This could be an issue when frequently detaching/attaching drives or when booting from a removable drive.

The solution is to install the boot loader to the default/fallback boot path.

Some motherboards may remove boot entries due to lack of free space in the NVRAM instead of giving an error at creation. To prevent this from occurring, reduce the amount of boot entries being added by minimizing your entry creation process, as well as reducing the amount of automatic drive boot entries by the Compatibility Support Module (CSM) by disabling it from your UEFI settings. See BBS#1608838.

Another reason why boot entries might have been removed is the fact that UEFI specification allows OEMs to do "NVRAM maintenance" during boot process. Those manufacturers do it simply: they just look up for EFI applications in predefined, hardcoded paths on the device. If they fail to find any, they conclude there is no operating system on the device and wipe all boot entries from NVRAM associated with it, because they assume the NVRAM contains some corrupted or outdated data. If you do not plan to install Windows and still want to load the Linux kernel directly from the firmware, one possible workaround is to create an empty file esp/EFI/BOOT/BOOTx64.EFI:

And restore the deleted boot entry. Now after reboot the motherboard will see the "Fake OS" and should not wipe other boot entries from NVRAM. You can change the fake operating system loader with an actual EFI application if you want, of course, as long as you keep the standard fallback name.

This article or section is a candidate for merging with Lenovo.

This article or section needs expansion.

On recent Lenovo ThinkPad laptops (e.g. T16 Gen 2 AMD models), users report that custom UEFI boot entries (created with efibootmgr or bootctl) are automatically deleted at each boot, with only Windows Boot Manager and Lenovo’s own entries (PXE, Recovery, Diagnostics) restored.

This is caused by the BIOS option "Restart / OS Optimized Defaults", which resets the UEFI boot variables at each reboot to defaults optimized for Windows.

Solution: Disable "OS Optimized Defaults" in the BIOS/UEFI setup. After doing so, manually created boot entries persist correctly, allowing systemd-boot or other custom boot managers to work as intended.

**Examples:**

Example 1 (unknown):
```unknown
$ cat /sys/firmware/efi/fw_platform_size
```

Example 2 (unknown):
```unknown
$ ioreg -l -p IODeviceTree | grep firmware-abi
```

Example 3 (unknown):
```unknown
msinfo32.exe
```

Example 4 (unknown):
```unknown
System Type: x64-based PC
```

---

## systemd

**URL:** https://wiki.archlinux.org/title/Drop-in_snippet

**Contents:**
- Basic systemctl usage
  - Using units
  - Power management
    - Soft reboot
- Writing unit files
  - Handling dependencies
  - Service types
  - Editing provided units
    - Replacement unit files
    - Drop-in files

From the project web page:

Historically, what systemd calls "service" was named daemon: any program that runs as a "background" process (without a terminal or user interface), commonly waiting for events to occur and offering services. A good example is a web server that waits for a request to deliver a page, or an ssh server waiting for someone trying to log in. While these are full featured applications, there are daemons whose work is not that visible. Daemons are for tasks like writing messages into a log file (e.g. syslog, metalog) or keeping your system time accurate (e.g. ntpd). For more information see daemon(7).

The main command used to introspect and control systemd is systemctl. Some of its uses are examining the system state and managing the system and services. See systemctl(1) for more details.

Units commonly include, but are not limited to, services (.service), mount points (.mount), devices (.device) and sockets (.socket).

When using systemctl, you generally have to specify the complete name of the unit file, including its suffix, for example sshd.socket. There are however a few short forms when specifying the unit in the following systemctl commands:

See systemd.unit(5) for details.

The commands in the below table operate on system units since --system is the implied default for systemctl. To instead operate on user units (for the calling user), use systemctl --user without root privileges. See also systemd/User#Basic setup to enable/disable user units for all users.

polkit is necessary for power management as an unprivileged user. If you are in a local systemd-logind user session and no other session is active, the following commands will work without root privileges. If not (for example, because another user is logged into a tty), systemd will automatically ask you for the root password.

Soft reboot is a special kind of a userspace-only reboot operation that does not involve the kernel. It is implemented by systemd-soft-reboot.service(8) and can be invoked through systemctl soft-reboot. As with kexec, it skips firmware re-initialization, but additionally the system does not go through kernel initialization and initramfs, and unlocked dm-crypt devices remain attached.

When /run/nextroot/ contains a valid root file system hierarchy (e.g. is the root mount of another distribution or another snapshot), soft-reboot would switch the system root into it, allowing for switching to another installation without losing states managed by kernel, e.g. networking.

The syntax of systemd's unit files (systemd.unit(5)) is inspired by XDG Desktop Entry Specification .desktop files, which are in turn inspired by Microsoft Windows .ini files. Unit files are loaded from multiple locations (to see the full list, run systemctl show --property=UnitPath), but the main ones are (listed from lowest to highest precedence):

Look at the units installed by your packages for examples, as well as systemd.service(5) § EXAMPLES.

systemd-analyze(1) can help verifying the work. See the systemd-analyze verify FILE... section of that page.

With systemd, dependencies can be resolved by designing the unit files correctly. The most typical case is when unit A requires unit B to be running before A is started. In that case add Requires=B and After=B to the [Unit] section of A. If the dependency is optional, add Wants=B and After=B instead. Note that Wants= and Requires= do not imply After=, meaning that if After= is not specified, the two units will be started in parallel.

Dependencies are typically placed on services and not on #Targets. For example, network.target is pulled in by whatever service configures your network interfaces, therefore ordering your custom unit after it is sufficient since network.target is started anyway.

There are several different start-up types to consider when writing a custom service file. This is set with the Type= parameter in the [Service] section:

See the systemd.service(5) § OPTIONS man page for a more detailed explanation of the Type values.

This article or section needs expansion.

To avoid conflicts with pacman, unit files provided by packages should not be directly edited. There are two safe ways to modify a unit without touching the original file: create a new unit file which overrides the original unit or create drop-in snippets which are applied on top of the original unit. For both methods, you must reload the unit afterwards to apply your changes. This can be done either by editing the unit with systemctl edit (which reloads the unit automatically) or by reloading all units with:

To replace the unit file /usr/lib/systemd/system/unit, create the file /etc/systemd/system/unit and reenable the unit to update the symlinks.

This opens /etc/systemd/system/unit in your editor (copying the installed version if it does not exist yet) and automatically reloads it when you finish editing.

To create drop-in files for the unit file /usr/lib/systemd/system/unit, create the directory /etc/systemd/system/unit.d/ and place .conf files there to override or add new options. systemd will parse and apply these files on top of the original unit.

The easiest way to do this is to run:

This opens the file /etc/systemd/system/unit.d/drop_in_name.conf in your text editor (creating it if necessary) and automatically reloads the unit when you are done editing. Omitting --drop-in= option will result in systemd using the default file name override.conf .

To revert any changes to a unit made using systemctl edit do:

For example, if you simply want to add an additional dependency to a unit, you may create the following file:

As another example, in order to replace the ExecStart directive, create the following file:

Note how ExecStart must be cleared before being re-assigned [2]. The same holds for every item that can be specified multiple times, e.g. OnCalendar for timers.

One more example to automatically restart a service:

For services that send logs directly to journald or syslog, you can control their verbosity by setting a numeric value between 0 and 6 for the LogLevelMax= parameter in the [Service] section using the methods described above. For example:

The standard log levels are identical to those used to filter the journal. Setting a lower number excludes all higher and less important log messages from your journal.

If a service is echoing stdout and/or stderr output, by default this will end up in the journal as well. This behavior can be suppressed by setting StandardOutput=null and/or StandardError=null in the [Service] section. Other values than null can be used to further tweak this behavior. See systemd.exec(5) § LOGGING_AND_STANDARD_INPUT/OUTPUT.

systemd uses targets to group units together via dependencies and as standardized synchronization points. They serve a similar purpose as runlevels but act a little differently. Each target is named instead of numbered and is intended to serve a specific purpose with the possibility of having multiple ones active at the same time. Some targets are implemented by inheriting all of the services of another target and adding additional services to it. There are systemd targets that mimic the common SystemVinit runlevels.

The following should be used under systemd instead of running runlevel:

The runlevels that held a defined meaning under sysvinit (i.e., 0, 1, 3, 5, and 6); have a 1:1 mapping with a specific systemd target. Unfortunately, there is no good way to do the same for the user-defined runlevels like 2 and 4. If you make use of those it is suggested that you make a new named systemd target as /etc/systemd/system/your target that takes one of the existing runlevels as a base (you can look at /usr/lib/systemd/system/graphical.target as an example), make a directory /etc/systemd/system/your target.wants, and then symlink the additional services from /usr/lib/systemd/system/ that you wish to enable.

In systemd, targets are exposed via target units. You can change them like this:

This will only change the current target, and has no effect on the next boot. This is equivalent to commands such as telinit 3 or telinit 5 in Sysvinit.

The standard target is default.target, which is a symlink to graphical.target. This roughly corresponds to the old runlevel 5.

To verify the current target with systemctl:

To change the default target to boot into, change the default.target symlink. With systemctl:

Alternatively, append one of the following kernel parameters to your boot loader:

systemd chooses the default.target according to the following order:

Some (not exhaustive) components of systemd are:

systemd is in charge of mounting the partitions and filesystems specified in /etc/fstab. The systemd-fstab-generator(8) translates all the entries in /etc/fstab into systemd units; this is performed at boot time and whenever the configuration of the system manager is reloaded.

systemd extends the usual fstab capabilities and offers additional mount options. These affect the dependencies of the mount unit. They can, for example, ensure that a mount is performed only once the network is up or only once another partition is mounted. The full list of specific systemd mount options, typically prefixed with x-systemd., is detailed in systemd.mount(5) § FSTAB.

An example of these mount options is automounting, which means mounting only when the resource is required rather than automatically at boot time. This is provided in fstab#Automount with systemd.

On UEFI-booted systems, GPT partitions such as root, home, swap, etc. can be mounted automatically following the Discoverable Partitions Specification. These partitions can thus be omitted from fstab, and if the root partition is automounted, then root= can be omitted from the kernel command line. See systemd-gpt-auto-generator(8).

The prerequisites are:

udev will create symlinks to the discovered partitions which can be used to reference the partitions and volumes in configuration files.

For /var automounting to work, the PARTUUID must match the SHA256 HMAC hash of the partition type UUID keyed by the machine ID. The required PARTUUID can be obtained using:

The primary role of systemd-sysvcompat (required by base) is to provide the traditional linux init binary. For systemd-controlled systems, init is just a symbolic link to its systemd executable.

In addition, it provides four convenience shortcuts that SysVinit users might be used to. The convenience shortcuts are halt(8), poweroff(8), reboot(8) and shutdown(8). Each one of those four commands is a symbolic link to systemctl, and is governed by systemd behavior. Therefore, the discussion at #Power management applies.

systemd-based systems can give up those System V compatibility methods by using the init= boot parameter (see, for example, /bin/init is in systemd-sysvcompat ?) and systemd native systemctl command arguments.

systemd-tmpfiles creates, deletes and cleans up volatile and temporary files and directories. It reads configuration files in /etc/tmpfiles.d/ and /usr/lib/tmpfiles.d/ to discover which actions to perform. Configuration files in the former directory take precedence over those in the latter directory.

Configuration files are usually provided together with service files, and they are named in the style of /usr/lib/tmpfiles.d/program.conf. For example, the Samba daemon expects the directory /run/samba to exist and to have the correct permissions. Therefore, the samba package ships with this configuration:

Configuration files may also be used to write values into certain files on boot. For example, if you used /etc/rc.local to disable wakeup from USB devices with echo USBE > /proc/acpi/wakeup, you may use the following tmpfile instead:

It is possible to write multiple lines to the same file, either with \n in the argument or using the w+ type on multiple lines (including the first one) for appending:

See the systemd-tmpfiles(8) and tmpfiles.d(5) man pages for details.

The factual accuracy of this article or section is disputed.

Configuration files provided by packages should not be directly edited to avoid conflicts with pacman updates. For this, many (but not all) systemd packages provide a way to modify the configuration, but without touching the original file by creation of drop-in snippets. Check the package manual to see if drop-in configuration files are supported.

To create a drop-in configuration file for the unit file /etc/systemd/unit.conf, create the directory /etc/systemd/unit.conf.d/ and place .conf files there to override or add new options. systemd will parse and apply these files on top of the original unit.

Check the overall configuration:

The applied drop-in snippets file(s) and content will be listed at the end. Restart the service for the changes to take effect.

Some packages provide a .socket unit. For example, cups provides a cups.socket unit[3]. If cups.socket is enabled (and cups.service is left disabled), systemd will not start CUPS immediately; it will just listen to the appropriate sockets. Then, whenever a program attempts to connect to one of these CUPS sockets, systemd will start cups.service and transparently hand over control of these ports to the CUPS process.

To delay a service until after the network is up, include the following dependencies in the .service file:

The network wait service of the network manager in use must also be enabled so that network-online.target properly reflects the network status.

For more detailed explanations, see the discussion in the Network configuration synchronization points.

If a service needs to perform DNS queries, it should additionally be ordered after nss-lookup.target:

See systemd.special(7) § Special Passive System Units.

For nss-lookup.target to have any effect it needs a service that pulls it in via Wants=nss-lookup.target and orders itself before it with Before=nss-lookup.target. Typically this is done by local DNS resolvers.

Check which active service, if any, is pulling in nss-lookup.target with:

This article or section needs expansion.

By default, Arch Linux does not make use of systemd presets, and does not enable most services on installation.

If you wish for systemd presets to be honored when packages are installed, you will need to create a a pacman hook in order to run systemctl preset or systemctl preset-all whenever a new systemd unit is installed. Something like

Arch Linux ships with /usr/lib/systemd/system-preset/99-default.preset containing disable *. This causes systemctl preset to disable all units by default, such that when a new package is installed, the user must manually enable the unit.

To enable units by default instead, simply create a symlink from /etc/systemd/system-preset/99-default.preset to /dev/null or replace with a file containing enable * in order to override the configuration file. This will cause systemctl preset to enable all units that get installed—regardless of unit type—unless specified in another file in one systemctl preset's configuration directories. User units are not affected. It is also possible to configure higher priority files with more specific rules on what should be enabled. See systemd.preset(5) for more information.

See systemd/Sandboxing.

In order to notify about service failures, an OnFailure= directive needs to be added to the according service file, for example by using a drop-in configuration file. Adding this directive to every service unit can be achieved with a top-level drop-in configuration file. For details about top-level drop-ins, see systemd.unit(5).

Create a top-level drop-in for services:

This adds OnFailure=failure-notification@%n.service to every service file. If some_service_unit fails, failure-notification@some_service_unit.service will be started to handle the notification delivery (or whatever task it is configured to perform).

Create the failure-notification@.service template unit:

You can create the failure-notification.sh script and define what to do or how to notify. Examples include sending e-mail, showing desktop notifications, using gotify, XMPP, etc. The %i will be the name of the failed service unit and will be passed as an argument to the script.

In order to prevent a recursion for starting instances of failure-notification@.service again and again if the start fails, create an empty drop-in configuration file with the same name as the top-level drop-in (the empty service-level drop-in configuration file takes precedence over the top-level drop-in and overrides the latter one):

You can set up systemd to send an e-mail when a unit fails. Cron sends mail to MAILTO if the job outputs to stdout or stderr, but many jobs are setup to only output on error. First you need two files: an executable for sending the mail and a .service for starting the executable. For this example, the executable is just a shell script using sendmail, which is in packages that provide smtp-forwarder.

Whatever executable you use, it should probably take at least two arguments as this shell script does: the address to send to and the unit file to get the status of. The .service we create will pass these arguments:

Where user is the user being emailed and address is that user's email address. Although the recipient is hard-coded, the unit file to report on is passed as an instance parameter, so this one service can send email for many other units. At this point you can start status_email_user@dbus.service to verify that you can receive the emails.

Then simply edit the service you want emails for and add OnFailure=status_email_user@%n.service to the [Unit] section. %n passes the unit's name to the template.

See udisks#Automatically turn off an external HDD at shutdown.

To find the systemd services which failed to start:

To find out why they failed, examine their log output. See systemd/Journal#Filtering output for details.

systemd has several options for diagnosing problems with the boot process. See boot debugging for more general instructions and options to capture boot messages before systemd takes over the boot process. Also see systemd debugging documentation.

If some systemd service misbehaves or you want to get more information about what is happening, set the SYSTEMD_LOG_LEVEL environment variable to debug. For example, to run the systemd-networkd daemon in debug mode:

Add a drop-in file for the service adding the two lines:

Or equivalently, set the environment variable manually:

then restart systemd-networkd and watch the journal for the service with the -f/--follow option.

If the shutdown process takes a very long time (or seems to freeze), most likely a service not exiting is to blame. systemd waits some time for each service to exit before trying to kill it. To find out whether you are affected, see Shutdown completes eventually in the systemd documentation.

A common problem is a stalled shutdown or suspend process. To verify whether that is the case, you could run either of these commands and check the outputs

The solution to this would be to cancel these jobs by running

and then trying shutdown or reboot again.

If running journalctl -u foounit as root does not show any output for a short lived service, look at the PID instead. For example, if systemd-modules-load.service fails, and systemctl status systemd-modules-load shows that it ran as PID 123, then you might be able to see output in the journal for that PID, i.e. by running journalctl -b _PID=123 as root. Metadata fields for the journal such as _SYSTEMD_UNIT and _COMM are collected asynchronously and rely on the /proc directory for the process existing. Fixing this requires fixing the kernel to provide this data via a socket connection, similar to SCM_CREDENTIALS. In short, it is a bug. Keep in mind that immediately failed services might not print anything to the journal as per design of systemd.

The factual accuracy of this article or section is disputed.

After using systemd-analyze a number of users have noticed that their boot time has increased significantly in comparison with what it used to be. After using systemd-analyze blame NetworkManager is being reported as taking an unusually large amount of time to start.

The problem for some users has been due to /var/log/journal becoming too large. This may have other impacts on performance, such as for systemctl status or journalctl. As such the solution is to remove every file within the folder (ideally making a backup of it somewhere, at least temporarily) and then setting a journal file size limit as described in systemd/Journal#Journal size limit.

Starting with systemd 219, /usr/lib/tmpfiles.d/systemd.conf specifies ACL attributes for directories under /var/log/journal and, therefore, requires ACL support to be enabled for the filesystem the journal resides on.

See Access Control Lists#Enable ACL for instructions on how to enable ACL on the filesystem that houses /var/log/journal.

You may want to disable emergency mode on a remote machine, for example, a virtual machine hosted at Azure or Google Cloud. It is because if emergency mode is triggered, the machine will be blocked from connecting to network.

To disable it, mask emergency.service and emergency.target.

You may be trying to start or enable a user unit as a system unit. systemd.unit(5) indicates, which units reside where. By default systemctl operates on system services.

See systemd/User for more details.

Some bootloaders only set the LoaderDevicePartUUID variable when it is empty. Consequently, even if the UUID of the EFI partition changes, the bootloader will not update LoaderDevicePartUUID. By deleting the EFI variable with the commands below, the bootloader will then repopulate it with the new UUID.

**Examples:**

Example 1 (unknown):
```unknown
-H user@host
```

Example 2 (unknown):
```unknown
sshd.socket
```

Example 3 (unknown):
```unknown
netctl.service
```

Example 4 (unknown):
```unknown
dev-sda2.device
```

---

## Activating numlock on bootup

**URL:** https://wiki.archlinux.org/title/Activating_numlock_on_bootup

**Contents:**
- Console
  - Early bootup (mkinitcpio)
  - With systemd service
  - Extending getty@.service
  - Bash alternative
- X.org
  - startx
  - MATE
  - KDE Plasma
  - GNOME

You can enable numlock right after the kernel boots in the initramfs. This is the only way to ensure numlock is on even during full-disk encryption password entry. Install mkinitcpio-numlockAUR and add the numlock mkinitcpio hook before encrypt in the /etc/mkinitcpio.conf HOOKS array:

Then regenerate the initramfs for the change to take effect.

An advantage of using this method is that the numlock setting will be replicated in the later boot process, and new virtual consoles will default to having numlock on.

First install the numlockx package.

Then create a script to set the numlock on relevant TTYs:

Once the script is created, you will need to make it executable. Otherwise the script cannot run.

Then create and enable a systemd service:

This is simpler than using a separate service and does not hardcode the number of VTs in a script. Create a drop-in snippet for getty@.service which are applied on top of the original unit:

To disable the numlock activation hint displaying on the login screen, edit getty@tty1.service and add --nohints to agetty options:

Add setleds -D +num to ~/.bash_profile. Note that, unlike the other methods, this will not take effect until after you log in.

Various methods are available.

Install the numlockx package and add it to the xinitrc file before any exec statement:

By default, MATE saves the last state on logout and restores it during the next login. To enable numlock on every login, you must change the following dconf properties:

Go to System Settings > Input & Output > Keyboard, in the Hardware tab, in the NumLock on Plasma Startup section, choose the desired NumLock behavior.

For this to work, make sure that System Settings > System > Session > Background Services > Keyboard Daemon is enabled.

Run the following command:

In order to remember last state of numlock key (whether you disabled or enabled), use:

Alternatively, you can use add numlockx on (from numlockx) to a startup script or ~/.profile.

Create the following keyfile:

And run the following command:

In the file ~/.config/xfce4/xfconf/xfce-perchannel-xml/keyboards.xml, make sure the following values are set to true:

In the file /etc/sddm.conf, under the [General] section, set Numlock=on:

In the file /etc/slim.conf, find the line and uncomment it (remove the #):

In the autostart file, add:

See LightDM#NumLock on by default.

Set the option in lxdm.conf:

Set the option in ~/.config/lxqt/session.conf:

See Sway#Initially enable CapsLock/NumLock.

Set the option in ~/.config/hypr/hyprland.conf:

The factual accuracy of this article or section is disputed.

Set the option in /var/lib/sddm/.config/kcminputrc:

Initialize the xkb_rules struct with the numpad:mac option:

**Examples:**

Example 1 (unknown):
```unknown
/etc/mkinitcpio.conf
```

Example 2 (unknown):
```unknown
/etc/mkinitcpio.conf
```

Example 3 (unknown):
```unknown
...
HOOKS=(base udev autodetect microcode modconf kms keyboard keymap consolefont numlock block encrypt lvm2 filesystems fsck)
...
```

Example 4 (unknown):
```unknown
numLockOnTty
```

---

## systemd-boot

**URL:** https://wiki.archlinux.org/title/Systemd-boot

**Contents:**
- Supported file systems
- Installation
- Installing the UEFI boot manager
  - Installation using XBOOTLDR
  - Updating the UEFI boot manager
    - Manual update
    - Automatic update
      - systemd service
      - pacman hook
  - Signing for Secure Boot

systemd-boot(7), previously called gummiboot (German for "rubber dinghy") and sometimes referred to as sd-boot, is an easy-to-configure UEFI boot manager. It provides a textual menu to select the boot entry and an editor for the kernel command line.

Note that systemd-boot can only start EFI executables (e.g., the Linux kernel EFI boot stub, UEFI shell, GRUB, or the Windows Boot Manager) from the EFI system partition it is installed to or from an Extended Boot Loader Partition (XBOOTLDR partition) on the same disk.

systemd-boot inherits the support for the file systems from the firmware (i.e. at least FAT12, FAT16 and FAT32). Additionally it loads any UEFI drivers placed in esp/EFI/systemd/drivers/.

systemd-boot is shipped with the systemd package which is a dependency of the base meta package, so no additional packages need to be installed manually.

To install systemd-boot, first make sure that the system is booted into UEFI mode and UEFI variables are accessible. This can be verified by running efivar --list or, if efivar is not installed, by running ls /sys/firmware/efi/efivars (if the directory exists, the system is booted into UEFI mode.)

Use bootctl(1) to install systemd-boot to the ESP:

This will copy the systemd-boot UEFI boot manager to the ESP, create a UEFI boot entry for it and set it as the first in the UEFI boot order.

The UEFI boot entry will be called "Linux Boot Manager" and will point to, depending on the UEFI bitness, either \EFI\systemd\systemd-bootx64.efi or \EFI\systemd\systemd-bootia32.efi on the ESP.

To conclude the installation, configure systemd-boot.

This article or section is a candidate for moving to Partitioning#Discrete partitions.

A separate /boot partition of type "Linux extended boot" (XBOOTLDR) can be created to keep the kernel and initramfs separate from the ESP. This is particularly helpful to dual boot with Windows with an existing ESP that is too small.

Prepare an ESP as usual and create another partition for XBOOTLDR on the same physical drive. The XBOOTLDR partition must have a partition type GUID of bc13c2ff-59e6-4262-a352-b275fd6f7172 [1] (ea00 type for gdisk, xbootldr type for fdisk). The size of the XBOOTLDR partition should be large enough to accommodate all of the kernels you are going to install.

During install, mount the ESP to /mnt/efi and the XBOOTLDR partition to /mnt/boot.

Once in chroot, use the command:

To conclude the installation, configure systemd-boot.

Whenever there is a new version of systemd-boot, the UEFI boot manager can be optionally reinstalled by the user. This can be done manually or automatically; the two approaches are described thereafter.

Use bootctl to update systemd-boot:

To update systemd-boot automatically, either use a systemd service or a pacman hook. The two methods are described below.

As of version 250, systemd ships with systemd-boot-update.service. Enabling this service will cause systemd-boot to run the following command on every boot:

Like in the manual update, this will attempt to locate the ESP at /efi, /boot or /boot/efi. The command will update all installed versions of systemd-boot in the ESP if a newer version is available in /usr/lib/systemd/boot/efi/. It will look for a systemd-boot file ending in .efi.signed first to allow users to sign the image for use with Secure Boot.

The package systemd-boot-pacman-hookAUR adds a pacman hook which is executed every time systemd is upgraded. This hook differs from the systemd service method in that it only attempts to update the boot manager when systemd is updated rather than every boot, and it does it immediately rather than waiting until after the next boot.

Rather than installing the AUR package, you may prefer to manually place the following file in /etc/pacman.d/hooks/:

If you have Secure Boot enabled, you may want to add a pacman hook to automatically sign the boot manager upon every upgrade of the package:

Replace /path/to/keyfile.key and /path/to/certificate.crt with your signing key and certificate respectively. For better understanding of this hook, consult systemd-sbsign(1).

The created /usr/lib/systemd/boot/efi/systemd-boot*.efi.signed will automatically be picked up by bootctl install or bootctl update. See bootctl(1) § SIGNED .EFI FILES.

As an alternative, use sbctl.

The loader configuration is stored in the file esp/loader/loader.conf. See loader.conf(5) § OPTIONS for details.

A loader configuration example is provided below:

The default can be changed to @saved in order to remember the last picked entry on startup. This is useful for when dual booting Windows and the surprise windows auto update pushes you into Linux.

Consult loader.conf(5) for more details.

systemd-boot will search for .conf files in /loader/entries/ on the EFI system partition it was launched from and additionally the XBOOTLDR partition on the same disk.

An example of loader files launching Arch from a volume using its UUID xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx is:

See the Boot Loader Specification for details on all configuration options.

systemd-boot will automatically check at boot time for Windows Boot Manager at the location /EFI/Microsoft/Boot/Bootmgfw.efi, Apple macOS Boot Manager in firmware, UEFI shell /shellx64.efi and EFI Default Loader /EFI/BOOT/bootx64.efi, as well as specially prepared kernel files found in /EFI/Linux/. When detected, corresponding entries with titles auto-windows, auto-osx, auto-efi-shell and auto-efi-default, respectively, will be generated. These entries do not require manual loader configuration. However, it does not auto-detect other EFI applications (unlike rEFInd), so for booting the Linux kernel, manual configuration entries must be created.

In case you installed a UEFI shell with the package edk2-shell, systemd-boot will auto-detect and create a new entry if the EFI file is placed in esp/shellx64.efi. To perform this an example command after installing the package would be:

Otherwise in case you installed other EFI applications into the ESP, you can use the following snippets.

You need to install memtest86+-efi for this to work. Also sign the EFI binary when using Secure Boot.

systemd-boot can chainload Netboot. Download the ipxe-arch.efi EFI binary and signature, verify it and place it as proposed in esp/EFI/arch_netboot/arch_netboot.efi.

systemd-boot can chainload GRUB. The location of the grubx64.efi binary matches the used --bootloader-id= when GRUB was installed to the ESP.

systemd-boot cannot launch EFI binaries from partitions other than the ESP it is launched from or the XBOOTLDR partition on the same disk, but it can direct the UEFI shell to do so.

First, install edk2-shell as described above. In the UEFI shell, use the map command to take notes of the FS alias (ex: HD0a66666a2, HD0b, FS1, or BLK7) of the partition with the corresponding PARTUUID.

Then, use the exit command to boot back into Linux, where you can create a new loader entry to run the target EFI program through the UEFI shell:

Ensure that the efi path matches the location where the shellx64.efi has been copied in the esp partition. Also, note that the shellx64.efi EFI file can be moved elsewhere to avoid the automatic entry creation by systemd-boot.

Replace HD0b with the previously noted FS alias.

systemd-boot will automatically add an entry to boot into UEFI firmware setup if your device's firmware supports rebooting into setup from the OS.

Alternatively you can install systemd-boot-passwordAUR which supports password basic configuration option. Use sbpctl generate to generate a value for this option.

Install systemd-boot-password with the following command:

With enabled editor you will be prompted for your password before you can edit kernel parameters.

You can use t and T while in the menu to adjust the menu timeout and e to edit the kernel parameters for this boot. Press h to see a short list of useful hotkeys. See systemd-boot(7) § KEY BINDINGS for the full list of available key bindings inside the boot menu.

The boot manager is integrated with the systemctl command, allowing you to choose what option you want to boot after a reboot. For example, suppose you have built a custom kernel and created an entry file esp/loader/entries/arch-custom.conf to boot into it, you can just launch

and your system will reboot into that entry maintaining the default option intact for subsequent boots. To see a list of possible entries pass the --boot-loader-entry=help option.

If you want to boot into the firmware of your motherboard directly, then you can use this command:

Unified kernel images (UKIs) in esp/EFI/Linux/ are automatically sourced by systemd-boot, and do not need an entry in esp/loader/entries. (Note that unified kernel images must have a .efi extension to be identified by systemd-boot.)

Grml is a small live system with a collection of software for system administration and rescue.

In order to install Grml on the ESP, we only need to copy the kernel vmlinuz, the initramfs initrd.img, and the squashed image grml64-small.squashfs from the iso file to the ESP. To do so, first download grml64-small.iso and mount the file (the mountpoint is henceforth denoted mnt); the kernel and initramfs are located in mnt/boot/grml64small/, and the squashed image resides in mnt/live/grml64-small/.

Next, create a directory for Grml in your ESP,

and copy the above-mentioned files in there:

In the last step, create an boot entry for systemd-boot: In esp/loader/entries create a grml.conf file with the following content:

For an overview of the available boot options, consult the cheatcode for Grml.

As with Grml it is possible to use the Arch Linux ISO. To do this we need to copy the kernel vmlinuz-linux, the initramfs initramfs-linux.img, and the squashfs image airootfs.sfs from the ISO file to the EFI system partition.

First download archlinux-YYYY.MM.DD-x86_64.iso.

Next, create a directory for archiso in your ESP:

Extract the contents of the arch directory in there:

In the last step, create a boot entry for the systemd-boot: In esp/loader/entries create a arch-rescue.conf file with the following content:

For an overview of the available boot options, consult the README.bootparams for mkinitcpio-archiso.

The official Arch ISO does not currently support Secure Boot. As a result, Secure Boot must be disabled to boot into the ISO for recovery or maintenance. This undermines system security and is not an ideal approach.

An alternative is to create a signed unified kernel image (UKI) using mkosi, assuming Secure Boot is already configured and functioning properly on your system. This allows you to boot into a signed recovery Arch environment without disabling Secure Boot or carrying an Arch ISO USB drive wherever your laptop goes.

https://swsnr.de/archlinux-rescue-image-with-mkosi/ describes how to set up Secure Boot-compatible Arch recovery images. A practical starting point with a sane mkosi configuration you can add your packages to is available at https://codeberg.org/swsnr/rescue-image.

If you need a boot loader for BIOS systems that follows The Boot Loader Specification, then systemd-boot can be pressed into service on BIOS systems. The Clover boot loader supports booting from BIOS systems and provides a emulated UEFI environment.

This may be caused by a variety of issues with the configuration file, such as the path to the kernel being specified incorrectly. To check, run:

If booted in BIOS mode, you can still install systemd-boot, however this process requires you to tell firmware to launch systemd-boot's EFI file at boot:

If you can do it, the installation is easier: go into your UEFI Shell or your firmware configuration interface and change your machine's default EFI file to esp/EFI/systemd/systemd-bootx64.efi.

If the bootctl install command failed, you can create a UEFI boot entry manually using efibootmgr:

where /dev/sdXY is the EFI system partition.

If for any reason you need to create a UEFI boot entry from Windows, you can use the following commands from an Administrator prompt:

Replace guid with the id returned by the first command. You can also set it as the default entry using

See UEFI#Windows changes boot order.

To stop BitLocker from requesting the recovery key, add the following to loader.conf:

This will set the BootNext UEFI variable, whereby Windows Boot Manager is loaded without BitLocker requiring the recovery key. This is a one-time change, and systemd-boot remains the default boot loader. There is no need to specify Windows as an entry if it was autodetected.

This is an experimental feature, so make sure to consult loader.conf(5).

**Examples:**

Example 1 (unknown):
```unknown
esp/EFI/systemd/drivers/
```

Example 2 (unknown):
```unknown
efivar --list
```

Example 3 (unknown):
```unknown
ls /sys/firmware/efi/efivars
```

Example 4 (unknown):
```unknown
# bootctl install
```

---

## Linux Containers

**URL:** https://wiki.archlinux.org/title/Linux_Containers

**Contents:**
- Privileged or unprivileged
- Setup
  - Required software
    - Enable support to run unprivileged containers (optional)
      - Unprivileged containers on linux-hardened and custom kernels
  - Host network configuration
    - Using lxc-net "Virtual NAT Router"
      - Enable lxc-net.service
      - Advanced Setup
      - Firewall considerations

Linux Containers (LXC) is a userspace interface for the Linux kernel containment features, providing a method for OS-level virtualization, using namespaces, cgroups and other Linux kernel capabilities(7) on the LXC host. lxc(7) is considered something in the middle between a chroot and a full-fledged virtual machine.

Incus or LXD can be used as a manager for LXC. This page deals with using LXC directly.

Alternatives for using containers comprise systemd-nspawn, Docker and Podman.

LXC supports two types of containers: privileged and unprivileged.

On privileged containers, the root UID—UID 0—within the container is mapped to the root UID on a host.

On unprivileged containers, the root UID within the container is mapped to an unprivileged UID on the host, which makes it more difficult for a hack inside the container to lead to consequences on the host system. In other words, if an attacker manages to escape the container, they should find themselves with limited or no rights on the host.

The Arch linux, linux-lts and linux-zen kernel packages currently provide out-of-the-box support for unprivileged containers. With the linux-hardened package, unprivileged containers are only available for the system administrator; hence additional kernel configuration changes are required to enable user namespaces for normal users there.

This article contains information for users to run either type of container, but additional steps may be required in order to use unprivileged containers.

To illustrate the power of UID mapping, consider the output below from a running, unprivileged container. Therein, we see the containerized processes owned by the containerized root user in the output of ps:

On the host, however, those containerized root processes are actually shown to be running as the mapped user (ID>99999), rather than the host's actual root user:

Installing lxc and arch-install-scripts will allow the host system to run privileged lxcs.

Modify /etc/lxc/default.conf to contain the following lines:

In other words, map a range of 65536 consecutive uids, starting from container-side uid 0, which shall be uid 100000 from the host’s point of view, up to and including container-side uid 65535, which the host will know as uid 165535. Apply that same mapping to gids.

Create or edit both subuid(5) at /etc/subuid and subgid(5) at /etc/subgid to contain the mapping to the containerized uid/gid pairs for each user who shall be able to run the containers. The example below is simply for the root user (and systemd system unit):

The factual accuracy of this article or section is disputed.

In addition, running unprivileged containers as an unprivileged user only works if you delegate a cgroup in advance (the cgroup2 delegation model enforces this restriction, not liblxc). Use the following systemd command to delegate the cgroup (per LXC - Getting started: Creating unprivileged containers as a user):

This works similarly for other lxc commands.

This article or section is a candidate for merging with cgroups#User delegation.

Alternatively, delegate unprivileged cgroups by creating a systemd unit (per Rootless Containers: Enabling CPU, CPUSET, and I/O delegation):

Users wishing to run unprivileged containers on linux-hardened or their custom kernel need to complete several additional setup steps.

Firstly, a kernel is required that has support for user namespaces (a kernel with CONFIG_USER_NS). All Arch Linux kernels have support for CONFIG_USER_NS. However, due to more general security concerns, the linux-hardened kernel does ship with user namespaces enabled only for the root user. There are two options to create unprivileged containers there:

LXC supports different virtual network types and devices (see lxc.container.conf(5) § NETWORK). A bridge device on the host is required for most types of virtual networking, which is illustrated in this section.

There are several main setups to consider:

The host bridge requires the host's network manager to manage a shared bridge interface. The host and any lxc will be assigned an IP address in the same network (for example 192.168.1.x). This might be more simplistic in cases where the goal is to containerize some network-exposed service like a webserver, or VPN server. The user can think of the lxc as just another PC on the physical LAN, and forward the needed ports in the router accordingly. The added simplicity can also be thought of as an added threat vector, again, if WAN traffic is being forwarded to the lxc, having it running on a separate range presents a smaller threat surface.

The NAT bridge does not require the host's network manager to manage the bridge. lxc ships with lxc-net which creates a NAT bridge called lxcbr0. The NAT bridge is a standalone bridge with a private network that is not bridged to the host's ethernet device or to a physical network. It exists as a private subnet in the host.

By default, lxc is shipped with the lxc-net shell script, which requires the optional dependency dnsmasq to be Install to function. lxc-net acts like a NAT router, similar to OpenWRT, and creates a Network bridge named lxcbr0 by default. It also manages nftables or iptables for traffic forwarding. Most configurations for this tool can be modified in its configuration file located at /etc/default/lxc-net.

Now you can start the lxc-net.service and among other things the default Bridge lxcbr0 should appear in

One can override some lxc-net defaults by creating /etc/default/lxc-net file or editing /etc/default/lxc file using the following template:

Optionally, create a configuration file to manually define the IP address of any containers:

Depending on which firewall the host machine is running, it might be necessary to allow inbound packets from lxcbr0 to the host, and outbound packets from lxcbr0 to traverse through the host to other networks. To test this, try to bring up a container configured to use DHCP for its IP assignment and see if lxc-net is able to assign an IP address to the container (check with lxc-ls -f. If no IP is assigned, the host's policies will need to be adjusted.

Users of ufw can simply run the following two lines to enable this:

Alternatively, users of nftables can modify /etc/nftables.conf (and reload it with nft -f /etc/nftables.conf; check if the config syntax is correct with nft -cf /etc/nftables.conf) to allow the container to have internet access (replace "eth0" with the device on your system that has internet access; list existing devices with ip link):

Additionally, since the container is running on the 10.0.3.x subnet, external access to services such as ssh, httpd, etc. will need to be actively forwarded to the lxc. In principle, the firewall on the host needs to forward incoming traffic on the expected port on the container.

The goal of this rule is to allow ssh traffic to the lxc:

This rule forwards tcp traffic originating on port 2221 to the IP address of the lxc on port 22.

To ssh into the container from another PC on the LAN, one needs to ssh on port 2221 to the host. The host will then forward that traffic to the container.

If using ufw, append the following at the bottom of /etc/ufw/before.rules to make this persistent:

To create and start containers as a non-root user, extra configuration must be applied.

Create the usernet file under /etc/lxc/lxc-usernet. According to lxc-usernet(5), the entry per line is:

Configure the file with the user needing to create containers. The bridge will be the same as defined in /etc/default/lxc-net.

A copy of the /etc/lxc/default.conf is needed in the non-root user's home directory, e.g. ~/.config/lxc/default.conf (create the directory if needed).

Running containers as a non-root user requires +x permissions on ~/.local/share/. Make that change with chmod before starting a container.

Containers are built using lxc-create(1) command. With the release of lxc-3.0.0-1, upstream has deprecated locally stored templates.

To create an Arch container:

To create a container by interactively choosing from a list of supported distributions:

To see a list of download template options:

The examples below can be used with privileged and unprivileged containers alike. Note that for unprivileged containers, additional lines will be present by default, which are not shown in the examples, including the lxc.idmap = u 0 100000 65536 and the lxc.idmap = g 0 100000 65536 values optionally defined in the #Enable support to run unprivileged containers (optional) section.

Configurations specific to a container, including system resources to be virtualized/isolated when a process is using the container, are defined in /var/lib/lxc/CONTAINER_NAME/config. Read lxc.container.conf(5) for the syntax and possible options of the configuration file.

A basic configuration file generated when creating containers using templates. Read lxc.conf(5) for more information.

By default, the creation process will make a minimum setup without networking support. Below is an example configuration with networking supplied by lxc-net.service:

One can create a host volume outside the container's rootfs and then mount that volume inside the container. This can be advantageous, for example if the same architecture is being containerized and one wants to share pacman packages between the host and container. Another example could be shared directories. Read lxc.container.conf(5) § MOUNT POINTS for more information. The general syntax is:

In order to run programs on the host's display, some bind mounts need to be defined so that the containerized programs can access the host's resources.

The factual accuracy of this article or section is disputed.

If still experiencing a permission denied error in the LXC guest, call xhost + in the host to allow the guest to connect to the host's display server. Take note of the security concerns of opening up the display server by doing this. In addition, add the following line before the above bind mount lines.

To run a containerized OpenVPN or WireGuard, see Linux Containers/Using VPNs.

To list all installed LXC containers:

Systemd can be used to start and to stop LXCs via lxc@CONTAINER_NAME.service. Enable lxc@CONTAINER_NAME.service to have it start when the host system boots.

Users can also start/stop LXCs without systemd. Start a container:

To login into a container:

Once logged, treat the container like any other linux system, set the root password, create users, install packages, etc.

To attach to a container:

That works nearly the same as lxc-console, but it causes starts with a root prompt inside the container, bypassing login. Without the --clear-env flag, the host will pass its own environment variables into the container (including $PATH, so some commands will not work when the containers are based on another distribution).

Users with a need to run multiple containers can simplify administrative overhead (user management, system updates, etc.) by using snapshots. The strategy is to setup and keep up-to-date a single base container, then, as needed, clone (snapshot) it. The power in this strategy is that the disk space and system overhead are truly minimized since the snapshots use an overlayfs mount to only write out to disk, only the differences in data. The base system is read-only but changes to it in the snapshots are allowed via the overlayfs.

This article or section needs expansion.

For example, setup a container as outlined above. We will call it "base" for the purposes of this guide. Now create 2 snapshots of "base" which we will call "snap1" and "snap2" with these commands:

The snapshots can be started/stopped like any other container. Users can optionally destroy the snapshots and all new data therein with the following command. Note that the underlying "base" lxc is untouched:

Systemd units and wrapper scripts to manage snapshots for pi-hole and OpenVPN are available to automate the process in lxc-service-snapshots.

Once the system has been configured to use unprivileged containers (see, #Enable support to run unprivileged containers (optional)), nsexec-bzrAUR contains a utility called uidmapshift which is able to convert an existing privileged container to an unprivileged container to avoid a total rebuild of the image.

Invoke the utility to convert over like so:

Additional options are available simply by calling uidmapshift without any arguments.

Either attach to or SSH into the target container and prefix the call to the program with the DISPLAY ID of the host's X session. For most simple setups, the display is always 0.

An example of running Firefox from the container in the host's display:

Alternatively, to avoid directly attaching to or connecting to the container, the following can be used on the host to automate the process:

In unprivileged containers, ping is likely to not work without an extra config step. Example error:

To fix this in container foo, on the host:

If presented with following error upon trying to login using lxc-console:

And the container's journal shows:

Delete /etc/securetty[3] and /usr/share/factory/etc/securetty on the container file system. Optionally add them to NoExtract in /etc/pacman.conf to prevent them from getting reinstalled. See FS#45903 for details.

Alternatively, create a new user in lxc-attach and use it for logging in to the system, then switch to root.

If you cannot access your LAN or WAN with a networking interface configured as veth and setup through /etc/lxc/containername/config. If the virtual interface gets the ip assigned and should be connected to the network correctly.

You may disable all the relevant static ip formulas and try setting the ip through the booted container-os like you would normaly do.

Example container/config

And then assign the IP through a preferred method inside the container, see also Network configuration#Network management.

The error may happen when a basic command (ls, cat, etc.) on an attached container is typed hen a different Linux distribution is containerized relative to the host system (e.g. Debian container in Arch Linux host system). Upon attaching, use the argument --clear-env:

Services in an unprivileged container may fail with the following message

Create a file /etc/lxc/unpriv.seccomp containing

Then add the following line to the container configuration after lxc.idmap

lxc-execute fails with the error message Unable to open lxc.init.static. See FS#63814 for details.

Starting containers using lxc-start works fine.

**Examples:**

Example 1 (unknown):
```unknown
[root@unprivileged_container /]# ps -ef | head -n 5
```

Example 2 (unknown):
```unknown
UID        PID  PPID  C STIME TTY          TIME CMD
root         1     0  0 17:49 ?        00:00:00 /sbin/init
root        14     1  0 17:49 ?        00:00:00 /usr/lib/systemd/systemd-journald
dbus        25     1  0 17:49 ?        00:00:00 /usr/bin/dbus-daemon --system --address=systemd: --nofork --nopidfile --systemd-activation
systemd+    26     1  0 17:49 ?        00:00:00 /usr/lib/systemd/systemd-networkd
```

Example 3 (unknown):
```unknown
[root@host /]# lxc-info -Ssip --name sandbox
```

Example 4 (unknown):
```unknown
State:          RUNNING
PID:            26204
CPU use:        10.51 seconds
BlkIO use:      244.00 KiB
Memory use:     13.09 MiB
KMem use:       7.21 MiB
```

---

## Runit

**URL:** https://wiki.archlinux.org/title/Runit

**Contents:**
- Installation
  - Using runit alongside systemd
    - Using BusyBox's implementation
    - Using standard runit
- Usage
  - The tools
  - Run levels and service directories
  - General use
- User Services
  - Example Service

Runit is a process supervisor. It includes runit-init, which can replace sysv's init as pid1, or can be run from inittab or your init system of choice. Runit's simple collection of tools can be used to build flexible dependency structures and distributed systems, or blazing fast parallel runlevel changes (including the initial boot). Runit can be used as a simple process supervisor, see the #User Services for an example.

BusyBox provides a minimal implementation of runit that can be used for simple processing supervision needs. First, create symbolic links to the BusyBox binary for the necessary tools that are going to be needed:

Afterwards, a systemd unit file can be created in order to run BusyBox's runit when needed:

Be sure to create the directory which is going to be supervised by runsvdir according to the systemd unit file created. It is also recommended to create a directory in which runit services can be stored (usually /etc/sv), and only enabled when needed by creating a symbolic link directed to them from the directory being supervised. See #General use for more details.

When everything is correctly configured, busybox-runit.service can be enabled and started.

It is possible to use runit as a simple process supervisor alongside the default Arch Linux's init system (systemd). For this purpose, install runit-systemdAUR, which provides a barebones runit installation without any stage scripts (/etc/runit/{1..3}) or runlevels (/etc/runit/runsvdir/*), which are generally only useful when using runit as the init system. The package provides a directory (/var/service) in which the desired runit services can be put and a systemd unit which starts runit monitoring that directory. Only the services configured in /var/service will be supervised by runit. Just enable and start runit.service.

See the manpages for usage details not covered below.

Runit uses directories of symlinks to specify runlevels, other than the 3 main ones, which are defined in /etc/runit/1, 2, and 3.

1 bootstraps the system, 2 starts runsvdir on /service, and 3 stops the system.

While in run level 2, you are not constrained to any amount of service levels (equivalent to runlevels in sysvinit). You can runschdir to any directory (full of service directory symlinks) you have made in /etc/runit/runsvdir/. This becomes very handy in cases where you have an HA (Failover) setup, and you have one machine that can take over services for many other machines, simply by runsvchdir <theservicedir>.

You can also run trees of dependent service levels by having user-level supervision directories. See User Level Services below.

By default, the runit-run package uses a very minimal service set, defined in /etc/runit/runsvdir/archlinux-default and symlinked to /etc/runit/runsvdir/default.

It only gives gettys on tty2 and tty3, so you will boot to just console scroll and a tidy 'runsvchdir: default: current'. This means when you start X it will be on tty4.

To go back to the standard arch consoles, remove the link /service/ngetty and link as many /etc/sv/*getty* services you like in /service, or edit the /etc/sv/ngetty/run file to get more getties. Better yet, create your own directory in /etc/runit/runsvdir and add the symlinks you want for just the services you desire. Remember to take any services you start with runit out of DAEMONS in /etc/rc.conf or systemctl disable them, they do not need to be started there, and runit will allow parallel startup without backgrounding them.

In this explanation, /var/service is the chosen service directory being supervised by runsvdir and /etc/sv is the chosen directory for containing the services that can be enabled.

There are two ways of creating a user supervision tree: using a Systemd/User service or with runit itself. See #Using BusyBox's implementation for an example systemd user service. To use runit itself refer to the section in the Voidlinux Handbook.

Create a directory where you will keep your services.

Create a directory for your service.

Create a run file in the service directory. You can find example run files in runit's site

To enable the service make a symlink to your service directory. This will make the service start automatically when the runit starts. The service should start immediately.

To start the service again:

**Examples:**

Example 1 (unknown):
```unknown
# busybox --list | awk '/^runsv|^chpst$|^sv/' | xargs -I{} ln -sv /usr/bin/busybox /usr/local/bin/{}
```

Example 2 (unknown):
```unknown
/etc/systemd/system/busybox-runit.service
```

Example 3 (unknown):
```unknown
[Unit]
Description=Runit service supervision - BusyBox implementation
Documentation=man:busybox(1) https://smarden.org/runit/

[Service]
Environment="PATH=/usr/local/sbin:/usr/local/bin:/usr/bin"
ExecStart=/usr/local/bin/runsvdir -P /var/service
KillSignal=SIGHUP
KillMode=process
Restart=always
SuccessExitStatus=111

[Install]
WantedBy=multi-user.target
```

Example 4 (unknown):
```unknown
${XDG_CONFIG_HOME:-$HOME/.config}/systemd/user/busybox-runit.service
```

---

## Kernel module

**URL:** https://wiki.archlinux.org/title/Kernel_module_parameter

**Contents:**
- Obtaining information
- Automatic module loading
  - Early module loading
  - systemd
- Manual module handling
- Setting module options
  - Using modprobe
  - Using modprobe.d
  - Using kernel command line
- Aliasing

Kernel modules are pieces of code that can be loaded and unloaded into the kernel upon demand. They extend the functionality of the kernel without the need to reboot the system.

To create a kernel module, you can read The Linux Kernel Module Programming Guide. A module can be configured as built-in or loadable. To dynamically load or remove a module, it has to be configured as a loadable module in the kernel configuration (the line related to the module will therefore display the letter M).

To rebuild a kernel module automatically when a new kernel is installed, see Dynamic Kernel Module Support (DKMS).

Usually modules depend on the kernel release and are stored in the /usr/lib/modules/kernel_release/ directory.

To show what kernel modules are currently loaded:

To show information about a module:

To list the options that are set for a loaded module use systool(1) from sysfsutils:

To display the comprehensive configuration of all the modules:

To display the configuration of a particular module:

List the dependencies of a module (or alias), including the module itself:

Today, all necessary modules loading is handled automatically by udev, so if you do not need to use any out-of-tree kernel modules, there is no need to put modules that should be loaded at boot in any configuration file. However, there are cases where you might want to load an extra module during the boot process, or blacklist another one for your computer to function properly.

Early module loading depends on the initramfs generator used:

Kernel modules can be explicitly listed in files under /etc/modules-load.d/ for systemd to load them during boot. Each configuration file is named in the style of /etc/modules-load.d/program.conf. Configuration files simply contain a list of kernel modules names to load, separated by newlines. Empty lines and lines whose first non-whitespace character is # or ; are ignored.

See modules-load.d(5) for more details.

Kernel modules are handled by tools provided by the kmod package, which is installed as a dependency of a kernel package. You can use these tools manually. To load a module:

To load a module by a file name—i.e. one that is not installed in the /usr/lib/modules/kernel_release/ directory—use any of:

To unload—remove—a module, use any of:

To pass a parameter to a kernel module, you can pass them manually with modprobe or assure certain parameters are always applied using a modprobe configuration file or by using the kernel command line. If the module is built into the kernel, the kernel command line must be used and other methods will not work.

The basic way to pass parameters to a module is using the modprobe command. Parameters are specified on command line using simple key=value assignments:

Configuration files in the /etc/modprobe.d/ directory can be used to pass module settings to udev, which will use modprobe to manage the loading of the modules during system boot. Files in this directory can have any name, given that they end with the .conf extension. The file name matters, see modprobe.d(5) § CONFIGURATION DIRECTORIES AND PRECEDENCE. To show the effective configuration:

Multiple module parameters are separated by spaces, in turn a parameter can receive a list of values which is separated by commas:

You can also pass options to the module using the kernel command line. This is the only working option for modules built into the kernel. For all common boot loaders, the following syntax is correct:

Simply add this to the appropriate line in your boot loader configuration, as described in Kernel parameters#Boot loader configuration.

Aliases are alternate names for a module. For example: alias my-mod really_long_modulename means you can use modprobe my-mod instead of modprobe really_long_modulename. You can also use shell-style wildcards, so alias my-mod* really_long_modulename means that modprobe my-mod-something has the same effect. Create an alias:

Aliases can be internal—contained in the module itself. Internal aliases are usually used for #Automatic module loading when it is needed by an application, e.g. when the kernel detects a new device. To see the module internal aliases:

To see both configured and internal aliases:

Blacklisting, in the context of kernel modules, is a mechanism to prevent the kernel module from loading. This could be useful if, for example, the associated hardware is not needed, or if loading that module causes problems: for instance there may be two kernel modules that try to control the same piece of hardware, and loading them together would result in a conflict.

Some modules are loaded as part of the initramfs. mkinitcpio -M will print out all automatically detected modules: to prevent the initramfs from loading some of those modules, blacklist them in a .conf file under /etc/modprobe.d and it shall be added in by the modconf hook during image generation. Running mkinitcpio -v will list all modules pulled in by the various hooks (e.g. filesystems hook, block hook, etc.). Remember to add that .conf file to the FILES array in /etc/mkinitcpio.conf if you do not have the modconf hook in your HOOKS array (e.g. you have deviated from the default configuration), and once you have blacklisted the modules regenerate the initramfs, and reboot afterwards.

Disable an alias by overriding. For example, to prevent Bluetooth module autoloading (assuming a module named off does not exist):

To disable all internal aliases for a given module use the blacklist keyword. For example, to prevent the pcspkr module from loading on boot to avoid sounds through the PC speaker:

There is a workaround for the behaviour described in the #alias and #blacklist notes. The install configuration command instructs modprobe to run a custom command instead of inserting the module in the kernel as normal, so you can simulate the successful module loading with:

You can force the module to always fail loading with /bin/false: this will effectively prevent the module—and any other that depends on it—from loading by any means, and a log error message may be produced.

You can also blacklist modules from the boot loader boot entry configuration.

Simply add module_blacklist=module_name_1,module_name_2,module_name_3 to your kernel command line, as described in Kernel parameters#Boot loader configuration.

Another use case for a command line option is to disable hardware-specific components of a module without disabling the module entirely. For example, disabling a microphone while retaining other sound out options. See BBS#303475 for a few examples.

In case a specific module does not load and the boot log (accessible by running journalctl -b as root) says that the module is blacklisted, but the directory /etc/modprobe.d/ does not show a corresponding entry, check another modprobe source directory at /usr/lib/modprobe.d/ for blacklisting entries.

A module will not be loaded if the "vermagic" string contained within the kernel module does not match the value of the currently running kernel. If it is known that the module is compatible with the current running kernel the "vermagic" check can be ignored with modprobe --force-vermagic.

**Examples:**

Example 1 (unknown):
```unknown
/usr/lib/modules/kernel_release/
```

Example 2 (unknown):
```unknown
uname --kernel-release
```

Example 3 (unknown):
```unknown
/etc/modprobe.d/
```

Example 4 (unknown):
```unknown
$ modinfo module_name
```

---

## BIND

**URL:** https://wiki.archlinux.org/title/BIND

**Contents:**
- Installation
- Configuration
  - Enable rndc access
  - Restrict access to localhost
  - Set up DNS forwarding
  - Serve DNS over TLS or HTTPS
- A configuration template for running a domain
  - Creating a zonefile
  - Configuring primary server
  - Configuring secondary server

BIND (or named) is the most widely used Domain Name System (DNS) server.

Install the bind package.

Start/enable the named.service systemd unit.

To use the DNS server locally, use the 127.0.0.1 nameserver (meaning clients like Firefox resolve via 127.0.0.1), see Domain name resolution. This will however require you to #Allow recursion while a firewall might block outside queries to your local named.

This article or section needs expansion.

BIND is configured in /etc/named.conf. The available options are documented in named.conf(5).

Reload the named.service unit to apply configuration changes.

In order to control BIND using rndc(8), you need to set up TSIG key for it and tell BIND to allow name server control with that key.

First, generate TSIG key for this purpose:

Copy the output to both /etc/named.conf and /etc/rndc.conf.

In /etc/rndc.conf, append default name server connection:

Allow rndc control from localhost in /etc/named.conf:

Now you can use rndc to control the local DNS server. For example, to check server status:

Reloading server configuration can also be done by:

BIND by default listens on port 53 of all interfaces and IP addresses. To only allow connections from localhost, add the following line to the options section in /etc/named.conf:

To make BIND forward DNS queries to another DNS server, add the forwarders clause to the options section.

For example, to make BIND forward to the Google Public DNS servers using DNS over TLS:

To enable serving DNS over TLS or HTTPS in BIND, define a tls block specifying your certificate, then add listen-on clauses enabling DNS over TLS and HTTPS listeners (as well as a standard DNS listener).

Note that tls{} is defined at the top level, not inside the options{} block.

Following is a simple home nameserver being set up, using domain.tld as the domain being served world-wide like this wiki's archlinux.org domain is.

A more elaborate example is DNS server with BIND9, while this shows how to set up internal network name resolution.

Create /var/named/domain.tld.zone.

$ORIGIN defines the default suffix for all names which do not already end with a . (dot), e.g. mail will be expanded to mail.$ORIGIN ⇒ mail.domain.tld. everywhere.

$TTL defines the default time-to-live (i.e. cache expiry time) for all records which do not have their own TTL specified. Here it is 2 hours.

Serial must be incremented manually before reloading named every time you change a resource record for the zone. Otherwise secondary servers (replicas or slaves) will not re-transfer the zone: they only do it if the serial is greater than that of the last time they transferred the zone. This example uses the somewhat common YYYYMMDDXX format, but this is not required; the serial number can also just start at 1.

Add your zone to /etc/named.conf:

Reload the named.service unit to apply the configuration change.

In case you have two or more servers, you can set up all but one of them as secondary, where the zone is retrieved from primary via AXFR transfer.

On primary, explicitly allow zone transfers from secondary:

On secondary, add the zone to /etc/named.conf:

Reload named.service or use rndc(8) to reload both primary and secondary servers configuration.

If you are running your own DNS server, you may want it to perform recursive lookups (i.e., resolve domains it is not authoritative for) on behalf of clients — either for your own machine or for a local network. This requires enabling recursion in BIND and carefully limiting which clients are allowed to use it.

By default, to mitigate DNS amplification attacks, recursion is disabled for all clients except the loopback interface in the default Arch /etc/named.conf. This is typically configured with:

To allow recursive queries from clients on your LAN (e.g., 192.168.0.0/24), you must explicitly define an ACL and use it with allow-recursion and allow-query-cache. For example:

See What has changed in the behavior of "allow-recursion" and "allow-query-cache"?

BIND makes it easy to serve DNSSEC-signed zones with Key and Signing Policy (KASP) facility, where it takes care of zone signing and key management automatically for you using user-defined policy.

Create separate directory to store keys, writable by the server:

Instruct BIND to store keys in aforementioned directory:

Now zones can be signed by applying the desired policy. In most cases, you can apply up-to-date DNSSEC best practices with default policy:

This will sign the zone with single ecdsap256sha256 combined signing key (CSK) with unlimited key lifetime, signed via inline-signing where the server generates the signed zone separately without having to set-up dynamic DNS.

If needed, a custom policy can be defined with dnssec-policy block. For example, the main policy signs the zone with traditional key signing key (KSK, rotated yearly) and zone signing key (ZSK, rotated monthly) split, alongside with timing parameters and NSEC3 denial-of-existence:

The custom policy can be applied in the same way as default policy earlier:

Increment the zone's serial and reload the server configuration.

The last step is to establish chain of trust with parent zone. First, check DNSSEC status of the zone:

Here, the KSK key ID is 58785. Yours will be different.

Generate the DS record (replace key ID as appropriate):

Submit DS to the parent zone (for example, by filling the web form on your registrar).

When you've confirmed that the DS record has been published in the parent zone, you can signal BIND as such by:

By default, BIND scans for new interfaces and stops listening on interfaces which no longer exist every hour. You can tune this value by adding:

The max value is 28 days (40320 min). You can disable this feature by setting its value to 0.

Then restart the service.

Running in a chroot environment is not required but improves security.

This article or section needs expansion.

In order to do this, we first need to create a place to keep the jail, we shall use /srv/named, and then put the required files into the jail.

Copy over required system files:

Set up required nodes in /dev/:

Set ownership of the files:

This should create the required file system for the jail.

Next we need a replacement unit file so that the service calls bind which will allow force bind into the chroot:

Now, reload systemd with daemon-reload, and start the named-chroot.service.

If you do not want to rely on third-party DNS services, you can serve the root zone locally following RFC:7706. This can be achieved by using BIND as a DNS recursive resolver.

To manage a recursive resolver, you typically need to configure a root hints file. This file contains the names and IP addresses of the authoritative name servers for the root zone.

Grab the file from IANA website and place it into /var/named.

Edit your server config, adding the respective file:

Recursion also should be allowed in the config. See #Allow recursion.

**Examples:**

Example 1 (unknown):
```unknown
named.service
```

Example 2 (unknown):
```unknown
/etc/resolv.conf
```

Example 3 (unknown):
```unknown
/etc/named.conf
```

Example 4 (unknown):
```unknown
named.service
```

---

## Users and groups

**URL:** https://wiki.archlinux.org/title/Wheel

**Contents:**
- Overview
- Permissions and ownership
- Shadow
- File list
- User management
  - Example adding a user
    - Changing user defaults
  - Example adding a system user
  - Change a user's login name or home directory
  - Other examples of user management

Users and groups are used on GNU/Linux for access control—that is, to control access to the system's files, directories, and peripherals. Linux offers relatively simple/coarse access control mechanisms by default. For more advanced options, see ACL, Capabilities and PAM#Configuration How-Tos.

A user is anyone who uses a computer. In this case, we are describing the names which represent those users. It may be Mary or Bill, and they may use the names Dragonlady or Pirate in place of their real name. All that matters is that the computer has a name for each account it creates, and it is this name by which a person gains access to use the computer. Some system services also run using restricted or privileged user accounts.

Managing users is done for the purpose of security by limiting access in certain specific ways. The superuser (root) has complete access to the operating system and its configuration; it is intended for administrative use only. Unprivileged users can use several programs for controlled privilege elevation.

Any individual may have more than one account as long as they use a different name for each account they create. Further, there are some reserved names which may not be used such as "root".

Users may be grouped together into a "group", and users may be added to an existing group to utilize the privileged access it grants.

From In UNIX Everything is a File:

From Extending UNIX File Abstraction for General-Purpose Networking:

Every file on a GNU/Linux system is owned by a user and a group. In addition, there are three types of access permissions: read, write, and execute. Different access permissions can be applied to a file's owning user, owning group, and others (those without ownership). One can determine a file's owners and permissions by viewing the long listing format of the ls command:

The first column displays the file's permissions (for example, the file initramfs-linux.img has permissions -rw-r--r--). The third and fourth columns display the file's owning user and group, respectively. In this example, all files are owned by the root user and the root group.

In this example, the sf_Shared directory is owned by the root user and the vboxsf group. It is also possible to determine a file's owners and permissions using the stat command:

Access permissions are displayed in three groups of characters, representing the permissions of the owning user, owning group, and others, respectively. For example, the characters -rw-r--r-- indicate that the file's owner has read and write permission, but not execute (rw-), whilst users belonging to the owning group and other users have only read permission (r-- and r--). Meanwhile, the characters drwxrwx--- indicate that the file's owner and users belonging to the owning group all have read, write, and execute permissions (rwx and rwx), whilst other users are denied access (---). The first character represents the file's type.

List files owned by a user or group with the find utility:

A file's owning user and group can be changed with the chown (change owner) command. A file's access permissions can be changed with the chmod (change mode) command.

See chown(1), chmod(1), and Linux file permissions for additional detail.

The user, group and password management tools on Arch Linux come from the shadow package, which is a dependency of the base meta package.

To list users currently logged on the system, the who command can be used. To list all existing user accounts including their properties stored in the user database, run passwd -Sa as root. See passwd(1) for the description of the output format.

To add a new user, use the useradd command:

If an initial login group is specified by name or number, it must refer to an already existing group. If not specified, the behaviour of useradd will depend on the USERGROUPS_ENAB variable contained in /etc/login.defs. The default behaviour (USERGROUPS_ENAB yes) is to create a group with the same name as the username.

When the login shell is intended to be non-functional, for example when the user account is created for a specific service, /usr/bin/nologin may be specified in place of a regular shell to politely refuse a login (see nologin(8)).

See useradd(8) for other supported options.

To add a new user named archie, creating its home directory and otherwise using all the defaults in terms of groups, directory names, shell used and various other parameters:

Although it is not required to protect the newly created user archie with a password, it is highly recommended to do so:

The above useradd command will also automatically create a group called archie and makes this the default group for the user archie. Making each user have their own group (with the group name same as the user name) is the preferred way to add users.

You could also make the default group something else using the -g option, but note that, in multi-user systems, using a single default group (e.g. users) for every user is not recommended. The reason is that typically, the method for facilitating shared write access for specific groups of users is setting user umask value to 002, which means that the default group will by default always have write access to any file you create. See also User Private Groups. If a user must be a member of a specific group specify that group as a supplementary group when creating the user.

In the recommended scenario, where the default group has the same name as the user name, all files are by default writeable only for the user who created them. To allow write access to a specific group, shared files/directories can be made writeable by default for everyone in this group and the owning group can be automatically fixed to the group which owns the parent directory by setting the setgid bit on this directory:

Otherwise the file creator's default group (usually the same as the user name) is used.

If a GID change is required temporarily you can also use the newgrp command to change the user's default GID to another GID at runtime. For example, after executing newgrp groupname files created by the user will be associated with the groupname GID, without requiring a re-login. To change back to the default GID, execute newgrp without a groupname.

The default values used for creating new accounts are set in /etc/default/useradd and can be displayed using useradd --defaults. For example, to change the default shell globally, set SHELL=/usr/bin/shell. A different shell can also be specified individually with the -s/--shell option. Use chsh -l to list valid login shells.

Files can also be specified to be added to newly created user home directories in /etc/skel. This is useful for minimalist window managers where config files need manual configuration to reach DE-familiar behavior. For example, to set up default shortcuts for all newly created users:

See also: Display manager#Run ~/.xinitrc as a session to add xinitrc as an option to all users on the display manager.

System users can be used to run processes/daemons under a different user, protecting (e.g. with chown) files and/or directories and more examples of computer hardening.

With the following command a system user without shell access and without a home directory is created (optionally append the -U parameter to create a group with the same name as the user, and add the user to this group):

If the system user requires a specific user and group ID, specify them with the -u/--uid and -g/--gid options when creating the user:

To change a user's home directory:

The -m option also automatically creates the new directory and moves the content there.

Make sure there is no trailing / on /my/old/home.

To change a user's login name:

Changing a username is safe and easy when done properly, just use the usermod command. If the user is associated to a group with the same name, you can rename this with the groupmod command.

Alternatively, the /etc/passwd file can be edited directly, see #User database for an introduction to its format.

Also keep in mind the following notes:

To enter user information for the GECOS comment (e.g. the full user name), type:

(this way chfn runs in interactive mode).

Alternatively the GECOS comment can be set more liberally with:

To mark a user's password as expired, requiring them to create a new password the first time they log in, type:

User accounts may be deleted with the userdel command:

The -r option specifies that the user's home directory and mail spool should also be deleted.

To change the user's login shell:

Local user information is stored in the plain-text /etc/passwd file: each of its lines represents a user account, and has seven fields delimited by colons.

Broken down, this means: user archie, whose password is in /etc/shadow, whose UID is 1001 and whose primary group is 1003. Archie is their full name and there is a comment associated to their account; their home directory is /home/archie and they are using Bash.

The pwck command can be used to verify the integrity of the user database. It can sort the user list by UID at the same time, which can be helpful for comparison:

Instead of running pwck/grpck manually, the systemd timer shadow.timer, which is part of, and is enabled by, installation of the shadow package, will start shadow.service daily. shadow.service will run pwck(8) and grpck(8) to verify the integrity of both password and group files.

If discrepancies are reported, group can be edited with the vigr(8) command and users with vipw(8). This provides an extra margin of protection in that these commands lock the databases for editing. Note that the default text editor is vi, but an alternative editor will be used if the EDITOR environment variable is set, then that editor will be used instead.

As packages adopt the change-sysusers-to-fully-locked-system-accounts, package created system users generated in the past will not inherit new package defaults for the increased security of locked/expired status. These users need to be modified manually for this change. user-analysis.sh does just that.

In addition, the script also identifies orphaned users (those created by a package no longer on the system) and can automatically delete them.

/etc/group is the file that defines the groups on the system (see group(5) for details). There is also its companion gshadow which is rarely used. Its details are at gshadow(5).

Display group membership with the groups command:

If user is omitted, the current user's group names are displayed.

The id command provides additional detail, such as the user's UID and associated GIDs:

To list all groups on the system:

Create new groups with the groupadd command:

Add users to a group with the gpasswd command (see FS#58262 regarding errors):

Alternatively, add a user to additional groups with usermod (replace additional_groups with a comma-separated list):

Modify an existing group with the groupmod command, e.g. to rename the old_group group to new_group:

To delete existing groups:

To remove users from a group:

The grpck command can be used to verify the integrity of the system's group files.

This section explains the purpose of the essential groups from the filesystem package. There are many other groups, which will be created with correct GID when the relevant package is installed. See the main page for the software for details.

Non-root workstation/desktop users often need to be added to some of following groups to allow access to hardware peripherals and facilitate system administration:

The following groups are used for system purposes, an assignment to users is only required for dedicated purposes:

Before arch migrated to systemd, users had to be manually added to these groups in order to be able to access the corresponding devices. This way has been deprecated in favour of udev marking the devices with a uaccess tag and logind assigning the permissions to users dynamically via ACLs according to which session is currently active. Note that the session must not be broken for this to work (see General troubleshooting#Session permissions to check it).

There are some notable exceptions which require adding a user to some of these groups: for example if you want to allow users to access the device even when they are not logged in. However, note that adding users to the groups can even cause some functionality to break (for example, the audio group will break fast user switching and allows applications to block software mixing).

Now solely for direct access to tapes if no custom udev rules is involved.[5][6][7].

Also required for manipulating some devices via udisks/udisksctl.

The following groups are currently not used for any purpose:

This article or section is a candidate for merging with #Shadow.

The factual accuracy of this article or section is disputed.

getent(1) can be used to read a particular record.

As warned in #User database, using specific utilities such as passwd and chfn, is a better way to change the databases. Nevertheless, there are times when editing them directly is looked after. For those times, vipw, vigr are provided. It is strongly recommended to use these tailored editors over using a general text editor as they lock the databases against concurrent editing. They also help prevent invalid entries and/or syntax errors. Note that Arch Linux prefers usage of specific tools, such as chage, for modifying the shadow database over using vipw -s and vigr -s from util-linux. See also FS#31414.

lslogins(1) and lastlog2(1) are some of the tools for viewing utmp related files.

**Examples:**

Example 1 (unknown):
```unknown
cat /dev/audio > myfile
```

Example 2 (unknown):
```unknown
cat myfile > /dev/audio
```

Example 3 (unknown):
```unknown
$ ls -l /boot/
```

Example 4 (unknown):
```unknown
total 13740
drwxr-xr-x 2 root root    4096 Jan 12 00:33 grub
-rw-r--r-- 1 root root 8570335 Jan 12 00:33 initramfs-linux-fallback.img
-rw-r--r-- 1 root root 1821573 Jan 12 00:31 initramfs-linux.img
-rw-r--r-- 1 root root 1457315 Jan  8 08:19 System.map26
-rw-r--r-- 1 root root 2209920 Jan  8 08:19 vmlinuz-linux
```

---

## systemd/Journal

**URL:** https://wiki.archlinux.org/title/Journal

**Contents:**
- Priority level
- Facility
- Filtering output
- Tips and tricks
  - Journal size limit
    - Per unit size limit by a journal namespace
  - Clean journal files manually
  - Journald in conjunction with syslog
  - Forward journald to /dev/tty12
  - Specify a different journal to view

systemd has its own logging system called the journal; running a separate logging daemon is not required. To read the log, use journalctl(1).

In Arch Linux, the directory /var/log/journal/ is a part of the systemd package, and the journal (when Storage= is set to auto in /etc/systemd/journald.conf) will write to /var/log/journal/. If that directory is deleted, systemd will not recreate it automatically and instead will write its logs to /run/log/journal/ in a non-persistent way. However, the directory will be recreated if Storage=persistent is added to journald.conf and systemd-journald.service is restarted (or the system is rebooted).

Systemd journal classifies messages by Priority level and Facility. Logging classification corresponds to classic Syslog protocol (RFC 5424).

A syslog severity code (in systemd called priority) is used to mark the importance of a message RFC 5424 6.2.1.

These rules are recommendations, and the priority level of a given error is at the application developer's discretion. It is always possible that the error will be at a higher or lower level than expected.

A syslog facility code is used to specify the type of program that is logging the message RFC 5424 6.2.1.

Useful facilities to watch: 0, 1, 3, 4, 9, 10, 15.

journalctl allows for the filtering of output by specific fields. If there are many messages to display, or if the filtering of large time spans has to be done, the output of this command can be extensively delayed.

See journalctl(1), systemd.journal-fields(7), or Lennart Poettering's blog post for details.

If the journal is persistent (non-volatile), its size limit is set to a default value of 10% of the size of the underlying file system but capped at 4 GiB. For example, with /var/log/journal/ located on a 20 GiB partition, journal data may take up to 2 GiB. On a 50 GiB partition, it would max at 4 GiB. To confirm current limits on your system review systemd-journald unit logs:

The maximum size of the persistent journal can be controlled by uncommenting and changing the following:

It is also possible to use the drop-in snippets configuration override mechanism rather than editing the global configuration file. In this case, place the overrides under the [Journal] header:

Restart the systemd-journald.service after changing this setting to apply the new limit.

See journald.conf(5) for more info.

Edit the unit file for the service you wish to configure (for example sshd) and add LogNamespace=ssh in the [Service] section.

Then create /etc/systemd/journald@ssh.conf by copying /etc/systemd/journald.conf. After that, edit journald@ssh.conf and adjust SystemMaxUse to your liking.

Restarting the service should automatically start the new journal service systemd-journald@ssh.service. The logs from the namespaced service can be viewed with journalctl --namespace ssh.

See systemd-journald.service(8) § JOURNAL NAMESPACES for details about journal namespaces.

Journal files can be globally removed from /var/log/journal/ using e.g. rm, or can be trimmed according to various criteria using journalctl. For example:

Journal files must have been rotated out and made inactive before they can be trimmed by vacuum commands. Rotation of journal files can be done by running journalctl --rotate. The --rotate argument can also be provided alongside one or more vacuum criteria arguments to perform rotation and then trim files in a single command.

See journalctl(1) for more info.

Compatibility with a classic, non-journald aware syslog implementation can be provided by letting systemd forward all messages via the socket /run/systemd/journal/syslog. To make the syslog daemon work with the journal, it has to bind to this socket instead of /dev/log (official announcement).

The default journald.conf for forwarding to the socket is ForwardToSyslog=no to avoid system overhead, because rsyslog or syslog-ng pull the messages from the journal by itself.

See Syslog-ng#Overview and Syslog-ng#syslog-ng and systemd journal, or rsyslog respectively, for details on configuration.

Create a drop-in directory /etc/systemd/journald.conf.d and create a fw-tty12.conf file in it:

Then restart systemd-journald.service.

There may be a need to check the logs of another system that is dead in the water, like booting from a live system to recover a production system. In such case, one can mount the disk in e.g. /mnt, and specify the journal path via -D/--directory, like so:

By default, a regular user only has access to their own per-user journal. To grant read access for the system journal as a regular user, you can add that user to the systemd-journal user group. Members of the adm and wheel groups are also given read access.

See journalctl(1) § DESCRIPTION and Users and groups#User groups for more information.

Desktop notifications can help you quickly notice error messages, improving awareness compared to manually checking logs or not noticing them at all.

The journalctl-desktop-notificationAUR package provides an automatic desktop notification for each error message logged by any process. For more details and configuration options, visit the GitLab project page.

**Examples:**

Example 1 (unknown):
```unknown
/var/log/journal/
```

Example 2 (unknown):
```unknown
/etc/systemd/journald.conf
```

Example 3 (unknown):
```unknown
/var/log/journal/
```

Example 4 (unknown):
```unknown
/run/log/journal/
```

---

## XDMCP

**URL:** https://wiki.archlinux.org/title/XDMCP

**Contents:**
- Setup graphical logins
  - XDM
  - GDM
  - LightDM
- Accessing X from a remote machine on your LAN
- Thin client setup
  - GUI-based Clients
- Troubleshooting
  - Session declined: Maximum Number of Sessions Reached

This allows for opening X sessions remotely.

Comment the last line of the configuration file with a !:

Then allow any host to get a login window in by un-commenting the following:

In case you have multiple network interfaces also add a line like this:

Where 192.168.0.10 is your server IP address.

Then restart xdm.service.

Enable XDMCP in the configuration file:

Then restart gdm.service.

Enable the XDMCP Server in the configuration file:

On a headless system, disable the automatic start of one seat so that LightDM can run in the background:

Then restart lightdm.service.

You can access your login manager on the network computer (using 192.168.0.10 in the following command). TCP and UDP streams are opened. So it is not possible to access the login manager via an SSH connection.

Or, with Xephyr, if you experience refreshing problems with Xnest:

Or, if you are on runlevel 3

Xserver should recognize your monitor and set appropriate resolution.

After allowing XDMCP access as described above, edit /etc/X11/xdm/Xservers and comment out:

Then launch XDM as root, e.g. xdm -config /etc/X11/xdm/archlinux/xdm-config

This article or section needs expansion.

First of all one should setup dhcp and tftp server. Dnsmasq has both of them. For network boot image check thinstation project. If your network card does not support PXE, you can try Etherboot

Edit /etc/gdm/custom.conf and increase MaxSessions. It is worth noting that there may be a limit regarding how many XDMCP connections are allowed at once, so it's worth increasing DisplaysPerHost as well[1] if multiple clients are connecting from the same IP.

**Examples:**

Example 1 (unknown):
```unknown
/etc/X11/xdm/xdm-config
```

Example 2 (unknown):
```unknown
!DisplayManager.requestPort:    0
```

Example 3 (unknown):
```unknown
/etc/X11/xdm/Xaccess
```

Example 4 (unknown):
```unknown
*             #any host can get a login window
```

---

## Users and groups

**URL:** https://wiki.archlinux.org/title/User_group

**Contents:**
- Overview
- Permissions and ownership
- Shadow
- File list
- User management
  - Example adding a user
    - Changing user defaults
  - Example adding a system user
  - Change a user's login name or home directory
  - Other examples of user management

Users and groups are used on GNU/Linux for access control—that is, to control access to the system's files, directories, and peripherals. Linux offers relatively simple/coarse access control mechanisms by default. For more advanced options, see ACL, Capabilities and PAM#Configuration How-Tos.

A user is anyone who uses a computer. In this case, we are describing the names which represent those users. It may be Mary or Bill, and they may use the names Dragonlady or Pirate in place of their real name. All that matters is that the computer has a name for each account it creates, and it is this name by which a person gains access to use the computer. Some system services also run using restricted or privileged user accounts.

Managing users is done for the purpose of security by limiting access in certain specific ways. The superuser (root) has complete access to the operating system and its configuration; it is intended for administrative use only. Unprivileged users can use several programs for controlled privilege elevation.

Any individual may have more than one account as long as they use a different name for each account they create. Further, there are some reserved names which may not be used such as "root".

Users may be grouped together into a "group", and users may be added to an existing group to utilize the privileged access it grants.

From In UNIX Everything is a File:

From Extending UNIX File Abstraction for General-Purpose Networking:

Every file on a GNU/Linux system is owned by a user and a group. In addition, there are three types of access permissions: read, write, and execute. Different access permissions can be applied to a file's owning user, owning group, and others (those without ownership). One can determine a file's owners and permissions by viewing the long listing format of the ls command:

The first column displays the file's permissions (for example, the file initramfs-linux.img has permissions -rw-r--r--). The third and fourth columns display the file's owning user and group, respectively. In this example, all files are owned by the root user and the root group.

In this example, the sf_Shared directory is owned by the root user and the vboxsf group. It is also possible to determine a file's owners and permissions using the stat command:

Access permissions are displayed in three groups of characters, representing the permissions of the owning user, owning group, and others, respectively. For example, the characters -rw-r--r-- indicate that the file's owner has read and write permission, but not execute (rw-), whilst users belonging to the owning group and other users have only read permission (r-- and r--). Meanwhile, the characters drwxrwx--- indicate that the file's owner and users belonging to the owning group all have read, write, and execute permissions (rwx and rwx), whilst other users are denied access (---). The first character represents the file's type.

List files owned by a user or group with the find utility:

A file's owning user and group can be changed with the chown (change owner) command. A file's access permissions can be changed with the chmod (change mode) command.

See chown(1), chmod(1), and Linux file permissions for additional detail.

The user, group and password management tools on Arch Linux come from the shadow package, which is a dependency of the base meta package.

To list users currently logged on the system, the who command can be used. To list all existing user accounts including their properties stored in the user database, run passwd -Sa as root. See passwd(1) for the description of the output format.

To add a new user, use the useradd command:

If an initial login group is specified by name or number, it must refer to an already existing group. If not specified, the behaviour of useradd will depend on the USERGROUPS_ENAB variable contained in /etc/login.defs. The default behaviour (USERGROUPS_ENAB yes) is to create a group with the same name as the username.

When the login shell is intended to be non-functional, for example when the user account is created for a specific service, /usr/bin/nologin may be specified in place of a regular shell to politely refuse a login (see nologin(8)).

See useradd(8) for other supported options.

To add a new user named archie, creating its home directory and otherwise using all the defaults in terms of groups, directory names, shell used and various other parameters:

Although it is not required to protect the newly created user archie with a password, it is highly recommended to do so:

The above useradd command will also automatically create a group called archie and makes this the default group for the user archie. Making each user have their own group (with the group name same as the user name) is the preferred way to add users.

You could also make the default group something else using the -g option, but note that, in multi-user systems, using a single default group (e.g. users) for every user is not recommended. The reason is that typically, the method for facilitating shared write access for specific groups of users is setting user umask value to 002, which means that the default group will by default always have write access to any file you create. See also User Private Groups. If a user must be a member of a specific group specify that group as a supplementary group when creating the user.

In the recommended scenario, where the default group has the same name as the user name, all files are by default writeable only for the user who created them. To allow write access to a specific group, shared files/directories can be made writeable by default for everyone in this group and the owning group can be automatically fixed to the group which owns the parent directory by setting the setgid bit on this directory:

Otherwise the file creator's default group (usually the same as the user name) is used.

If a GID change is required temporarily you can also use the newgrp command to change the user's default GID to another GID at runtime. For example, after executing newgrp groupname files created by the user will be associated with the groupname GID, without requiring a re-login. To change back to the default GID, execute newgrp without a groupname.

The default values used for creating new accounts are set in /etc/default/useradd and can be displayed using useradd --defaults. For example, to change the default shell globally, set SHELL=/usr/bin/shell. A different shell can also be specified individually with the -s/--shell option. Use chsh -l to list valid login shells.

Files can also be specified to be added to newly created user home directories in /etc/skel. This is useful for minimalist window managers where config files need manual configuration to reach DE-familiar behavior. For example, to set up default shortcuts for all newly created users:

See also: Display manager#Run ~/.xinitrc as a session to add xinitrc as an option to all users on the display manager.

System users can be used to run processes/daemons under a different user, protecting (e.g. with chown) files and/or directories and more examples of computer hardening.

With the following command a system user without shell access and without a home directory is created (optionally append the -U parameter to create a group with the same name as the user, and add the user to this group):

If the system user requires a specific user and group ID, specify them with the -u/--uid and -g/--gid options when creating the user:

To change a user's home directory:

The -m option also automatically creates the new directory and moves the content there.

Make sure there is no trailing / on /my/old/home.

To change a user's login name:

Changing a username is safe and easy when done properly, just use the usermod command. If the user is associated to a group with the same name, you can rename this with the groupmod command.

Alternatively, the /etc/passwd file can be edited directly, see #User database for an introduction to its format.

Also keep in mind the following notes:

To enter user information for the GECOS comment (e.g. the full user name), type:

(this way chfn runs in interactive mode).

Alternatively the GECOS comment can be set more liberally with:

To mark a user's password as expired, requiring them to create a new password the first time they log in, type:

User accounts may be deleted with the userdel command:

The -r option specifies that the user's home directory and mail spool should also be deleted.

To change the user's login shell:

Local user information is stored in the plain-text /etc/passwd file: each of its lines represents a user account, and has seven fields delimited by colons.

Broken down, this means: user archie, whose password is in /etc/shadow, whose UID is 1001 and whose primary group is 1003. Archie is their full name and there is a comment associated to their account; their home directory is /home/archie and they are using Bash.

The pwck command can be used to verify the integrity of the user database. It can sort the user list by UID at the same time, which can be helpful for comparison:

Instead of running pwck/grpck manually, the systemd timer shadow.timer, which is part of, and is enabled by, installation of the shadow package, will start shadow.service daily. shadow.service will run pwck(8) and grpck(8) to verify the integrity of both password and group files.

If discrepancies are reported, group can be edited with the vigr(8) command and users with vipw(8). This provides an extra margin of protection in that these commands lock the databases for editing. Note that the default text editor is vi, but an alternative editor will be used if the EDITOR environment variable is set, then that editor will be used instead.

As packages adopt the change-sysusers-to-fully-locked-system-accounts, package created system users generated in the past will not inherit new package defaults for the increased security of locked/expired status. These users need to be modified manually for this change. user-analysis.sh does just that.

In addition, the script also identifies orphaned users (those created by a package no longer on the system) and can automatically delete them.

/etc/group is the file that defines the groups on the system (see group(5) for details). There is also its companion gshadow which is rarely used. Its details are at gshadow(5).

Display group membership with the groups command:

If user is omitted, the current user's group names are displayed.

The id command provides additional detail, such as the user's UID and associated GIDs:

To list all groups on the system:

Create new groups with the groupadd command:

Add users to a group with the gpasswd command (see FS#58262 regarding errors):

Alternatively, add a user to additional groups with usermod (replace additional_groups with a comma-separated list):

Modify an existing group with the groupmod command, e.g. to rename the old_group group to new_group:

To delete existing groups:

To remove users from a group:

The grpck command can be used to verify the integrity of the system's group files.

This section explains the purpose of the essential groups from the filesystem package. There are many other groups, which will be created with correct GID when the relevant package is installed. See the main page for the software for details.

Non-root workstation/desktop users often need to be added to some of following groups to allow access to hardware peripherals and facilitate system administration:

The following groups are used for system purposes, an assignment to users is only required for dedicated purposes:

Before arch migrated to systemd, users had to be manually added to these groups in order to be able to access the corresponding devices. This way has been deprecated in favour of udev marking the devices with a uaccess tag and logind assigning the permissions to users dynamically via ACLs according to which session is currently active. Note that the session must not be broken for this to work (see General troubleshooting#Session permissions to check it).

There are some notable exceptions which require adding a user to some of these groups: for example if you want to allow users to access the device even when they are not logged in. However, note that adding users to the groups can even cause some functionality to break (for example, the audio group will break fast user switching and allows applications to block software mixing).

Now solely for direct access to tapes if no custom udev rules is involved.[5][6][7].

Also required for manipulating some devices via udisks/udisksctl.

The following groups are currently not used for any purpose:

This article or section is a candidate for merging with #Shadow.

The factual accuracy of this article or section is disputed.

getent(1) can be used to read a particular record.

As warned in #User database, using specific utilities such as passwd and chfn, is a better way to change the databases. Nevertheless, there are times when editing them directly is looked after. For those times, vipw, vigr are provided. It is strongly recommended to use these tailored editors over using a general text editor as they lock the databases against concurrent editing. They also help prevent invalid entries and/or syntax errors. Note that Arch Linux prefers usage of specific tools, such as chage, for modifying the shadow database over using vipw -s and vigr -s from util-linux. See also FS#31414.

lslogins(1) and lastlog2(1) are some of the tools for viewing utmp related files.

**Examples:**

Example 1 (unknown):
```unknown
cat /dev/audio > myfile
```

Example 2 (unknown):
```unknown
cat myfile > /dev/audio
```

Example 3 (unknown):
```unknown
$ ls -l /boot/
```

Example 4 (unknown):
```unknown
total 13740
drwxr-xr-x 2 root root    4096 Jan 12 00:33 grub
-rw-r--r-- 1 root root 8570335 Jan 12 00:33 initramfs-linux-fallback.img
-rw-r--r-- 1 root root 1821573 Jan 12 00:31 initramfs-linux.img
-rw-r--r-- 1 root root 1457315 Jan  8 08:19 System.map26
-rw-r--r-- 1 root root 2209920 Jan  8 08:19 vmlinuz-linux
```

---

## Linux console/Keyboard configuration

**URL:** https://wiki.archlinux.org/title/Linux_console/Keyboard_configuration

**Contents:**
- Viewing keyboard settings
- Keymaps
  - Listing keymaps
  - Loadkeys
  - Persistent configuration
  - Creating a custom keymap
    - Adding directives
    - Other examples
    - Saving changes
- Adjusting typematic delay and rate

Keyboard mappings, console fonts and console maps are provided by the kbd package (a dependency of systemd), which also provides low-level tools for managing the text console. In addition, systemd provides the localectl(1) tool, which can control both the system locale and keyboard layout settings for both the console and Xorg.

Use localectl status to view the current keyboard configuration.

The keymap files are stored in the /usr/share/kbd/keymaps/ directory tree. A keymap file fully describes the keyboard layout, possibly with symbols for different languages and layout switching is simulated via AltGr_Lock keysym usage.

The include statement can be used to share common parts of keymap files. Where to look for an include file is described in the source code only.

For more details see keymaps(5).

The naming conventions of console keymaps are somewhat arbitrary, but usually they are based on:

For a list of all the available keymaps, use the command:

To search for a keymap, use the following command, replacing search_term with the code for your language, country, or layout:

Alternatively, using find:

It is possible to set a keymap just for the current session. This is useful for testing different keymaps, solving problems etc. The loadkeys tool is used for this purpose:

See loadkeys(1) for details. The same tool is used internally by systemd-vconsole-setup(8) when loading the keymap configured in /etc/vconsole.conf.

A persistent keymap can be set in /etc/vconsole.conf, which is read by systemd on start-up. The KEYMAP variable is used for specifying the keymap. If the variable is empty or not set, the us keymap is used as default value. See vconsole.conf(5) for all options. For example:

For convenience, localectl may be used to set the console keymap. It will change the KEYMAP variable in /etc/vconsole.conf and also set the keymap for the current session:

The --no-convert option can be used to prevent localectl from automatically changing the Xorg keymap to the nearest match. See localectl(1) for more information.

If required, the keymap from /etc/vconsole.conf can be loaded during early userspace by the keymap mkinitcpio hook.

When using the console, you can use hotkeys to print a specific character. Moreover we can also print a sequence of characters and some escape sequences. Thus, if we print the sequence of characters constituting a command and afterwards an escape character for a new line, that command will be executed.

One method of doing this is editing the keymap file. However, since it will be rewritten anytime the package it belongs to is updated, editing this file is discouraged. It is better to integrate the existing keymap with a personal keymap. The loadkeys utility can do this.

First, create a keymap file. This keymap file can be anywhere, but one method is to mimic the directory hierarchy in /usr/local: create the /usr/local/share/kbd/keymaps directory, then edit /usr/local/share/kbd/keymaps/personal.map.

As a side note, it is worth noting that such a personal keymap is useful also to redefine the behaviour of keys already treated by the default keymap: when loaded with loadkeys, the directives in the default keymap will be replaced when they conflict with the new directives and conserved otherwise. This way, only changes to the keymap must be specified in the personal keymap.

Two kinds of directives are required in this personal keymap. First of all, the keycode directives, which matches the format seen in the default keymaps. These directives associate a keycode with a keysym. Keysyms represent keyboard actions. The actions available include outputting character codes or character sequences, switching consoles or keymaps, booting the machine, and many other actions. The full currently active keymap can be obtained with

Most keysyms are intuitive. For example, to set key 112 to output an 'e', the directive will be:

To set key 112 to output a euro symbol, the directive will be:

Some keysym are not immediately connected to a keyboard actions. In particular, the keysyms prefixed by a capital F and one to three digits (F1-F246) constituting a number greater than 30 are always free. This is useful directing a hotkey to output a sequence of characters and other actions:

Then, F70 can be bound to output a specific string:

When key 112 is pressed, it will output the contents of F70. In order to execute a printed command in a terminal, a newline escape character must be appended to the end of the command string. For example, to enter a system into hibernation, the following keymap is added:

In order to make use of the personal keymap, it must be loaded with loadkeys:

However this keymap is only active for the current session. In order to load the keymap at boot, specify the full path to the file in the KEYMAP variable in /etc/vconsole.conf. The file does not have to be gzipped as the official keymaps provided by kbd.

The typematic delay indicates the amount of time (typically in milliseconds) a key needs to be pressed and held in order for the repeating process to begin. After the repeating process has been triggered, the character will be repeated with a certain frequency (usually given in Hz) specified by the typematic rate. These values can be changed using the kbdrate command. Note that these settings are configured separately for the console and for Xorg.

For example to set a typematic delay to 200ms and a typematic rate to 30Hz, use the following command:

Issuing the command without specifying the delay and rate will reset the typematic values to their respective defaults; a delay of 250ms and a rate of 11Hz:

A systemd service can be used to set the keyboard rate. For example:

Then start/enable the kbdrate.service systemd service.

Layout switching can only be simulated by establishing a different layout on one of the higher layers (typically the 3rd, AltGr).

For a list of layouts that likely support this, run

You can test this by passing main and augmenting layout to loadkeys, e.g.

If it worked, the 3rd level shift (AltGr) will allow you to access the second layout, the AltGr_Lock (often Alt+Shift, you will have to inspect the keymap file with zless) will provide an effective toggle.

For a permanent configuration, set the KEYMAP_TOGGLE next to the KEYMAP in /etc/vconsole.conf.

**Examples:**

Example 1 (unknown):
```unknown
localectl status
```

Example 2 (unknown):
```unknown
/usr/share/kbd/keymaps/
```

Example 3 (unknown):
```unknown
$ localectl list-keymaps
```

Example 4 (unknown):
```unknown
search_term
```

---

## Access Control Lists

**URL:** https://wiki.archlinux.org/title/Access_Control_Lists

**Contents:**
- Installation
- Enable ACL
- Usage
  - Set ACL
  - Show ACL
- Examples
  - Output of ls command
  - Execution permissions for private files
- See also

Access control list (ACL) provides an additional, more flexible permission mechanism for file systems. It is designed to assist with UNIX file permissions. ACL allows you to give permissions for any user or group to any disk resource.

The acl package is a dependency of systemd, it should already be installed.

To enable ACL, the filesystem must be mounted with the acl option. You can use fstab entries to make it permanent on your system.

There is a possibility that the acl option is already active as one of the default mount options on the filesystem. Btrfs and Ext2/3/4 filesystems are affected by this. Use the following command to check ext2/3/4 formatted partitions for the option:

Also check that the default mount options are not overridden, in such case you will see noacl in /proc/mounts in the relevant line.

You can set the default mount options of a filesystem using the tune2fs -o option partition command, for example:

Using the default mount options instead of an entry in /etc/fstab is very useful for external drives, such partition will be mounted with acl option also on other Linux machines. There is no need to edit /etc/fstab on every machine.

The ACL can be modified using the setfacl command.

To set permissions for a user (user is either the user name or ID):

To set permissions for a group (group is either the group name or ID):

To set permissions for others:

To allow all newly created files or directories to inherit entries from the parent directory (this will not affect files which will be moved into the directory):

To remove a specific entry:

To remove the default entries:

To remove all entries (entries of the owner, group and others are retained):

The mask then acts as a limiting filter: each affected entry's effective permissions are the intersection (bitwise AND) of its own permissions and the mask. For example, if user:bob is granted rwx and the mask is r-x, Bob’s effective permissions will be limited to r-x. This behavior ensures consistent access semantics, especially when sharing files across systems that do or do not support ACLs.

The example below helps clarify two distinct steps in how the ACL mask works in setfacl, especially in the context of the --no-mask and default behavior:

The 2003 USENIX document POSIX Access Control Lists on Linux has more information.

To show permissions, use:

Set all permissions for user johnny to file named abc:

Change permissions for user johnny:

Remove all ACL entries:

You will notice that there is an ACL for a given file because it will exhibit a + (plus sign) after its Unix permissions in the output of ls -l.

The following technique describes how a process like a web server can be granted access to files that reside in a user's home directory, without compromising security by giving the whole world access.

In the following we assume that the web server runs as the user http and grant it access to geoffrey's home directory /home/geoffrey.

The first step is granting execution permissions for the user http:

Since the user http is now able to access files in /home/geoffrey, others no longer need access:

Use getfacl to verify the changes:

As the above output shows, other's no longer have any permissions, but the user http is still able to access the files, thus security might be considered increased.

If you need to give write access for the user http on specific directories and/or files, run:

**Examples:**

Example 1 (unknown):
```unknown
# tune2fs -l /dev/sdXY | grep "Default mount options:"
```

Example 2 (unknown):
```unknown
Default mount options:    user_xattr acl
```

Example 3 (unknown):
```unknown
/proc/mounts
```

Example 4 (unknown):
```unknown
tune2fs -o option partition
```

---

## SELinux

**URL:** https://wiki.archlinux.org/title/SELinux

**Contents:**
- Current status in Arch Linux
- Concepts: Mandatory Access Controls
- Installing SELinux
  - Package description
    - SELinux aware system utilities
    - SELinux userspace utilities
    - SELinux policy packages
    - Other SELinux tools
  - Installation
    - Via binary package on GitHub

Security-Enhanced Linux (SELinux) is a Linux feature that provides a variety of security policies, including U.S. Department of Defense style Mandatory Access Control (MAC), through the use of Linux Security Modules (LSM) in the Linux kernel. It is not a Linux distribution, but rather a set of modifications that can be applied to Unix-like operating systems, such as Linux and BSD.

Running SELinux under a Linux distribution requires three things: An SELinux enabled kernel, SELinux Userspace tools and libraries, and SELinux Policies (mostly based on the Reference Policy). Some common Linux programs will also need to be patched/compiled with SELinux features.

SELinux is not officially supported (see [1][2]). The status of unofficial support is:

Summary of changes in AUR as compared to official core packages:

All of the other SELinux-related packages may be included without changes nor risks.

Before you enable SELinux, it is worth understanding what it does. Simply and succinctly, SELinux enforces Mandatory Access Controls (MACs) on Linux. In contrast to SELinux, the traditional user/group/rwx permissions are a form of Discretionary Access Control (DAC). MACs are different from DACs because security policy and its execution are completely separated.

An example would be the use of the sudo command. When DACs are enforced, sudo allows temporary privilege escalation to root, giving the process so spawned unrestricted systemwide access. However, when using MACs, if the security administrator deems the process to have access only to a certain set of files, then no matter what the kind of privilege escalation used, unless the security policy itself is changed, the process will remain constrained to simply that set of files. So if sudo is tried on a machine with SELinux running in order for a process to gain access to files its policy does not allow, it will fail.

Another set of examples are the traditional (-rwxr-xr-x) type permissions given to files. When under DAC, these are user-modifiable. However, under MAC, a security administrator can choose to freeze the permissions of a certain file by which it would become impossible for any user to change these permissions until the policy regarding that file is changed.

As you may imagine, this is particularly useful for processes which have the potential to be compromised, i.e. web servers and the like. If DACs are used, then there is a particularly good chance of havoc being wreaked by a compromised program which has access to privilege escalation.

For further information, visit Wikipedia:Mandatory access control.

All SELinux related packages belong to the selinux group in the AUR. Before you manually install any of these, read #Installation to see recommended options for a comprehensive installation.

This article or section needs expansion.

There are three methods to install the requisite SELinux packages.

All packages are available from the selinux unofficial repository. the base package can be replaced with base-selinux during the arch-bootstrap stage of system installation.

This repository also contains a script named build_and_install_all.sh which builds and installs (or updates) all packages in the needed order. Here is an example of a way this script can be used in a user shell to install all packages (with downloading the GPG keys which are used to verify the source tarballs of the package):

Of course, it is possible to modify the content of build_and_install_all.sh before running it, for example if you already have SELinux support in your kernel.

After all these steps, you can install a SELinux kernel (like linux) and a policy (like selinux-refpolicy-archAUR or selinux-refpolicy-gitAUR).

To enable SELinux as default security model on every boot, set the following kernel parameter:

When compiling the kernel, it is required to set at least the following options:

To enable the SELinux Linux security model by default and omit the need to set kernel parameters, additionally set the CONFIG_LSM option and specify selinux as the first "major" module in the list:

A correctly set-up PAM is important to get the proper security context after login. Check for the presence of the following lines in /etc/pam.d/system-login:

Policies are the mainstay of SELinux. They are what govern its behaviour. The only policy currently available in the AUR is the Reference Policy. In order to install it, you should use the source files, which may be got from the package selinux-refpolicy-srcAUR or by downloading the latest release on https://github.com/SELinuxProject/refpolicy/wiki/DownloadRelease#current-release. When using the AUR package, navigate to /etc/selinux/refpolicy/src/policy and run the following commands:

to install the reference policy as it is. Those who know how to write SELinux policies can tweak them to their heart's content before running the commands written above. The command takes a while to do its job and taxes one core of your system completely, so do not worry. Just sit back and let the command run for as long as it takes.

To load the reference policy run:

Then, make the file /etc/selinux/config with the following contents (Only works if you used the defaults as mentioned above. If you decided to change the name of the policy, you need to tweak the file):

Now, you may reboot. After rebooting, run:

to label your filesystem.

Now, make a file requiredmod.te with the contents:

and run the following commands:

This is required to remove a few messages from /var/log/audit/audit.log which are a nuisance to deal with in the reference policy. This is an ugly hack and it should be made very clear that the policy so installed simply patches the reference policy in order to hide the effects of incorrect labelling.

It is possible to use Vagrant to provision a virtual Arch Linux machine with SELinux configured. This is a convenient way to test an Arch Linux system running SELinux without modifying a current system. Here are commands which can be used to achieve this:

You can check that SELinux is working with sestatus. You should get something like:

To maintain correct context, you can enable restorecond.service.

To switch to enforcing mode without rebooting, you can use:

If you have a swap file instead of a swap partition, issue the following commands in order to set the appropriate security context:

SELinux defines security using a different mechanism than traditional Unix access controls. The best way to understand it is by example. For example, the SELinux security context of the apache homepage looks like the following:

The first three and the last columns should be familiar to any (Arch) Linux user. The fourth column is new and has the format:

This is important in case you wish to understand how to build your own policies, for these are the basic building blocks of SELinux. However, for most purposes, there is no need to, for the reference policy is sufficiently mature. However, if you are a power user or someone with very specific needs, then it might be ideal for you to learn how to make your own SELinux policies.

This is a great series of articles for someone seeking to understand how to work with SELinux.

The place to look for SELinux errors is the systemd journal. In order to see SELinux messages related to the label system_u:system_r:policykit_t:s0 (for example), you would need to run:

There are some tools/commands that can greatly help with SELinux.

Please report issues on GitHub: https://github.com/archlinuxhardened/selinux/issues

**Examples:**

Example 1 (unknown):
```unknown
--with-selinux
```

Example 2 (unknown):
```unknown
--with-selinux
```

Example 3 (unknown):
```unknown
--enable-libaudit
```

Example 4 (unknown):
```unknown
--enable-selinux
```

---

## systemd-homed

**URL:** https://wiki.archlinux.org/title/Systemd-homed

**Contents:**
- Installation
- Usage
  - Utilities
    - homectl
    - userdbctl
  - Storage mechanism
    - LUKS home directory
    - fscrypt directory
    - Directory or Btrfs subvolume
    - CIFS server

systemd-homed(8) is a systemd service providing portable human-user accounts that are not dependent on current system configuration.

It achieves portability by moving all user-related information into a storage medium, optionally encrypted, and creating an ~/.identity file that contains signed information about the user, password, what groups they belong to, UID/GID and other information that would typically be scattered over multiple files in /.

This approach allows not only for a home directory portability, but also provides security by automatically managing a home directory encryption on login and locking it if the system is suspended.

systemd-homed is part of and packaged with systemd. The pambase package since version 20200721.1-2 comes with the necessary PAM configuration to allow systemd-homed user sessions.

Start/enable the systemd-homed.service service.

homectl is the main utility you will use for homed. With it, you can create, update, and inspect users; their home directories; and their ~/.identity files controlled by the systemd-homed(8) service.

The simplest usage of homectl(1) is:

This command will create a user with the specified username, a free UID in range 60001–60513, create a group with the same name and a GID equal to the chosen UID, set the specified user as its member, and set the user's default shell to /bin/bash.

The home directory mount point is set to /home/username. The storage mechanism is chosen in this order:

The image path for the LUKS mechanism is set to /home/username.home. The directory path for the directory mechanism is set to /home/username.homedir.

A query tool used to inspect users, groups and group memberships provided by both classic UNIX mechanisms and systemd-homed.

This article or section needs expansion.

A user home directory is stored in a Linux file system, inside an encrypted LUKS (Linux Unified Key Setup) volume inside a loopback file or any removable media. To use this mechanism provide --storage=luks to homectl.

If you are using a loopback file, in order to save space, the LUKS2 volume can be made to discard deleted data transparently. To use this mechanism provide --luks-discard=true and --luks-offline-discard=true to homectl. Doing so can, however, reduce security under certain situations.

If you are using a removable media, make sure that these conditions are met:

A user home directory is stored the same way as when using the above method, but this time a native filesystem encryption is used. To use this mechanism provide --storage=fscrypt to homectl.

A user home directory is stored in /home/username.homedir and mounted to /home/username using bind mount on unlocking. When this method is used no encryption is provided. To use this mechanism provide --storage=directory or --storage=subvolume to homectl.

Here, the home directory is mounted from a CIFS (Common Internet File System) server at login. Note that CIFS is implemented via the Samba protocol. Use --storage=cifs on the homectl command line. The local password of the user is used to log into the CIFS service.

You can view an entire user record with:

You can modify or add to the user record with:

See homectl(1) for more options.

Create a user with LUKS encryption:

Create a user with fscrypt encryption (make sure that fscrypt is enabled on the file system):

Create a user with a specific UID, shell and groups:

Other options can be found in homectl(1) § USER RECORD PROPERTIES.

It is possible to delete several users at the same time:

By default, the home directory will be mounted with compress=zstd:1 and noacl. See [2].

You can override this and/or add extra options with --luks-extra-mount-options. For example, to enable ACLs, use the standard level 3 zstd compression level, and allow deleting subvolumes as a regular user, use

The suspend option can be used with pam_systemd_home.so entries in the files in /etc/pam.d/ to enable forget key on suspend. No session manager at the moment supports this feature. Furthermore, TTY sessions do not support the reauthentication mechanism. So, when session managers start supporting this feature, the suspend option should only be enabled for them. Read pam_systemd_home(8) and the Linux-PAM System Administrators' Guide for more details.

systemd-homed encrypts your home directory using your password, so SSH configured for public key authentication cannot mount it or read authorized_keys. A possible solution is to add authorized keys to your user record and require both public key and password for authentication. Add the following to /etc/ssh/sshd_config:

Update your user record with your authorized keys while the user is unlocked using:

From now on, SSH will ask you to enter your password after completing key-based authentication. systemd-homed will use it to unlock and mount your home directory.

If you need to mount a systemd-homed-encrypted directory from a rescue disk or another machine, you will need to decrypt the directory outside of the systemd-homed framework. You may wish to keep a text file or script of this solution from the forums on your rescue disk for emergencies:

This setup requires a FIDO2 security device, and asks for a pin to login and decrypt the home directory:

Setting up a recovery key is recommended in case the device is lost or broken. The recovery key will be used like a password to access the user and files. Instead of using a device pin, it is also possible to ask only for user presence, which requires touching the security device. Currently, homectl also requires the user to set a password as alternative login which also works as a backup secret.

After logging out of Plasma, there may still be active user processes (e.g. dbus-daemon) that prevent the home directory's deactivation.

This can be solved by enabling Plasma's systemd startup.

There always is an umask set in /etc/login.defs, so the user's own umask set via homectl is currently ignored. See systemd issue 23007.

To solve this, set the umask in your login shell's startup files e.g. ~/.bash_profile or ~/.zprofile.

In the event of a forceful or unexpected shutdown or power outage, it is possible for the home directory to be left in a dirty state. In this case, the user will be locked out and authenticating to the home directory may require manual intervention. The following steps assumes that the LUKS2 encrypted loop back file storage mechanism is used, and fsck can be used in the underlying filesystem to repair it.

First, confirm the state of the loop back file is dirty and determine the path of the user's loop back file:

Next, using the path of the loop back file, follow the steps in #Mounting encrypted home directory for rescue.

Finally, attempt to repair the filesystem using fsck on the opened LUKS2 device:

At this point, if the repair was successful, close the LUKS2 container, detach the loop back device, then attempt to re-authenticate normally via the systemd-homed service.

This article or section needs expansion.

**Examples:**

Example 1 (unknown):
```unknown
~/.identity
```

Example 2 (unknown):
```unknown
systemd-homed.service
```

Example 3 (unknown):
```unknown
~/.identity
```

Example 4 (unknown):
```unknown
# homectl create username
```

---

## Network tools

**URL:** https://wiki.archlinux.org/title/Traceroute

**Contents:**
- Traceroute
- Netcat
- Whois
- inetd
- See also

This page lists various network tools. ping and ip are covered by Network configuration.

Traceroute is a tool to display the path of packets across an IP network.

There are several implementations available:

See also Wikipedia:Netcat.

A more complex alternative is socat.

See also Wikipedia:WHOIS.

Arch Linux does not have inetd but you can use systemd or xinetd (xinetdAUR) instead.

---

## chroot

**URL:** https://wiki.archlinux.org/title/Chroot

**Contents:**
- Reasoning
- Requirements
- Prepare new root location
- Usage
  - Using arch-chroot
    - Enter a chroot
    - Exit a chroot
    - Run a single command and exit
  - Running on Btrfs
  - Using chroot

A chroot is an operation that changes the apparent root directory for the current running process and their children. A program that is run in such a modified environment cannot access files and commands outside that environmental directory tree. This modified environment is called a chroot jail.

Changing root is commonly done for performing system maintenance on systems where booting and/or logging in is no longer possible. Common examples are:

See also Wikipedia:Chroot#Limitations.

In order to use chroot, you need another Linux installation or installation media (of any distribution), with:

The chroot target should be a directory which contains a file system hierarchy.

In the installation guide, this directory would be /mnt. For an existing installation, you need to mount existing partitions into /mnt yourself:

Run lsblk and note the partition layout of your installation. It will be usually something like /dev/sdXY or if you have an NVMe drive /dev/nvme0nXpY.

Mount the file system:

If you have an EFI system partition and need to make changes in it (e.g. updating the vmlinuz or initramfs images):

If you have any discrete partitions, mount them too.

In the following examples, /path/to/new/root is the directory where the new root resides (e.g. /mnt).

There are two main options for using chroot, described below.

The bash script arch-chroot(8) is part of the arch-install-scripts package. arch-chroot wraps the chroot(1) command while ensuring that important functionality is available, e.g. mounting /dev, /proc and other API filesystems, or exposing /etc/resolv.conf to the chroot.

Run arch-chroot with the new root directory as first argument:

You can now do most of the operations available from your existing installation. Some tasks which needs D-Bus will not work as noted in #Usage.

To exit the chroot, use:

To run a command from the chroot and exit again, append the command to the end of the line:

For example, to run mkinitcpio -p linux for a chroot located at /mnt/arch, do:

On a Btrfs root file system with subvolumes, you have to make sure that all subvolumes are properly mounted as specified in fstab before entering chroot.

An example with the Btrfs default setup from archinstall:

If you run chroot directly, below steps are needed before actual chroot.

First, mount the temporary API filesystems:

If you are running a UEFI system, you will also need access to EFI variables. Otherwise, when installing GRUB, you will receive a message similar to: UEFI variables not supported on this machine:

Next, in order to use an internet connection in the chroot environment, copy over the DNS details:

Finally, to change root into /path/to/new/root using a bash shell:

After chrooting, it may be necessary to load the local Bash configuration:

When finished with the chroot, you can exit it via:

Then unmount the temporary file systems:

If you have an X server running on your system, you can start graphical applications from the chroot environment.

To allow the chroot environment to connect to an X server, open a virtual terminal inside the X server (i.e. inside the desktop of the user that is currently logged in), then run the xhost command, which gives permission to anyone to connect to the user's X server (see also Xhost):

Then, to direct the applications to the X server from chroot, set the DISPLAY environment variable inside the chroot to match the DISPLAY variable of the user that owns the X server. So for example, run:

as the user that owns the X server to see the value of DISPLAY. If the value is ":0" (for example), then run the following in the chroot environment:

Chroot requires root privileges, which may not be desirable or possible for the user to obtain in certain situations. There are, however, various ways to simulate chroot-like behavior using alternative implementations.

PRoot may be used to change the apparent root directory and use mount --bind without root privileges. This is useful for confining applications to a single directory or running programs built for a different CPU architecture, but it has limitations due to the fact that all files are owned by the user on the host system. PRoot provides a --root-id argument that can be used as a workaround for some of these limitations in a similar (albeit more limited) manner to fakeroot.

fakechroot is a library shim which intercepts the chroot call and fakes the results. It can be used in conjunction with fakeroot to simulate a chroot as a regular user.

unshare(1), part of util-linux, can be used to create a new kernel namespace. This works with the usual chroot command. For example:

systemd-detect-virt --chroot detect whether invoked in a chroot environment. See systemd-detect-virt(1) for detection of other virtualized environments. For a broader discussion, and usage of traditional tools, see how-do-i-tell-im-running-in-a-chroot.

Upon executing arch-chroot /path/to/new/root, a warning is issued:

See arch-chroot(8) for an explanation and an example of using bind mounting to make the chroot directory a mountpoint.

**Examples:**

Example 1 (unknown):
```unknown
# swapon /dev/sdxY
```

Example 2 (unknown):
```unknown
/dev/nvme0nXpY
```

Example 3 (unknown):
```unknown
# mount /dev/sdXY /mnt
```

Example 4 (unknown):
```unknown
# mount /dev/sdXZ /mnt/esp
```

---

## Kexec

**URL:** https://wiki.archlinux.org/title/Kexec

**Contents:**
- Installation
- Rebooting using kexec
  - Manually
  - systemd
    - Custom unit file
    - Separate /boot partition
- Troubleshooting
  - System hangs or reboots after "kexec_core: Starting new kernel"
  - No kernel mode-setting (Nvidia)
- See also

Kexec is a system call that enables you to load and boot into another kernel from the currently running kernel. This is useful for kernel developers or other people who need to reboot very quickly without waiting for the whole BIOS boot process to finish. Note that kexec may not work correctly for you due to devices not fully re-initializing when using this method, however this is rarely the case.

Install the kexec-tools package.

You can manually invoke kexec using:

It is also possible to load kernel manually and then let systemd handle service shutdown and kexec for you.

By default, if systemd-boot is used and no kernel was loaded manually using kexec -l before, systemd will load the kernel specified in the default boot loader entry. For example, to reboot into the newer kernel after a system update, you may simply run:

The command will refuse to execute if you have several initrd entries (e.g. for Microcode updates) which are currently not supported.

If the default behavior does not work for you or you desire to conveniently load custom kernels, you may wrap the kernel loading into a service unit. Create a new unit file, kexec-load@.service, that will load the specified kernel to be kexec'ed:

Then enable the service file for the kernel you want to load, (e.g. for linux it will be kexec-load@linux.service)

Ensure that the shutdown hook is not part of your initramfs image by removing it from the HOOKS array in /etc/mkinitcpio.conf. If it is, remove it and regenerate the initramfs.

If you wish to load a different kernel for the next kexec, for example linux-lts, disable the service for the current kernel and enable the one for the new kernel.

The above systemd unit file will fail if /boot is not on the root file system, as systemd will likely unmount /boot before it runs the kexec-load unit file. An alternative approach is to load a "hook" unit file that does nothing on startup and invokes kexec upon termination. By making this unit file conflict with kexec.target and only kexec.target, you can ensure the new kernel gets loaded early enough and only after a systemctl kexec command. Here is an alternate /etc/systemd/system/kexec-load@.service file that follows this strategy:

Note that Conflicts=shutdown.target is not really needed, as it is implicitly guaranteed by strict ordering on sysinit.target which itself Conflicts= with shutdown.target.

The troubleshooting information on General troubleshooting#Boot problems may be helpful for diagnosing the problem.

In some cases a hanging system might be an acpi related problem which can be checked on-the-fly like this:

Please adapt the name of the initramfs image and the kernel according to your output of ls -al /boot/.

Adding the acpi_rsdp kernel parameter to the kexec command line has been suggested in [1] and may solve the issue in some cases without the need to completely disable ACPI via acpi=off.

The graphics driver needs to be unloaded before a kexec, or the next kernel will not be able to gain exclusive control of the device. This is difficult to achieve manually because any programs which need exclusive control over the GPU (Xorg, display managers) must not be running. Below is an example systemd service that will unload the KMS driver right before kexec, which requires that you use systemctl kexec.

Afterwards, enable unmodeset.service

**Examples:**

Example 1 (unknown):
```unknown
# kexec -l /boot/vmlinuz-linux --initrd=/boot/initramfs-linux.img --reuse-cmdline
# kexec -e
```

Example 2 (unknown):
```unknown
# kexec -l /boot/vmlinuz-linux --initrd=/boot/initramfs-linux.img --reuse-cmdline
# systemctl kexec
```

Example 3 (unknown):
```unknown
# systemctl kexec
```

Example 4 (unknown):
```unknown
kexec-load@.service
```

---

## udev

**URL:** https://wiki.archlinux.org/title/Udev

**Contents:**
- Installation
- Introduction to udev rules
  - Example
  - List the attributes of a device
  - Testing rules before loading
  - Loading new rules
- Components of udev rules
  - Action matching
    - The change action
  - Device attributes

udev (user space /dev) is a user space system that enables the operating system administrator to register user space handlers for events. The events received by udev's daemon are mainly generated by the Linux kernel in response to physical events relating to peripheral devices.

As such, udev main purpose is to act upon peripheral detection and hot-plugging, including actions that return control to the kernel, e.g. loading kernel modules or device firmware. Another component of this detection is adjusting the permissions of the device to be accessible to non-root users and groups. udev manages device nodes in the /dev directory by adding, symlinking and renaming them.

udev is part of systemd and thus installed by default. See systemd-udevd.service(8) for more information.

udev rules written by the administrator go in /etc/udev/rules.d/, their file name has to end with .rules. The udev rules shipped with various packages are found in /usr/lib/udev/rules.d/. If there are two files by the same name under /usr/lib and /etc, the ones in /etc take precedence.

To learn about udev rules, refer to the udev(7) manual. Also see Writing udev rules and some practical examples are provided within the guide: Writing udev rules - Examples.

Below is an example of a rule that creates a symlink /dev/video-cam when a webcamera is connected.

Let say this camera is currently connected and has loaded with the device name /dev/video2. The reason for writing this rule is that at the next boot, the device could show up under a different name, like /dev/video0.

To identify the webcamera, from the video4linux device we use KERNEL=="video2" and SUBSYSTEM=="video4linux", then walking up two levels above, we match the webcamera using vendor and product ID's from the usb parent SUBSYSTEMS=="usb", ATTRS{idVendor}=="05a9" and ATTRS{idProduct}=="4519". Note that this matching is case sensitive, so "05A9" can not be used to match the idVendor in this example.

We are now able to create a rule match for this device as follows:

Here we create a symlink using SYMLINK+="video-cam" but we could easily set user OWNER="john" or group using GROUP="video" or set the permissions using MODE="0660".

If you intend to write a rule to do something when a device is being removed, be aware that device attributes may not be accessible. In this case, you will have to work with preset device environment variables. To monitor those environment variables, execute the following command while unplugging your device:

In this command's output, you will see value pairs such as ID_VENDOR_ID and ID_MODEL_ID, which match the previously used attributes idVendor and idProduct. A rule that uses device environment variables instead of device attributes may look like this:

To get a list of all of the attributes of a device you can use to write rules, run this command:

Replace device_name with the device present in the system, such as /dev/sda or /dev/ttyUSB0.

If you do not know the device name you can also list all attributes of a specific system path:

To narrow down the search for a device, figure out the class and run:

You can use the symlink outright or what it points as the input to --name. For example:

To get the path of a bare USB device which does not populate any subordinate device you have to use the full USB device path. Start monitor mode and then plug in the USB device to get it:

You can just choose the deepest path and --attribute-walk will show all parent's attributes anyway:

This will not perform all actions in your new rules but it will however process symlink rules on existing devices which might come in handy if you are unable to load them otherwise. You can also directly provide the path to the device you want to test the udev rule for:

udev automatically detects changes to rules files, so changes take effect immediately without requiring udev to be restarted. However, the rules are not re-triggered automatically on already existing devices. Hot-pluggable devices, such as USB devices, will probably have to be reconnected for the new rules to take effect, or at least unloading and reloading the ohci-hcd and ehci-hcd kernel modules and thereby reloading all USB drivers.

If rules fail to reload automatically:

To manually force udev to trigger your rules:

ACTION=="" is used to only match against a device when something specific is happening, usually when it is appearing or disappearing. There are eight types of actions that can be raised for udev rules to match against:

The change action is somewhat special, because not all drivers use it the same way, if at all. A change event is only emitted as a result of the driver manually raising a userspace event (or uevent) of type KOBJ_CHANGE. This normally signals that something has happened to that device, but it takes some additional information to determine what exactly.

Since that event is only raised under a limited number of situations, here is a (best-effort, non-exhaustive) list of subsystems that do raise change events, and under what conditions.

Device attributes, listed as ATTR{my_attr}=="..." (or ATTRS{my_attr}=="..." for parent devices) in udev rules and udevadm info --attribute-walk, correspond to all the device information that is made available through sysfs, which exposes information from the "kobject" classes used by the kernel to keep track of device state. This means that there is no need for udev-specific tools to examine these, and that a simple ls /sys/class/thermal/thermal_zone0/ or cat /sys/class/input/input0/name is all that is needed to explore device attributes, and some can even be changed with something like echo 1 > "/sys/class/leds/input2::capslock/brightness". It is, however, a lot more conveinient to use udevadm info --attribute-walk /sys/class/input/mouse0, which has the advantage of showing all matchable attributes for that device all at once in the same format that udev rules expect (as well as the attributes for any parent devices, also in their readily usable format).

Most of these attributes have well-documented meanings, behaviors and accepted values based on what kernel submodule is handling the device, such as sysfs-bus-usb for USB devices and sysfs-class-typec for USB type-C ports, for instance. Other attributes are present as subtrees, such as sysfs-devices-physical_location or sysfs-devices-power, which use slashes to separate levels, just like directories, giving ATTR{power/control}=="auto".

The "environment" of an event is a set of device properties and event properties (and rarely global properties), both written as ENV{MY_PROPERTY}=="..." in udev rules and both reported as E: MY_PROPERTY=... by udevadm monitor --property and udevadm info (without --attribute-walk). Despite being called "environment", they have nothing to do with environment variables (although they do get passed as environment variables to programs started by RUN+="..."). These contain context information added to an event by the kernel module or other udev rules to make that information available to downstream rules or components. The only difference between event properties and device properties is that devices properties are stored and can be examined with udevadm info, whereas event properties are transient and can only be seen if the event is caught by udevadm monitor --property. A lot of device properties are also available as device attributes with similar names.

Unlike with attributes, udev rules are allowed to set event properties arbitrarily, and there is also no concept of "parent properties" to inspect beyond the ones that are set on the event (which is why there is no ENVS{...}=="" directive). Note that setting a property with a udev rule sets a device property, which will be stored until the device is removed and will thus appear in every event raised by that device (unless the property's name starts with a period, like ENV{.PART_SUFFIX}, which will be added to the event properties with the leading dot in the name and be usable by other rules, but not stored).

Tags (added using TAG+="foo", removed with TAG-="foo", matched with TAG=="foo") are used by userspace software interacting with udev to list and identify all devices they should be acting upon. Those can also be arbitrary, but there are a few well-known ones.

To mount removable drives, do not call mount from udev rules. This is ill-advised for two reasons:

There are some options that work:

Programs started by udev will block further events from that device, and any tasks spawned from a udev rule will be killed after event handling is completed. If you need to spawn a long-running process with udev, the intended way is to have a systemd unit which handles running the actual command, and a udev rule that merely signals that this unit should run. However, using systemctl in a udev rule is discouraged, since it is meant for user interaction and may block, among other things.

The correct way of doing this is to have the rule tag the device as needing a systemd device unit (see systemd.device(5)) using TAG+="systemd" and adding an device property of either ENV{SYSTEMD_WANTS}+= for services that would run with systemctl --system or ENV{SYSTEMD_USER_WANTS}+= for services that should run with systemctl --user. For example:

SYSTEMD_WANTS is equivalent to the Wants= directive elsewhere in systemd, meaning the device will not be affected if the service fails, does not exists, or completes successfully at any point.

When a kernel driver initializes a device, the default state of the device node is to be owned by root:root, with permissions 600. [1] This makes devices inaccessible to regular users unless the driver changes the default, or a udev rule in user space changes the permissions.

The OWNER, GROUP, and MODE udev values can be used to provide access, though one encounters the issue of how to make a device usable to all users without an overly permissive mode. Ubuntu's approach is to create a plugdev group that devices are added to, but this practice is not only discouraged by the systemd developers, [2] but considered a bug when shipped in udev rules on Arch (FS#35602). Another approach historically employed, as described in Users and groups#Pre-systemd groups, is to have different groups corresponding to categories of devices.

The modern recommended approach for systemd systems is to use a MODE of 660 to let the group use the device, and then attach a TAG named uaccess [3]. This special tag makes udev apply a dynamic user ACL to the device node, which coordinates with systemd-logind(8) to make the device usable to logged-in users. For an example of a udev rule implementing this:

Create the rule /etc/udev/rules.d/95-hdmi-plug.rules with the following content:

Create the rule /etc/udev/rules.d/95-monitor-hotplug.rules with the following content to launch arandr on plug in of a VGA monitor cable:

Some display managers store the .Xauthority outside the user home directory. You will need to update the ENV{XAUTHORITY} accordingly. As an example GNOME Display Manager looks as follows:

If your eSATA drive is not detected when you plug it in, there are a few things you can try. You can reboot with the eSATA plugged in. Or you could try:

Or you could install scsiaddAUR (from the AUR) and try:

Hopefully, your drive is now in /dev. If it is not, you could try the above commands while running:

to see if anything is actually happening.

If you connected an eSATA bay or another eSATA adapter, the system will still recognize this disk as an internal SATA drive. GNOME and KDE will ask you for your root password all the time. The following rule will mark the specified SATA-Port as an external eSATA-Port. With that, a normal GNOME user can connect their eSATA drives to that port like a USB drive, without any root password and so on.

Because udev loads all modules asynchronously, they are initialized in a different order. This can result in devices randomly switching names. A udev rule can be added to use static device names. See also Persistent block device naming for block devices and Network configuration#Change interface name for network devices.

For setting up the webcam in the first place, refer to Webcam setup.

Using multiple webcams will assign video devices as /dev/video* randomly on boot. The recommended solution is to create symlinks using a udev rule as in the #Example:

If you use multiple printers, /dev/lp[0-9] devices will be assigned randomly on boot, which will break e.g. CUPS configuration.

You can create the following rule, which will create symlinks under /dev/lp/by-id and /dev/lp/by-path, similar to Persistent block device naming scheme:

To perform some action on a specific disk device /dev/sdX identified permanently by its unique serial ID_SERIAL_SHORT as displayed with udevadm info /dev/sdX, one can use the below rule. It is passing as a parameter the device name found if any to illustrate:

A udev rule can be useful to enable the wakeup triggers of a USB device, like a mouse or a keyboard, so that it can be used to wake the system from sleep.

First, identify the vendor and product identifiers of the USB device. They will be used to recognize it in the udev rule. For example:

Then, find where the device is connected to using:

Now create the rule to change the power/wakeup attribute of both the device and the USB controller it is connected to whenever it is added:

USB devices need to be reset after exiting a suspended state (whether from the system resuming from sleep or from a port being turned off when the device was idle for power saving), which the Linux kernel handles mostly transparently in a process called reset-resume, so that USB drives do not look like they have been disconnected and reconnected every time. This is mostly desirable, but some devices, like USB TTY interfaces, do need to be manually reconfigured after being power cycled, which is not signalled by any uevent from the relevent drivers.

The one uevent that does get raised, however, is the one from losing and regaining power/wakeup, since USB devices are unusable while unconfigured and would be unable to wake up the system from sleep in this state. These two events have no unique event properties, but the first one can still be easily identified because DEVNUM is set to zero (which is not a valid device number) immediately before the device is unconfigured and loses power/wakeup, raising the uevent. When that happens, one can simply touch the bConfigurationValue sysfs attribute to force the system to reconfigure the device non-transparently, as if it had been disconnected during sleep, which unbinds all drivers and removes all children devices before adding them back when the device is ready again.

This article or section is a candidate for merging with #Testing rules before loading.

It can be useful to trigger various udev events. For example, you might want to simulate a USB device disconnect on a remote machine. In such cases, use udevadm trigger:

This command will trigger a USB remove event on all USB devices with vendor ID abcd.

To trigger a desktop notification from a udev rule, use systemd-run(1) as explained in Desktop notifications#Send notifications to another user:

To launch multiple or long commands, an executable script can be given to systemd-run:

In rare cases, udev can make mistakes and load the wrong modules. To prevent it from doing this, you can blacklist modules. Once blacklisted, udev will never load that module – not at boot-time and not even later on when a hot-plug event is received (e.g., you plug in your USB flash drive).

To get hardware debug info, use the kernel parameter udev.log-priority=debug. Alternatively you can set

This option can also be compiled into your initramfs by adding the configuration file to your FILES array:

and then regenerate the initramfs.

After migrating to LDAP or updating an LDAP-backed system, udevd can hang at boot at the message "Starting UDev Daemon". This is usually caused by udevd trying to look up a name from LDAP but failing, because the network is not up yet. The solution is to ensure that all system group names are present locally.

Extract the group names referenced in udev rules and the group names actually present on the system:

To see the differences, do a side-by-side diff:

In this case, the pcscd group is for some reason not present in the system. Add the missing groups. Also, make sure that local resources are looked up before resorting to LDAP. /etc/nsswitch.conf should contain the following line:

You need to create a custom udev rule for that particular device. To get definitive information of the device you can use either ID_SERIAL or ID_SERIAL_SHORT (remember to change /dev/sdb if needed):

Then we set UDISKS_AUTO="1" to mark the device for automounting and UDISKS_SYSTEM="0" to mark the device as "removable". See udisks(8) for details.

Remember to reload udev rules with udevadm control --reload. Next time you plug your device in, it will be treated as an external drive.

If the group ID of your optical drive is set to disk and you want to have it set to optical, you have to create a custom udev rule:

When xrandr or another X-based program tries to connect to an X server, it falls back to a TCP connection on failure. However, due to IPAddressDeny in the systemd-udev service configuration, this hangs. Eventually the program will be killed and event processing will resume.

If the rule is for a drm device and the hang causes event processing to complete once the X server has started, this can cause 3D acceleration to stop working with a failed to authenticate magic error.

**Examples:**

Example 1 (unknown):
```unknown
…/by-path/…
```

Example 2 (unknown):
```unknown
…/by-uuid/…
```

Example 3 (unknown):
```unknown
/etc/udev/rules.d/
```

Example 4 (unknown):
```unknown
/usr/lib/udev/rules.d/
```

---

## PAM

**URL:** https://wiki.archlinux.org/title/PAM

**Contents:**
- Installation
- Configuration
  - Security parameters
  - PAM base-stack
    - Examples
- Configuration How-Tos
  - Security parameter configuration
  - PAM stack and module configuration
- Further PAM packages
- Tips and tricks

The Linux Pluggable Authentication Modules (PAM) provides a framework for system-wide user authentication. To quote the project:

This article explains the Arch Linux base set-up defaults for PAM to authenticate local and remote users. Applying changes to the defaults is subject of crosslinked specialized per topic articles.

The pam package is a dependency of the base meta package and, thereby, normally installed on an Arch system. The PAM modules are installed into /usr/lib/security exclusively.

The repositories contain a number of optional PAM packages, the #Configuration How-Tos show examples.

A number of /etc paths are relevant for PAM; execute pacman --query --list pam | grep /etc to see the default configuration files created. They relate to either #Security parameters for the modules, or the #PAM base-stack configuration.

The path /etc/security contains system-specific configuration for variables the authentication methods offer. The base install populates it with default upstream configuration files.

Note Arch Linux does not provide distribution-specific configuration for these files. For example, the /etc/security/pwquality.conf file can be used to define system-wide defaults for password quality. Yet, to enable it, the pam_pwquality.so module has to be added to the #PAM base-stack of modules, which is not the case per default.

See #Security parameter configuration for some of the possibilities.

The /etc/pam.d/ path is exclusive for the PAM configuration to link the applications to the individual systems' authentication schemes. During installation of the system base, it is populated by:

The different configuration files of the base installation link together and are stacked during runtime. For example, on a local user logon, the login application sources the system-local-login policy, which in turn sources others:

For a different application, a different path may apply. For example, openssh installs its sshd PAM policy:

Consequently, the choice of the configuration file in the stack matters. For the above example, a special authentication method could be required for sshd only, or all remote logins by changing system-remote-login; both changes would not affect local logins. Applying the change to system-login or system-auth instead would affect local and remote logins.

Like the example of sshd, any pam-aware application is required to install its policy to /etc/pam.d in order to integrate and rely on the PAM stack appropriately. If an application fails to do it, the /etc/pam.d/other default policy to deny and log a warning is applied.

PAM is dynamically linked at runtime. For example:

the login application is pam-aware and must, therefore, have a policy.

The PAM package manual pages pam(8) and pam.d(5) describe the standardized content of the configuration files. In particular, they explain the four PAM groups: account, authentication, password, and session management, as well as the control values that may be used to configure stacking and behavior of the modules.

Additionally, extensive documentation is installed to /usr/share/doc/Linux-PAM/index.html which, among various guides, contains browsable man pages for each of the standard modules.

The factual accuracy of this article or section is disputed.

Two short examples to illustrate the above warning.

First, we take the following two lines from a historic Arch default:

- the latter being what pam_permit.so is used for. Simply swapping the control values required and optional for both lines is enough to disable password authentication, i.e. any user may logon without providing a password.

Second, as the contrary example, per default configuration of pam_nologin.so in /etc/pam.d/login, creating the following file:

results in that no user other than root may login (if root logins are allowed, see Security#Restricting root login). To allow logins again, remove the file again before your logout from the console you created it with.

With that as background, see #PAM stack and module configuration for particular use-case configuration.

This section provides an overview of content detailing how to apply changes to the PAM configuration and how to integrate special new PAM modules into the PAM stack. Note the man pages for the modules can generally be reached dropping the .so extension.

The following sections describe examples to change the default PAM parameter configuration:

The following articles detail how to change the #PAM base-stack for special use-cases.

Other than those packages mentioned so far, the Arch User Repository contains a number of additional PAM modules and tools.

A general purpose utility relating to PAM is:

Note the AUR features a keyword tag for PAM, but not all available packages are updated to include it. Hence, searching the package description may be necessary.

If PAM has locked you out, perhaps by typing the wrong password too many times, see Security#Lock out user after three failed login attempts.

**Examples:**

Example 1 (unknown):
```unknown
/usr/lib/security
```

Example 2 (unknown):
```unknown
pacman --query --list pam | grep /etc
```

Example 3 (unknown):
```unknown
/etc/security
```

Example 4 (unknown):
```unknown
/etc/security/pwquality.conf
```

---

## Universal Wayland Session Manager

**URL:** https://wiki.archlinux.org/title/Uwsm

**Contents:**
- Installation
- Configuration
  - Service startup notification and vars set by compositor
  - Environment variables
- Usage
  - Startup
    - TTY
    - Display manager
  - Session termination
- Tips and tricks

The Universal Wayland Session Manager (uwsm) wraps standalone Wayland compositors into a set of systemd units on the fly. This provides robust session management including environment, XDG Autostart support, bi-directional binding with login session, and clean shutdown.

Install the uwsm package.

In order to find the current compositor, a Wayland application run as a systemd service needs the WAYLAND_DISPLAY environment variable (or DISPLAY if they are intended to run through Xwayland). Therefore this and other useful environment variables should be put into the systemd/dbus activation environment once the compositor has set their values.

The command uwsm finalize puts WAYLAND_DISPLAY, DISPLAY and other environment variables listed via the white-space separated UWSM_FINALIZE_VARNAMES list into the activation environment. It is recommended to execute this command after the compositor is ready.

If other variables set by the compositors are needed in the activation environment, they can be passed as arguments to uwsm finalize or put into a white-space separated list in UWSM_FINALIZE_VARNAMES. See the examples below:

All environment variables set in ${XDG_CONFIG_HOME}/uwsm/env are sourced by uwsm and available to all managed compositors and graphical applications run inside such a session.

If you need certain environment variables to be set only for a specific compositor (and graphical applications in that graphical session), then put them in ${XDG_CONFIG_HOME}/uwsm/env-compositor instead.

An example of such a file can be seen below:

uwsm can be started both by TTY and by a display manager.

Add in your ~/.profile file:

If you want to always start the same compositor, then you can use instead in your ~/.profile file:

You can create a custom session desktop entry which starts your compositor through uwsm:

If you want to terminate the current uwsm session, then you should use either loginctl terminate-user "" (terminates the entire user session) or uwsm stop (executes code after uwsm start or terminates user session, if it replaced the login shell).

By default uwsm launches compositors through a custom systemd service in session.slice. Many Wayland compositors allow you to start other applications that would then be launched inside the compositor service, which would uselessly consume compositor resources or even interfere with notification sockets.

To start applications as separate systemd scope units you can use uwsm app, which can launch both executables

By default uwsm puts launched scope units in app-graphical.slice slice. If you want to put them in background-graphical.slice or session-graphical.slice, then you should respectively use options -s b, -s s:

Instead of uwsm app (which runs as a Python script), you can use a faster alternative:

**Examples:**

Example 1 (unknown):
```unknown
loginctl terminate-user ""
```

Example 2 (unknown):
```unknown
WAYLAND_DISPLAY
```

Example 3 (unknown):
```unknown
uwsm finalize
```

Example 4 (unknown):
```unknown
WAYLAND_DISPLAY
```

---

## Display manager

**URL:** https://wiki.archlinux.org/title/Display_manager

**Contents:**
- List of display managers
  - Console
  - Graphical
  - Login daemons
- Loading the display manager
  - Using systemd-logind
- Session configuration
  - Run ~/.xinitrc as a session
  - Starting applications without a window manager
- Tips and tricks

A display manager, or login manager, is typically a graphical user interface that is displayed at the end of the boot process in place of the default shell. There are various implementations of display managers, just as there are various types of window managers and desktop environments. There is usually a certain amount of customization and themeability available with each one.

To enable graphical login, enable the appropriate systemd service. For example, for SDDM, enable sddm.service.

This should work out of the box. If not, you might have to reset a custom default.target symlink to point to the default graphical.target. See systemd#Change default target to boot into.

After enabling SDDM a symlink display-manager.service should be set in /etc/systemd/system/. You may need to use --force to override old symlinks.

In order to check the status of your user session, you can use loginctl. All polkit actions like suspending the system or mounting external drives will work out of the box.

Many display managers read available sessions from /usr/share/xsessions/ directory. It contains standard desktop entry files for each desktop environment or window manager. Some display managers use a separate /usr/share/wayland-sessions/ to list Wayland-specific sessions.

To add/remove entries to your display manager's session list, create/remove the .desktop files in /usr/share/xsessions/ as desired. A typical .desktop file will look something like:

Installing xinit-xsessionAUR will provide an option to run your xinitrc as a session. Simply set xinitrc as the session in your display manager's settings and make sure that the ~/.xinitrc file is executable.

You can also launch an application without any decoration, desktop, or window management. For example to launch google-chromeAUR create a web-browser.desktop file in /usr/share/xsessions/ like this:

In this case, once you login, the application set with Exec will be launched immediately. When you close the application, you will be taken back to the login manager (same as logging out of a normal desktop environment or window manager).

It is important to remember that most graphical applications are not intended to be launched this way and you might have manual tweaking to do or limitations to live with (there is no window manager, so do not expect to be able to move or resize any windows, including dialogs; nonetheless, you might be able to set the window geometry in the application's configuration files).

See also xinitrc#Starting applications without a window manager.

Most display managers source /etc/xprofile, ~/.xprofile and /etc/X11/xinit/xinitrc.d/. For more details, see xprofile.

For display managers that use AccountsService the locale for the user session can be set by editing:

where your_locale is a value such as en_GB.UTF-8.

Alternatively, you can achieve this using D-Bus: busctl call org.freedesktop.Accounts /org/freedesktop/Accounts/User$UID org.freedesktop.Accounts.User SetLanguage s your_locale

GNOME and KDE users can also set languages in GNOME or KDE settings.

Log out and then back in again for the changes to take effect.

This article or section needs expansion.

Since systemd version 255, pam_systemd_loadkey can be used to unlock a GNOME/Kwallet keyring, if the user password matches the LUKS passphrase of the system. For this to work, you need to enable autologin in the display manager.

**Examples:**

Example 1 (unknown):
```unknown
sddm.service
```

Example 2 (unknown):
```unknown
default.target
```

Example 3 (unknown):
```unknown
graphical.target
```

Example 4 (unknown):
```unknown
display-manager.service
```

---

## Kernel live patching

**URL:** https://wiki.archlinux.org/title/Kernel_live_patching

**Contents:**
- kpatch
  - Installation
  - Usage
- kGraft
- See also

Kernel Live Patching (KLP) allows quick fixes to the kernel space without rebooting the whole system. Since version 4.0, related patches have been accepted [1][2][3], so one can configure their kernel to enable this feature. Generally, KLP is achieved by the following steps:

Some projects provide the live patching utilities before KLP was officially supported, such as Oracle's ksplice, SuSE's #kGraft, and Red Hat's #kpatch. They implemented the KLP functionality in different ways. The minimalistic functional set of patches that entered the mainstream kernel were derived from kpatch and kGraft.

Install kpatchAUR for an appropriate kernel and kpatch-gitAUR for userspace tools.

You can also manually build a kernel that supports kpatch usage, by enabling CONFIG_LIVEPATCH, CONFIG_DEBUG_INFO, and CONFIG_KALLSYMS.

Once both packages are successfully built and after reboot, you may

Assume that you have done some modifications and have a patch some.patch (against the source tree after a makepkg -o, not the vanilla kernel of version x.y) in the working directory. Launch the kpatch utility,

This command involves two kernel builds, the original one and the patched one, so it may take a period of time to complete. After the build is over, there should be a kpatch-some.ko module in the same directory. And then,

For further information, please check the manpages or the GitHub repository.

This article or section needs expansion.

KGraft has not been tested in Arch environment.

**Examples:**

Example 1 (unknown):
```unknown
CONFIG_LIVEPATCH
```

Example 2 (unknown):
```unknown
CONFIG_DEBUG_INFO
```

Example 3 (unknown):
```unknown
CONFIG_KALLSYMS
```

Example 4 (unknown):
```unknown
$ export ROOTDIR=some/dir/aur/linux-kpatch/src/linux-x-y
$ cd $ROOTDIR
```

---

## mkinitcpio

**URL:** https://wiki.archlinux.org/title/Regenerate_the_initramfs

**Contents:**
- Installation
- Image creation and activation
  - Automated generation
  - Manual generation
  - Customized generation
  - Unified kernel images
- Configuration
  - MODULES
  - BINARIES and FILES
  - HOOKS

mkinitcpio is a Bash script used to create initramfs images. See Arch boot process#initramfs for a general introduction.

It is important to note that there are two distinct approaches how the various tasks during initial ramdisk phase are performed:

The concrete variant is determined by the absence or presence of the systemd hook in the HOOKS array of /etc/mkinitcpio.conf. See #Common hooks for more details.

mkinitcpio has been developed by the Arch Linux developers and from community contributions. See the public Git repository.

Install the mkinitcpio package, which is a dependency of the linux package, so most users will already have it installed.

Advanced users may wish to install the latest development version of mkinitcpio from Git with the mkinitcpio-gitAUR package.

Every time a kernel is installed or upgraded, a pacman hook automatically generates a .preset file saved in /etc/mkinitcpio.d/. For example linux.preset for the official stable linux kernel package. A preset is simply a list of information required to create initial ramdisk images, instead of manually specifying the various parameters and the location of the output files. By default, it contains the instructions to create two images:

After creating the preset, the pacman hook calls the mkinitcpio script which generates the two images, using the information provided in the preset.

To run the script manually, refer to the mkinitcpio(8) manual page for instructions. In particular, to (re-)generate an initramfs image based on the preset provided by a kernel package, use the -p/--preset option followed by the preset to utilize. For example, for the linux package, use the command:

To (re-)generate initramfs images based on all existing presets, use the -P/--allpresets switch. This is typically used to regenerate all the initramfs images after a change of the global #Configuration:

Users may create any number of initramfs images with a variety of different configurations. The desired image must be specified in the respective boot loader configuration file.

Users can generate an image using an alternative configuration file. For example, the following will generate an initial ramdisk image according to the directions in /etc/mkinitcpio-custom.conf and save it as /boot/initramfs-custom.img.

If generating an image for a kernel other than the one currently running, add the kernel release version to the command line. The installed kernel releases can be found in /usr/lib/modules/, the syntax is consistent with the output of the command uname -r for each kernel.

mkinitcpio can create unified kernel images (UKIs) either by itself or via systemd-ukify. If systemd-ukify is absent or explicitly disabled using --no-ukify, the UKI will be assembled by mkinitcpio itself. Advanced features of ukify will not be available then.

See unified kernel image for details about UKI generation.

The primary configuration file for mkinitcpio is /etc/mkinitcpio.conf. Drop-in configuration files are also supported, e.g. /etc/mkinitcpio.conf.d/myhooks.conf (they aren't taken into account if mkinitcpio is called with -c option and/or use a preset containing ALL_config). Additionally, preset definitions are provided by kernel packages in the /etc/mkinitcpio.d directory (e.g. /etc/mkinitcpio.d/linux.preset).

Users can modify seven variables within the configuration file, see mkinitcpio.conf(5) § VARIABLES for more details:

The MODULES array is used to specify modules to load before anything else is done.

Modules suffixed with a ? will not throw errors if they are not found. This might be useful for custom kernels that compile in modules which are listed explicitly in a hook or configuration file.

These options allow users to add files to the image. Both BINARIES and FILES are added before hooks are run, and may be used to override files used or provided by a hook. BINARIES are auto-located within a standard PATH and are dependency-parsed, meaning any required libraries will also be added. FILES are added as-is. For example:

Note that as both BINARIES and FILES are Bash arrays, multiple entries can be added delimited with spaces.

The HOOKS array is the most important setting in the file. Hooks are small scripts which describe what will be added to the image. For some hooks, they will also contain a runtime component which provides additional behavior, such as starting a daemon, or assembling a stacked block device. Hooks are referred to by their name, and executed in the order they exist in the HOOKS array of the configuration file.

The default HOOKS setting should be sufficient for most simple, single disk setups. For root devices which are stacked or multi-block devices such as LVM, RAID, or dm-crypt, see the respective wiki pages for further necessary configuration.

Build hooks are found in /usr/lib/initcpio/install/, custom build hooks can be placed in /etc/initcpio/install/. These files are sourced by the bash shell during runtime of mkinitcpio and should contain two functions: build and help. The build function describes the modules, files, and binaries which will be added to the image. An API, documented by mkinitcpio(8), serves to facilitate the addition of these items. The help function outputs a description of what the hook accomplishes.

For a list of all available hooks:

Use mkinitcpio's -H/--hookhelp option to output help for a specific hook, for example:

Runtime hooks are found in /usr/lib/initcpio/hooks/, custom runtime hooks can be placed in /etc/initcpio/hooks/. For any runtime hook, there should always be a build hook of the same name, which calls add_runscript to add the runtime hook to the image. These files are sourced by the busybox ash shell during early userspace. With the exception of cleanup hooks, they will always be run in the order listed in the HOOKS setting. Runtime hooks may contain several functions:

run_earlyhook: Functions of this name will be run once the API file systems have been mounted and the kernel command line has been parsed. This is generally where additional daemons, such as udev, which are needed for the early boot process are started from.

run_hook: Functions of this name are run shortly after the early hooks. This is the most common hook point, and operations such as assembly of stacked block devices should take place here.

run_latehook: Functions of this name are run after the root device has been mounted. This should be used, sparingly, for further setup of the root device, or for mounting other file systems, such as /usr.

run_cleanuphook: Functions of this name are run as late as possible, and in the reverse order of how they are listed in the HOOKS array in the configuration file. These hooks should be used for any last minute cleanup, such as shutting down any daemons started by an early hook.

Post hooks are executables or shell scripts located in /usr/lib/initcpio/post/ (hooks provided by packages) and /etc/initcpio/post/ (custom hooks). These files are executed after an image is (re)generated in order to perform additional tasks like signing.

To each executable the following arguments are passed in this order:

Additionally, the following environment variables are set—KERNELVERSION the full kernel version, KERNELDESTINATION the default location where the kernel should be located on order to be booted.

A table of common hooks and how they affect image creation and runtime follows. Note that this table is not complete, as packages can provide custom hooks.

This article or section needs expansion.

Optional when using the systemd hook as it only provides a busybox recovery shell. In addition to enabling base, you'll need to temporarily add SYSTEMD_SULOGIN_FORCE=1 to your kernel parameters to use the shell.

If the autodetect hook runs before this hook, it will only add early microcode update files for the processor of the system the image is built on.

The use of this hook replaces the now deprecated --microcode flag, and the microcode option in the preset files. This also allows you to drop the microcode initrd lines from your boot configuration as they are now packed together with the main initramfs image. || class="archwiki-table-cell" data-sort-value=5 |–

For sd-encrypt see dm-crypt/System configuration#Using systemd-cryptsetup-generator.

The use of this hook requires the rw parameter to be set on the kernel command line (discussion). See fsck#Boot time checking for more details. || class="archwiki-table-cell" data-sort-value=5 |–

The kernel supports several formats for compression of the initramfs: gzip, bzip2, lzma (xz), xz, lzo (lzop), lz4 and zstd. By default mkinitcpio uses zstd compression for kernel version 5.9 and newer and gzip for kernel versions older than 5.9.

The provided mkinitcpio.conf has the various COMPRESSION options commented out. Uncomment one if you wish to switch to another compression method and make sure you have the corresponding compression utility installed. If none is specified, the default method is used. If you wish to create an uncompressed image, specify COMPRESSION=cat in the configuration file or use -z cat on the command line.

These are additional flags passed to the program specified by COMPRESSION, such as:

This option can be left empty; mkinitcpio will ensure that any supported compression method has the necessary flags to produce a working image.

With the default zstd compression, to save space for custom kernels (especially with a dual boot setup when using the EFI system partition as /boot), the --long option is very effective. However, systems with limited RAM may not be able to decompress initramfs using this option. The -v option may also be desired to see details during the initramfs generation. For example:

Highest, but slowest, compression can be achieved by using xz with the -9e compression level and also decompressing the loadable kernel modules and firmware:

MODULES_DECOMPRESS controls whether the kernel module and firmware files are decompressed during initramfs creation. The default value is no.

Arch compresses its kernel modules and linux-firmware with zstd at level 19. When using a higher compression than that for the initramfs, setting MODULES_DECOMPRESS="yes" will allow to reduce the initramfs size even further. This comes at the expense of increased RAM and CPU usage at early boot which negatively affects systems with limited RAM or weak CPUs since the kernel will spend more time to decompress the whole initramfs image than it would take to decompress the individual modules and firmware upon loading them.

This article or section needs expansion.

Runtime configuration options can be passed to init and certain hooks via the kernel command line. Kernel command-line parameters are often supplied by the boot loader. The options discussed below can be appended to the kernel command line to alter default behavior. See Kernel parameters and Arch boot process for more information.

See Boot debugging and mkinitcpio(8) for other parameters.

See RAID#Configure mkinitcpio.

net requires the mkinitcpio-nfs-utils package.

Comprehensive and up-to-date information can be found in the official kernel documentation.

This parameter tells the kernel how to configure IP addresses of devices and also how to set up the IP routing table. It can take up to nine arguments separated by colons: ip=<client-ip>:<server-ip>:<gw-ip>:<netmask>:<hostname>:<device>:<autoconf>:<dns0-ip>:<dns1-ip>:<ntp0-ip>.

If this parameter is missing from the kernel command line, all fields are assumed to be empty, and the defaults mentioned in the kernel documentation apply. In general this means that the kernel tries to configure everything using autoconfiguration.

The <autoconf> parameter can appear alone as the value to the ip parameter (without all the : characters before). If the value is ip=off or ip=none, no autoconfiguration will take place, otherwise autoconfiguration will take place. The most common way to use this is ip=dhcp.

For parameters explanation, see the kernel documentation.

If you have multiple network cards, this parameter can include the MAC address of the interface you are booting from. This is often useful as interface numbering may change, or in conjunction with pxelinux IPAPPEND 2 or IPAPPEND 3 option. If not given, eth0 will be used.

If the nfsroot parameter is NOT given on the command line, the default /tftpboot/%s will be used.

Run mkinitcpio -H net for parameter explanation.

If your root device is on LVM, see Install Arch Linux on LVM#Adding mkinitcpio hooks.

If using an encrypted root see dm-crypt/System configuration#mkinitcpio for detailed information on which hooks to include.

If you keep /usr as a separate partition, you must adhere to the following requirements:

The generation of fallback images can be disabled:

If you are curious about what is inside the initramfs image, you can extract it and poke at the files inside of it.

The initramfs image is an SVR4 CPIO archive, generated via the find and bsdcpio commands, optionally compressed with a compression scheme understood by the kernel. For more information on the compression schemes, see #COMPRESSION.

mkinitcpio includes a utility called lsinitcpio(1) which will list and/or extract the contents of initramfs images.

You can list the files in the image with:

And to extract them all in the current directory:

You can also get a more human-friendly listing of the important parts in the image:

Invoke the build_image function of the /usr/bin/mkinitcpio script with parameters

It can be done by creating a new script with the contents of the build_image function and running it with the above parameters. This will compress the contents present in the current directory in a file named outfile.

The test used by mkinitcpio to determine if /dev is mounted is to see if /dev/fd/ is there. If everything else looks fine, it can be "created" manually by:

(Obviously, /proc must be mounted as well. mkinitcpio requires that anyway, and that is the next thing it will check.)

When initramfs are being rebuilt after a kernel update, you might get warnings:

If these messages appear when generating a default initramfs image, then, as the warning says, installing additional firmware may be required. Most common firmware files can be acquired by installing the linux-firmware package. For other packages providing firmware see the table below or try searching for the module name in the official repositories or AUR.

Otherwise, if the messages only appear when generating the fallback initramfs image you have the following options:

For unavailable firmware, you can suppress the warnings by creating dummy files, e.g.:

On some motherboards (mostly ancient ones, but also a few new ones), the i8042 controller cannot be automatically detected. It is rare, but some people will surely be without keyboard. You can detect this situation in advance. If you have a PS/2 port and get i8042: PNP: No PS/2 controller found. Probing ports directly message, add atkbd to the MODULES array.[2]

With an improper initial ram-disk a system often is unbootable. So follow a system rescue procedure like below:

mkinitcpio's autodetect hook filters unneeded kernel modules in the primary initramfs scanning /sys and the modules loaded at the time it is run. If you transfer your /boot directory to another machine and the boot sequence fails during early userspace, it may be because the new hardware is not detected due to missing kernel modules. Note that USB 2.0 and 3.0 need different kernel modules.

To fix, first try choosing the fallback image from your boot loader, as it is not filtered by autodetect. Once booted, run mkinitcpio on the new machine to rebuild the primary image with the correct modules. If the fallback image fails, try booting into an Arch Linux live CD/USB, chroot into the installation, and run mkinitcpio on the new machine. As a last resort, try manually adding modules to the initramfs.

The systemd hook disables the root account. To enable the emergency shell, temporarily add SYSTEMD_SULOGIN_FORCE=1 to the kernel parameters.

Alternatively, you can use initcpio-hook-shadowcopyAUR, by installing it and adding the shadowcopy hook after systemd in /etc/mkinitcpio.conf, and regenerating initramfs with mkinitcpio -P. More documentation is available in its GitHub repo.

**Examples:**

Example 1 (unknown):
```unknown
/etc/crypttab.initramfs
```

Example 2 (unknown):
```unknown
/etc/mkinitcpio.conf
```

Example 3 (unknown):
```unknown
/etc/mkinitcpio.d/
```

Example 4 (unknown):
```unknown
linux.preset
```

---

## GDM

**URL:** https://wiki.archlinux.org/title/GNOME_Display_Manager

**Contents:**
- Installation
- Starting
  - Autostarting applications
- Configuration
  - Login screen background image
  - dconf configuration
    - Login screen logo
    - Changing the cursor theme
    - Changing the icon theme
    - Larger font for log-in screen

From GDM - GNOME Display Manager: "The GNOME Display Manager (GDM) is a program that manages graphical display servers and handles graphical user logins."

Display managers provide X Window System and Wayland users with a graphical login prompt.

GDM can be installed with the gdm package, and is installed as part of the gnome group.

To start GDM at boot time, enable gdm.service.

To automatically start applications after logging in, follow the instructions in Autostarting#On desktop environment startup that pertain to your desktop environment.

The factual accuracy of this article or section is disputed.

Firstly, you need to extract the existing GNOME Shell theme to a directory in your home directory. You can do this using the following script:

Navigate to the created directory. You should find that the theme files have been extracted to it (under a theme subfolder). Now copy your preferred background image to this directory.

Next, you need to create a file under the theme directory with the following content (the list below is for Gnome 47, just in case verify that the contents of the theme subfolder match the list here, including the background image):

Replace filename with the filename of your background image or remove the line to use a hex color value instead.

Now, open the gnome-shell-light.css and gnome-shell-dark.css files and change the #lockDialogGroup definition as follows:

Set background-size to the resolution that GDM uses; this might not necessarily be the resolution of the image. For a list of display resolutions, see Display resolution.

Again, set filename to be the name of the background image, filename should be within quotes in the CSS. background-size can also be set to auto if you want the image scaled to fill.

If using multiple monitors, the image ends up spanning across the monitors, so it might be better to use an SVG file for the background.

If you only want to change the background color, adjust the #lockDialogGroup definition as follows:

where color is the new hex-encoded background color.

Next, compile the theme using the following command:

Then, copy the resulting gnome-shell-theme.gresource file to the /usr/share/gnome-shell directory (keep a backup of the original).

Finally, logout and you should find that gdm is using your preferred background image (you might need to restart gdm if the changes aren't applied immediately).

If a subsequent update resets the gnome-shell-theme.gresource file, simply repeat the above steps, verifying that the contents of the XML match the extracted file list.

For more information, please see the following forum thread. A shell script to automate the above steps is available on DimaZirix's github repository.

Some GDM settings are stored in a dconf database. They can be configured either by adding keyfiles to the /etc/dconf/db/gdm.d directory and then recompiling the GDM database by running dconf update as root or by logging into the GDM user on the system and changing the setting directly using the gsettings command line tool. Note that for the former approach, a GDM profile file is required—this must be created manually as it is no longer shipped upstream, see below:

For the latter approach, you can log into the GDM user with the command below:

Create the following keyfile:

Then recompile the GDM database.

Alternatively, execute the following as the GDM user to change the logo:

To disable the logo, you can set the value to an empty string:

GDM disregards GNOME cursor theme settings and it also ignores the cursor theme set according to the XDG specification. To change the cursor theme used in GDM, create the following keyfile:

Then recompile the GDM database. Alternatively, execute the following as the GDM user temporarily and change the cursor theme:

The same methods can be used to change the icon theme. Create the following keyfile:

Then, recompile the GDM database. Alternatively, execute the following as the GDM user temporarily and change the icon theme:

Click on the accessibility icon at the top right of the screen (a white circle with the silhouette of a person in the centre) and check the Large Text option.

To set a specific scaling factor, create the following keyfile:

Then recompile the GDM database. Alternatively, execute the following as the GDM user temporarily and change the font:

This tweak disables the audible feedback heard when the system volume is adjusted (via keyboard) on the login screen.

Create the following keyfile:

Then recompile the GDM database. Alternatively execute the following as the GDM user temporarily and turn off the sound:

Create the following keyfile:

Then recompile the GDM database. Alternatively, execute the following as the GDM user temporarily and configure the behavior:

where action can be one of nothing, suspend or hibernate.

Tap-to-click is disabled in GDM (and GNOME) by default, but you can easily enable it with a dconf setting.

To enable tap-to-click, create the following keyfile:

Then recompile the GDM database. Alternatively, execute the following as the GDM user temporarily and enable the action:

This article or section is out of date.

To disable or enable the Accessibility Menu, create the following keyfile:

Then recompile the GDM database. Alternatively, execute the following as the GDM user temporarily and change the status:

The menu is disabled when the key is false, enabled when it is true.

To enable Night Light on GDM, run

GDM requires the XKBLAYOUT parameter to be set in /etc/vconsole.conf; without it it defaults to a standard us (qwerty) layout, i.e it will not honor the value set in KEYMAP.

One generally applicable way to do so is to use localectl --no-convert set-x11-keymap [keymap]: see Keyboard configuration in Xorg#Setting keyboard layout for details.

The system language will be applied to GDM. If a system has multiple users, it is possible to set a language for GDM different to the system language. In this case, firstly ensure that gnome-control-center is installed. Then, start gnome-control-center and choose Region & Language. In the header bar, check the Login Screen toggle button. Finally, click on Language and choose your language from the list. You will be prompted for your root password. Note that the Login Screen button will not be visible in the header bar unless multiple users are present on the system [5].

To enable automatic login with GDM, add the following to /etc/gdm/custom.conf (replace username with your own):

or for an automatic login with a delay:

You can set the session used for automatic login (replace gnome-xorg with desired session):

If you want to bypass the password prompt in GDM then simply add the following line on the first line of /etc/pam.d/gdm-password:

Then, add the group nopasswdlogin to your system. See User group for group descriptions and group management commands.

Now, add your user to the nopasswdlogin group and you will only have to click on your username to login.

When using a fingerprint to login, it will not unlock the keychain, so you will still be prompted for the keychain password. You might want to disallow login and keep the fingerprint to unlock your session. To do this, just disable fingerprint for the GDM user.

Execute the following as the GDM user temporarily and change this setting:

GDM uses polkit and logind to gain permissions for shutdown. You can shutdown the system when multiple users are logged in by setting:

You can find all available logind options (e.g. reboot-multiple-sessions) in org.freedesktop.login1(5).

It is not advised to login as root, but if necessary you can edit /etc/pam.d/gdm-password and add the following line before the line auth required pam_deny.so:

The file should look something like this:

You should be able to login as root after restarting GDM.

The users for the gdm user list are gathered by AccountsService. It will automatically hide system users (UID < 1000). To hide ordinary users from the login list create or edit a file named after the user to hide in /var/lib/AccountsService/users/ to contain at least:

Remote login can be configured via graphical interface by navigating to System > Remote Desktop > Remote Login in Gnome Settings app.

To display current status and credentials, the following command can be used:

To generate new TLS key and certificate:

If -n rdp-tls part is omitted, hostname will be used as the name instead.

To set TLS key and certificate:

Finally, to enable Remote Login:

Some desktop environments store display settings in ~/.config/monitors.xml, based on which xrandr commands are generated. GDM has a file serving a similar purpose located at /etc/xdg/monitors.xml (This is the global mutter configuration and will impact new GNOME session too).

If you have your monitors setup as you like (resolution, refresh rate, orientation, scaling, primary and so on) in ~/.config/monitors.xml and want GDM to honor those settings:

To automatically re-configure the monitor setup on each boot, use a drop-in file for gdm.service:

The relevant parts of monitors.xml for screen rotation and scaling are:

Changes will take effect on logout. This is necessary because GDM does not respect xorg.conf.

If you have modified Mutter's experimental-features setting (usual done for fractional scaling or Variable), make sure the dconf setup is done globally not just to you user, see fractional scaling. This due to these setting can make the monitors.xml incompatible, see GDM bug 1028. i

You can use the xhost command to configure X server access permissions.

For instance, to grant GDM the right to access the X server, use the following command:

To use Wayland in GDM with the NVIDIA driver, you must fulfill the three following conditions:

If, instead of GDM, a black screen appears, try disabling integrated graphics in your computer's BIOS settings.

In some cases, GNOME fails to start and transfers control back to GDM, which in turn causes the login screen to reappear. You may try setting the following environment variable as suggested in BBS#2126478:

If GDM starts up properly on boot, but fails after repeated attempts on logout, try adding this line to the daemon section of /etc/gdm/custom.conf:

See Xorg#Rootless Xorg.

The Wayland backend is used by default, and the Xorg backend is used only if the Wayland backend cannot be started. You may wish to use the Xorg backend instead if, for example:

To use the Xorg backend by default, uncomment the following line in /etc/gdm/custom.conf:

After removing gdm, systemd may report the following:

To remove this warning, login as root and delete the primary user gdm and then delete the group gdm:

Verify that gdm is successfully removed via pwck and grpck with root privileges. To round it off, you may want to double-check no unowned files for gdm remain.

GDM uses a separate dconf database to control power management. To apply your user's power settings, copy them to GDM's dconf database:

where username is your username.

To only disable auto-suspend on AC, run:

(To also disable auto-suspend on battery, run the command with battery instead of ac.)

Restart GDM to activate your changes.

For newer versions of GNOME (Tested on 49.1):

Supplementary forum links:

Wayland requires Kernel Mode Setting (KMS) running in order to work, and on some machines the GDM process start earlier than KMS, resulting in GDM unable to see Wayland and working only with X.Org. This might result in messages like the following showing up in your log:

Alternatively, the same issue may lead to GDM not appearing or monitor only displaying the TTY output.

You can solve this problem by starting KMS earlier. You may also wish to just verify that Wayland is enabled in the GDM config (see above).

At first, without an NVIDIA device, GDM starts and works normally on Wayland, but stops working once an NVIDIA eGPU is plugged in (or the nvidia module is loaded for other reasons). A typical symptom of the problem is a black screen with a blinking cursor upon logouts and GDM restarts and the following message in GDM's logs (accessed by running journalctl -u gdm -b as root):

The solution is the same as #GDM ignores Wayland and uses X.Org by default: Prevent /usr/lib/gdm-disable-wayland from running upon nvidia module loading.

Notice that GDM on Wayland will no longer work once /usr/lib/gdm-disable-wayland has run. This is because WaylandEnable=false has been written into /run/gdm/custom.conf, which overrides /etc/gdm/custom.conf. To fix the situation without a reboot, remove /run/gdm/custom.conf and then restart GDM.

See systemd/FAQ#Failure to enable unit due to preexisting symlink.

**Examples:**

Example 1 (unknown):
```unknown
gdm.service
```

Example 2 (unknown):
```unknown
extractgst.sh
```

Example 3 (unknown):
```unknown
#!/bin/sh
gst=/usr/share/gnome-shell/gnome-shell-theme.gresource
workdir=${HOME}/shell-theme

for r in `gresource list $gst`; do
	r=${r#\/org\/gnome\/shell/}
	if [ ! -d $workdir/${r%/*} ]; then
	  mkdir -p $workdir/${r%/*}
	fi
done

for r in `gresource list $gst`; do
        gresource extract $gst $r >$workdir/${r#\/org\/gnome\/shell/}
done
```

Example 4 (unknown):
```unknown
gnome-shell-theme.gresource.xml
```

---

## InterPlanetary File System

**URL:** https://wiki.archlinux.org/title/IPFS

**Contents:**
- Installation
- Using a service to start the daemon
  - Starting the service with a different command line
- Firewall
- File sharing
- Simple hosting with name resolution
- Usage as makepkg DLAGENT
- See also

The InterPlanetary File System (IPFS) is a distributed hypermedia protocol, addressed by content and identities. IPFS enables the creation of completely distributed applications, datastores as well as websites and aims to make the web faster, safer, and more open.

It seeks to connect all computers which do peer-to-peer exchanges of the requested file transfers. In some ways this is similar to the original goals of the Web, but instead of using a client and server role model and location-based addressing, all computers in the network share the information about where which files are stored in a distributed fashion. This way a user does not need to know which server contains which files, but instead only needs to know which id the content has to receive it. The network locates the corresponding nodes and requests the data in a tamper-proof way. Files are stored in objects similar to the way git works, and the metadata storage and the file transfers have similarities to BitTorrent.

Files added to an IPFS node get chunked and hashed. The individual chunks are added to a MerkleDAG - a tree object - which gets stored alongside the chunks. The hash of the tree object is the tamper-proof content-id (CID). The node then publishes all hashes in the distributed hash-table of the network with its node-id. This way the content can be located by the CID to specific nodes which can provide the data on requests.

Install the kubo package.

To start using IPFS you must first issue

as a user. This creates a ~/.ipfs directory with all the necessary files in it.

Now you can start the IPFS daemon:

This starts your node, available via the ipfs cli, or the web interface on localhost:5001/webui. Additionally, a local gateway goes up on localhost:8080 (the default port can be changed in ~/.ipfs/config).

For convenience, you can automate the startup of the IPFS daemon using the Systemd/User service included in kubo. This ensures that the daemon starts when you log in and that it is restarted if it crashes.

You can start/enable ipfs.service as a user unit.

If you want the service to always run regardless of the user session status, enable the system-wide service instead by starting/enabling ipfs@username.service.

You may also want to limit the bandwidth IPFS uses by using trickleAUR (c.f. ipfs/go-ipfs#3429). You can edit this Systemd/User service file to $HOME/.config/systemd/user/go-ipfs.service:

This will both start IPFS with trickle and pass the argument --routing=dhtclient. You may of course modify it as needed, or base your version on the package's /usr/lib/systemd/user/ipfs.service. Start/enable go-ipfs.service as a user unit.

IPFS requires port 4001 TCP and UDP. It attempts to configure port forwarding on the router using UPnP/IGD. Routers without UPnP/IGD support will need to do a manual port forward.

To share a file using IPFS you need the daemon to be running.

returns a hash. If someone shared this file via IPFS before, the hash would match that previous upload, making you the second source of the file.

To retrieve a file via the IPFS hash, use ipfs cat:

You can pipe this into any other application, for example, to watch a video with mpv:

Or you can download the file:

There is also an ipgetAUR utility, which acts like wget for IPFS. In addition, it includes a bootstrap node, so you will not have to have ipfs daemon running or installed to use it. To download a file:

You can share both files and folders. Folders should be shared recursively:

To view all the files and caches in a folder (if the hash is a folder):

Every file shared with network is accessible via the IPFS gateway on localhost:8080 like this:

There are public gateways, allowing users with no IPFS node running to access files on the network. For example, the official website:

In IPFS, files shared are never deleted, and with any change of a file its hash changes as well. This makes tasks such as website hosting difficult, as any changes to a webpage, for example to an index.html, would result in it having a different hash, and the old webpage would be still accessible from the old hash. It is one of a network's goals to store all the content persistently with full history. IPFS offers a name service you can use to generate persistent caches - IPNS. IPNS allows you to bind any hash to your node's unique ID, generated at initialization. You can view your ID like this:

And to bind any hash to it:

This assigns the new hash generated by ipfs add after the file change to your node ID and hence makes the updated version of a folder/file accessible at the same address.

Note that when using IPNS the address has an ipns prefix instead of ipfs:

PKGBUILDs containing URIs pointing to IPFS resources need the ipfs-dlagentAUR download agent installed and configured to be correctly built.

**Examples:**

Example 1 (unknown):
```unknown
$ ipfs init
```

Example 2 (unknown):
```unknown
$ ipfs daemon
```

Example 3 (unknown):
```unknown
~/.ipfs/config
```

Example 4 (unknown):
```unknown
ipfs.service
```

---

## Universal Wayland Session Manager

**URL:** https://wiki.archlinux.org/title/Universal_Wayland_Session_Manager

**Contents:**
- Installation
- Configuration
  - Service startup notification and vars set by compositor
  - Environment variables
- Usage
  - Startup
    - TTY
    - Display manager
  - Session termination
- Tips and tricks

The Universal Wayland Session Manager (uwsm) wraps standalone Wayland compositors into a set of systemd units on the fly. This provides robust session management including environment, XDG Autostart support, bi-directional binding with login session, and clean shutdown.

Install the uwsm package.

In order to find the current compositor, a Wayland application run as a systemd service needs the WAYLAND_DISPLAY environment variable (or DISPLAY if they are intended to run through Xwayland). Therefore this and other useful environment variables should be put into the systemd/dbus activation environment once the compositor has set their values.

The command uwsm finalize puts WAYLAND_DISPLAY, DISPLAY and other environment variables listed via the white-space separated UWSM_FINALIZE_VARNAMES list into the activation environment. It is recommended to execute this command after the compositor is ready.

If other variables set by the compositors are needed in the activation environment, they can be passed as arguments to uwsm finalize or put into a white-space separated list in UWSM_FINALIZE_VARNAMES. See the examples below:

All environment variables set in ${XDG_CONFIG_HOME}/uwsm/env are sourced by uwsm and available to all managed compositors and graphical applications run inside such a session.

If you need certain environment variables to be set only for a specific compositor (and graphical applications in that graphical session), then put them in ${XDG_CONFIG_HOME}/uwsm/env-compositor instead.

An example of such a file can be seen below:

uwsm can be started both by TTY and by a display manager.

Add in your ~/.profile file:

If you want to always start the same compositor, then you can use instead in your ~/.profile file:

You can create a custom session desktop entry which starts your compositor through uwsm:

If you want to terminate the current uwsm session, then you should use either loginctl terminate-user "" (terminates the entire user session) or uwsm stop (executes code after uwsm start or terminates user session, if it replaced the login shell).

By default uwsm launches compositors through a custom systemd service in session.slice. Many Wayland compositors allow you to start other applications that would then be launched inside the compositor service, which would uselessly consume compositor resources or even interfere with notification sockets.

To start applications as separate systemd scope units you can use uwsm app, which can launch both executables

By default uwsm puts launched scope units in app-graphical.slice slice. If you want to put them in background-graphical.slice or session-graphical.slice, then you should respectively use options -s b, -s s:

Instead of uwsm app (which runs as a Python script), you can use a faster alternative:

**Examples:**

Example 1 (unknown):
```unknown
loginctl terminate-user ""
```

Example 2 (unknown):
```unknown
WAYLAND_DISPLAY
```

Example 3 (unknown):
```unknown
uwsm finalize
```

Example 4 (unknown):
```unknown
WAYLAND_DISPLAY
```

---

## systemd/Timers

**URL:** https://wiki.archlinux.org/title/Systemd/Timers

**Contents:**
- Timer units
- Service units
- Management
- Examples
  - Monotonic timer
  - Realtime timer
- Transient timer units
- As a cron replacement
  - Benefits
  - Caveats

Timers are systemd unit files whose name ends in .timer that control .service files or events. Timers can be used as an alternative to cron (read #As a cron replacement). Timers have built-in support for calendar time events, monotonic time events, and can be run asynchronously.

Timers are systemd unit files with a suffix of .timer. Timers are like other unit configuration files and are loaded from the same paths but include a [Timer] section which defines when and how the timer activates. Timers are defined as one of two types:

For a full explanation of timer options, see the systemd.timer(5). The argument syntax for calendar events and time spans is defined in systemd.time(7).

For each .timer file, a matching .service file exists (e.g. foo.timer and foo.service). The .timer file activates and controls the .service file. The .service does not require an [Install] section as it is the timer units that are enabled. If necessary, it is possible to control a differently-named unit using the Unit= option in the timer's [Timer] section.

To use a timer unit enable and start it like any other unit (remember to add the .timer suffix). To view all started timers, run:

A service unit file can be scheduled with a timer out-of-the-box. The following examples schedule foo.service to be run with a corresponding timer called foo.timer.

A timer which will start 15 minutes after boot and again every week while the system is running.

A timer which starts once a week (at 12:00am on Monday). When activated, it triggers the service immediately if it missed the last start time (option Persistent=true), for example due to the system being powered off:

When more specific dates and times are required, OnCalendar events uses the following format:

An asterisk may be used to specify any value and commas may be used to list possible values. Two values separated by .. indicate a contiguous range.

In the below example the service is run the first four days of each month at 12:00 PM, but only if that day is a Monday or a Tuesday.

To run a service on the first Saturday of every month, use:

When using the DayOfWeek part, at least one weekday has to be specified. If you want something to run every day at 4am, use:

To run a service at different times, OnCalendar may be specified more than once. In the example below, the service runs at 22:30 on weekdays and at 20:00 on weekends.

You can also specify a timezone at the end of the directive (use timedatectl list-timezones to list accepted values)

More information is available in systemd.time(7).

One can use systemd-run to create transient .timer units. That is, one can set a command to run at a specified time without having a service file. For example the following command touches a file after 30 seconds:

One can also specify a pre-existing service file that does not have a timer file. For example, the following starts the systemd unit named someunit.service after 12.5 hours have elapsed:

See systemd-run(1) for more information and examples.

Although cron is arguably the most well-known job scheduler, systemd timers can be an alternative.

The main benefits of using timers come from each job having its own systemd service. Some of these benefits are:

Some things that are easy to do with cron are difficult to do with timer units alone:

Also note that user timer units will only run during an active user login session by default. However, lingering can enable services to run at boot even when the user has no active login session.

Several of the caveats can be worked around by installing a package that parses a traditional crontab to configure the timers. systemd-cron-next-gitAUR and systemd-cronAUR are two such packages. These can provide the missing MAILTO feature.

Also, like with crontabs, a unified view of all scheduled jobs can be obtained with systemctl. See #Management.

Outside of migrating from an existing crontab, using the same periodicity as cron can be desired. To avoid the tedious task of creating a timer for each service to start periodically, use a template unit, for example:

Then one only needs to enable/start monthly@unit_name.timer.

Some software will track the time elapsed since they last ran, for example blocking the update of a database if the last download ended less than 24 hours ago.

By default, timers do not track when the task they launched has ended. To work around this, we can use OnUnitInactiveSeconds:

The systemd-timer-notifyAUR provides an automatic desktop notification that helps you notice when a systemd service is triggered by a timer and is running. The notification will automatically close when the service finishes.

This can be helpful for understanding why CPU usage is high or for preventing a shutdown when a backup service hasn't finished.

For more details and configuration options, visit https://gitlab.com/Zesko/systemd-timer-notify

**Examples:**

Example 1 (unknown):
```unknown
OnCalendar=
```

Example 2 (unknown):
```unknown
OnUnitActiveSec
```

Example 3 (unknown):
```unknown
timers.target
```

Example 4 (unknown):
```unknown
WantedBy=timers.target
```

---

## systemd/Journal

**URL:** https://wiki.archlinux.org/title/Systemd_journal

**Contents:**
- Priority level
- Facility
- Filtering output
- Tips and tricks
  - Journal size limit
    - Per unit size limit by a journal namespace
  - Clean journal files manually
  - Journald in conjunction with syslog
  - Forward journald to /dev/tty12
  - Specify a different journal to view

systemd has its own logging system called the journal; running a separate logging daemon is not required. To read the log, use journalctl(1).

In Arch Linux, the directory /var/log/journal/ is a part of the systemd package, and the journal (when Storage= is set to auto in /etc/systemd/journald.conf) will write to /var/log/journal/. If that directory is deleted, systemd will not recreate it automatically and instead will write its logs to /run/log/journal/ in a non-persistent way. However, the directory will be recreated if Storage=persistent is added to journald.conf and systemd-journald.service is restarted (or the system is rebooted).

Systemd journal classifies messages by Priority level and Facility. Logging classification corresponds to classic Syslog protocol (RFC 5424).

A syslog severity code (in systemd called priority) is used to mark the importance of a message RFC 5424 6.2.1.

These rules are recommendations, and the priority level of a given error is at the application developer's discretion. It is always possible that the error will be at a higher or lower level than expected.

A syslog facility code is used to specify the type of program that is logging the message RFC 5424 6.2.1.

Useful facilities to watch: 0, 1, 3, 4, 9, 10, 15.

journalctl allows for the filtering of output by specific fields. If there are many messages to display, or if the filtering of large time spans has to be done, the output of this command can be extensively delayed.

See journalctl(1), systemd.journal-fields(7), or Lennart Poettering's blog post for details.

If the journal is persistent (non-volatile), its size limit is set to a default value of 10% of the size of the underlying file system but capped at 4 GiB. For example, with /var/log/journal/ located on a 20 GiB partition, journal data may take up to 2 GiB. On a 50 GiB partition, it would max at 4 GiB. To confirm current limits on your system review systemd-journald unit logs:

The maximum size of the persistent journal can be controlled by uncommenting and changing the following:

It is also possible to use the drop-in snippets configuration override mechanism rather than editing the global configuration file. In this case, place the overrides under the [Journal] header:

Restart the systemd-journald.service after changing this setting to apply the new limit.

See journald.conf(5) for more info.

Edit the unit file for the service you wish to configure (for example sshd) and add LogNamespace=ssh in the [Service] section.

Then create /etc/systemd/journald@ssh.conf by copying /etc/systemd/journald.conf. After that, edit journald@ssh.conf and adjust SystemMaxUse to your liking.

Restarting the service should automatically start the new journal service systemd-journald@ssh.service. The logs from the namespaced service can be viewed with journalctl --namespace ssh.

See systemd-journald.service(8) § JOURNAL NAMESPACES for details about journal namespaces.

Journal files can be globally removed from /var/log/journal/ using e.g. rm, or can be trimmed according to various criteria using journalctl. For example:

Journal files must have been rotated out and made inactive before they can be trimmed by vacuum commands. Rotation of journal files can be done by running journalctl --rotate. The --rotate argument can also be provided alongside one or more vacuum criteria arguments to perform rotation and then trim files in a single command.

See journalctl(1) for more info.

Compatibility with a classic, non-journald aware syslog implementation can be provided by letting systemd forward all messages via the socket /run/systemd/journal/syslog. To make the syslog daemon work with the journal, it has to bind to this socket instead of /dev/log (official announcement).

The default journald.conf for forwarding to the socket is ForwardToSyslog=no to avoid system overhead, because rsyslog or syslog-ng pull the messages from the journal by itself.

See Syslog-ng#Overview and Syslog-ng#syslog-ng and systemd journal, or rsyslog respectively, for details on configuration.

Create a drop-in directory /etc/systemd/journald.conf.d and create a fw-tty12.conf file in it:

Then restart systemd-journald.service.

There may be a need to check the logs of another system that is dead in the water, like booting from a live system to recover a production system. In such case, one can mount the disk in e.g. /mnt, and specify the journal path via -D/--directory, like so:

By default, a regular user only has access to their own per-user journal. To grant read access for the system journal as a regular user, you can add that user to the systemd-journal user group. Members of the adm and wheel groups are also given read access.

See journalctl(1) § DESCRIPTION and Users and groups#User groups for more information.

Desktop notifications can help you quickly notice error messages, improving awareness compared to manually checking logs or not noticing them at all.

The journalctl-desktop-notificationAUR package provides an automatic desktop notification for each error message logged by any process. For more details and configuration options, visit the GitLab project page.

**Examples:**

Example 1 (unknown):
```unknown
/var/log/journal/
```

Example 2 (unknown):
```unknown
/etc/systemd/journald.conf
```

Example 3 (unknown):
```unknown
/var/log/journal/
```

Example 4 (unknown):
```unknown
/run/log/journal/
```

---

## Linux console

**URL:** https://wiki.archlinux.org/title/Linux_console

**Contents:**
- Implementation
  - Virtual consoles
  - Text mode
  - Framebuffer console
- Keyboard shortcuts
- Fonts
  - Preview and temporary changes
  - Persistent configuration
- Cursor appearance
  - Cursor size

According to Wikipedia:

This article describes the basics of the Linux console and how to configure the font display. Keyboard configuration is described in the /Keyboard configuration subpage. For alternative console solutions offering more features (full Unicode fonts, modern graphics adapters etc.), see KMSCON or similar projects.

The console, unlike most services that interact directly with users, is implemented in the kernel. This contrasts with terminal emulation software, such as Xterm, which is implemented in user space as a normal application. The console has always been part of released Linux kernels, but has undergone changes in its history, most notably the transition to using the framebuffer and support for Unicode.

Despite many improvements in the console, its full backward compatibility with legacy hardware means it is limited compared to a graphical terminal emulator. The main difference between the Linux console and graphical terminal emulators is the shells in the Linux console are attached directly to TTY devices (/dev/tty*), whereas the shells in a graphical terminal emulator are attached to pseudo-TTYs (/dev/pty*).

Also, graphical terminal emulators can have many more features than the Linux console, including a richer set of available fonts, multiple tabs/windows, split views, scrollback buffers/sliders, background colors/images (optionally with transparency), etc. Some of these features can be used in the Linux console with terminal multiplexers, such as Tmux or GNU Screen, or in certain text user interface programs (TUI) typically relying on libraries such as ncurses and the like, e.g. Vim, nano, or Emacs. These can also be used in graphical terminal emulators, if desired.

The console is presented to the user as a series of virtual consoles. These give the impression that several independent terminals are running concurrently; each virtual console can be logged in with different users, run its own shell and have its own font settings. The virtual consoles each use a device /dev/ttyX, and you can switch between them by pressing Alt+Fx (where x is equal to the virtual console number, beginning with 1). The device /dev/console is automatically mapped to the active virtual console.

See also chvt(1), openvt(1) and deallocvt(1).

Since Linux originally began as a kernel for PC hardware, the console was developed using standard IBM CGA/EGA/VGA graphics, which all PCs supported at the time. The graphics operated in VGA text mode, which provides a simple 80x25 character display with 16 colours. This legacy mode is similar to the capabilities of dedicated text terminals, such as the DEC VT100 series. It is still possible to boot in text mode (with vga=0 nomodeset) if the system hardware supports it, but almost all modern distributions (including Arch Linux) use the framebuffer console instead.

As Linux was ported to other non-PC architectures, a better solution was required, since other architectures do not use VGA-compatible graphics adapters, and may not support text modes at all. The framebuffer console was implemented to provide a standard console across all platforms, and so presents the same VGA-style interface regardless of the underlying graphics hardware. As such, the Linux console is not a terminal emulator, but a terminal in its own right. It uses the terminal type linux, and is largely compatible with VT100.

See also console_codes(4).

The Linux console uses UTF-8 encoding by default, but because the standard VGA-compatible framebuffer is used, a console font is limited to either a standard 256, or 512 glyphs. If the font has more than 256 glyphs, the number of colours is reduced from 16 to 8. In order to assign correct symbol to be displayed to the given Unicode value, a special translation map, often called unimap, is needed. Nowadays, most of the console fonts have the unimap built-in; historically, it had to be loaded separately.

By default, the virtual console uses the kernel built-in font with a CP437 character set[1], but this can be easily changed. The kernel offers about 15 built in fonts to choose from, from which the officially supported kernels provide two: VGA 8x16 font (CONFIG_FONT_8x16) and Terminus 16x32 font (CONFIG_FONT_TER16x32). The kernel chooses the one to use based on its evaluation of the screen resolution. Another builtin font can be forced upon by kernel parameters boot parameter setting such as fbcon=font:TER16x32.

The kbd package provides tools to override the kernel decision for virtual console font and font mapping. Available fonts are provided in the /usr/share/kbd/consolefonts/ directory; those ending with .psfu or .psfu.gz have a Unicode translation map built-in.

Keymaps, the connection between the key pressed and the character used by the computer, are found in the subdirectories of /usr/share/kbd/keymaps/; see /Keyboard configuration for details.

shows a table of glyphs or letters of a font.

setfont temporarily changes the font if passed a font name (in /usr/share/kbd/consolefonts/) such as

Font names are case-sensitive. With no parameter, setfont returns the console to the default font.

So to have a small 8x8 font, with that font installed like seen below, use e.g.:

To have a bigger font, the Terminus font (terminus-font) is available in many sizes, such as ter-132b which is large.

You can also append -d for double size. This would be using a 64*64 font:

The FONT variable in /etc/vconsole.conf is used to set the font at boot, persistently for all consoles. See vconsole.conf(5) for details.

For displaying characters such as Č, ž, đ, š or Ł, ę, ą, ś using the font lat2-16.psfu.gz:

It means that second part of ISO/IEC 8859 characters are used with size 16. You can change font size using other values (e.g. lat2-08). For the regions determined by 8859 specification, look at the Wikipedia:ISO/IEC 8859#The parts of ISO/IEC 8859.

Since mkinitcpio v33, the font specified in /etc/vconsole.conf gets automatically loaded during early userspace by default via the consolefont hook, which adds the font to the initramfs. See Mkinitcpio#HOOKS for more information.

You may also need to restart systemd-vconsole-setup.service after changing /etc/vconsole.conf.

If the fonts appear to not change on boot, or change only temporarily, it is most likely that they got reset when graphics driver was initialized and console was switched to framebuffer. By default, all in-tree kernel drivers are loaded early, NVIDIA users should see NVIDIA#Early loading to load their graphics driver before /etc/vconsole.conf is applied.

This subject is poorly documented. You should read the following articles:

The console cursor can be adjusted with Device Attributes (DA) control function. The sequence of parameters must be preceded by a single question mark (despite console_codes(4) says the opposite).

Here is an example for full block non-blinking green cursor with black symbols under it:

The same can be expressed with the octal and characters instead of hex codes:

The same can be applied as permanent configuration with kernel parameter:

The first parameter, despite it's name cursor size, with number 16 (the rightmost two hex digits of the kernel parameter are 10) means "use software cursor."

If you want to change hardware cursor shape - use corresponding number (from 0 to 6, see the table above).

The second parameter, called toggle mask, flips corresponding bits of the color.

(symbol under the cursor)

In our case the second parameter is 15 (the middle hex digits of the kernel parameter are 0f), so all four foreground (symbol) bits will be flipped. The most important rule is: toggling (the second parameter) is applied after the setting (the third parameter).

The third parameter is called set mask. It sets corresponding character attribute bits. We use 47 (the leftmost hex digits of the kernel parameter are 2f) in our example, which means two things:

See HiDPI#Linux console (tty).

**Examples:**

Example 1 (unknown):
```unknown
/dev/console
```

Example 2 (unknown):
```unknown
vga=0 nomodeset
```

Example 3 (unknown):
```unknown
Ctrl+Alt+Del
```

Example 4 (unknown):
```unknown
/usr/lib/systemd/system/ctrl-alt-del.target
```

---

## DNS-over-HTTPS

**URL:** https://wiki.archlinux.org/title/DNS-over-HTTPS

**Contents:**
- Installation
- Client startup
  - Disable any services bound to port 53
  - Change system DNS server
  - Startup
  - Test configuration
- Client configuration
  - Select preferred upstream DNS server
- Troubleshooting
  - Service does not start properly in wired connection

DNS-over-HTTPS is an implementation of DNS over HTTPS. It can act as a stub resolver.

Install the dns-over-https package.

To see if any programs are using port 53, run:

If the output contains more than the first line of column names, you need to disable whatever service is using port 53. You are ready to proceed once the above command outputs nothing more than the following line:

Change your system's DNS server to an address in the listen = section of the configuration file. If you don't know what you're doing, 127.0.0.1 is recommended.

This can be accomplished either through your Network Manager or through editing /etc/resolv.conf.

Start or enable doh-client.service.

To test if your system's DNS works, type nslookup www.google.com into the command line. Note that this will work before DNS-over-HTTPS is configured, assuming you had a DNS configuration before installing this.

The client configuration file is /etc/dns-over-https/doh-client.conf by default

To select a preferred DNS server, uncomment one of the profiles.

If your preferred server is not listed, you may use the following template in the [upstream] section.

As explained by the developer:

Upstream suggests to use a drop-in snippet to your service file to:

**Examples:**

Example 1 (unknown):
```unknown
$ ss -lp 'sport = :domain'
```

Example 2 (unknown):
```unknown
Netid State   Recv-Q  Send-Q   Local Address:Port     Peer Address:Port Process
```

Example 3 (unknown):
```unknown
doh-client.service
```

Example 4 (unknown):
```unknown
nslookup www.google.com
```

---

## System maintenance

**URL:** https://wiki.archlinux.org/title/Backup

**Contents:**
- Check for errors
  - Failed systemd services
  - Log files
- Backup
  - Configuration files
  - List of installed packages
  - Pacman database
  - Encryption metadata
  - System and user data
- Upgrading the system

Regular system maintenance is necessary for the proper functioning of Arch over a period of time. Timely maintenance is a practice many users get accustomed to.

Check if any systemd services have failed:

See systemd#Using units for more information.

Look for errors in the log files located in /var/log/, as well as messages logged in the systemd journal:

See Xorg#Troubleshooting for information on where and how Xorg logs errors.

Having backups of important data is a necessary measure to take, since human and machine processing errors are very likely to generate corruption as time passes, and also the physical media where the data is stored is inevitably destined to fail.

See Synchronization and backup programs for many alternative applications that may better suit your case. See Category:System recovery for other articles of interest.

It is highly encouraged to automate backups and test the recovery process to ensure everything works as intended. For automation see System backup#Automation.

Before editing any configuration files, create a backup so that you can revert to a working version in case of problems. Editors like vim and emacs can do this automatically. On a larger scale, consider using a configuration manager.

For dotfiles (configuration files in the home directory), see dotfiles#Tracking dotfiles directly with Git.

Maintain a list of all installed packages so that if a complete re-installation is inevitable, it is easier to re-create the original environment.

See pacman/Tips and tricks#List of installed packages for details.

See pacman/Tips and tricks#Back up the pacman database.

See Data-at-rest encryption#Backup for disk encryption scenarios.

It is recommended to perform full system upgrades regularly via pacman#Upgrading packages, to enjoy both the latest bug fixes and security updates, and also to avoid having to deal with too many package upgrades that require manual intervention at once. When requesting support from the community, it will usually be assumed that the system is up to date.

Make sure to have the Arch install media or another Linux "live" CD/USB available so you can easily rescue your system if there is a problem after updating. If you are running Arch in a production environment, or cannot afford downtime for any reason, test changes to configuration files, as well as updates to software packages, on a non-critical duplicate system first. Then, if no problems arise, roll out the changes to the production system.

If the system has packages from the AUR, carefully upgrade all of them.

pacman is a powerful package management tool, but it does not attempt to handle all corner cases. Users must be vigilant and take responsibility for maintaining their own system.

Before upgrading, users are expected to visit the Arch Linux home page to check the latest news, or alternatively subscribe to the RSS feed or the arch-announce mailing list. When updates require out-of-the-ordinary user intervention (more than what can be handled simply by following the instructions given by pacman), an appropriate news post will be made.

Before upgrading fundamental software (such as the kernel, xorg, systemd, or glibc) to a new version, look over the appropriate forum to see if there have been any reported problems.

Users must equally be aware that upgrading packages can raise unexpected problems that could need immediate intervention; therefore, it is discouraged to upgrade a stable system shortly before it is required for carrying out an important task. Instead, wait to upgrade until there is enough time available to resolve any post-upgrade issues.

Avoid doing partial upgrades. In other words, never run pacman -Sy; instead, always use pacman -Syu.

Generally avoid using the --overwrite option with pacman. The --overwrite option takes an argument containing a glob. When used, pacman will bypass file conflict checks for files that match the glob. In a properly maintained system, it should only be used when explicitly recommended by the Arch Developers. See the #Read before upgrading the system section.

Avoid using the -d option with pacman. pacman -Rdd package skips dependency checks during package removal. As a result, a package providing a critical dependency could be removed, resulting in a broken system.

Arch Linux is a rolling release distribution. That means when new library versions are pushed to the repositories, the Developers and Package Maintainers rebuild all the packages in the repositories that need to be rebuilt against the libraries. For example, if two packages depend on the same library, upgrading only one package might also upgrade the library (as a dependency), which might then break the other package which depends on an older version of the library.

That is why partial upgrades are not supported. Do not use:

When refreshing the package database, always do a full upgrade with pacman -Syu.

Be very careful when using IgnorePkg and IgnoreGroup for the same reason. If the system has locally built packages (such as AUR packages), users will need to rebuild them when their dependencies receive a soname bump.

If a partial upgrade scenario has been created, and binaries are broken because they cannot find the libraries they are linked against, do not "fix" the problem simply by symlinking. Libraries receive soname bumps when they are not backwards compatible. A simple pacman -Syu to a properly synced mirror will fix the problem as long as pacman is not broken.

When upgrading the system, be sure to pay attention to the alert notices provided by pacman. If any additional actions are required by the user, be sure to take care of them right away. If a pacman alert is confusing, search the forums or check the latest news on the Arch Linux homepage (see #Read before upgrading the system) for more detailed instructions.

When pacman is invoked, .pacnew and .pacsave files can be created. Pacman provides notice when this happens and users must deal with these files promptly. Users are referred to the pacman/Pacnew and Pacsave wiki page for detailed instructions.

Also, think about other configuration files you may have copied or created. If a package had an example configuration that you copied to your home directory, check to see if a new one has been created.

Upgrades are typically not applied to existing processes. You must restart processes to fully apply the upgrade.

The archlinux-contrib package provides a script called checkservices which runs pacdiff to merge .pacnew files then checks for processes running with outdated libraries and prompts the user if they want them to be restarted.

The kernel is particularly difficult to patch without a reboot. A reboot is always the most secure option, but if this is very inconvenient kernel live patching can be used to apply upgrades without a reboot.

If a package update is expected/known to cause problems, packagers will ensure that pacman displays an appropriate message when the package is updated. If experiencing trouble after an update, double-check pacman's output by looking at /var/log/pacman.log.

At this point, only after ensuring there is no information available through pacman, there is no relevant news on https://archlinux.org/, and there are no forum posts regarding the update, consider seeking help on the forum or over IRC. Downgrading the offending package to revert broken updates should be considered as a last resort.

After upgrading you may now have packages that are no longer needed or that are no longer in the official repositories.

Use pacman -Qtd to check for packages that were installed as a dependency but now, no other packages depend on them. If an orphaned package is still needed, it is recommended to change the installation reason to explicit. Otherwise, if the package is no longer needed, it can be removed. See pacman/Tips and tricks#Removing unused packages (orphans) for details.

Additionally, some packages may no longer be in the remote repositories, but they still may be on your local system. To list all foreign packages use pacman -Qm. Note that this list will include packages that have been installed manually (e.g., from the AUR). To exclude packages that are (still) available on the AUR, use the script from BBS#288205 or try the ancient-packagesAUR tool.

Pacman does a much better job than you at keeping track of files. If you install things manually you will, sooner or later, forget what you did, forget where you installed to, install conflicting software, install to the wrong locations, etc.

To clean up improperly installed files, see pacman/Tips and tricks#Identify files not owned by any package.

Always try open source drivers before resorting to proprietary drivers. Most of the time, open source drivers are more stable and reliable than proprietary drivers. Open source driver bugs are fixed more easily and quickly. While proprietary drivers can offer more features and capabilities, this can come at the cost of stability. To avoid this dilemma, try to choose hardware components known to have mature open source driver support with full features. Information about hardware with open source Linux drivers is available at linux-drivers.org.

Use precaution when using packages from the AUR or an unofficial user repository. Most are supplied by regular users and thus may not have the same standards as those in the official repositories. Always check PKGBUILDs for sanity and signs of mistake or malicious code before building and/or installing the package.

To simplify maintenance, limit the amount of unofficial packages used. Make periodic checks on which are in actual use, and remove (or replace with their official counterparts) any others. See pacman/Tips and tricks#Maintenance for useful commands. Following system upgrade, use rebuild-detector to identify any unofficial packages that may need to be rebuilt.

Update pacman's mirrorlist, as the quality of mirrors can vary over time, and some might go offline or their download rate might degrade.

See mirrors for details.

Programs that help with this can be found in List of applications/Utilities#Disk cleaning.

When looking for files to remove, it is important to find the files that take up the most disk space. Programs that help with this can be found in List of applications/Utilities#Disk usage display.

Remove unwanted .pkg files from /var/cache/pacman/pkg/ to free up disk space.

See pacman#Cleaning the package cache for more information.

Remove unused packages from the system to free up disk space and simplify maintenance.

See #Check for orphans and dropped packages.

Old configuration files may conflict with newer software versions, or corrupt over time. Remove unneeded configurations periodically, particularly in your home directory and ~/.config. For similar reasons, be careful when sharing home directories between installations.

Look for the following directories:

See XDG Base Directory support for more information about these directories.

To keep the home directory clean from temporary files created at the wrong place, it is a good idea to manage a list of unwanted files and remove them regularly, for example with rmshit.py.

rmlint-gitAUR can be used to find and optionally remove duplicate files, empty files, recursive empty directories and broken symlinks.

Old, broken symbolic links might be sitting around your system; you should remove them. Examples on achieving this can be found here and here. However, you should not blindly delete all broken symbolic links, as some of them serve a purpose [1].

To quickly list all the broken symlinks of permanent files on your system, use:

Then inspect and remove unnecessary entries from this list.

The following tips are generally not required, but certain users may find them useful.

Arch's rolling releases can be a boon for users who want to try the latest features and get upstream updates as soon as possible, but they can also make system maintenance more difficult. To simplify maintenance and improve stability, try to avoid cutting edge software and install only mature and proven software. Such packages are less likely to receive difficult upgrades such as major configuration changes or feature removals. Prefer software that has a strong and active development community, as well as a high number of competent users, in order to simplify support in the event of a problem.

Avoid any use of the testing repository, even individual packages from testing. These packages are experimental and not suitable for a stable system. Similarly, avoid packages which are built directly from upstream development sources. These are usually found in the AUR, with names including things like: "dev", "devel", "svn", "cvs", "git", etc.

The linux-lts package is an alternative Arch kernel package, and is available in the core repository. This particular kernel version has long-term support (LTS) from upstream, including security and bug fixes. It is useful if you use out-of-tree kernel modules and want to ensure their compatibility or if you want a fallback kernel in case a new kernel version causes problems.

To make it available as a boot option, you will need to update your boot loader's configuration file to use the LTS kernel and ram disk: vmlinuz-linux-lts and initramfs-linux-lts.img.

**Examples:**

Example 1 (unknown):
```unknown
$ systemctl --failed
```

Example 2 (unknown):
```unknown
# journalctl -b
```

Example 3 (unknown):
```unknown
pacman -Syu
```

Example 4 (unknown):
```unknown
--overwrite
```

---

## Kernel module

**URL:** https://wiki.archlinux.org/title/Blacklisting

**Contents:**
- Obtaining information
- Automatic module loading
  - Early module loading
  - systemd
- Manual module handling
- Setting module options
  - Using modprobe
  - Using modprobe.d
  - Using kernel command line
- Aliasing

Kernel modules are pieces of code that can be loaded and unloaded into the kernel upon demand. They extend the functionality of the kernel without the need to reboot the system.

To create a kernel module, you can read The Linux Kernel Module Programming Guide. A module can be configured as built-in or loadable. To dynamically load or remove a module, it has to be configured as a loadable module in the kernel configuration (the line related to the module will therefore display the letter M).

To rebuild a kernel module automatically when a new kernel is installed, see Dynamic Kernel Module Support (DKMS).

Usually modules depend on the kernel release and are stored in the /usr/lib/modules/kernel_release/ directory.

To show what kernel modules are currently loaded:

To show information about a module:

To list the options that are set for a loaded module use systool(1) from sysfsutils:

To display the comprehensive configuration of all the modules:

To display the configuration of a particular module:

List the dependencies of a module (or alias), including the module itself:

Today, all necessary modules loading is handled automatically by udev, so if you do not need to use any out-of-tree kernel modules, there is no need to put modules that should be loaded at boot in any configuration file. However, there are cases where you might want to load an extra module during the boot process, or blacklist another one for your computer to function properly.

Early module loading depends on the initramfs generator used:

Kernel modules can be explicitly listed in files under /etc/modules-load.d/ for systemd to load them during boot. Each configuration file is named in the style of /etc/modules-load.d/program.conf. Configuration files simply contain a list of kernel modules names to load, separated by newlines. Empty lines and lines whose first non-whitespace character is # or ; are ignored.

See modules-load.d(5) for more details.

Kernel modules are handled by tools provided by the kmod package, which is installed as a dependency of a kernel package. You can use these tools manually. To load a module:

To load a module by a file name—i.e. one that is not installed in the /usr/lib/modules/kernel_release/ directory—use any of:

To unload—remove—a module, use any of:

To pass a parameter to a kernel module, you can pass them manually with modprobe or assure certain parameters are always applied using a modprobe configuration file or by using the kernel command line. If the module is built into the kernel, the kernel command line must be used and other methods will not work.

The basic way to pass parameters to a module is using the modprobe command. Parameters are specified on command line using simple key=value assignments:

Configuration files in the /etc/modprobe.d/ directory can be used to pass module settings to udev, which will use modprobe to manage the loading of the modules during system boot. Files in this directory can have any name, given that they end with the .conf extension. The file name matters, see modprobe.d(5) § CONFIGURATION DIRECTORIES AND PRECEDENCE. To show the effective configuration:

Multiple module parameters are separated by spaces, in turn a parameter can receive a list of values which is separated by commas:

You can also pass options to the module using the kernel command line. This is the only working option for modules built into the kernel. For all common boot loaders, the following syntax is correct:

Simply add this to the appropriate line in your boot loader configuration, as described in Kernel parameters#Boot loader configuration.

Aliases are alternate names for a module. For example: alias my-mod really_long_modulename means you can use modprobe my-mod instead of modprobe really_long_modulename. You can also use shell-style wildcards, so alias my-mod* really_long_modulename means that modprobe my-mod-something has the same effect. Create an alias:

Aliases can be internal—contained in the module itself. Internal aliases are usually used for #Automatic module loading when it is needed by an application, e.g. when the kernel detects a new device. To see the module internal aliases:

To see both configured and internal aliases:

Blacklisting, in the context of kernel modules, is a mechanism to prevent the kernel module from loading. This could be useful if, for example, the associated hardware is not needed, or if loading that module causes problems: for instance there may be two kernel modules that try to control the same piece of hardware, and loading them together would result in a conflict.

Some modules are loaded as part of the initramfs. mkinitcpio -M will print out all automatically detected modules: to prevent the initramfs from loading some of those modules, blacklist them in a .conf file under /etc/modprobe.d and it shall be added in by the modconf hook during image generation. Running mkinitcpio -v will list all modules pulled in by the various hooks (e.g. filesystems hook, block hook, etc.). Remember to add that .conf file to the FILES array in /etc/mkinitcpio.conf if you do not have the modconf hook in your HOOKS array (e.g. you have deviated from the default configuration), and once you have blacklisted the modules regenerate the initramfs, and reboot afterwards.

Disable an alias by overriding. For example, to prevent Bluetooth module autoloading (assuming a module named off does not exist):

To disable all internal aliases for a given module use the blacklist keyword. For example, to prevent the pcspkr module from loading on boot to avoid sounds through the PC speaker:

There is a workaround for the behaviour described in the #alias and #blacklist notes. The install configuration command instructs modprobe to run a custom command instead of inserting the module in the kernel as normal, so you can simulate the successful module loading with:

You can force the module to always fail loading with /bin/false: this will effectively prevent the module—and any other that depends on it—from loading by any means, and a log error message may be produced.

You can also blacklist modules from the boot loader boot entry configuration.

Simply add module_blacklist=module_name_1,module_name_2,module_name_3 to your kernel command line, as described in Kernel parameters#Boot loader configuration.

Another use case for a command line option is to disable hardware-specific components of a module without disabling the module entirely. For example, disabling a microphone while retaining other sound out options. See BBS#303475 for a few examples.

In case a specific module does not load and the boot log (accessible by running journalctl -b as root) says that the module is blacklisted, but the directory /etc/modprobe.d/ does not show a corresponding entry, check another modprobe source directory at /usr/lib/modprobe.d/ for blacklisting entries.

A module will not be loaded if the "vermagic" string contained within the kernel module does not match the value of the currently running kernel. If it is known that the module is compatible with the current running kernel the "vermagic" check can be ignored with modprobe --force-vermagic.

**Examples:**

Example 1 (unknown):
```unknown
/usr/lib/modules/kernel_release/
```

Example 2 (unknown):
```unknown
uname --kernel-release
```

Example 3 (unknown):
```unknown
/etc/modprobe.d/
```

Example 4 (unknown):
```unknown
$ modinfo module_name
```

---

## AppArmor

**URL:** https://wiki.archlinux.org/title/AppArmor

**Contents:**
- Installation
  - Custom kernel
- Usage
  - Display current status
  - Parsing profiles
  - Disabling loading
- Configuration
  - Auditing and generating profiles
  - Understanding profiles
- Tips and tricks

AppArmor is a Mandatory Access Control (MAC) system, implemented upon the Linux Security Modules (LSM).

AppArmor, like most other LSMs, supplements rather than replaces the default Discretionary Access Control (DAC). As such it is impossible to grant a process more privileges than it had in the first place.

Ubuntu, SUSE and a number of other distributions use it by default. RHEL (and its variants) use SELinux which requires good userspace integration to work properly. SELinux attaches labels to all files, processes and objects and is therefore very flexible. However configuring SELinux is considered to be very complicated and requires a supported filesystem. AppArmor on the other hand works using file paths and its configuration can be easily adapted.

AppArmor proactively protects the operating system and applications from external or internal threats and even zero-day attacks by enforcing a specific rule set on a per application basis. Security policies completely define what system resources individual applications can access, and with what privileges. Access is denied by default if no profile says otherwise. A few default policies are included with AppArmor and using a combination of advanced static analysis and learning-based tools, AppArmor policies for even very complex applications can be deployed successfully in a matter of hours.

Every breach of policy triggers a message in the system log, and AppArmor can be configured to notify users with real-time violation warnings popping up on the desktop.

AppArmor is available in all officially supported kernels.

Install apparmor for userspace tools and libraries to control AppArmor. To load all AppArmor profiles on startup, enable apparmor.service.

To enable AppArmor as default security model on every boot, set the following kernel parameter:

When compiling the kernel, it is required to set at least the following options:

To enable the AppArmor Linux security model by default and omit the need to set kernel parameters, additionally set the CONFIG_LSM option and specify apparmor as the first "major" module in the list:

To test if AppArmor has been correctly enabled:

To display the current loaded status use aa-status(8):

In complain mode, policy violations of profiles are allowed but are logged. Complain mode is used for testing new profiles. Note that deny rules of profiles are enforced/blocked even in complain mode.

In enforce mode, policy violations of profiles are blocked and logged.

To load (enforce or complain), unload, reload, cache and stat profiles use apparmor_parser. The default action (-a) is to load a new profile in enforce mode, loading it in complain mode is possible using the -C switch, in order to overwrite an existing profile use the -r option and to remove a profile use -R. Each action may also apply to multiple profiles. Refer to apparmor_parser(8) man page for more information.

Disable AppArmor by unloading all profiles for the current session:

To prevent AppArmor profiles from loading at the next boot disable apparmor.service. To prevent the kernel from loading AppArmor, append apparmor=0 to the kernel parameters, or remove the lsm= kernel parameter that was added when setting up AppArmor.

To create new profiles the Audit framework should be running. This is because Arch Linux adopted systemd and does not do kernel logging to file by default. AppArmor can grab kernel audit logs from the userspace auditd daemon, allowing you to build a profile.

New AppArmor profiles can be created by utilizing aa-genprof(8) or aa-autodep(8). The profile is first created in complain mode: in this mode policy violations are only reported but not enforced. The rules are interactively created by the aa-logprof(8) tool available in apparmor package. Finally the profile should be set into enforce mode with aa-enforce(8). In this mode the policy defined by the rules in the respecting profile are enforced. If necessary, additional rules can be added by repeatedly executing aa-logprof(8), or the profile can be set back to complain mode with aa-complain(8). Detailed guide about using those tools is available at AppArmor wiki - Profiling with tools.

Note that aa-logprof(8) also offers deny rules which are actually not strictly necessary as according to the basic AppArmor logic everything is forbidden which is not explicitly allowed by a rule. However, deny rules serve two purposes:

Alternatively profiles may be also created manually, see guide available at AppArmor wiki - Profiling by hand.

In addition to the default profiles in /etc/apparmor.d/, there are more predefined profiles in /usr/share/apparmor/extra-profiles/. Note that those are not necessarily deemed production-ready, so manual intervention or usage of aa-logprof(8) may be required.

Another set of AppArmor profiles can be found in the apparmor.d project, however, as of writing, the project is not currently stable.

Profiles are human readable text files residing under /etc/apparmor.d/ describing how binaries should be treated when executed. A basic profile looks similar to this:

Strings preceded by a @ symbol are variables defined by abstractions (/etc/apparmor.d/abstractions/), tunables (/etc/apparmor.d/tunables/) or by the profile itself. #include includes other profile-files directly. Paths followed by a set of characters are access permissions. Pattern matching is done using AppArmor's globbing syntax.

Most common use cases are covered by the following statements:

Remember that those permission do not allow binaries to exceed the permission dictated by the Discretionary Access Control (DAC).

This is merely a short overview, for a more detailed guide be sure to have a look at the apparmor.d(5) man page and documentation.

The notification daemon displays desktop notifications whenever AppArmor denies a program access. To automatically start aa-notify daemon on login follow below steps:

Install the Audit framework and enable and start the userspace Linux Audit daemon. Allow your desktop user to read audit logs in /var/log/audit by adding it to audit user group:

Add audit group to auditd.conf:

Install python-notify2 and python-psutil.

Create a desktop launcher with the below content:

Reboot and check if the aa-notify process is running:

For more information, see aa-notify(8).

Since AppArmor has to translate the configured profiles into a binary format it may significantly increase the boot time. You can check current AppArmor startup time with:

To enable caching AppArmor profiles, uncomment:

To change default cache location add:

Reboot and check AppArmor startup time again to see improvement:

See Samba#Permission issues on AppArmor.

In rare cases, after upgrading to AppArmor version 4, it becomes impossible to log into any system account.

The system journal might contain errors like these:

This might be caused by /etc/shadow and/or /etc/gshadow not being readable by the root user (i.e. the permission bits of those files might be entirely unset). Thus, a possible solution would be to:

Information you find often is about AppArmor on Ubuntu, which can be confusing, since Ubuntu carries a lot of kernel patches regarding AppArmor. Other distributions may also carry their own kernel patches, while Arch Linux uses a close-to-mainline kernel.

For example, apparmor.d(5) documented dbus rules years prior their support in the mainline kernel in 6.17 [5]. Support also varies by D-Bus implementation - the d-bus package on Arch is built without AppArmor support necessary for applying dbus AppArmor rules.

AppArmor-specific kernel patches applied by Ubuntu can be found at (replace jammy with the codename of the Ubuntu version you are interested in):

The ABI versions supported by the userland tools can be found in /etc/apparmor.d/abi/. The ABI supported by the currently running kernel can be shown with:

**Examples:**

Example 1 (unknown):
```unknown
apparmor.service
```

Example 2 (unknown):
```unknown
lsm=landlock,lockdown,yama,integrity,apparmor,bpf
```

Example 3 (unknown):
```unknown
zgrep CONFIG_LSM= /proc/config.gz
```

Example 4 (unknown):
```unknown
cat /sys/kernel/security/lsm
```

---

## Users and groups

**URL:** https://wiki.archlinux.org/title/Password

**Contents:**
- Overview
- Permissions and ownership
- Shadow
- File list
- User management
  - Example adding a user
    - Changing user defaults
  - Example adding a system user
  - Change a user's login name or home directory
  - Other examples of user management

Users and groups are used on GNU/Linux for access control—that is, to control access to the system's files, directories, and peripherals. Linux offers relatively simple/coarse access control mechanisms by default. For more advanced options, see ACL, Capabilities and PAM#Configuration How-Tos.

A user is anyone who uses a computer. In this case, we are describing the names which represent those users. It may be Mary or Bill, and they may use the names Dragonlady or Pirate in place of their real name. All that matters is that the computer has a name for each account it creates, and it is this name by which a person gains access to use the computer. Some system services also run using restricted or privileged user accounts.

Managing users is done for the purpose of security by limiting access in certain specific ways. The superuser (root) has complete access to the operating system and its configuration; it is intended for administrative use only. Unprivileged users can use several programs for controlled privilege elevation.

Any individual may have more than one account as long as they use a different name for each account they create. Further, there are some reserved names which may not be used such as "root".

Users may be grouped together into a "group", and users may be added to an existing group to utilize the privileged access it grants.

From In UNIX Everything is a File:

From Extending UNIX File Abstraction for General-Purpose Networking:

Every file on a GNU/Linux system is owned by a user and a group. In addition, there are three types of access permissions: read, write, and execute. Different access permissions can be applied to a file's owning user, owning group, and others (those without ownership). One can determine a file's owners and permissions by viewing the long listing format of the ls command:

The first column displays the file's permissions (for example, the file initramfs-linux.img has permissions -rw-r--r--). The third and fourth columns display the file's owning user and group, respectively. In this example, all files are owned by the root user and the root group.

In this example, the sf_Shared directory is owned by the root user and the vboxsf group. It is also possible to determine a file's owners and permissions using the stat command:

Access permissions are displayed in three groups of characters, representing the permissions of the owning user, owning group, and others, respectively. For example, the characters -rw-r--r-- indicate that the file's owner has read and write permission, but not execute (rw-), whilst users belonging to the owning group and other users have only read permission (r-- and r--). Meanwhile, the characters drwxrwx--- indicate that the file's owner and users belonging to the owning group all have read, write, and execute permissions (rwx and rwx), whilst other users are denied access (---). The first character represents the file's type.

List files owned by a user or group with the find utility:

A file's owning user and group can be changed with the chown (change owner) command. A file's access permissions can be changed with the chmod (change mode) command.

See chown(1), chmod(1), and Linux file permissions for additional detail.

The user, group and password management tools on Arch Linux come from the shadow package, which is a dependency of the base meta package.

To list users currently logged on the system, the who command can be used. To list all existing user accounts including their properties stored in the user database, run passwd -Sa as root. See passwd(1) for the description of the output format.

To add a new user, use the useradd command:

If an initial login group is specified by name or number, it must refer to an already existing group. If not specified, the behaviour of useradd will depend on the USERGROUPS_ENAB variable contained in /etc/login.defs. The default behaviour (USERGROUPS_ENAB yes) is to create a group with the same name as the username.

When the login shell is intended to be non-functional, for example when the user account is created for a specific service, /usr/bin/nologin may be specified in place of a regular shell to politely refuse a login (see nologin(8)).

See useradd(8) for other supported options.

To add a new user named archie, creating its home directory and otherwise using all the defaults in terms of groups, directory names, shell used and various other parameters:

Although it is not required to protect the newly created user archie with a password, it is highly recommended to do so:

The above useradd command will also automatically create a group called archie and makes this the default group for the user archie. Making each user have their own group (with the group name same as the user name) is the preferred way to add users.

You could also make the default group something else using the -g option, but note that, in multi-user systems, using a single default group (e.g. users) for every user is not recommended. The reason is that typically, the method for facilitating shared write access for specific groups of users is setting user umask value to 002, which means that the default group will by default always have write access to any file you create. See also User Private Groups. If a user must be a member of a specific group specify that group as a supplementary group when creating the user.

In the recommended scenario, where the default group has the same name as the user name, all files are by default writeable only for the user who created them. To allow write access to a specific group, shared files/directories can be made writeable by default for everyone in this group and the owning group can be automatically fixed to the group which owns the parent directory by setting the setgid bit on this directory:

Otherwise the file creator's default group (usually the same as the user name) is used.

If a GID change is required temporarily you can also use the newgrp command to change the user's default GID to another GID at runtime. For example, after executing newgrp groupname files created by the user will be associated with the groupname GID, without requiring a re-login. To change back to the default GID, execute newgrp without a groupname.

The default values used for creating new accounts are set in /etc/default/useradd and can be displayed using useradd --defaults. For example, to change the default shell globally, set SHELL=/usr/bin/shell. A different shell can also be specified individually with the -s/--shell option. Use chsh -l to list valid login shells.

Files can also be specified to be added to newly created user home directories in /etc/skel. This is useful for minimalist window managers where config files need manual configuration to reach DE-familiar behavior. For example, to set up default shortcuts for all newly created users:

See also: Display manager#Run ~/.xinitrc as a session to add xinitrc as an option to all users on the display manager.

System users can be used to run processes/daemons under a different user, protecting (e.g. with chown) files and/or directories and more examples of computer hardening.

With the following command a system user without shell access and without a home directory is created (optionally append the -U parameter to create a group with the same name as the user, and add the user to this group):

If the system user requires a specific user and group ID, specify them with the -u/--uid and -g/--gid options when creating the user:

To change a user's home directory:

The -m option also automatically creates the new directory and moves the content there.

Make sure there is no trailing / on /my/old/home.

To change a user's login name:

Changing a username is safe and easy when done properly, just use the usermod command. If the user is associated to a group with the same name, you can rename this with the groupmod command.

Alternatively, the /etc/passwd file can be edited directly, see #User database for an introduction to its format.

Also keep in mind the following notes:

To enter user information for the GECOS comment (e.g. the full user name), type:

(this way chfn runs in interactive mode).

Alternatively the GECOS comment can be set more liberally with:

To mark a user's password as expired, requiring them to create a new password the first time they log in, type:

User accounts may be deleted with the userdel command:

The -r option specifies that the user's home directory and mail spool should also be deleted.

To change the user's login shell:

Local user information is stored in the plain-text /etc/passwd file: each of its lines represents a user account, and has seven fields delimited by colons.

Broken down, this means: user archie, whose password is in /etc/shadow, whose UID is 1001 and whose primary group is 1003. Archie is their full name and there is a comment associated to their account; their home directory is /home/archie and they are using Bash.

The pwck command can be used to verify the integrity of the user database. It can sort the user list by UID at the same time, which can be helpful for comparison:

Instead of running pwck/grpck manually, the systemd timer shadow.timer, which is part of, and is enabled by, installation of the shadow package, will start shadow.service daily. shadow.service will run pwck(8) and grpck(8) to verify the integrity of both password and group files.

If discrepancies are reported, group can be edited with the vigr(8) command and users with vipw(8). This provides an extra margin of protection in that these commands lock the databases for editing. Note that the default text editor is vi, but an alternative editor will be used if the EDITOR environment variable is set, then that editor will be used instead.

As packages adopt the change-sysusers-to-fully-locked-system-accounts, package created system users generated in the past will not inherit new package defaults for the increased security of locked/expired status. These users need to be modified manually for this change. user-analysis.sh does just that.

In addition, the script also identifies orphaned users (those created by a package no longer on the system) and can automatically delete them.

/etc/group is the file that defines the groups on the system (see group(5) for details). There is also its companion gshadow which is rarely used. Its details are at gshadow(5).

Display group membership with the groups command:

If user is omitted, the current user's group names are displayed.

The id command provides additional detail, such as the user's UID and associated GIDs:

To list all groups on the system:

Create new groups with the groupadd command:

Add users to a group with the gpasswd command (see FS#58262 regarding errors):

Alternatively, add a user to additional groups with usermod (replace additional_groups with a comma-separated list):

Modify an existing group with the groupmod command, e.g. to rename the old_group group to new_group:

To delete existing groups:

To remove users from a group:

The grpck command can be used to verify the integrity of the system's group files.

This section explains the purpose of the essential groups from the filesystem package. There are many other groups, which will be created with correct GID when the relevant package is installed. See the main page for the software for details.

Non-root workstation/desktop users often need to be added to some of following groups to allow access to hardware peripherals and facilitate system administration:

The following groups are used for system purposes, an assignment to users is only required for dedicated purposes:

Before arch migrated to systemd, users had to be manually added to these groups in order to be able to access the corresponding devices. This way has been deprecated in favour of udev marking the devices with a uaccess tag and logind assigning the permissions to users dynamically via ACLs according to which session is currently active. Note that the session must not be broken for this to work (see General troubleshooting#Session permissions to check it).

There are some notable exceptions which require adding a user to some of these groups: for example if you want to allow users to access the device even when they are not logged in. However, note that adding users to the groups can even cause some functionality to break (for example, the audio group will break fast user switching and allows applications to block software mixing).

Now solely for direct access to tapes if no custom udev rules is involved.[5][6][7].

Also required for manipulating some devices via udisks/udisksctl.

The following groups are currently not used for any purpose:

This article or section is a candidate for merging with #Shadow.

The factual accuracy of this article or section is disputed.

getent(1) can be used to read a particular record.

As warned in #User database, using specific utilities such as passwd and chfn, is a better way to change the databases. Nevertheless, there are times when editing them directly is looked after. For those times, vipw, vigr are provided. It is strongly recommended to use these tailored editors over using a general text editor as they lock the databases against concurrent editing. They also help prevent invalid entries and/or syntax errors. Note that Arch Linux prefers usage of specific tools, such as chage, for modifying the shadow database over using vipw -s and vigr -s from util-linux. See also FS#31414.

lslogins(1) and lastlog2(1) are some of the tools for viewing utmp related files.

**Examples:**

Example 1 (unknown):
```unknown
cat /dev/audio > myfile
```

Example 2 (unknown):
```unknown
cat myfile > /dev/audio
```

Example 3 (unknown):
```unknown
$ ls -l /boot/
```

Example 4 (unknown):
```unknown
total 13740
drwxr-xr-x 2 root root    4096 Jan 12 00:33 grub
-rw-r--r-- 1 root root 8570335 Jan 12 00:33 initramfs-linux-fallback.img
-rw-r--r-- 1 root root 1821573 Jan 12 00:31 initramfs-linux.img
-rw-r--r-- 1 root root 1457315 Jan  8 08:19 System.map26
-rw-r--r-- 1 root root 2209920 Jan  8 08:19 vmlinuz-linux
```

---

## systemd

**URL:** https://wiki.archlinux.org/title/Network-online.target

**Contents:**
- Basic systemctl usage
  - Using units
  - Power management
    - Soft reboot
- Writing unit files
  - Handling dependencies
  - Service types
  - Editing provided units
    - Replacement unit files
    - Drop-in files

From the project web page:

Historically, what systemd calls "service" was named daemon: any program that runs as a "background" process (without a terminal or user interface), commonly waiting for events to occur and offering services. A good example is a web server that waits for a request to deliver a page, or an ssh server waiting for someone trying to log in. While these are full featured applications, there are daemons whose work is not that visible. Daemons are for tasks like writing messages into a log file (e.g. syslog, metalog) or keeping your system time accurate (e.g. ntpd). For more information see daemon(7).

The main command used to introspect and control systemd is systemctl. Some of its uses are examining the system state and managing the system and services. See systemctl(1) for more details.

Units commonly include, but are not limited to, services (.service), mount points (.mount), devices (.device) and sockets (.socket).

When using systemctl, you generally have to specify the complete name of the unit file, including its suffix, for example sshd.socket. There are however a few short forms when specifying the unit in the following systemctl commands:

See systemd.unit(5) for details.

The commands in the below table operate on system units since --system is the implied default for systemctl. To instead operate on user units (for the calling user), use systemctl --user without root privileges. See also systemd/User#Basic setup to enable/disable user units for all users.

polkit is necessary for power management as an unprivileged user. If you are in a local systemd-logind user session and no other session is active, the following commands will work without root privileges. If not (for example, because another user is logged into a tty), systemd will automatically ask you for the root password.

Soft reboot is a special kind of a userspace-only reboot operation that does not involve the kernel. It is implemented by systemd-soft-reboot.service(8) and can be invoked through systemctl soft-reboot. As with kexec, it skips firmware re-initialization, but additionally the system does not go through kernel initialization and initramfs, and unlocked dm-crypt devices remain attached.

When /run/nextroot/ contains a valid root file system hierarchy (e.g. is the root mount of another distribution or another snapshot), soft-reboot would switch the system root into it, allowing for switching to another installation without losing states managed by kernel, e.g. networking.

The syntax of systemd's unit files (systemd.unit(5)) is inspired by XDG Desktop Entry Specification .desktop files, which are in turn inspired by Microsoft Windows .ini files. Unit files are loaded from multiple locations (to see the full list, run systemctl show --property=UnitPath), but the main ones are (listed from lowest to highest precedence):

Look at the units installed by your packages for examples, as well as systemd.service(5) § EXAMPLES.

systemd-analyze(1) can help verifying the work. See the systemd-analyze verify FILE... section of that page.

With systemd, dependencies can be resolved by designing the unit files correctly. The most typical case is when unit A requires unit B to be running before A is started. In that case add Requires=B and After=B to the [Unit] section of A. If the dependency is optional, add Wants=B and After=B instead. Note that Wants= and Requires= do not imply After=, meaning that if After= is not specified, the two units will be started in parallel.

Dependencies are typically placed on services and not on #Targets. For example, network.target is pulled in by whatever service configures your network interfaces, therefore ordering your custom unit after it is sufficient since network.target is started anyway.

There are several different start-up types to consider when writing a custom service file. This is set with the Type= parameter in the [Service] section:

See the systemd.service(5) § OPTIONS man page for a more detailed explanation of the Type values.

This article or section needs expansion.

To avoid conflicts with pacman, unit files provided by packages should not be directly edited. There are two safe ways to modify a unit without touching the original file: create a new unit file which overrides the original unit or create drop-in snippets which are applied on top of the original unit. For both methods, you must reload the unit afterwards to apply your changes. This can be done either by editing the unit with systemctl edit (which reloads the unit automatically) or by reloading all units with:

To replace the unit file /usr/lib/systemd/system/unit, create the file /etc/systemd/system/unit and reenable the unit to update the symlinks.

This opens /etc/systemd/system/unit in your editor (copying the installed version if it does not exist yet) and automatically reloads it when you finish editing.

To create drop-in files for the unit file /usr/lib/systemd/system/unit, create the directory /etc/systemd/system/unit.d/ and place .conf files there to override or add new options. systemd will parse and apply these files on top of the original unit.

The easiest way to do this is to run:

This opens the file /etc/systemd/system/unit.d/drop_in_name.conf in your text editor (creating it if necessary) and automatically reloads the unit when you are done editing. Omitting --drop-in= option will result in systemd using the default file name override.conf .

To revert any changes to a unit made using systemctl edit do:

For example, if you simply want to add an additional dependency to a unit, you may create the following file:

As another example, in order to replace the ExecStart directive, create the following file:

Note how ExecStart must be cleared before being re-assigned [2]. The same holds for every item that can be specified multiple times, e.g. OnCalendar for timers.

One more example to automatically restart a service:

For services that send logs directly to journald or syslog, you can control their verbosity by setting a numeric value between 0 and 6 for the LogLevelMax= parameter in the [Service] section using the methods described above. For example:

The standard log levels are identical to those used to filter the journal. Setting a lower number excludes all higher and less important log messages from your journal.

If a service is echoing stdout and/or stderr output, by default this will end up in the journal as well. This behavior can be suppressed by setting StandardOutput=null and/or StandardError=null in the [Service] section. Other values than null can be used to further tweak this behavior. See systemd.exec(5) § LOGGING_AND_STANDARD_INPUT/OUTPUT.

systemd uses targets to group units together via dependencies and as standardized synchronization points. They serve a similar purpose as runlevels but act a little differently. Each target is named instead of numbered and is intended to serve a specific purpose with the possibility of having multiple ones active at the same time. Some targets are implemented by inheriting all of the services of another target and adding additional services to it. There are systemd targets that mimic the common SystemVinit runlevels.

The following should be used under systemd instead of running runlevel:

The runlevels that held a defined meaning under sysvinit (i.e., 0, 1, 3, 5, and 6); have a 1:1 mapping with a specific systemd target. Unfortunately, there is no good way to do the same for the user-defined runlevels like 2 and 4. If you make use of those it is suggested that you make a new named systemd target as /etc/systemd/system/your target that takes one of the existing runlevels as a base (you can look at /usr/lib/systemd/system/graphical.target as an example), make a directory /etc/systemd/system/your target.wants, and then symlink the additional services from /usr/lib/systemd/system/ that you wish to enable.

In systemd, targets are exposed via target units. You can change them like this:

This will only change the current target, and has no effect on the next boot. This is equivalent to commands such as telinit 3 or telinit 5 in Sysvinit.

The standard target is default.target, which is a symlink to graphical.target. This roughly corresponds to the old runlevel 5.

To verify the current target with systemctl:

To change the default target to boot into, change the default.target symlink. With systemctl:

Alternatively, append one of the following kernel parameters to your boot loader:

systemd chooses the default.target according to the following order:

Some (not exhaustive) components of systemd are:

systemd is in charge of mounting the partitions and filesystems specified in /etc/fstab. The systemd-fstab-generator(8) translates all the entries in /etc/fstab into systemd units; this is performed at boot time and whenever the configuration of the system manager is reloaded.

systemd extends the usual fstab capabilities and offers additional mount options. These affect the dependencies of the mount unit. They can, for example, ensure that a mount is performed only once the network is up or only once another partition is mounted. The full list of specific systemd mount options, typically prefixed with x-systemd., is detailed in systemd.mount(5) § FSTAB.

An example of these mount options is automounting, which means mounting only when the resource is required rather than automatically at boot time. This is provided in fstab#Automount with systemd.

On UEFI-booted systems, GPT partitions such as root, home, swap, etc. can be mounted automatically following the Discoverable Partitions Specification. These partitions can thus be omitted from fstab, and if the root partition is automounted, then root= can be omitted from the kernel command line. See systemd-gpt-auto-generator(8).

The prerequisites are:

udev will create symlinks to the discovered partitions which can be used to reference the partitions and volumes in configuration files.

For /var automounting to work, the PARTUUID must match the SHA256 HMAC hash of the partition type UUID keyed by the machine ID. The required PARTUUID can be obtained using:

The primary role of systemd-sysvcompat (required by base) is to provide the traditional linux init binary. For systemd-controlled systems, init is just a symbolic link to its systemd executable.

In addition, it provides four convenience shortcuts that SysVinit users might be used to. The convenience shortcuts are halt(8), poweroff(8), reboot(8) and shutdown(8). Each one of those four commands is a symbolic link to systemctl, and is governed by systemd behavior. Therefore, the discussion at #Power management applies.

systemd-based systems can give up those System V compatibility methods by using the init= boot parameter (see, for example, /bin/init is in systemd-sysvcompat ?) and systemd native systemctl command arguments.

systemd-tmpfiles creates, deletes and cleans up volatile and temporary files and directories. It reads configuration files in /etc/tmpfiles.d/ and /usr/lib/tmpfiles.d/ to discover which actions to perform. Configuration files in the former directory take precedence over those in the latter directory.

Configuration files are usually provided together with service files, and they are named in the style of /usr/lib/tmpfiles.d/program.conf. For example, the Samba daemon expects the directory /run/samba to exist and to have the correct permissions. Therefore, the samba package ships with this configuration:

Configuration files may also be used to write values into certain files on boot. For example, if you used /etc/rc.local to disable wakeup from USB devices with echo USBE > /proc/acpi/wakeup, you may use the following tmpfile instead:

It is possible to write multiple lines to the same file, either with \n in the argument or using the w+ type on multiple lines (including the first one) for appending:

See the systemd-tmpfiles(8) and tmpfiles.d(5) man pages for details.

The factual accuracy of this article or section is disputed.

Configuration files provided by packages should not be directly edited to avoid conflicts with pacman updates. For this, many (but not all) systemd packages provide a way to modify the configuration, but without touching the original file by creation of drop-in snippets. Check the package manual to see if drop-in configuration files are supported.

To create a drop-in configuration file for the unit file /etc/systemd/unit.conf, create the directory /etc/systemd/unit.conf.d/ and place .conf files there to override or add new options. systemd will parse and apply these files on top of the original unit.

Check the overall configuration:

The applied drop-in snippets file(s) and content will be listed at the end. Restart the service for the changes to take effect.

Some packages provide a .socket unit. For example, cups provides a cups.socket unit[3]. If cups.socket is enabled (and cups.service is left disabled), systemd will not start CUPS immediately; it will just listen to the appropriate sockets. Then, whenever a program attempts to connect to one of these CUPS sockets, systemd will start cups.service and transparently hand over control of these ports to the CUPS process.

To delay a service until after the network is up, include the following dependencies in the .service file:

The network wait service of the network manager in use must also be enabled so that network-online.target properly reflects the network status.

For more detailed explanations, see the discussion in the Network configuration synchronization points.

If a service needs to perform DNS queries, it should additionally be ordered after nss-lookup.target:

See systemd.special(7) § Special Passive System Units.

For nss-lookup.target to have any effect it needs a service that pulls it in via Wants=nss-lookup.target and orders itself before it with Before=nss-lookup.target. Typically this is done by local DNS resolvers.

Check which active service, if any, is pulling in nss-lookup.target with:

This article or section needs expansion.

By default, Arch Linux does not make use of systemd presets, and does not enable most services on installation.

If you wish for systemd presets to be honored when packages are installed, you will need to create a a pacman hook in order to run systemctl preset or systemctl preset-all whenever a new systemd unit is installed. Something like

Arch Linux ships with /usr/lib/systemd/system-preset/99-default.preset containing disable *. This causes systemctl preset to disable all units by default, such that when a new package is installed, the user must manually enable the unit.

To enable units by default instead, simply create a symlink from /etc/systemd/system-preset/99-default.preset to /dev/null or replace with a file containing enable * in order to override the configuration file. This will cause systemctl preset to enable all units that get installed—regardless of unit type—unless specified in another file in one systemctl preset's configuration directories. User units are not affected. It is also possible to configure higher priority files with more specific rules on what should be enabled. See systemd.preset(5) for more information.

See systemd/Sandboxing.

In order to notify about service failures, an OnFailure= directive needs to be added to the according service file, for example by using a drop-in configuration file. Adding this directive to every service unit can be achieved with a top-level drop-in configuration file. For details about top-level drop-ins, see systemd.unit(5).

Create a top-level drop-in for services:

This adds OnFailure=failure-notification@%n.service to every service file. If some_service_unit fails, failure-notification@some_service_unit.service will be started to handle the notification delivery (or whatever task it is configured to perform).

Create the failure-notification@.service template unit:

You can create the failure-notification.sh script and define what to do or how to notify. Examples include sending e-mail, showing desktop notifications, using gotify, XMPP, etc. The %i will be the name of the failed service unit and will be passed as an argument to the script.

In order to prevent a recursion for starting instances of failure-notification@.service again and again if the start fails, create an empty drop-in configuration file with the same name as the top-level drop-in (the empty service-level drop-in configuration file takes precedence over the top-level drop-in and overrides the latter one):

You can set up systemd to send an e-mail when a unit fails. Cron sends mail to MAILTO if the job outputs to stdout or stderr, but many jobs are setup to only output on error. First you need two files: an executable for sending the mail and a .service for starting the executable. For this example, the executable is just a shell script using sendmail, which is in packages that provide smtp-forwarder.

Whatever executable you use, it should probably take at least two arguments as this shell script does: the address to send to and the unit file to get the status of. The .service we create will pass these arguments:

Where user is the user being emailed and address is that user's email address. Although the recipient is hard-coded, the unit file to report on is passed as an instance parameter, so this one service can send email for many other units. At this point you can start status_email_user@dbus.service to verify that you can receive the emails.

Then simply edit the service you want emails for and add OnFailure=status_email_user@%n.service to the [Unit] section. %n passes the unit's name to the template.

See udisks#Automatically turn off an external HDD at shutdown.

To find the systemd services which failed to start:

To find out why they failed, examine their log output. See systemd/Journal#Filtering output for details.

systemd has several options for diagnosing problems with the boot process. See boot debugging for more general instructions and options to capture boot messages before systemd takes over the boot process. Also see systemd debugging documentation.

If some systemd service misbehaves or you want to get more information about what is happening, set the SYSTEMD_LOG_LEVEL environment variable to debug. For example, to run the systemd-networkd daemon in debug mode:

Add a drop-in file for the service adding the two lines:

Or equivalently, set the environment variable manually:

then restart systemd-networkd and watch the journal for the service with the -f/--follow option.

If the shutdown process takes a very long time (or seems to freeze), most likely a service not exiting is to blame. systemd waits some time for each service to exit before trying to kill it. To find out whether you are affected, see Shutdown completes eventually in the systemd documentation.

A common problem is a stalled shutdown or suspend process. To verify whether that is the case, you could run either of these commands and check the outputs

The solution to this would be to cancel these jobs by running

and then trying shutdown or reboot again.

If running journalctl -u foounit as root does not show any output for a short lived service, look at the PID instead. For example, if systemd-modules-load.service fails, and systemctl status systemd-modules-load shows that it ran as PID 123, then you might be able to see output in the journal for that PID, i.e. by running journalctl -b _PID=123 as root. Metadata fields for the journal such as _SYSTEMD_UNIT and _COMM are collected asynchronously and rely on the /proc directory for the process existing. Fixing this requires fixing the kernel to provide this data via a socket connection, similar to SCM_CREDENTIALS. In short, it is a bug. Keep in mind that immediately failed services might not print anything to the journal as per design of systemd.

The factual accuracy of this article or section is disputed.

After using systemd-analyze a number of users have noticed that their boot time has increased significantly in comparison with what it used to be. After using systemd-analyze blame NetworkManager is being reported as taking an unusually large amount of time to start.

The problem for some users has been due to /var/log/journal becoming too large. This may have other impacts on performance, such as for systemctl status or journalctl. As such the solution is to remove every file within the folder (ideally making a backup of it somewhere, at least temporarily) and then setting a journal file size limit as described in systemd/Journal#Journal size limit.

Starting with systemd 219, /usr/lib/tmpfiles.d/systemd.conf specifies ACL attributes for directories under /var/log/journal and, therefore, requires ACL support to be enabled for the filesystem the journal resides on.

See Access Control Lists#Enable ACL for instructions on how to enable ACL on the filesystem that houses /var/log/journal.

You may want to disable emergency mode on a remote machine, for example, a virtual machine hosted at Azure or Google Cloud. It is because if emergency mode is triggered, the machine will be blocked from connecting to network.

To disable it, mask emergency.service and emergency.target.

You may be trying to start or enable a user unit as a system unit. systemd.unit(5) indicates, which units reside where. By default systemctl operates on system services.

See systemd/User for more details.

Some bootloaders only set the LoaderDevicePartUUID variable when it is empty. Consequently, even if the UUID of the EFI partition changes, the bootloader will not update LoaderDevicePartUUID. By deleting the EFI variable with the commands below, the bootloader will then repopulate it with the new UUID.

**Examples:**

Example 1 (unknown):
```unknown
-H user@host
```

Example 2 (unknown):
```unknown
sshd.socket
```

Example 3 (unknown):
```unknown
netctl.service
```

Example 4 (unknown):
```unknown
dev-sda2.device
```

---

## Access Control Lists

**URL:** https://wiki.archlinux.org/title/ACL

**Contents:**
- Installation
- Enable ACL
- Usage
  - Set ACL
  - Show ACL
- Examples
  - Output of ls command
  - Execution permissions for private files
- See also

Access control list (ACL) provides an additional, more flexible permission mechanism for file systems. It is designed to assist with UNIX file permissions. ACL allows you to give permissions for any user or group to any disk resource.

The acl package is a dependency of systemd, it should already be installed.

To enable ACL, the filesystem must be mounted with the acl option. You can use fstab entries to make it permanent on your system.

There is a possibility that the acl option is already active as one of the default mount options on the filesystem. Btrfs and Ext2/3/4 filesystems are affected by this. Use the following command to check ext2/3/4 formatted partitions for the option:

Also check that the default mount options are not overridden, in such case you will see noacl in /proc/mounts in the relevant line.

You can set the default mount options of a filesystem using the tune2fs -o option partition command, for example:

Using the default mount options instead of an entry in /etc/fstab is very useful for external drives, such partition will be mounted with acl option also on other Linux machines. There is no need to edit /etc/fstab on every machine.

The ACL can be modified using the setfacl command.

To set permissions for a user (user is either the user name or ID):

To set permissions for a group (group is either the group name or ID):

To set permissions for others:

To allow all newly created files or directories to inherit entries from the parent directory (this will not affect files which will be moved into the directory):

To remove a specific entry:

To remove the default entries:

To remove all entries (entries of the owner, group and others are retained):

The mask then acts as a limiting filter: each affected entry's effective permissions are the intersection (bitwise AND) of its own permissions and the mask. For example, if user:bob is granted rwx and the mask is r-x, Bob’s effective permissions will be limited to r-x. This behavior ensures consistent access semantics, especially when sharing files across systems that do or do not support ACLs.

The example below helps clarify two distinct steps in how the ACL mask works in setfacl, especially in the context of the --no-mask and default behavior:

The 2003 USENIX document POSIX Access Control Lists on Linux has more information.

To show permissions, use:

Set all permissions for user johnny to file named abc:

Change permissions for user johnny:

Remove all ACL entries:

You will notice that there is an ACL for a given file because it will exhibit a + (plus sign) after its Unix permissions in the output of ls -l.

The following technique describes how a process like a web server can be granted access to files that reside in a user's home directory, without compromising security by giving the whole world access.

In the following we assume that the web server runs as the user http and grant it access to geoffrey's home directory /home/geoffrey.

The first step is granting execution permissions for the user http:

Since the user http is now able to access files in /home/geoffrey, others no longer need access:

Use getfacl to verify the changes:

As the above output shows, other's no longer have any permissions, but the user http is still able to access the files, thus security might be considered increased.

If you need to give write access for the user http on specific directories and/or files, run:

**Examples:**

Example 1 (unknown):
```unknown
# tune2fs -l /dev/sdXY | grep "Default mount options:"
```

Example 2 (unknown):
```unknown
Default mount options:    user_xattr acl
```

Example 3 (unknown):
```unknown
/proc/mounts
```

Example 4 (unknown):
```unknown
tune2fs -o option partition
```

---

## systemd-resolved

**URL:** https://wiki.archlinux.org/title/Systemd-resolved

**Contents:**
- Installation
- Configuration
  - DNS
    - Setting DNS servers
      - Automatically
      - Manually
      - Fallback
    - DNSSEC
    - DNS over TLS
      - Global DNS over TLS

systemd-resolved is a systemd service that provides network name resolution to local applications via a D-Bus interface, the resolve NSS service (nss-resolve(8)), and a local DNS stub listener on 127.0.0.53. See systemd-resolved(8) for the usage.

systemd-resolved is a part of the systemd package that is installed by default.

systemd-resolved provides resolver services for Domain Name System (DNS) (including DNSSEC and DNS over TLS), Multicast DNS (mDNS) and Link-Local Multicast Name Resolution (LLMNR).

The resolver can be configured by editing /etc/systemd/resolved.conf and/or drop-in .conf files in /etc/systemd/resolved.conf.d/. See resolved.conf(5).

To use systemd-resolved start and enable systemd-resolved.service.

Software that relies on glibc's getaddrinfo(3) (or similar) will work out of the box, since, by default, /etc/nsswitch.conf is configured to use nss-resolve(8) if it is available.

To provide domain name resolution for software that reads /etc/resolv.conf directly, such as web browsers, Go, GnuPG and QEMU when using user networking, systemd-resolved has four different modes for handling the file—stub, static, uplink and foreign. They are described in systemd-resolved(8) § /ETC/RESOLV.CONF. We will focus here only on the recommended mode, i.e. the stub mode which uses /run/systemd/resolve/stub-resolv.conf.

/run/systemd/resolve/stub-resolv.conf contains the local stub 127.0.0.53 as the only DNS server and a list of search domains. This is the recommended mode of operation that propagates the systemd-resolved managed configuration to all clients. To use it, replace /etc/resolv.conf with a symbolic link to it:

systemd-resolved will work out of the box with a network manager using /etc/resolv.conf. No particular configuration is required since systemd-resolved will be detected by following the /etc/resolv.conf symlink. This is going to be the case with systemd-networkd, NetworkManager, and iwd.

However, if the DHCP and VPN clients use the resolvconf program to set name servers and search domains (see openresolv#Users for a list of software that use resolvconf), the additional package systemd-resolvconf is needed to provide the /usr/bin/resolvconf symlink.

In stub and static modes, custom DNS server(s) can be set in the resolved.conf(5) file:

For more information on per-link configuration see systemd.network(5).

If systemd-resolved does not receive DNS server addresses from the network manager and no DNS servers are configured manually then systemd-resolved falls back to the fallback DNS addresses to ensure that DNS resolution always works.

The addresses can be changed by setting FallbackDNS in resolved.conf(5). E.g.:

To disable the fallback DNS functionality set the FallbackDNS option without specifying any addresses:

The systemd package is built with DNSSEC validation disabled by default. This can be changed with the DNSSEC setting in resolved.conf(5).

Test DNSSEC validation by querying a domain name with no signature:

Now test a domain with valid signature:

DNS over TLS is disabled by default. To enable it change the DNSOverTLS setting in the [Resolve] section in resolved.conf(5). To enable validation of your DNS provider's server certificate, include their hostname in the DNS setting in the format ip_address#hostname. For example:

ngrep can be used to test if DNS over TLS is working since DNS over TLS always uses port 853 and never port 53. The command ngrep port 53 should produce no output when a hostname is resolved with DNS over TLS and ngrep port 853 should produce encrypted output.

Wireshark can be used for more detailed packet inspection of DNS over TLS queries.

Enabling DNS over TLS for specific connections depends on the network manager:

systemd-resolved answers DNS requests to local applications via loopback interface per default. To make systemd-resolved answer DNS requests on additional interfaces or addresses than the default one, set the option DNSStubListenerExtra for every additional interface in resolved.conf(5). For example:

systemd-resolved is capable of working as a multicast DNS resolver and responder.

The resolver provides hostname resolution using a "hostname.local" naming scheme.

mDNS will only be activated for a connection if both systemd-resolved's mDNS support is enabled, and if the configuration for the currently active network manager enables mDNS for the connection.

systemd-resolved's mDNS support is enabled by default. It can be disabled by its MulticastDNS setting (see resolved.conf(5) § OPTIONS).

Enabling per-connection mDNS support depends on the network manager:

Link-Local Multicast Name Resolution is a hostname resolution protocol created by Microsoft.

LLMNR will only be activated for the connection if both the systemd-resolved's global setting (LLMNR in resolved.conf(5) § OPTIONS) and the network manager's per-connection setting is enabled. By default systemd-resolved enables LLMNR responder; systemd-networkd and NetworkManager[3] enable it for connections.

If you plan to use LLMNR and use a firewall, make sure to open UDP and TCP ports 5355.

To query DNS records, mDNS or LLMNR hosts you can use the resolvectl utility.

For example, to query a DNS record:

systemd-resolved may not search the local domain when given just the hostname, even when UseDomains=yes or Domains=[domain-list] is present in the appropriate systemd-networkd's .network file, and that file produces the expected search [domain-list] in resolv.conf. You can run networkctl status or resolvectl status to check if the search domains are actually being picked up.

Possible workarounds:

To make systemd-resolved resolve hostnames that are not fully qualified domain names, add ResolveUnicastSingleLabel=yes to /etc/systemd/resolved.conf.

This only seems to work with LLMNR disabled (LLMNR=no).

If you are using systemd-networkd, you might want the domain supplied by the DHCP server or IPv6 Router Advertisement to be used as a search domain. This is disabled by default, to enable it add to the interface's .network file:

You can check what systemd-resolved has for each interface with:

**Examples:**

Example 1 (unknown):
```unknown
/etc/systemd/resolved.conf
```

Example 2 (unknown):
```unknown
/etc/systemd/resolved.conf.d/
```

Example 3 (unknown):
```unknown
systemd-resolved.service
```

Example 4 (unknown):
```unknown
/etc/nsswitch.conf
```

---

## File systems

**URL:** https://wiki.archlinux.org/title/Create_a_file_system

**Contents:**
- Types of file systems
  - Journaling
  - FUSE-based file systems
  - Stackable file systems
  - Read-only file systems
  - Clustered file systems
  - Shared-disk file system
- Identify existing file systems
- Create a file system
- Mount a file system

Individual drive partitions can be set up using one of the many different available file systems. Each has its own advantages, disadvantages, and unique idiosyncrasies. A brief overview of supported file systems follows; the links are to Wikipedia pages that provide much more information.

See filesystems(5) for a general overview and Wikipedia:Comparison of file systems for a detailed feature comparison. File systems already loaded by the kernel or built-in are listed in /proc/filesystems, while all the installed modules can be seen with ls /lib/modules/$(uname -r)/kernel/fs.

xfs.html xfs-delayed-logging-design.html xfs-self-describing-metadata.html

The ext3/4, HFS+, JFS, NTFS, ReiserFS, and XFS file systems use journaling. Journaling provides fault-resilience by logging changes before they are committed to the file system. In the event of a system crash or power failure, such file systems are faster to bring back online and less likely to become corrupted. The logging takes place in a dedicated area of the file system.

ext3/4 offer data-mode journaling, which can optionally log data in addition to the metadata. Data-mode journaling comes with a speed penalty, because it does two write operations: first to the journal and then to the disk. Therefore, data-mode journaling is not enabled by default. The trade-off between system speed and data safety should be considered when choosing the file system type and features.

In the same vein, Reiser4 offers configurable "transaction models": a special model called wandering logs, which eliminates the need to write to the disk twice; write-anywhere, a pure copy-on-write approach; and a combined approach called hybrid which heuristically alternates between the two.

File systems based on copy-on-write (also known as write-anywhere), such as Reiser4, Btrfs, Bcachefs and ZFS, by design operate on full atomicity and also provide checksums for both metadata and inline data (operations entirely occur, or they entirely do not, and in properly functioning hardware data does not corrupt due to operations half-occurring). Therefore, these file systems are by design much less prone to data loss than other file systems and have no need to use traditional journal to protect metadata, because they are never updated in-place. Although Btrfs still has a journal-like log tree, it is only used to speed-up fdatasync/fsync.

FAT, exFAT, ext2, and HFS provide neither journaling nor atomicity, They are for temporary or legacy use and not recommended for use when reliable storage is needed.

To identify existing file systems, you can use lsblk:

An existing file system, if present, will be shown in the FSTYPE column. If mounted, it will appear in the MOUNTPOINT column.

File systems are usually created on a partition, inside logical containers such as LVM, RAID and dm-crypt, or on a regular file (see Wikipedia:Loop device). This section describes the partition case.

Before continuing, identify the device where the file system will be created and whether or not it is mounted. For example:

Mounted file systems must be unmounted before proceeding. In the above example an existing file system is on /dev/sda2 and is mounted at /mnt. It would be unmounted with:

To find just mounted file systems, see #List mounted file systems.

To create a new file system, use mkfs(8). See #Types of file systems for the exact type, as well as userspace utilities you may wish to install for a particular file system.

For example, to create a new file system of type ext4 (common for Linux data partitions) on /dev/sda1, run:

The new file system can now be mounted to a directory of choice.

To manually mount a file system located on a device (e.g., a partition) to a directory, use mount(8). This example mounts /dev/sda1 to /mnt.

This attaches the file system on /dev/sda1 at the directory /mnt, making the contents of the file system visible. Any data that existed at /mnt before this action is made invisible until the device is unmounted.

fstab contains information on how devices should be automatically mounted if present. See the fstab article for more information on how to modify this behavior.

If a device is specified in /etc/fstab and only the device or mount point is given on the command line, that information will be used in mounting. For example, if /etc/fstab contains a line indicating that /dev/sda1 should be mounted to /mnt, then the following will automatically mount the device to that location:

mount contains several options, many of which depend on the file system specified. The options can be changed, either by:

See these related articles and the article of the file system of interest for more information.

To list all mounted file systems, use findmnt(8):

findmnt takes a variety of arguments which can filter the output and show additional information. For example, it can take a device or mount point as an argument to show only information on what is specified:

findmnt gathers information from /etc/fstab, /etc/mtab, and /proc/self/mounts.

To unmount a file system use umount(8). Either the device containing the file system (e.g., /dev/sda1) or the mount point (e.g., /mnt) can be specified:

Unmount the file system and run fsck on the problematic volume.

**Examples:**

Example 1 (unknown):
```unknown
/proc/filesystems
```

Example 2 (unknown):
```unknown
ls /lib/modules/$(uname -r)/kernel/fs
```

Example 3 (unknown):
```unknown
NAME   FSTYPE LABEL     UUID                                 MOUNTPOINT
sdb
└─sdb1 vfat   Transcend 4A3C-A9E9
```

Example 4 (unknown):
```unknown
NAME   FSTYPE   LABEL       UUID                                 MOUNTPOINT
sda
├─sda1                      C4DA-2C4D
├─sda2 ext4                 5b1564b2-2e2c-452c-bcfa-d1f572ae99f2 /mnt
└─sda3                      56adc99b-a61e-46af-aab7-a6d07e504652
```

---

## Secure Shell

**URL:** https://wiki.archlinux.org/title/SSH

**Contents:**
- Software
  - Server only
  - Client only
- Securing
- See also

This article or section is a candidate for merging with OpenSSH.

According to Wikipedia:

Examples of services that can use SSH are Git, rsync and X11 forwarding. Services that always use SSH are SCP and SFTP.

An SSH server, by default, listens on the standard TCP port 22. An SSH client program is typically used for establishing connections to an sshd daemon accepting remote connections. Both are commonly present on most modern operating systems, including macOS, GNU/Linux, Solaris and OpenVMS. Proprietary, freeware and open source versions of various levels of complexity and completeness exist.

---

## cgroups

**URL:** https://wiki.archlinux.org/title/Control_groups

**Contents:**
- Installing
- With systemd
  - Hierarchy
  - Find cgroup of a process
  - cgroup resource usage
  - Custom cgroups
  - As service
    - Service unit file
    - Grouping unit under a slice
  - As root

Control groups (or cgroups as they are commonly known) are a feature provided by the Linux kernel to manage, restrict, and audit groups of processes. Compared to other approaches like the nice(1) command or /etc/security/limits.conf, cgroups are more flexible as they can operate on (sub)sets of processes (possibly with different system users).

Control groups can be accessed with various tools:

For Arch Linux, systemd is the preferred and easiest method of invoking and configuring cgroups as it is a part of the default installation.

Make sure you have one of these packages installed for automated cgroup handling:

Current cgroup hierarchy can be seen with systemctl status or systemd-cgls command.

The cgroup name of a process can be found in /proc/PID/cgroup.

For example, the cgroup of the shell:

The systemd-cgtop command can be used to see the resource usage:

systemd.slice(5) systemd unit files can be used to define a custom cgroup configuration. They must be placed in a systemd directory, such as /etc/systemd/system/. The resource control options that can be assigned are documented in systemd.resource-control(5).

This is an example slice unit that only allows 30% of one CPU to be used:

Remember to do a daemon-reload to pick up any new or changed .slice files.

Resources can be directly specified in service definition or as a drop-in file:

This example limits the service to 1 gigabyte.

Service can be specified what slice to run in:

systemd-run can be used to run a command in a specific slice.

--uid=username option can be used to spawn the command as specific user.

The --shell option can be used to spawn a command shell inside the slice.

Unprivileged users can divide the resources provided to them into new cgroups, if some conditions are met.

Cgroups v2 must be utilized for a non-root user to be allowed managing cgroup resources.

Not all resources can be controlled by user.

For user to control cpu and io resources, the resources need to be delegated. This can be done with a drop-in file.

For example if your user id is 1000:

Reboot and verify that the slice your user session is under has cpu and io controller:

The user slice files can be placed in ~/.config/systemd/user/.

To run the command under certain slice:

You can also run your login shell inside the slice:

cgroups resources can be adjusted at run-time using systemctl set-property command. Option syntax is the same as in systemd.resource-control(5).

For example, cutting off internet access for all user sessions:

One layer lower than management with systemd is the cgroup virtual file system. "libcgroup" provides a library and utilities for making management easier, so we will use them here as well.

The reason for using the lower level is simple: systemd does not provide an interface for every single interface file in cgroups and nor should it be expected for it to provide them at any point in the future. It is completely harmless to read from them for additional insights on a cgroup's resource use.

One cgroup should only have one set of programs writing to it to avoid race conditions, the "single-writer rule". This is not enforced by the kernel, but following this recommendation prevents hard-to-debug issues from happening. To set the boundary at which systemd stops managing child cgroups, see the Delegate= property. Otherwise do not be surprised if system to overwrites what you have set.

One of the powers of cgroups is that you can create "ad-hoc" groups on the fly. You can even grant the privileges to create custom groups to regular users. groupname is the cgroup name:

Now all the tunables in the group groupname are writable by your user:

Cgroups are hierarchical, so you can create as many subgroups as you like. If a normal user wants to make new subgroup called foo:

As previously mentioned, only one thing should write to a cgroup at any point. This does not affect non-write operations including spawning new processes inside a group, moving processes to a group, or reading properties from cgroup files.

libcgroup contains a simple tool for running new processes inside a cgroup. If a normal user wants to run a bash shell under a our previous groupname/foo:

Inside of the shell, we can confirm which cgroup it belongs to with:

This makes use of /proc/$PID/cgroup, a file that exists in every process. Manually writing to the file causes the cgroup to change as well.

To move all 'bash' commands to this group:

Internally (i.e. without cgclassify the kernel provides two ways to move processes between cgroups. These two are equivalent:

A new subdirectory is crated for groupname/foo at its creation, located at /sys/fs/cgroup/groupname/foo. These files can be read and written to change the group's properties. (Again, writing is not recommended unless delegation is done!)

Let us try to see how much memory all the processes in our group is taking up:

To limit the RAM (not swap) usage of all processes, run the following:

To change the CPU priority of this group (the default is 100):

You can find more tunables or statistics by listing the cgroup directory.

If you want your cgroups to be created at boot, you can define them in /etc/cgconfig.conf instead. This causes a service booted at launch to configure your cgroups. See the relevant manual page about the syntax of this file; we will make no instruction on how to use a truly deprecated mechanism.

The following example shows a cgroup that constrains a given command to 2GB of memory.

The following example shows a command restricted to 20% of one CPU core.

Doing large calculations in MATLAB can crash your system, because Matlab does not have any protection against taking all your machine's memory or CPU. The following examples show a cgroup that constrains Matlab to first 6 CPU cores and 5 GB of memory.

Launch Matlab like this (be sure to use the right path):

For commands and configuration files, see relevant man pages, e.g. cgcreate(1) or cgrules.conf(5)

Before our current cgroup v2 there was an earlier version called v1. V1 allowed a lot of additional flexibility including a non-unified hierarchy and thread-granular management. This was, in retrospect, a bad idea (see the rationales for v2):

To avoid further chaos, cgroup v2 has two key design rules on top of the removal of features:

Before systemd v258, the kernel parameters SYSTEMD_CGROUP_ENABLE_LEGACY_FORCE=1 systemd.unified_cgroup_hierarchy=0 could be used to force booting with cgroup-v1 (the first parameter was added in v256 to make it harder to use cgroup-v1). However, this feature has now been removed. It is still worth knowing about because some software like to put systemd.unified_cgroup_hierarchy=0 in your kernel command-line without telling you, causing your entire system to break.

**Examples:**

Example 1 (unknown):
```unknown
/etc/security/limits.conf
```

Example 2 (unknown):
```unknown
/etc/cgrules.conf
```

Example 3 (unknown):
```unknown
cgconfig.service
```

Example 4 (unknown):
```unknown
cgconfig.conf
```

---

## Xorg/Keyboard configuration

**URL:** https://wiki.archlinux.org/title/Xorg/Keyboard_configuration

**Contents:**
- Viewing keyboard settings
  - Third party utilities
- Setting keyboard layout
  - Using X configuration files
    - Using localectl
  - Using setxkbmap
- Frequently used XKB options
  - Switching between keyboard layouts
    - Switch languages using Alt Shift
  - Terminating Xorg with Ctrl+Alt+Backspace

This article describes the basics of Xorg keyboard configuration. For advanced topics such as keyboard layout modification or additional key mappings, see X keyboard extension or Extra keyboard keys respectively.

The Xorg server uses the X keyboard extension (XKB) to define keyboard layouts. Optionally, xmodmap can be used to access the internal keymap table directly, although this is not recommended for complex tasks. Also systemd's localectl can be used to define the keyboard layout for both the Xorg server and the virtual console.

You can use the following command to see the actual XKB settings:

There are some "unofficial" utilities which allow to print specific information about the currently used keyboard layout.

Keyboard layout in Xorg can be set in multiple ways. Here is an explanation of used options:

The layout name is usually a 2-letter country code. To see a full list of keyboard models, layouts, variants and options, along with a short description, open /usr/share/X11/xkb/rules/base.lst. Alternatively, you may use one of the following commands to see a list without a description:

Examples in the following subsections will have the same effect, they will set pc104 model, cz as primary layout, us as secondary layout, dvorak variant for us layout and the Super+Space combination for switching between layouts. See xkeyboard-config(7) for more detailed information.

The syntax of X configuration files is explained in Xorg#Configuration. This method creates system-wide configuration which is persistent across reboots.

For convenience, the tool localectl may be used instead of manually editing X configuration files. It will save the configuration in /etc/X11/xorg.conf.d/00-keyboard.conf, this file should not be manually edited, because localectl will overwrite the changes on next start.

The usage is as follows:

To set a model, variant or options, all preceding fields need to be specified, but the preceding fields can be skipped by passing an empty string with "". Unless the --no-convert option is passed, the specified keymap is also converted to the closest matching console keymap and applied to the console configuration in vconsole.conf. See localectl(1) for more information.

To create a /etc/X11/xorg.conf.d/00-keyboard.conf like the above:

This article or section needs expansion.

setxkbmap sets the keyboard layout for the current X session only, but can be made persistent in xinitrc or xprofile. This overrides system-wide configuration specified following #Using X configuration files.

The usage is as follows (see setxkbmap(1)):

To change just the layout (-layout is the default flag):

For multiple customizations:

To be able to easily switch keyboard layouts, first specify multiple layouts between which you want to switch (the first one is the default). Then specify a key (or key combination), which will be used for switching. For example, to switch between a US and a Swedish layout using the CapsLock key, use us,se as an argument of XkbLayout and grp:caps_toggle as an argument of XkbOptions. The number of XkbLayouts should match that of the XkbVariants — if you want to switch solely between different variants, then duplicate the layout accordingly (e.g. de,de).

The list of available layouts (and variants) can be found in xkeyboard-config(7) § LAYOUTS. The key combinations available for layout switching are listed in xkeyboard-config(7) § Switching to another layout.

Note that the grp:alts_toggle option is unreliable and unlikely to be fixed; prefer other combinations.

To set Alt+Shift as a layout shortcut, use grp:alt_shift_toggle in XkbOptions.

However, there is a known issue with XKB that causes other shortcuts of the type Alt+Shift+any_key to break. Moreover, XKB may set the right Alt to be AltGr by default in some keyboard layouts, making RAlt+RShift not working for layout switching.

As a workaround, sxhkd may be used to switch layouts by adding the following to sxhkdrc:

Note that for some reason, Alt must be pressed before Shift to be detected by sxhkd.

By default, the key combination Ctrl+Alt+Backspace is disabled. You can enable it by passing terminate:ctrl_alt_bksp to XkbOptions. This can also be done by binding a key to Terminate_Server in xmodmap (which undoes any existing XkbOptions setting). In order for either method to work, one also needs to have DontZap set to "off" in ServerFlags: since 2004 [2] this is the default.

To swap Caps Lock with Left Control key, add ctrl:swapcaps to XkbOptions. Run the following command to see similar options along with their descriptions:

Mouse keys, not to be confused with the keys of the mouse, is disabled by default and has to be manually enabled by passing keypad:pointerkeys to XkbOptions. This will make the Shift+NumLock shortcut toggle mouse keys.

See also X keyboard extension#Mouse control for advanced configuration.

The AltGr (Alternate Graphic) key can be used to access additional characters and symbols on a keyboard. It functions as a modifier key similar to Shift but provides access to a third level of key mappings. Note that mapping levels work as follows:

2nd level characters are usually printed on keyboard keys and are easy to find. On the other hand, to check the characters on additional levels, you can use xmodmap -pk or look up your keyboard mapping on /usr/share/X11/xkb/symbols.

Though typically not on traditional keyboards, a Compose key can be configured to an existent key.

The Compose key begins a keypress sequence that involves (usually two) additional keypresses. Usage is typically either for entering characters in a language that the keyboard was not designed for, or for other less-used characters that are not covered with the AltGr modifier. For example, pressing Compose ' e produces é, or Compose - - - will produce an "em dash": —.

Though a few more eccentric keyboards feature a Compose key, its availability is usually through substituting an already existing key to it. For example, to make the Menu key a Compose key use the Desktop environment configuration, or pass compose:menu to XkbOptions (or setxkbmap: setxkbmap -option compose:menu). Allowed key substitutions are defined in /usr/share/X11/xkb/rules/base.lst:

If the desired mapping is not found in that file, an alternative is to use xmodmap to map the desired key to the Multi_key keysym, which acts as a compose key by default (note that xmodmap settings are reset by setxkbmap).

The default combinations for the compose keys depend on the locale configured for the session and are stored in /usr/share/X11/locale/used_locale/Compose, where used_locale is for example en_US.UTF-8.

You can define your own compose key combinations by copying the default file to ~/.XCompose and editing it. Alternatively, create an empty ~/.XCompose and include the default one using include "%L", for example:

The compose key (denoted as <Multi_key> in the ~/.XCompose file) works with any of the thousands of valid Unicode characters, including those outside the Basic Multilingual Plane. Take a look at the Compose(5) man page, it explains the format of the XCompose files.

However, GTK does not use XIM by default and therefore does not follow ~/.XCompose keys. This can be fixed by forcing GTK to use XIM by configure the graphical environment variables GTK_IM_MODULE=xim and/or XMODIFIERS="@im=none".

Most European keyboards have a Euro sign (€) printed on on the 5 key. For example, to access it with Alt+5, use the lv3:ralt_switch and eurosign:5 options.

The Rupee sign (₹) can be used the same way with rupeesign:4.

Those who prefer typing capital letters with the Caps Lock key may experience a short delay when Caps Lock state is switched, resulting in two or more capital letters (e.g. THe, ARch LInux). This occurs because Caps Lock is enabled immediately once the Caps Lock key is pressed, but is only disabled upon release of the second key-press. This behaviour stems from typewriters where a Caps Lock function was achieved by physically locking the shifted typebars in place, and the release of a shift key-press was the action that caused the release of the lock.

Some more popular operating systems have removed this behaviour, either voluntarily (as it can be confusing to some) or by mistake, however this is a question of preference. Bug reports have been filed on the Xserver bug tracker, as there is currently no easy way to switch to the behaviour reflected by those other operating systems. For anyone who would like to follow up the issue, bug reports and latest working progress can be found at [3] and [4].

First, export your keyboard configurations to a file:

In the file xkbmap, locate the Caps Lock section which begins with key <CAPS>:

and replace whole section with the following code:

Save and reload keyboard configurations:

Consider making it a service launching after X starts, since reloaded configurations do not survive a system reboot.

To assign an additional one-click function to a modifier key, you can use xcape. For example it is possible to have CapsLock work as Escape when pressed alone, and as Control when used with another key. First set the Control swapping using setxkbmap as mentioned earlier, and xcape to set the Escape association:

You can set multiple associations separated with a semicolon, e.g.: Caps_Lock=Escape;Shift_L=Escape.

If you hold a key for longer than the timeout value (default 500 ms), xcape will not generate a key event.

The typematic delay indicates the amount of time (typically in milliseconds) a key needs to be pressed and held in order for the repeating process to begin. After the repeating process has been triggered, the character will be repeated with a certain frequency (usually given in Hz) specified by the typematic rate. Note that these settings are configured separately for Xorg and for the virtual console.

The tool xset, provided by xorg-xset, can be used to set the typematic delay and rate for an active X server, though certain actions during runtime may cause the X server to reset these changes and revert instead to its seat defaults.

For example to set a typematic delay to 200ms and a typematic rate to 30Hz, use the following command:

Issuing the command without specifying the delay and rate will reset the typematic values to their respective defaults; a delay of 660ms and a rate of 25Hz:

xautocfgAUR can apply repeat rate settings for newly connected devices automatically. It watches for X11 events and applies repeat rate configuration to newly connected keyboards.

Adjust the configuration:

If graphical-session.target is started by your window manager or desktop environment, enable the systemd/User xautocfg.service. Alternatively, launch xautocfg manually.

To persist the configuration system-wide, change the seat defaults with an Xorg configuration file as described in #Using X configuration files, and add a AutoRepeat section entry: [5]

The parameters for AutoRepeat are delay and interval in milliseconds. If you like 25 Hz rate of xset, the corresponding interval 1000 / 25 = 40 milliseconds.

Another method of persisting the configuration is to pass the desired settings to the X server on its startup using the following options:

See Xserver(1) for a full list of X server options and refer to your display manager for information about how to pass these options.

To manually modify or create a layout, some low-level knowledge is needed.

To physical keys, numbers assigned by kernel. They are listed in /usr/include/linux/input-event-codes.h, provided linux-api-headers—a dependency of linux-headers—is installed, like this:

These values are used by libevdev. For input remap utilities that rely on libevdev, key names are often these entries in lowercase, without the prefix KEY_. Xorg's "keycode" value is the one in the file input-event-codes.h + 8. (For example KEY_Q is 16. The X's keycode of the key printed "Q" 24.)

In XKB however, i.e. for X and Wayland, the key names look like <AD11> or <TLDE>. Most of them are easy to guess; for example if you have an Italian keyboard, just look at /usr/share/X11/xkb/symbols/it. It has the lines:

When it is not sufficient, look at files in /usr/share/X11/xkb/keycodes, in particular the file evdev. These lines define the keycode for the key name.

Finally, what are possible symbols that can be assigned to keys, like notsign or brokenbar in the above example? They are defined in /usr/include/X11/keysymdef.h

Some symbols with the prefix XF86 is listed in the separate file /usr/include/X11/XF86keysym.h.

This key is called XF86MonBrightnessUp in X. (Again rip off XK_, but in such cases the middle of the name.)

**Examples:**

Example 1 (unknown):
```unknown
$ setxkbmap -print -verbose 10
```

Example 2 (unknown):
```unknown
Setting verbose level to 10
locale is C
Applied rules from evdev:
model:      evdev
layout:     us
options:    terminate:ctrl_alt_bksp
Trying to build keymap using the following components:
keycodes:   evdev+aliases(qwerty)
types:      complete
compat:     complete
symbols:    pc+us+inet(evdev)+terminate(ctrl_alt_bksp)
geometry:   pc(pc104)
xkb_keymap {
        xkb_keycodes  { include "evdev+aliases(qwerty)" };
        xkb_types     { include "complete"      };
        xkb_compat    { include "complete"      };
        xkb_symbols   { include "pc+us+inet(evdev)+terminate(ctrl_alt_bksp)"    };
        xkb_geometry  { include "pc(pc104)"     };
};
```

Example 3 (unknown):
```unknown
$ xkb-switch
```

Example 4 (unknown):
```unknown
$ xkblayout-state print "%s"
```

---

## TOMOYO Linux

**URL:** https://wiki.archlinux.org/title/TOMOYO

**Contents:**
- Introduction
- Branches of development
- TOMOYO Linux 1.x
  - Initializing configuration
- AKARI
  - Limitations of AKARI
  - Installation
  - Initializing configuration
- TOMOYO Linux 2.x
  - Limitations of TOMOYO Linux 2.x

TOMOYO Linux is a Mandatory Access Control (MAC) implementation for Linux. It was launched in March 2003 and was sponsored by NTT Data Corporation. TOMOYO Linux focuses on the behaviour of a system, allowing each process to declare behaviours and resources needed to achieve its purpose. It can be used as a system analysis tool as well as an access restriction tool.

The security goal of TOMOYO Linux is to provide "MAC that covers practical requirements for most users and keeps usable for most administrators". TOMOYO Linux is not a tool for just security professionals, but also for average users and administrators.

TOMOYO Linux attempts to make the system where everything is prearranged in an easy to understand way:

Unlike AppArmor, TOMOYO Linux is intended to protect the whole system from attackers exploiting vulnerabilities in applications. TOMOYO Linux addresses this threat by recording the behaviour of all applications in the test environment and then forcing all applications to act within these recorded behaviours in the production environment.

TOMOYO Linux is not for users wanting ready-made policy files supplied by others. It involves creating policy from scratch, aided by the "learning mode" which can automatically generate policy files with necessary and sufficient permissions for a specific system. TOMOYO Linux reports what is happening within the Linux system and can therefore be used as a system analysis tool. It resembles strace and reports what is being executed by each program and what files/networks are accessed.

This table provides a comprehensive comparison of TOMOYO Linux with AppArmor, SELinux and SMACK.

TOMOYO Linux 1.x is the original branch of development. TOMOYO Linux was first released on 11th November 2005. It was implemented as a patch that can be applied to the Linux kernel and is still in active development. It can coexist with other security modules such as SELinux, SMACK and AppArmor.

TOMOYO Linux 2.x is the Linux mainline kernel branch of development. In June 2009, TOMOYO was merged into the Linux kernel version 2.6.30 and it uses standard Linux Security Module (LSM) hooks. However, the LSM hooks must be extended further in order to port the full MAC functionality of TOMOYO Linux into the Linux kernel. Thus, it does not yet provide equal functionality with the 1.x branch of development. This chart compares the differences between each branch.

AKARI is based on the TOMOYO Linux 1.x branch and is implemented as a Loadable Kernel Module (LKM). It therefore has the advantage of not requiring the user to patch and recompile the kernel. This table provides a comprehensive comparison of AKARI with the TOMOYO Linux 1.x and 2.x branches.

Implementing TOMOYO Linux 1.x using a kernel patched with ccs-patch provides the full functionality obtainable from the TOMOYO Linux project. However, implementation of this branch requires the most hurdles to be overcome, as the kernel must be patched with ccs-patch and subsequently recompiled.

Both a patched kernel and the userspace tools must be installed. A package for ccs-toolsAUR is available on the AUR.

The policy must first be initialized:

The policy files are saved in the /etc/css/ directory and can be edited by running:

If using the TOMOYO Linux project purely for system analysis, then AKARI is the easiest method of achieving this. If using the TOMOYO Linux project for system restriction, it is a minimal effort way to gain most of the functionality of the TOMOYO Linux 1.x branch. However, there are a few limitations that must be considered:

This table provides a comprehensive comparison of AKARI with the TOMOYO Linux 1.x and 2.x branches.

Both AKARI and the userspace tools must be installed. A package for akariAUR and a package for ccs-toolsAUR are available on the AUR.

To activate AKARI, set the following kernel parameter:

The policy must first be initialized:

The policy files are saved in the /etc/css/ directory and can be edited by running:

The implementation of TOMOYO Linux 2.x into the Linux mainline kernel is not yet complete but is very close to 1.x since 2.5.x. There are a few features that still need to be implemented as compared to the 1.x branch. This chart has a comprehensive comparison of the differences between each branch of development.

TOMOYO is supported by all officially supported kernels.

For custom kernels make sure that the following kernel options are set:

Install the 2.6 branch of the userspace tools with tomoyo-toolsAUR.

To activate TOMOYO Linux, set the following kernel parameters to enable the Linux Security Module:

TOMOYO will load all saved policies from /etc/tomoyo/policy/current when /usr/lib/systemd/systemd executes.

Next, check whether the activation was successful. You should have the following lines (or similar) from dmesg:

For first time, you may want to auto-save in-memory policies to filesystem when computer goes to shutdown/reboot. If yes, create the following systemd unit:

Enable tomoyo-savepolicy.service.

Remove tomoyo from the lsm= kernel parameter or remove lsm= entirely.

The policy must first be initialized:

The policy files are saved in the /etc/tomoyo/ directory and can be edited by running:

By default, tomoyo will start with "Disabled" profile (see profile-table below). You may want to enable learning mode for everybody right now. Just switch profile for <kernel> namespace in /etc/tomoyo/policy/current/domain_policy.conf:

If unsure if such wide learning is needed, just ignore this step. You can switch profiles later using tomoyo-editpolicy in "Domain transition editor" by pressing S on any selected domain (domains).

Now, the computer should be restarted.

For tomoyo exists the log-daemon tomoyo-auditd. It is useful for monitoring the behavior of closed-source applications. The initial configuration file is well explained and can be found in /etc/tomoyo/tools/auditd.conf whereas the log files can be found in /var/log/tomoyo/.

To use it with systemd create the file /etc/systemd/system/tomoyo-auditd.service with the content described in chapter 4.6 in the official documentation.

It is important to consult the relevant documentation in order to use TOMOYO Linux or AKARI effectively:

Run the policy editor to begin editing. If using TOMOYO Linux 1.x or AKARI, then ccs-tools should be used:

If using TOMOYO Linux 2.x, then tomoyo-tools should be used:

As the system runs, TOMOYO Linux will create domains and add them to the tree. The access analysis/restriction in TOMOYO Linux is applied via domains. Every process belongs to a single domain and the process will transit to a different domain whenever it executes a program. The name of a domain is a concatenated string expression for the process execution history. For example, the name of the domain which the kernel belongs to is "<kernel>"; the name of domain which /sbin/init invoked by the kernel belongs to is "<kernel> /sbin/init"; if /sbin/init invokes /etc/rc.d/rc then the domain it belongs to is "<kernel> /sbin/init /etc/rc.d/rc". You can suppress or initialize domain transitions as needed.

Profiles can be assigned to each domain. There are four default profiles:

The learning profile can be used to analyse the system or a specific application. Once all of the desired access requests of a domain have been identified, the policy for that domain can be edited as required before selecting the enforcing profile. This can be done for any and all domains from the start of system boot.

**Examples:**

Example 1 (unknown):
```unknown
# /usr/lib/ccs/init_policy
```

Example 2 (unknown):
```unknown
# ccs-editpolicy
```

Example 3 (unknown):
```unknown
CONFIG_SECURITY=y [required]
CONFIG_KALLSYMS=y [required]
CONFIG_PROC_FS=y [required]
CONFIG_MODULES=y [required]
CONFIG_SECURITY_PATH=y [optional: for using absolute pathnames]
CONFIG_SECURITY_NETWORK=y [optional: for providing network restriction]
```

Example 4 (unknown):
```unknown
init=/usr/bin/ccs-init
```

---

## EFI system partition

**URL:** https://wiki.archlinux.org/title/ESP

**Contents:**
- Check for an existing partition
- Create the partition
  - GPT partitioned disks
  - MBR partitioned disks
- Format the partition
- Mount the partition
  - Typical mount points
  - Alternative mount points
    - Using bind mount
    - Using systemd

The EFI system partition (also called ESP) is an OS independent partition that acts as the storage place for the UEFI boot loaders, applications and drivers to be launched by the UEFI firmware. It is mandatory for UEFI boot.

If you are installing Arch Linux on an UEFI-capable computer with an installed operating system, like Windows 10 for example, it is very likely that you already have an EFI system partition.

To find out the disk partition scheme and the system partition, use fdisk as root on the disk you want to boot from:

If you found an acceptable existing EFI system partition, simply proceed to #Mount the partition. If you did not find one, you will need to create it, proceed to #Create the partition.

The following two sections show how to create an EFI system partition (ESP).

The partition size should provide adequate space for storing boot loaders and other files required for booting.

It is recommended to make the partition 1 GiB in size to ensure it has adequate space for multiple kernels or unified kernel images, a boot loader, firmware updates files and any other operating system or OEM files. If still in doubt, 4 GiB ought to be enough for anybody, e.g., for tools like Limine boot loader with Snapper integration for Btrfs, which supports creating multiple bootable snapshots.

EFI system partition on a GUID Partition Table is identified by the partition type GUID C12A7328-F81F-11D2-BA4B-00A0C93EC93B.

Choose one of the following methods to create an ESP for a GPT partitioned disk:

After creating the partition, it should be formatted with a file system. Proceed to the #Format the partition section below.

See also Partitioning#Choosing between GPT and MBR for the limits of MBR and the advantages of GPT in general.

EFI system partition on a Master Boot Record partition table is identified by the partition type ID EF.

Choose one of the following methods to create an ESP for a MBR partitioned disk:

After creating the partition, it should be formatted with a file system. Proceed to the #Format the partition section below.

The UEFI specification mandates support for the FAT12, FAT16, and FAT32 file systems (see UEFI specification version 2.11, section 13.3.1.1), but any conformant vendor can optionally add support for additional file systems; for example, the firmware in Apple Macs supports the HFS+ file system.

To prevent potential issues with other operating systems and since the UEFI specification says that UEFI "encompasses the use of FAT32 for a system partition, and FAT12 or FAT16 for removable media"[5], it is recommended to use FAT32. Use the mkfs.fat(8) utility from dosfstools:

If you get the message WARNING: Number of clusters for 32 bit FAT is less than suggested minimum. and you cannot create a larger ESP, reduce cluster size with mkfs.fat -s 2 -F 32 ... or -s 1; otherwise the partition may be unreadable by UEFI. See mkfs.fat(8) for supported cluster sizes.

For partitions smaller than 32 MiB using FAT32 may not be possible. In which case, format it to FAT16 or even FAT12. For example, a 2 MiB ESP will only be able to support FAT12:

The kernels, initramfs files, and, in most cases, the processor's microcode, need to be accessible by the boot loader or UEFI itself to successfully boot the system. Thus if you want to keep the setup simple, your boot loader choice limits the available mount points for EFI system partition.

Alternatively preload the required kernel modules on boot, e.g.:

The three typical scenarios for mounting the EFI system partition are:

If you do not use one of the #Typical mount points, you will need to copy your boot files to ESP (referred to hereafter as esp).

Furthermore, you will need to keep the files on the ESP up-to-date with later kernel updates. Failure to do so could result in an unbootable system. The following sections discuss several mechanisms for automating it.

Instead of mounting the ESP itself to /boot, you can mount a directory of the ESP to /boot using a bind mount (see mount(8)). This allows pacman to update the kernel directly while keeping the ESP organized to your liking.

Just like in #Alternative mount points, copy all boot files to a directory on your ESP, but mount the ESP outside /boot. Then bind mount the directory:

After verifying success, edit your Fstab to make the changes persistent:

systemd features event triggered tasks. In this particular case, the ability to detect a change in path is used to sync the EFISTUB kernel and initramfs files when they are updated in /boot/. The file watched for changes is initramfs-linux-fallback.img since this is the last file built by mkinitcpio, to make sure all files have been built before starting the copy. The systemd path and service files to be created are:

Then enable and start efistub-update.path.

Filesystem events can be used to run a script syncing the EFISTUB Kernel after kernel updates. An example with incron follows.

In order to use this method, enable the incrond.service.

As the presets in /etc/mkinitcpio.d/ support shell scripting, the kernel and initramfs can be copied by just editing the presets.

Edit the file /etc/mkinitcpio.d/linux.preset:

To test that, just run:

A mkinitcpio post hook can be used to copy kernels and initramfs images to a desired directory after the initramfs is generated.

Create the following file and make it executable:

A last option relies on the pacman hooks that are run at the end of the transaction.

The first file is a hook that monitors the relevant files, and it is run if they were modified in the former transaction.

The second file is the script itself. Create the file and make it executable:

On a disk with a preexisting operating system, the EFI system partition may be smaller than recommended in #Create the partition. E.g. Windows Setup creates a measly 100 MiB EFI system partition on non-4Kn drives.

In such cases, it may be a good idea to create a new, larger EFI system partition to prevent running out of space on it.

In Windows, partitions can be either managed graphically with Disk Management (diskmgmt.msc) or from the command line with the diskpart.exe utility.

Run diskmgmt.msc as Administrator.

There should now be a 4 GiB unallocated space after the "(C:)" partition.

Boot into Arch Linux or the Arch Linux installation medium to proceed to creating a new partition.

First, make sure to backup the contents of the EFI system partition. For example, with esp being its mountpoint:

Unmount the EFI system partition:

Run blkid and take note of the UUID and PARTUUID values. They will later be reused for the new partition.

Delete the old partition using sgdisk from gptfdisk:

Create a new partition in the largest unallocated space while reusing the old PARTUUID and PARTLABEL:

Tell the kernel to reread the partition table using partprobe(8) from parted:

Confirm the new, 4 GiB in size, EFI system partition is created by listing the partitions with fdisk:

Partition numbers are not resorted when deleting and creating partitions, so the EFI system partition number on the disk will likely be the same as before.

Format the partition to FAT32 reusing the old UUID (while removing the dash from it):

Finally, mount the new partition and restore its contents from backup:

If you previously stopped esp.automount, start it again.

If there is a swap partition right after the EFI system partition, you can sacrifice it to give space for enlarging the EFI system partition. E.g. with a layout similar to:

First, deactivate the swap partition and remove it from fstab.

Use fdisk to delete the swap partition and enlarge the EFI system partition.

After the partition is resized, you need to resize the file system in it. Since fatresize(1) does not work and libparted cannot resize FAT volumes of certain sizes, the only option is to backup the files from the existing file system and create a new one that takes up all space of the partition.

Take note of the file system UUID to allow reusing it for the new file system:

Backup the contents of the EFI system partition. For example, with esp being its mountpoint:

Unmount the EFI system partition:

Wipe the file system signature from the partition to avoid any artifacts from the old file system:

Format the partition to FAT32 reusing the old UUID (while removing the dash from it):

Finally, mount the new partition and restore its contents from backup:

If you previously stopped esp.automount, start it again.

Now that the swap partition is gone, set up swap on a swap file.

It is possible to make the ESP part of a RAID1 array, but doing so brings the risk of data corruption, and further considerations need to be taken when creating the ESP. See [9] and [10] for details and UEFI booting and RAID1 for an in-depth guide with a solution.

The key part is to use --metadata 1.0 in order to keep the RAID metadata at the end of the partition, otherwise the firmware will not be able to access it:

Alternatively, as the ESP is not often updated, a secondary ESP can be managed by copying the primary ESP to the secondary one on a different disk during relevant updates. A boot entry for the secondary ESP can then be added manually using efibootmgr. See Debian:UEFI#RAID for the EFI System Partition for an implementation example. Note that while this avoids some risks of the RAID approach, it only works when using a single OS.

If you give the FAT file system a volume name (i.e. file system label), be sure to name it something other than EFI. That can trigger a bug in some firmwares (due to the volume name matching the EFI directory name) that will cause the firmware to act like the EFI directory does not exist.

If you are running a multi boot system (including but not limited to dual booting with Windows) and want to be able to boot into your other system while your main Arch Linux is hibernated, you must take extra caution not to mount the ESP in both systems, as this will likely cause data corruption and I/O errors on usage.

There are three possible mitigation strategies:

By successfully applying one of the mitigation strategies above you may hibernate this system, but not the other systems unless you apply one of the mitigation strategies to them as well. For example, applying one of the solutions to your main Arch Linux allows you to hibernate Arch Linux and boot into another e.g. Ubuntu Linux, but not to hibernate that Ubuntu Linux and boot into your main Arch Linux. Allowing this with Windows requires separate EFI system partitions.

Strictly speaking, this issue of mounting the same filesystem multiple times at once is neither limited to the EFI system partition, nor to hibernating the system. However, it is particularly relevant for the ESP, because the ESP is expected to be shared across multiple systems. The mitigation strategies can be adapted for other use cases as well.

If you choose option 3 above, you can create and enable the following systemd system service that unmounts the ESP before hibernating the system and remounts it after resuming:

However, since efi.mount is pulled in as a requirement for local-fs.target by default, stopping efi.mount also permanently brings down local-fs.target. This might have all sorts of negative side-effects, including ordering issues when shutting down the system, which might trigger systemd to auto-mask efi.mount as defective. This can be mitigated by telling systemd that efi.mount is not required by local-fs.target, but just wanted by it. To ensure this you must add the following mount options to /efi in /etc/fstab:

**Examples:**

Example 1 (unknown):
```unknown
# fdisk -l /dev/sdx
```

Example 2 (unknown):
```unknown
Disklabel type: gpt
```

Example 3 (unknown):
```unknown
Disklabel type: dos
```

Example 4 (unknown):
```unknown
EFI (FAT-12/16/32)
```

---

## Kernel module

**URL:** https://wiki.archlinux.org/title/Kernel_module

**Contents:**
- Obtaining information
- Automatic module loading
  - Early module loading
  - systemd
- Manual module handling
- Setting module options
  - Using modprobe
  - Using modprobe.d
  - Using kernel command line
- Aliasing

Kernel modules are pieces of code that can be loaded and unloaded into the kernel upon demand. They extend the functionality of the kernel without the need to reboot the system.

To create a kernel module, you can read The Linux Kernel Module Programming Guide. A module can be configured as built-in or loadable. To dynamically load or remove a module, it has to be configured as a loadable module in the kernel configuration (the line related to the module will therefore display the letter M).

To rebuild a kernel module automatically when a new kernel is installed, see Dynamic Kernel Module Support (DKMS).

Usually modules depend on the kernel release and are stored in the /usr/lib/modules/kernel_release/ directory.

To show what kernel modules are currently loaded:

To show information about a module:

To list the options that are set for a loaded module use systool(1) from sysfsutils:

To display the comprehensive configuration of all the modules:

To display the configuration of a particular module:

List the dependencies of a module (or alias), including the module itself:

Today, all necessary modules loading is handled automatically by udev, so if you do not need to use any out-of-tree kernel modules, there is no need to put modules that should be loaded at boot in any configuration file. However, there are cases where you might want to load an extra module during the boot process, or blacklist another one for your computer to function properly.

Early module loading depends on the initramfs generator used:

Kernel modules can be explicitly listed in files under /etc/modules-load.d/ for systemd to load them during boot. Each configuration file is named in the style of /etc/modules-load.d/program.conf. Configuration files simply contain a list of kernel modules names to load, separated by newlines. Empty lines and lines whose first non-whitespace character is # or ; are ignored.

See modules-load.d(5) for more details.

Kernel modules are handled by tools provided by the kmod package, which is installed as a dependency of a kernel package. You can use these tools manually. To load a module:

To load a module by a file name—i.e. one that is not installed in the /usr/lib/modules/kernel_release/ directory—use any of:

To unload—remove—a module, use any of:

To pass a parameter to a kernel module, you can pass them manually with modprobe or assure certain parameters are always applied using a modprobe configuration file or by using the kernel command line. If the module is built into the kernel, the kernel command line must be used and other methods will not work.

The basic way to pass parameters to a module is using the modprobe command. Parameters are specified on command line using simple key=value assignments:

Configuration files in the /etc/modprobe.d/ directory can be used to pass module settings to udev, which will use modprobe to manage the loading of the modules during system boot. Files in this directory can have any name, given that they end with the .conf extension. The file name matters, see modprobe.d(5) § CONFIGURATION DIRECTORIES AND PRECEDENCE. To show the effective configuration:

Multiple module parameters are separated by spaces, in turn a parameter can receive a list of values which is separated by commas:

You can also pass options to the module using the kernel command line. This is the only working option for modules built into the kernel. For all common boot loaders, the following syntax is correct:

Simply add this to the appropriate line in your boot loader configuration, as described in Kernel parameters#Boot loader configuration.

Aliases are alternate names for a module. For example: alias my-mod really_long_modulename means you can use modprobe my-mod instead of modprobe really_long_modulename. You can also use shell-style wildcards, so alias my-mod* really_long_modulename means that modprobe my-mod-something has the same effect. Create an alias:

Aliases can be internal—contained in the module itself. Internal aliases are usually used for #Automatic module loading when it is needed by an application, e.g. when the kernel detects a new device. To see the module internal aliases:

To see both configured and internal aliases:

Blacklisting, in the context of kernel modules, is a mechanism to prevent the kernel module from loading. This could be useful if, for example, the associated hardware is not needed, or if loading that module causes problems: for instance there may be two kernel modules that try to control the same piece of hardware, and loading them together would result in a conflict.

Some modules are loaded as part of the initramfs. mkinitcpio -M will print out all automatically detected modules: to prevent the initramfs from loading some of those modules, blacklist them in a .conf file under /etc/modprobe.d and it shall be added in by the modconf hook during image generation. Running mkinitcpio -v will list all modules pulled in by the various hooks (e.g. filesystems hook, block hook, etc.). Remember to add that .conf file to the FILES array in /etc/mkinitcpio.conf if you do not have the modconf hook in your HOOKS array (e.g. you have deviated from the default configuration), and once you have blacklisted the modules regenerate the initramfs, and reboot afterwards.

Disable an alias by overriding. For example, to prevent Bluetooth module autoloading (assuming a module named off does not exist):

To disable all internal aliases for a given module use the blacklist keyword. For example, to prevent the pcspkr module from loading on boot to avoid sounds through the PC speaker:

There is a workaround for the behaviour described in the #alias and #blacklist notes. The install configuration command instructs modprobe to run a custom command instead of inserting the module in the kernel as normal, so you can simulate the successful module loading with:

You can force the module to always fail loading with /bin/false: this will effectively prevent the module—and any other that depends on it—from loading by any means, and a log error message may be produced.

You can also blacklist modules from the boot loader boot entry configuration.

Simply add module_blacklist=module_name_1,module_name_2,module_name_3 to your kernel command line, as described in Kernel parameters#Boot loader configuration.

Another use case for a command line option is to disable hardware-specific components of a module without disabling the module entirely. For example, disabling a microphone while retaining other sound out options. See BBS#303475 for a few examples.

In case a specific module does not load and the boot log (accessible by running journalctl -b as root) says that the module is blacklisted, but the directory /etc/modprobe.d/ does not show a corresponding entry, check another modprobe source directory at /usr/lib/modprobe.d/ for blacklisting entries.

A module will not be loaded if the "vermagic" string contained within the kernel module does not match the value of the currently running kernel. If it is known that the module is compatible with the current running kernel the "vermagic" check can be ignored with modprobe --force-vermagic.

**Examples:**

Example 1 (unknown):
```unknown
/usr/lib/modules/kernel_release/
```

Example 2 (unknown):
```unknown
uname --kernel-release
```

Example 3 (unknown):
```unknown
/etc/modprobe.d/
```

Example 4 (unknown):
```unknown
$ modinfo module_name
```

---

## Dropbear

**URL:** https://wiki.archlinux.org/title/Dropbear

**Contents:**
- Installation
- Configuration
  - Disable password and root logins
  - Set SSH key from GitLab
- Dropbear in Docker container

Dropbear is lightweight SSH server that is commonly run on routers and other low memory devices. It is also often configured to run during the boot process.

Install the dropbear package.

Dropbear server does not have any configuration file. It is configured with command line options.

Enable/start dropbear.service.

Edit the dropbear.service. This creates a new drop-in file. Adding content to it will override corresponding sections in main configuration.

Override the command to start Dropbear:

Change GitLab URL to your own.

This article or section is being considered for removal.

systemctl does not work in `archlinux` Docker container.

You need to start Dropbear manually.

**Examples:**

Example 1 (unknown):
```unknown
dropbear.service
```

Example 2 (unknown):
```unknown
dropbear.service
```

Example 3 (unknown):
```unknown
[Service]
ExecStart=
ExecStart=/usr/bin/dropbear -F -P /run/dropbear.pid -R -w -s
```

Example 4 (unknown):
```unknown
$ mkdir -p ~/.ssh -m 0700
$ curl -sSLf https://gitlab.com/example.keys >> ~/.ssh/authorized_keys
```

---

## Users and groups

**URL:** https://wiki.archlinux.org/title/User

**Contents:**
- Overview
- Permissions and ownership
- Shadow
- File list
- User management
  - Example adding a user
    - Changing user defaults
  - Example adding a system user
  - Change a user's login name or home directory
  - Other examples of user management

Users and groups are used on GNU/Linux for access control—that is, to control access to the system's files, directories, and peripherals. Linux offers relatively simple/coarse access control mechanisms by default. For more advanced options, see ACL, Capabilities and PAM#Configuration How-Tos.

A user is anyone who uses a computer. In this case, we are describing the names which represent those users. It may be Mary or Bill, and they may use the names Dragonlady or Pirate in place of their real name. All that matters is that the computer has a name for each account it creates, and it is this name by which a person gains access to use the computer. Some system services also run using restricted or privileged user accounts.

Managing users is done for the purpose of security by limiting access in certain specific ways. The superuser (root) has complete access to the operating system and its configuration; it is intended for administrative use only. Unprivileged users can use several programs for controlled privilege elevation.

Any individual may have more than one account as long as they use a different name for each account they create. Further, there are some reserved names which may not be used such as "root".

Users may be grouped together into a "group", and users may be added to an existing group to utilize the privileged access it grants.

From In UNIX Everything is a File:

From Extending UNIX File Abstraction for General-Purpose Networking:

Every file on a GNU/Linux system is owned by a user and a group. In addition, there are three types of access permissions: read, write, and execute. Different access permissions can be applied to a file's owning user, owning group, and others (those without ownership). One can determine a file's owners and permissions by viewing the long listing format of the ls command:

The first column displays the file's permissions (for example, the file initramfs-linux.img has permissions -rw-r--r--). The third and fourth columns display the file's owning user and group, respectively. In this example, all files are owned by the root user and the root group.

In this example, the sf_Shared directory is owned by the root user and the vboxsf group. It is also possible to determine a file's owners and permissions using the stat command:

Access permissions are displayed in three groups of characters, representing the permissions of the owning user, owning group, and others, respectively. For example, the characters -rw-r--r-- indicate that the file's owner has read and write permission, but not execute (rw-), whilst users belonging to the owning group and other users have only read permission (r-- and r--). Meanwhile, the characters drwxrwx--- indicate that the file's owner and users belonging to the owning group all have read, write, and execute permissions (rwx and rwx), whilst other users are denied access (---). The first character represents the file's type.

List files owned by a user or group with the find utility:

A file's owning user and group can be changed with the chown (change owner) command. A file's access permissions can be changed with the chmod (change mode) command.

See chown(1), chmod(1), and Linux file permissions for additional detail.

The user, group and password management tools on Arch Linux come from the shadow package, which is a dependency of the base meta package.

To list users currently logged on the system, the who command can be used. To list all existing user accounts including their properties stored in the user database, run passwd -Sa as root. See passwd(1) for the description of the output format.

To add a new user, use the useradd command:

If an initial login group is specified by name or number, it must refer to an already existing group. If not specified, the behaviour of useradd will depend on the USERGROUPS_ENAB variable contained in /etc/login.defs. The default behaviour (USERGROUPS_ENAB yes) is to create a group with the same name as the username.

When the login shell is intended to be non-functional, for example when the user account is created for a specific service, /usr/bin/nologin may be specified in place of a regular shell to politely refuse a login (see nologin(8)).

See useradd(8) for other supported options.

To add a new user named archie, creating its home directory and otherwise using all the defaults in terms of groups, directory names, shell used and various other parameters:

Although it is not required to protect the newly created user archie with a password, it is highly recommended to do so:

The above useradd command will also automatically create a group called archie and makes this the default group for the user archie. Making each user have their own group (with the group name same as the user name) is the preferred way to add users.

You could also make the default group something else using the -g option, but note that, in multi-user systems, using a single default group (e.g. users) for every user is not recommended. The reason is that typically, the method for facilitating shared write access for specific groups of users is setting user umask value to 002, which means that the default group will by default always have write access to any file you create. See also User Private Groups. If a user must be a member of a specific group specify that group as a supplementary group when creating the user.

In the recommended scenario, where the default group has the same name as the user name, all files are by default writeable only for the user who created them. To allow write access to a specific group, shared files/directories can be made writeable by default for everyone in this group and the owning group can be automatically fixed to the group which owns the parent directory by setting the setgid bit on this directory:

Otherwise the file creator's default group (usually the same as the user name) is used.

If a GID change is required temporarily you can also use the newgrp command to change the user's default GID to another GID at runtime. For example, after executing newgrp groupname files created by the user will be associated with the groupname GID, without requiring a re-login. To change back to the default GID, execute newgrp without a groupname.

The default values used for creating new accounts are set in /etc/default/useradd and can be displayed using useradd --defaults. For example, to change the default shell globally, set SHELL=/usr/bin/shell. A different shell can also be specified individually with the -s/--shell option. Use chsh -l to list valid login shells.

Files can also be specified to be added to newly created user home directories in /etc/skel. This is useful for minimalist window managers where config files need manual configuration to reach DE-familiar behavior. For example, to set up default shortcuts for all newly created users:

See also: Display manager#Run ~/.xinitrc as a session to add xinitrc as an option to all users on the display manager.

System users can be used to run processes/daemons under a different user, protecting (e.g. with chown) files and/or directories and more examples of computer hardening.

With the following command a system user without shell access and without a home directory is created (optionally append the -U parameter to create a group with the same name as the user, and add the user to this group):

If the system user requires a specific user and group ID, specify them with the -u/--uid and -g/--gid options when creating the user:

To change a user's home directory:

The -m option also automatically creates the new directory and moves the content there.

Make sure there is no trailing / on /my/old/home.

To change a user's login name:

Changing a username is safe and easy when done properly, just use the usermod command. If the user is associated to a group with the same name, you can rename this with the groupmod command.

Alternatively, the /etc/passwd file can be edited directly, see #User database for an introduction to its format.

Also keep in mind the following notes:

To enter user information for the GECOS comment (e.g. the full user name), type:

(this way chfn runs in interactive mode).

Alternatively the GECOS comment can be set more liberally with:

To mark a user's password as expired, requiring them to create a new password the first time they log in, type:

User accounts may be deleted with the userdel command:

The -r option specifies that the user's home directory and mail spool should also be deleted.

To change the user's login shell:

Local user information is stored in the plain-text /etc/passwd file: each of its lines represents a user account, and has seven fields delimited by colons.

Broken down, this means: user archie, whose password is in /etc/shadow, whose UID is 1001 and whose primary group is 1003. Archie is their full name and there is a comment associated to their account; their home directory is /home/archie and they are using Bash.

The pwck command can be used to verify the integrity of the user database. It can sort the user list by UID at the same time, which can be helpful for comparison:

Instead of running pwck/grpck manually, the systemd timer shadow.timer, which is part of, and is enabled by, installation of the shadow package, will start shadow.service daily. shadow.service will run pwck(8) and grpck(8) to verify the integrity of both password and group files.

If discrepancies are reported, group can be edited with the vigr(8) command and users with vipw(8). This provides an extra margin of protection in that these commands lock the databases for editing. Note that the default text editor is vi, but an alternative editor will be used if the EDITOR environment variable is set, then that editor will be used instead.

As packages adopt the change-sysusers-to-fully-locked-system-accounts, package created system users generated in the past will not inherit new package defaults for the increased security of locked/expired status. These users need to be modified manually for this change. user-analysis.sh does just that.

In addition, the script also identifies orphaned users (those created by a package no longer on the system) and can automatically delete them.

/etc/group is the file that defines the groups on the system (see group(5) for details). There is also its companion gshadow which is rarely used. Its details are at gshadow(5).

Display group membership with the groups command:

If user is omitted, the current user's group names are displayed.

The id command provides additional detail, such as the user's UID and associated GIDs:

To list all groups on the system:

Create new groups with the groupadd command:

Add users to a group with the gpasswd command (see FS#58262 regarding errors):

Alternatively, add a user to additional groups with usermod (replace additional_groups with a comma-separated list):

Modify an existing group with the groupmod command, e.g. to rename the old_group group to new_group:

To delete existing groups:

To remove users from a group:

The grpck command can be used to verify the integrity of the system's group files.

This section explains the purpose of the essential groups from the filesystem package. There are many other groups, which will be created with correct GID when the relevant package is installed. See the main page for the software for details.

Non-root workstation/desktop users often need to be added to some of following groups to allow access to hardware peripherals and facilitate system administration:

The following groups are used for system purposes, an assignment to users is only required for dedicated purposes:

Before arch migrated to systemd, users had to be manually added to these groups in order to be able to access the corresponding devices. This way has been deprecated in favour of udev marking the devices with a uaccess tag and logind assigning the permissions to users dynamically via ACLs according to which session is currently active. Note that the session must not be broken for this to work (see General troubleshooting#Session permissions to check it).

There are some notable exceptions which require adding a user to some of these groups: for example if you want to allow users to access the device even when they are not logged in. However, note that adding users to the groups can even cause some functionality to break (for example, the audio group will break fast user switching and allows applications to block software mixing).

Now solely for direct access to tapes if no custom udev rules is involved.[5][6][7].

Also required for manipulating some devices via udisks/udisksctl.

The following groups are currently not used for any purpose:

This article or section is a candidate for merging with #Shadow.

The factual accuracy of this article or section is disputed.

getent(1) can be used to read a particular record.

As warned in #User database, using specific utilities such as passwd and chfn, is a better way to change the databases. Nevertheless, there are times when editing them directly is looked after. For those times, vipw, vigr are provided. It is strongly recommended to use these tailored editors over using a general text editor as they lock the databases against concurrent editing. They also help prevent invalid entries and/or syntax errors. Note that Arch Linux prefers usage of specific tools, such as chage, for modifying the shadow database over using vipw -s and vigr -s from util-linux. See also FS#31414.

lslogins(1) and lastlog2(1) are some of the tools for viewing utmp related files.

**Examples:**

Example 1 (unknown):
```unknown
cat /dev/audio > myfile
```

Example 2 (unknown):
```unknown
cat myfile > /dev/audio
```

Example 3 (unknown):
```unknown
$ ls -l /boot/
```

Example 4 (unknown):
```unknown
total 13740
drwxr-xr-x 2 root root    4096 Jan 12 00:33 grub
-rw-r--r-- 1 root root 8570335 Jan 12 00:33 initramfs-linux-fallback.img
-rw-r--r-- 1 root root 1821573 Jan 12 00:31 initramfs-linux.img
-rw-r--r-- 1 root root 1457315 Jan  8 08:19 System.map26
-rw-r--r-- 1 root root 2209920 Jan  8 08:19 vmlinuz-linux
```

---

## File systems

**URL:** https://wiki.archlinux.org/title/File_system

**Contents:**
- Types of file systems
  - Journaling
  - FUSE-based file systems
  - Stackable file systems
  - Read-only file systems
  - Clustered file systems
  - Shared-disk file system
- Identify existing file systems
- Create a file system
- Mount a file system

Individual drive partitions can be set up using one of the many different available file systems. Each has its own advantages, disadvantages, and unique idiosyncrasies. A brief overview of supported file systems follows; the links are to Wikipedia pages that provide much more information.

See filesystems(5) for a general overview and Wikipedia:Comparison of file systems for a detailed feature comparison. File systems already loaded by the kernel or built-in are listed in /proc/filesystems, while all the installed modules can be seen with ls /lib/modules/$(uname -r)/kernel/fs.

xfs.html xfs-delayed-logging-design.html xfs-self-describing-metadata.html

The ext3/4, HFS+, JFS, NTFS, ReiserFS, and XFS file systems use journaling. Journaling provides fault-resilience by logging changes before they are committed to the file system. In the event of a system crash or power failure, such file systems are faster to bring back online and less likely to become corrupted. The logging takes place in a dedicated area of the file system.

ext3/4 offer data-mode journaling, which can optionally log data in addition to the metadata. Data-mode journaling comes with a speed penalty, because it does two write operations: first to the journal and then to the disk. Therefore, data-mode journaling is not enabled by default. The trade-off between system speed and data safety should be considered when choosing the file system type and features.

In the same vein, Reiser4 offers configurable "transaction models": a special model called wandering logs, which eliminates the need to write to the disk twice; write-anywhere, a pure copy-on-write approach; and a combined approach called hybrid which heuristically alternates between the two.

File systems based on copy-on-write (also known as write-anywhere), such as Reiser4, Btrfs, Bcachefs and ZFS, by design operate on full atomicity and also provide checksums for both metadata and inline data (operations entirely occur, or they entirely do not, and in properly functioning hardware data does not corrupt due to operations half-occurring). Therefore, these file systems are by design much less prone to data loss than other file systems and have no need to use traditional journal to protect metadata, because they are never updated in-place. Although Btrfs still has a journal-like log tree, it is only used to speed-up fdatasync/fsync.

FAT, exFAT, ext2, and HFS provide neither journaling nor atomicity, They are for temporary or legacy use and not recommended for use when reliable storage is needed.

To identify existing file systems, you can use lsblk:

An existing file system, if present, will be shown in the FSTYPE column. If mounted, it will appear in the MOUNTPOINT column.

File systems are usually created on a partition, inside logical containers such as LVM, RAID and dm-crypt, or on a regular file (see Wikipedia:Loop device). This section describes the partition case.

Before continuing, identify the device where the file system will be created and whether or not it is mounted. For example:

Mounted file systems must be unmounted before proceeding. In the above example an existing file system is on /dev/sda2 and is mounted at /mnt. It would be unmounted with:

To find just mounted file systems, see #List mounted file systems.

To create a new file system, use mkfs(8). See #Types of file systems for the exact type, as well as userspace utilities you may wish to install for a particular file system.

For example, to create a new file system of type ext4 (common for Linux data partitions) on /dev/sda1, run:

The new file system can now be mounted to a directory of choice.

To manually mount a file system located on a device (e.g., a partition) to a directory, use mount(8). This example mounts /dev/sda1 to /mnt.

This attaches the file system on /dev/sda1 at the directory /mnt, making the contents of the file system visible. Any data that existed at /mnt before this action is made invisible until the device is unmounted.

fstab contains information on how devices should be automatically mounted if present. See the fstab article for more information on how to modify this behavior.

If a device is specified in /etc/fstab and only the device or mount point is given on the command line, that information will be used in mounting. For example, if /etc/fstab contains a line indicating that /dev/sda1 should be mounted to /mnt, then the following will automatically mount the device to that location:

mount contains several options, many of which depend on the file system specified. The options can be changed, either by:

See these related articles and the article of the file system of interest for more information.

To list all mounted file systems, use findmnt(8):

findmnt takes a variety of arguments which can filter the output and show additional information. For example, it can take a device or mount point as an argument to show only information on what is specified:

findmnt gathers information from /etc/fstab, /etc/mtab, and /proc/self/mounts.

To unmount a file system use umount(8). Either the device containing the file system (e.g., /dev/sda1) or the mount point (e.g., /mnt) can be specified:

Unmount the file system and run fsck on the problematic volume.

**Examples:**

Example 1 (unknown):
```unknown
/proc/filesystems
```

Example 2 (unknown):
```unknown
ls /lib/modules/$(uname -r)/kernel/fs
```

Example 3 (unknown):
```unknown
NAME   FSTYPE LABEL     UUID                                 MOUNTPOINT
sdb
└─sdb1 vfat   Transcend 4A3C-A9E9
```

Example 4 (unknown):
```unknown
NAME   FSTYPE   LABEL       UUID                                 MOUNTPOINT
sda
├─sda1                      C4DA-2C4D
├─sda2 ext4                 5b1564b2-2e2c-452c-bcfa-d1f572ae99f2 /mnt
└─sda3                      56adc99b-a61e-46af-aab7-a6d07e504652
```

---

## WirePlumber

**URL:** https://wiki.archlinux.org/title/WirePlumber

**Contents:**
- Installation
- Configuration
  - Configuration file layout
  - Modifying the configuration
  - Obtain interface name for rules matching
  - Changing a device/node property
  - Disable a device/node
  - Setting node priority
  - Simultaneous output to multiple sinks on the same sound card
  - Simultaneous output to transient devices

WirePlumber is a powerful session and policy manager for PipeWire. Based on a modular design, with Lua plugins that implement the actual management functionality, it is highly configurable and extendable.

Install the wireplumber package. Any conflicting PipeWire Session Managers will be uninstalled.

WirePlumber uses systemd user units to manage the server.

WirePlumber's configuration comprises global PipeWire-flavored JSON objects such as context and alsa_monitor that are modified to change its behavior. The configuration files are read from ~/.config/wireplumber/ (user configuration), /etc/wireplumber/ (global configuration), and then /usr/share/wireplumber/ (stock configuration).

WirePlumber starts by reading the main configuration file. This is a JSON-like file that sets up the PipeWire context, SPA plugins, modules, and components.

The single-instance configuration file at /usr/share/wireplumber/wireplumber.conf is the default configuration, which includes the functionality of all the other configurations within one process. See the documentation for the ALSA objects, "access" objects and Bluetooth objects.

The recommended way to configure WirePlumber is to add an SPA-JSON file to the appropriate wireplumber.conf.d/ directory within /etc/wireplumber/ or ~/.config/wireplumber/.

User configuration files have a higher priority than system files. Configuration files with the same name but in a lower priority location will be ignored. Within each configuration directory, the individual files are opened in alphanumerical order. See the documentation for details.

In the configuration, you need to specify matches rules with a property from an object of the interface you want to configure.

Use the command wpctl status to show all objects managed by WirePlumber. Find the ID assigned to the target interface. Then use command wpctl inspect ID to get a needed property.

For example, if your target interface is HD Audio Controller Analog Stereo, consider the following output:

the interface ID is 48.

Then use the inspect command to view the object's detail and list all properties in that object:

Choose the device.name or node.name property to use with the matches rules in the configuration.

Avoid using device.id; it is dynamic and changes often.

Multiple properties in matches rules are possible; see the alsa_monitor.rules section in the documentation for the WirePlumber ALSA configuration.

To change a device or node property, such as its description or nick, you must create an SPA-JSON file and add it into /etc/wireplumber/ or ~/.config/wireplumber/ under the proper path and name.

For instance, to change the description of an ALSA node, you would create a file such as:

If instead you wish to change something on a Bluetooth node or device, you could create a file such as:

The properties that you can change as well as the matching rules to select devices or nodes are documented at ALSA configuration and Bluetooth configuration.

Since WirePlumber v0.4.7, users could now disable any devices or nodes by property device.disabled or node.disabled:

For the name of alsa_card.* in your system, see #Obtain interface name for rules matching.

To change which sink or source is automatically selected, you need to set its priority.driver and priority.session values:

This example sets the priority of the speaker output sink of the on-board audio on the Asus B650E-F motherboard to 100, lowered from the original value of 1000. Consequently, the other output sinks, such as the S/PDIF output of the onboard audio device or the plugged in headphones, will be selected by default due to their higher priorities, instead of the analog speaker output.

See PipeWire#Simultaneous output to multiple sinks on the same sound card.

You may wish to output sound to onboard and external devices simultaneously - even when the external devices are not always plugged in. To accomplish this we create a virtual node that will always be present, regardless of what hardware is plugged in. We then link the transient hardware (in this example USB headphones) to the virtual node whenever they are plugged in.

First, create a script to run during login; this is usually easiest to do via your window manager's Startup function.

In the above example, initially the only output device is our 'normal' on-board soundcard (alsa_output.pci-0000_05_04.0.analog-stereo). You can find the designator for your card by running wpctl status and wpctl inspect.

Run the following script when your USB headphones are inserted in order to link them to the virtual sink:

Ideally you would run this script automatically when your headphones are inserted. The instructions on the udev page describe how you would create a custom rule for that. (Although note that you cannot run this script directly - because udev will not load drivers until after any specified script has run. So you will have to have an intermediate script with some nohup trickery or something like that). You will also need to modify the above script so that the XDG_RUNTIME_DIR matches your user id number and user1 will need to be replaced with your username.

You can add any arbitrary number of devices to this virtual sink in the same manner.

If you are having trouble keeping track what devices are connected where, the qpwgraph tool is excellent for getting a visual representation of which devices are connected to each other.

If the settings of WirePlumber are corrupted it is possible to delete all user settings. Stop the wireplumber.service user unit, delete the settings with:

Then, you can start the wireplumber.service user unit back.

See Keyboard shortcuts#Xorg to bind the following commands to your volume keys: XF86AudioRaiseVolume, XF86AudioLowerVolume, XF86AudioMute and XF86AudioMicMute.

To raise the volume, with a limit of 150%:

To mute/unmute the volume:

To mute/unmute the microphone:

To get the volume level and mute status of the default sink:

See PipeWire#Automatic profile selection.

Since 0.4.8 the requirement to support.logind has to be disabled for the bluez seat-monitoring.

If you do not want PipeWire/Wireplumber to take over control of your audio devices because you are opting for a different audio solution (e.g. PulseAudio/JACK/ALSA) but still want it to be available for screen sharing/video purposes you can make use of the wireplumber@.service template unit to enable a different set of default profiles. Wireplumber ships with a profile configuration that enables only the video parts and disables audio integration (including Bluetooth audio) by enabling the video-only template user unit.

Disable wireplumber.service user unit and enable wireplumber@video-only.service user unit.

Some applications (e.g. launchers) play sound too briefly to adjust their volume using GUI mixers. WirePlumber stores per-application and per-node volume and mute states in a text file at $HOME/.local/state/wireplumber/stream-properties. This file can be edited manually to change these settings. Ensure WirePlumber is not running while editing, or it may overwrite the changes.

Since WirePlumber only exists to manage PipeWire sessions, WirePlumber-related fixes may be found in PipeWire#Troubleshooting.

**Examples:**

Example 1 (unknown):
```unknown
alsa_monitor
```

Example 2 (unknown):
```unknown
~/.config/wireplumber/
```

Example 3 (unknown):
```unknown
/etc/wireplumber/
```

Example 4 (unknown):
```unknown
/usr/share/wireplumber/
```

---

## systemd/User

**URL:** https://wiki.archlinux.org/title/Systemd/User

**Contents:**
- How it works
- Basic setup
  - Environment variables
    - systemd user instance
    - Service example
    - Re-using the shell login environment
    - DISPLAY and XAUTHORITY
    - PATH
    - pam_env
  - Automatic start-up of systemd user instances

systemd offers the ability to manage services under the user's control with a per-user systemd instance, enabling them to start, stop, enable, and disable their own user units. This is convenient for daemons and other services that are commonly run for a single user, such as mpd, or to perform automated tasks like fetching mail.

As per default configuration in /etc/pam.d/system-login, the pam_systemd module automatically launches a systemd --user instance when the user logs in for the first time. This process will survive as long as there is some session for that user, and will be killed as soon as the last session for the user is closed. When #Automatic start-up of systemd user instances is enabled, the instance is started on boot and will not be killed. The systemd user instance is responsible for managing user services, which can be used to run daemons or automated tasks with all the benefits of systemd, such as socket activation, timers, dependency system, and strict process control via cgroups.

Similar to system units, user units are located in the following directories (ordered by ascending precedence):

When a systemd user instance starts, it brings up the per user target default.target. Other units can be controlled manually with systemctl --user. See systemd.special(7) § UNITS MANAGED BY THE USER SERVICE MANAGER.

All the user units will be placed in ~/.config/systemd/user/. If you want to start units on first login, execute systemctl --user enable unit for any unit you want to be autostarted.

Units started by user instance of systemd do not inherit any of the environment variables set in places like .bashrc etc. There are several ways to set environment variables for them:

One variable you may want to set is PATH.

After configuration, the command systemctl --user show-environment can be used to verify that the values are correct. You may need to run systemctl --user daemon-reload for changes to take effect immediately.

The above only addresses default environment variables for user units. However, the systemd user instance itself is also affected by some environment variables. In particular, certain specifiers (see systemd.unit(5) § SPECIFIERS) are affected by XDG variables.

However, the systemd user instance will only use environment variables that are set when it is started. In particular, it will not try parsing files, see upstream bug #29414 (closed WONTFIX). Therefore, if such environment variables are needed, they should be set in a drop-in configuration file, see #Service example.

systemd does not provide introspection tools to check these values, however, something like the following service can be used to help checking that the specifiers expand as expected:

Create the drop-in directory /etc/systemd/system/user@.service.d/ and inside create a file that has the extension .conf (e.g. local.conf):

If you normally set your environment through the shell login mechanisms (i.e. in ~/.profile, ~/.bash_profile, ~/.zprofile, or similar), the shell login environment can be read into a systemd user instance using the systemd.environment-generator(7) logic (as above). Create the following script:

The script invokes your $SHELL as a login shell, and dumps the resulting environment, while removing ephemeral shell variables. This is executed only once, on manager start, and can be reloaded on demand, using systemctl --user daemon-reload.

It provides the same environment block one gets with a non-interactive login shell — the same environment one would see after loging in through Getty or SSH, but not including anything set in ~/.bashrc, ~/.zshrc, and friends — including the system-wide environment from /etc/profile and /etc/profile.d. This is similar to what e.g. gnome-shell does, which is starting a login shell, and updating systemd with the resulting environment.

DISPLAY is used by any X application to know which display to use and XAUTHORITY to provide a path to the user's .Xauthority file and thus the cookie needed to access the X server. If you plan on launching X applications from systemd units, these variables need to be set. systemd provides a script in /etc/X11/xinit/xinitrc.d/50-systemd-user.sh to import those variables into the systemd user session on X launch. [3] So unless you start X in a nonstandard way, user services should be aware of the DISPLAY and XAUTHORITY.

If you customize your PATH and plan on launching applications that make use of it from systemd units, you should make sure the modified PATH is set on the systemd environment. Assuming you set your PATH in .bash_profile, the best way to make systemd aware of your modified PATH is by adding the following to .bash_profile after the PATH variable is set:

Environment variables can be made available through use of the pam_env.so module. See Environment variables#Using pam_env for configuration details.

The systemd user instance is started after the first login of a user and killed after the last session of the user is closed. Sometimes it may be useful to start it right after boot, and keep the systemd user instance running after the last session closes, for instance to have some user process running without any open session. Lingering is used to that effect. Use the following command to enable lingering for your own user, if polkit is installed:

Without polkit or to enable lingering for a different user:

To list all users which have the permit for lingering see column "LINGER" with yes:

or inspect /var/lib/systemd/linger. To revoke lingering:

See systemd#Writing unit files for general information about writing systemd unit files.

The following is an example of a user version of the mpd service:

The factual accuracy of this article or section is disputed.

The following is a user service used by foldingathomeAUR, which takes into account variable home directories where Folding@home can find certain files:

As detailed in systemd.unit(5) § SPECIFIERS, the %h variable is replaced by the home directory of the user running the service. There are other variables that can be taken into account in the systemd manpages.

The journal for the user can be read using the analogous command:

To specify a unit, one can use

systemd-tmpfiles allows users to manage custom volatile and temporary files and directories just like in the system-wide way (see systemd#systemd-tmpfiles - temporary files). User-specific configuration files are read from ~/.config/user-tmpfiles.d/ and ~/.local/share/user-tmpfiles.d/, in that order. For this functionality to be used, it is needed to enable the necessary systemd user units for your user:

The syntax of the configuration files is the same than those used system-wide. See the systemd-tmpfiles(8) and tmpfiles.d(5) man pages for details.

This article or section needs expansion.

There are several ways to run xorg within systemd units. Below there are 3 options, either by starting a new user session with an xorg process, launching xorg from a systemd user service, or launching xinit and application as a service.

Alternatively, xorg can be run from within a systemd user service. This is nice since other X-related units can be made to depend on xorg, etc, but on the other hand, it has some drawbacks explained below.

xorg-server provides integration with systemd in two ways:

Unfortunately, to be able to run xorg in unprivileged mode, it needs to run inside a session. So, right now the handicap of running xorg as user service is that it must be run with root privileges (like before 1.16), and cannot take advantage of the unprivileged mode introduced in 1.16.

This is how to launch xorg from a user service:

1. Make xorg run with root privileges for any user, by editing /etc/X11/Xwrapper.config. This builds on Xorg#Xorg as Root by adding the stipulation that this need not be done from a physical console. That is, allowed_user's default of console is being overwritten with anybody; see Xorg.wrap(1).

2. Add the following units to ~/.config/systemd/user

where ${XDG_VTNR} is the virtual terminal where xorg will be launched, either hard-coded in the service unit, or set in the systemd environment with

3. Make sure to configure the DISPLAY environment variable as explained above.

4. Then, to enable socket activation for xorg on display 0 and tty 2 one would do:

Now running any X application will launch xorg on virtual terminal 2 automatically.

The environment variable XDG_VTNR can be set in the systemd environment from .bash_profile, and then one could start any X application, including a window manager, as a systemd unit that depends on xorg@0.socket.

The factual accuracy of this article or section is disputed.

The service below is an example to run xinit and mate-session with user privilege.

To run a window manager as a systemd service, you first need to run #Xorg as a systemd user service. In the following we will use awesome as an example:

Rather than logging you into a window manager session for your user session by default, you may want to automatically run a terminal multiplexer (such as screen or tmux) in the background.

Create the following:

Separating login from X login is most likely only useful for those who boot to a TTY instead of to a display manager (in which case you can simply bundle everything you start in mystuff.target).

The dependency cruft.target, like the mystuff.target above, allows starting anything which should run before the multiplexer starts (or which you want started at boot regardless of timing), such as a GnuPG daemon session.

You then need to create a service for your multiplexer session. Here is a sample service, using tmux as an example and sourcing a gpg-agent session which wrote its information to /tmp/gpg-agent-info. This sample session, when you start X, will also be able to run X programs, since $DISPLAY is set:

Enable tmux.service, multiplexer.target and any services you created to be run by cruft.target, start user@.service as usual and you should be done.

Arch Linux builds the systemd package with --without-kill-user-processes, setting KillUserProcesses to no by default. This setting causes user processes not to be killed when the user logs out. To change this behavior in order to have all user processes killed on the user's logout, set KillUserProcesses=yes in /etc/systemd/logind.conf.

Note that changing this setting breaks terminal multiplexers such as tmux and GNU Screen. If you change this setting, you can still use a terminal multiplexer by using systemd-run as follows:

For example, to run screen you would do:

Using systemd-run will keep the process running after logout only while the user is logged in at least once somewhere else in the system and user@.service is still running.

After the user logs out of all sessions, user@.service will be terminated too, by default, unless the user has "lingering" enabled [9]. To effectively allow users to run long-term tasks even if they are completely logged out, lingering must be enabled for them. See #Automatic start-up of systemd user instances and loginctl(1) for details.

If you see errors such as this and your login session is broken, it is possible that another system (non-user) service on your system is creating this directory. This can happen for example if you use a docker container that has a bind mount to /run/user/1000. To fix this, you can either fix the container by removing the mount, or disable/delay the docker service.

If you see this message during shutdown, usually with a 2 minute timeout, it means that one of the user services did not stop in a timely manner. This can be caused by a misbehaving application which spawned a transient service earlier. You can simply wait for the timeout to expire, but if this bothers you, you can either create an override for the misbehaving service or reduce the global timeout for all user services.

To troubleshoot this problem, start the systemd debug shell:

Then, reboot or shut down the system. When the problem occurs, switch to the debug shell using Ctrl+Alt+F9. To find out which service is preventing the shutdown, run:

For most open source applications, this problem should be reported to the respective maintainers such that an override isn't necessary. For closed source applications, however, an override can be created like so:

This will shorten the timeout of that particular service to 1 second. The --force parameter is only required for transient services which do not create a .service file on disk. The override will work regardless. Instead of the timeout, KillSignal=SIGKILL can be used. This will cause the service to be killed immediately when the user manager is stopped. Only use this if you know the service can handle it.

If you don't care which service is preventing the shutdown, you can change the global timeout for all user services in a similar manner:

After this timeout, any user services which haven't gracefully stopped will be killed, which is equivalent to a sudden power loss. Adjust this value for your particular use case. Setting the timeout too low may cause data corruption depending on the application.

**Examples:**

Example 1 (unknown):
```unknown
/etc/pam.d/system-login
```

Example 2 (unknown):
```unknown
pam_systemd
```

Example 3 (unknown):
```unknown
systemd --user
```

Example 4 (unknown):
```unknown
/usr/lib/systemd/user/
```

---

## OpenNTPD

**URL:** https://wiki.archlinux.org/title/OpenNTPD

**Contents:**
- Installation
- Configuration
  - Client
  - Server
- Usage
  - Start OpenNTPD at boot
  - Making openntpd dependent upon network access
    - Using NetworkManager dispatcher
    - Using dhclient hooks
    - Using dhcpcd hooks

OpenNTPD (part of the OpenBSD project) is a daemon that can be used to synchronize the system clock to internet time servers using the Network Time Protocol, and can also act as a time server itself if needed. It implements the Simple Network Time Protocol version 4, as described in RFC:5905, and the Network Time Protocol version 3, as described in RFC:1305.

Install the openntpd package. The default configuration is actually usable if all you want is to sync the time of the local computer.

To configure OpenNTPD, you need to edit /etc/ntpd.conf. See ntpd.conf(5) for all available options.

This article or section needs expansion.

To sync to a single particular server, edit the server directive.

The servers directive works the same as the server directive. However, if the DNS name resolves to multiple IP address, all of them will be synced to. The default, 2.arch.pool.ntp.org, is working and should be acceptable in most cases. You can find the server's URL in your area at www.pool.ntp.org/zone/@.

Any number of server or servers directives may be used.

If you want the computer you run OpenNTPD on to also be a time server, simply uncomment and edit the "listen" directive.

will listen on all interfaces, and

will only listen on the loopback interface.

Your time server will only begin to serve time after it has synchronized itself to a high resolution. This may take hours, or days, depending on the accuracy of your system.

Enable openntpd.service.

If you have intermittent network access (you roam around on a laptop, you use dial-up, etc.), it does not make sense to have openntpd running as a system daemon on start up. Here are a few ways you can control openntpd based on the presence of a network connection.

OpenNTPD can be brought up/down along with a network connection through the use of NetworkManager's dispatcher scripts.

Install networkmanager-dispatcher-openntpdAUR.

This article or section needs expansion.

Another possibility is to use dhclient hooks to start and stop openntpd. When dhclient detects a change in state, it will run the following scripts:

See dhclient-script(8).

See dhcpcd-run-hooks(8).

If you find your time set incorrectly and in the log, you see:

This is also how you would manually sync your system.

OpenNTPD will fail to start on a system with AppArmor if HTTPS constraints are configured in /etc/ntpd.conf. The journal will show constraint: failed to load constraint ca.

This is because AppArmor's usr.sbin.ntpd profile does not have read access to LibreSSL's CA certificate file /etc/libressl/cert.pem.[1]

The solution is to grant access with a local override:

After editing, reload the AppArmor profile:

**Examples:**

Example 1 (unknown):
```unknown
/etc/ntpd.conf
```

Example 2 (unknown):
```unknown
/etc/ntpd.conf
```

Example 3 (unknown):
```unknown
server time.cloudflare.com
```

Example 4 (unknown):
```unknown
2.arch.pool.ntp.org
```

---

## EFI system partition

**URL:** https://wiki.archlinux.org/title/EFI_system_partition

**Contents:**
- Check for an existing partition
- Create the partition
  - GPT partitioned disks
  - MBR partitioned disks
- Format the partition
- Mount the partition
  - Typical mount points
  - Alternative mount points
    - Using bind mount
    - Using systemd

The EFI system partition (also called ESP) is an OS independent partition that acts as the storage place for the UEFI boot loaders, applications and drivers to be launched by the UEFI firmware. It is mandatory for UEFI boot.

If you are installing Arch Linux on an UEFI-capable computer with an installed operating system, like Windows 10 for example, it is very likely that you already have an EFI system partition.

To find out the disk partition scheme and the system partition, use fdisk as root on the disk you want to boot from:

If you found an acceptable existing EFI system partition, simply proceed to #Mount the partition. If you did not find one, you will need to create it, proceed to #Create the partition.

The following two sections show how to create an EFI system partition (ESP).

The partition size should provide adequate space for storing boot loaders and other files required for booting.

It is recommended to make the partition 1 GiB in size to ensure it has adequate space for multiple kernels or unified kernel images, a boot loader, firmware updates files and any other operating system or OEM files. If still in doubt, 4 GiB ought to be enough for anybody, e.g., for tools like Limine boot loader with Snapper integration for Btrfs, which supports creating multiple bootable snapshots.

EFI system partition on a GUID Partition Table is identified by the partition type GUID C12A7328-F81F-11D2-BA4B-00A0C93EC93B.

Choose one of the following methods to create an ESP for a GPT partitioned disk:

After creating the partition, it should be formatted with a file system. Proceed to the #Format the partition section below.

See also Partitioning#Choosing between GPT and MBR for the limits of MBR and the advantages of GPT in general.

EFI system partition on a Master Boot Record partition table is identified by the partition type ID EF.

Choose one of the following methods to create an ESP for a MBR partitioned disk:

After creating the partition, it should be formatted with a file system. Proceed to the #Format the partition section below.

The UEFI specification mandates support for the FAT12, FAT16, and FAT32 file systems (see UEFI specification version 2.11, section 13.3.1.1), but any conformant vendor can optionally add support for additional file systems; for example, the firmware in Apple Macs supports the HFS+ file system.

To prevent potential issues with other operating systems and since the UEFI specification says that UEFI "encompasses the use of FAT32 for a system partition, and FAT12 or FAT16 for removable media"[5], it is recommended to use FAT32. Use the mkfs.fat(8) utility from dosfstools:

If you get the message WARNING: Number of clusters for 32 bit FAT is less than suggested minimum. and you cannot create a larger ESP, reduce cluster size with mkfs.fat -s 2 -F 32 ... or -s 1; otherwise the partition may be unreadable by UEFI. See mkfs.fat(8) for supported cluster sizes.

For partitions smaller than 32 MiB using FAT32 may not be possible. In which case, format it to FAT16 or even FAT12. For example, a 2 MiB ESP will only be able to support FAT12:

The kernels, initramfs files, and, in most cases, the processor's microcode, need to be accessible by the boot loader or UEFI itself to successfully boot the system. Thus if you want to keep the setup simple, your boot loader choice limits the available mount points for EFI system partition.

Alternatively preload the required kernel modules on boot, e.g.:

The three typical scenarios for mounting the EFI system partition are:

If you do not use one of the #Typical mount points, you will need to copy your boot files to ESP (referred to hereafter as esp).

Furthermore, you will need to keep the files on the ESP up-to-date with later kernel updates. Failure to do so could result in an unbootable system. The following sections discuss several mechanisms for automating it.

Instead of mounting the ESP itself to /boot, you can mount a directory of the ESP to /boot using a bind mount (see mount(8)). This allows pacman to update the kernel directly while keeping the ESP organized to your liking.

Just like in #Alternative mount points, copy all boot files to a directory on your ESP, but mount the ESP outside /boot. Then bind mount the directory:

After verifying success, edit your Fstab to make the changes persistent:

systemd features event triggered tasks. In this particular case, the ability to detect a change in path is used to sync the EFISTUB kernel and initramfs files when they are updated in /boot/. The file watched for changes is initramfs-linux-fallback.img since this is the last file built by mkinitcpio, to make sure all files have been built before starting the copy. The systemd path and service files to be created are:

Then enable and start efistub-update.path.

Filesystem events can be used to run a script syncing the EFISTUB Kernel after kernel updates. An example with incron follows.

In order to use this method, enable the incrond.service.

As the presets in /etc/mkinitcpio.d/ support shell scripting, the kernel and initramfs can be copied by just editing the presets.

Edit the file /etc/mkinitcpio.d/linux.preset:

To test that, just run:

A mkinitcpio post hook can be used to copy kernels and initramfs images to a desired directory after the initramfs is generated.

Create the following file and make it executable:

A last option relies on the pacman hooks that are run at the end of the transaction.

The first file is a hook that monitors the relevant files, and it is run if they were modified in the former transaction.

The second file is the script itself. Create the file and make it executable:

On a disk with a preexisting operating system, the EFI system partition may be smaller than recommended in #Create the partition. E.g. Windows Setup creates a measly 100 MiB EFI system partition on non-4Kn drives.

In such cases, it may be a good idea to create a new, larger EFI system partition to prevent running out of space on it.

In Windows, partitions can be either managed graphically with Disk Management (diskmgmt.msc) or from the command line with the diskpart.exe utility.

Run diskmgmt.msc as Administrator.

There should now be a 4 GiB unallocated space after the "(C:)" partition.

Boot into Arch Linux or the Arch Linux installation medium to proceed to creating a new partition.

First, make sure to backup the contents of the EFI system partition. For example, with esp being its mountpoint:

Unmount the EFI system partition:

Run blkid and take note of the UUID and PARTUUID values. They will later be reused for the new partition.

Delete the old partition using sgdisk from gptfdisk:

Create a new partition in the largest unallocated space while reusing the old PARTUUID and PARTLABEL:

Tell the kernel to reread the partition table using partprobe(8) from parted:

Confirm the new, 4 GiB in size, EFI system partition is created by listing the partitions with fdisk:

Partition numbers are not resorted when deleting and creating partitions, so the EFI system partition number on the disk will likely be the same as before.

Format the partition to FAT32 reusing the old UUID (while removing the dash from it):

Finally, mount the new partition and restore its contents from backup:

If you previously stopped esp.automount, start it again.

If there is a swap partition right after the EFI system partition, you can sacrifice it to give space for enlarging the EFI system partition. E.g. with a layout similar to:

First, deactivate the swap partition and remove it from fstab.

Use fdisk to delete the swap partition and enlarge the EFI system partition.

After the partition is resized, you need to resize the file system in it. Since fatresize(1) does not work and libparted cannot resize FAT volumes of certain sizes, the only option is to backup the files from the existing file system and create a new one that takes up all space of the partition.

Take note of the file system UUID to allow reusing it for the new file system:

Backup the contents of the EFI system partition. For example, with esp being its mountpoint:

Unmount the EFI system partition:

Wipe the file system signature from the partition to avoid any artifacts from the old file system:

Format the partition to FAT32 reusing the old UUID (while removing the dash from it):

Finally, mount the new partition and restore its contents from backup:

If you previously stopped esp.automount, start it again.

Now that the swap partition is gone, set up swap on a swap file.

It is possible to make the ESP part of a RAID1 array, but doing so brings the risk of data corruption, and further considerations need to be taken when creating the ESP. See [9] and [10] for details and UEFI booting and RAID1 for an in-depth guide with a solution.

The key part is to use --metadata 1.0 in order to keep the RAID metadata at the end of the partition, otherwise the firmware will not be able to access it:

Alternatively, as the ESP is not often updated, a secondary ESP can be managed by copying the primary ESP to the secondary one on a different disk during relevant updates. A boot entry for the secondary ESP can then be added manually using efibootmgr. See Debian:UEFI#RAID for the EFI System Partition for an implementation example. Note that while this avoids some risks of the RAID approach, it only works when using a single OS.

If you give the FAT file system a volume name (i.e. file system label), be sure to name it something other than EFI. That can trigger a bug in some firmwares (due to the volume name matching the EFI directory name) that will cause the firmware to act like the EFI directory does not exist.

If you are running a multi boot system (including but not limited to dual booting with Windows) and want to be able to boot into your other system while your main Arch Linux is hibernated, you must take extra caution not to mount the ESP in both systems, as this will likely cause data corruption and I/O errors on usage.

There are three possible mitigation strategies:

By successfully applying one of the mitigation strategies above you may hibernate this system, but not the other systems unless you apply one of the mitigation strategies to them as well. For example, applying one of the solutions to your main Arch Linux allows you to hibernate Arch Linux and boot into another e.g. Ubuntu Linux, but not to hibernate that Ubuntu Linux and boot into your main Arch Linux. Allowing this with Windows requires separate EFI system partitions.

Strictly speaking, this issue of mounting the same filesystem multiple times at once is neither limited to the EFI system partition, nor to hibernating the system. However, it is particularly relevant for the ESP, because the ESP is expected to be shared across multiple systems. The mitigation strategies can be adapted for other use cases as well.

If you choose option 3 above, you can create and enable the following systemd system service that unmounts the ESP before hibernating the system and remounts it after resuming:

However, since efi.mount is pulled in as a requirement for local-fs.target by default, stopping efi.mount also permanently brings down local-fs.target. This might have all sorts of negative side-effects, including ordering issues when shutting down the system, which might trigger systemd to auto-mask efi.mount as defective. This can be mitigated by telling systemd that efi.mount is not required by local-fs.target, but just wanted by it. To ensure this you must add the following mount options to /efi in /etc/fstab:

**Examples:**

Example 1 (unknown):
```unknown
# fdisk -l /dev/sdx
```

Example 2 (unknown):
```unknown
Disklabel type: gpt
```

Example 3 (unknown):
```unknown
Disklabel type: dos
```

Example 4 (unknown):
```unknown
EFI (FAT-12/16/32)
```

---

## systemd

**URL:** https://wiki.archlinux.org/title/Systemd-tmpfiles

**Contents:**
- Basic systemctl usage
  - Using units
  - Power management
    - Soft reboot
- Writing unit files
  - Handling dependencies
  - Service types
  - Editing provided units
    - Replacement unit files
    - Drop-in files

From the project web page:

Historically, what systemd calls "service" was named daemon: any program that runs as a "background" process (without a terminal or user interface), commonly waiting for events to occur and offering services. A good example is a web server that waits for a request to deliver a page, or an ssh server waiting for someone trying to log in. While these are full featured applications, there are daemons whose work is not that visible. Daemons are for tasks like writing messages into a log file (e.g. syslog, metalog) or keeping your system time accurate (e.g. ntpd). For more information see daemon(7).

The main command used to introspect and control systemd is systemctl. Some of its uses are examining the system state and managing the system and services. See systemctl(1) for more details.

Units commonly include, but are not limited to, services (.service), mount points (.mount), devices (.device) and sockets (.socket).

When using systemctl, you generally have to specify the complete name of the unit file, including its suffix, for example sshd.socket. There are however a few short forms when specifying the unit in the following systemctl commands:

See systemd.unit(5) for details.

The commands in the below table operate on system units since --system is the implied default for systemctl. To instead operate on user units (for the calling user), use systemctl --user without root privileges. See also systemd/User#Basic setup to enable/disable user units for all users.

polkit is necessary for power management as an unprivileged user. If you are in a local systemd-logind user session and no other session is active, the following commands will work without root privileges. If not (for example, because another user is logged into a tty), systemd will automatically ask you for the root password.

Soft reboot is a special kind of a userspace-only reboot operation that does not involve the kernel. It is implemented by systemd-soft-reboot.service(8) and can be invoked through systemctl soft-reboot. As with kexec, it skips firmware re-initialization, but additionally the system does not go through kernel initialization and initramfs, and unlocked dm-crypt devices remain attached.

When /run/nextroot/ contains a valid root file system hierarchy (e.g. is the root mount of another distribution or another snapshot), soft-reboot would switch the system root into it, allowing for switching to another installation without losing states managed by kernel, e.g. networking.

The syntax of systemd's unit files (systemd.unit(5)) is inspired by XDG Desktop Entry Specification .desktop files, which are in turn inspired by Microsoft Windows .ini files. Unit files are loaded from multiple locations (to see the full list, run systemctl show --property=UnitPath), but the main ones are (listed from lowest to highest precedence):

Look at the units installed by your packages for examples, as well as systemd.service(5) § EXAMPLES.

systemd-analyze(1) can help verifying the work. See the systemd-analyze verify FILE... section of that page.

With systemd, dependencies can be resolved by designing the unit files correctly. The most typical case is when unit A requires unit B to be running before A is started. In that case add Requires=B and After=B to the [Unit] section of A. If the dependency is optional, add Wants=B and After=B instead. Note that Wants= and Requires= do not imply After=, meaning that if After= is not specified, the two units will be started in parallel.

Dependencies are typically placed on services and not on #Targets. For example, network.target is pulled in by whatever service configures your network interfaces, therefore ordering your custom unit after it is sufficient since network.target is started anyway.

There are several different start-up types to consider when writing a custom service file. This is set with the Type= parameter in the [Service] section:

See the systemd.service(5) § OPTIONS man page for a more detailed explanation of the Type values.

This article or section needs expansion.

To avoid conflicts with pacman, unit files provided by packages should not be directly edited. There are two safe ways to modify a unit without touching the original file: create a new unit file which overrides the original unit or create drop-in snippets which are applied on top of the original unit. For both methods, you must reload the unit afterwards to apply your changes. This can be done either by editing the unit with systemctl edit (which reloads the unit automatically) or by reloading all units with:

To replace the unit file /usr/lib/systemd/system/unit, create the file /etc/systemd/system/unit and reenable the unit to update the symlinks.

This opens /etc/systemd/system/unit in your editor (copying the installed version if it does not exist yet) and automatically reloads it when you finish editing.

To create drop-in files for the unit file /usr/lib/systemd/system/unit, create the directory /etc/systemd/system/unit.d/ and place .conf files there to override or add new options. systemd will parse and apply these files on top of the original unit.

The easiest way to do this is to run:

This opens the file /etc/systemd/system/unit.d/drop_in_name.conf in your text editor (creating it if necessary) and automatically reloads the unit when you are done editing. Omitting --drop-in= option will result in systemd using the default file name override.conf .

To revert any changes to a unit made using systemctl edit do:

For example, if you simply want to add an additional dependency to a unit, you may create the following file:

As another example, in order to replace the ExecStart directive, create the following file:

Note how ExecStart must be cleared before being re-assigned [2]. The same holds for every item that can be specified multiple times, e.g. OnCalendar for timers.

One more example to automatically restart a service:

For services that send logs directly to journald or syslog, you can control their verbosity by setting a numeric value between 0 and 6 for the LogLevelMax= parameter in the [Service] section using the methods described above. For example:

The standard log levels are identical to those used to filter the journal. Setting a lower number excludes all higher and less important log messages from your journal.

If a service is echoing stdout and/or stderr output, by default this will end up in the journal as well. This behavior can be suppressed by setting StandardOutput=null and/or StandardError=null in the [Service] section. Other values than null can be used to further tweak this behavior. See systemd.exec(5) § LOGGING_AND_STANDARD_INPUT/OUTPUT.

systemd uses targets to group units together via dependencies and as standardized synchronization points. They serve a similar purpose as runlevels but act a little differently. Each target is named instead of numbered and is intended to serve a specific purpose with the possibility of having multiple ones active at the same time. Some targets are implemented by inheriting all of the services of another target and adding additional services to it. There are systemd targets that mimic the common SystemVinit runlevels.

The following should be used under systemd instead of running runlevel:

The runlevels that held a defined meaning under sysvinit (i.e., 0, 1, 3, 5, and 6); have a 1:1 mapping with a specific systemd target. Unfortunately, there is no good way to do the same for the user-defined runlevels like 2 and 4. If you make use of those it is suggested that you make a new named systemd target as /etc/systemd/system/your target that takes one of the existing runlevels as a base (you can look at /usr/lib/systemd/system/graphical.target as an example), make a directory /etc/systemd/system/your target.wants, and then symlink the additional services from /usr/lib/systemd/system/ that you wish to enable.

In systemd, targets are exposed via target units. You can change them like this:

This will only change the current target, and has no effect on the next boot. This is equivalent to commands such as telinit 3 or telinit 5 in Sysvinit.

The standard target is default.target, which is a symlink to graphical.target. This roughly corresponds to the old runlevel 5.

To verify the current target with systemctl:

To change the default target to boot into, change the default.target symlink. With systemctl:

Alternatively, append one of the following kernel parameters to your boot loader:

systemd chooses the default.target according to the following order:

Some (not exhaustive) components of systemd are:

systemd is in charge of mounting the partitions and filesystems specified in /etc/fstab. The systemd-fstab-generator(8) translates all the entries in /etc/fstab into systemd units; this is performed at boot time and whenever the configuration of the system manager is reloaded.

systemd extends the usual fstab capabilities and offers additional mount options. These affect the dependencies of the mount unit. They can, for example, ensure that a mount is performed only once the network is up or only once another partition is mounted. The full list of specific systemd mount options, typically prefixed with x-systemd., is detailed in systemd.mount(5) § FSTAB.

An example of these mount options is automounting, which means mounting only when the resource is required rather than automatically at boot time. This is provided in fstab#Automount with systemd.

On UEFI-booted systems, GPT partitions such as root, home, swap, etc. can be mounted automatically following the Discoverable Partitions Specification. These partitions can thus be omitted from fstab, and if the root partition is automounted, then root= can be omitted from the kernel command line. See systemd-gpt-auto-generator(8).

The prerequisites are:

udev will create symlinks to the discovered partitions which can be used to reference the partitions and volumes in configuration files.

For /var automounting to work, the PARTUUID must match the SHA256 HMAC hash of the partition type UUID keyed by the machine ID. The required PARTUUID can be obtained using:

The primary role of systemd-sysvcompat (required by base) is to provide the traditional linux init binary. For systemd-controlled systems, init is just a symbolic link to its systemd executable.

In addition, it provides four convenience shortcuts that SysVinit users might be used to. The convenience shortcuts are halt(8), poweroff(8), reboot(8) and shutdown(8). Each one of those four commands is a symbolic link to systemctl, and is governed by systemd behavior. Therefore, the discussion at #Power management applies.

systemd-based systems can give up those System V compatibility methods by using the init= boot parameter (see, for example, /bin/init is in systemd-sysvcompat ?) and systemd native systemctl command arguments.

systemd-tmpfiles creates, deletes and cleans up volatile and temporary files and directories. It reads configuration files in /etc/tmpfiles.d/ and /usr/lib/tmpfiles.d/ to discover which actions to perform. Configuration files in the former directory take precedence over those in the latter directory.

Configuration files are usually provided together with service files, and they are named in the style of /usr/lib/tmpfiles.d/program.conf. For example, the Samba daemon expects the directory /run/samba to exist and to have the correct permissions. Therefore, the samba package ships with this configuration:

Configuration files may also be used to write values into certain files on boot. For example, if you used /etc/rc.local to disable wakeup from USB devices with echo USBE > /proc/acpi/wakeup, you may use the following tmpfile instead:

It is possible to write multiple lines to the same file, either with \n in the argument or using the w+ type on multiple lines (including the first one) for appending:

See the systemd-tmpfiles(8) and tmpfiles.d(5) man pages for details.

The factual accuracy of this article or section is disputed.

Configuration files provided by packages should not be directly edited to avoid conflicts with pacman updates. For this, many (but not all) systemd packages provide a way to modify the configuration, but without touching the original file by creation of drop-in snippets. Check the package manual to see if drop-in configuration files are supported.

To create a drop-in configuration file for the unit file /etc/systemd/unit.conf, create the directory /etc/systemd/unit.conf.d/ and place .conf files there to override or add new options. systemd will parse and apply these files on top of the original unit.

Check the overall configuration:

The applied drop-in snippets file(s) and content will be listed at the end. Restart the service for the changes to take effect.

Some packages provide a .socket unit. For example, cups provides a cups.socket unit[3]. If cups.socket is enabled (and cups.service is left disabled), systemd will not start CUPS immediately; it will just listen to the appropriate sockets. Then, whenever a program attempts to connect to one of these CUPS sockets, systemd will start cups.service and transparently hand over control of these ports to the CUPS process.

To delay a service until after the network is up, include the following dependencies in the .service file:

The network wait service of the network manager in use must also be enabled so that network-online.target properly reflects the network status.

For more detailed explanations, see the discussion in the Network configuration synchronization points.

If a service needs to perform DNS queries, it should additionally be ordered after nss-lookup.target:

See systemd.special(7) § Special Passive System Units.

For nss-lookup.target to have any effect it needs a service that pulls it in via Wants=nss-lookup.target and orders itself before it with Before=nss-lookup.target. Typically this is done by local DNS resolvers.

Check which active service, if any, is pulling in nss-lookup.target with:

This article or section needs expansion.

By default, Arch Linux does not make use of systemd presets, and does not enable most services on installation.

If you wish for systemd presets to be honored when packages are installed, you will need to create a a pacman hook in order to run systemctl preset or systemctl preset-all whenever a new systemd unit is installed. Something like

Arch Linux ships with /usr/lib/systemd/system-preset/99-default.preset containing disable *. This causes systemctl preset to disable all units by default, such that when a new package is installed, the user must manually enable the unit.

To enable units by default instead, simply create a symlink from /etc/systemd/system-preset/99-default.preset to /dev/null or replace with a file containing enable * in order to override the configuration file. This will cause systemctl preset to enable all units that get installed—regardless of unit type—unless specified in another file in one systemctl preset's configuration directories. User units are not affected. It is also possible to configure higher priority files with more specific rules on what should be enabled. See systemd.preset(5) for more information.

See systemd/Sandboxing.

In order to notify about service failures, an OnFailure= directive needs to be added to the according service file, for example by using a drop-in configuration file. Adding this directive to every service unit can be achieved with a top-level drop-in configuration file. For details about top-level drop-ins, see systemd.unit(5).

Create a top-level drop-in for services:

This adds OnFailure=failure-notification@%n.service to every service file. If some_service_unit fails, failure-notification@some_service_unit.service will be started to handle the notification delivery (or whatever task it is configured to perform).

Create the failure-notification@.service template unit:

You can create the failure-notification.sh script and define what to do or how to notify. Examples include sending e-mail, showing desktop notifications, using gotify, XMPP, etc. The %i will be the name of the failed service unit and will be passed as an argument to the script.

In order to prevent a recursion for starting instances of failure-notification@.service again and again if the start fails, create an empty drop-in configuration file with the same name as the top-level drop-in (the empty service-level drop-in configuration file takes precedence over the top-level drop-in and overrides the latter one):

You can set up systemd to send an e-mail when a unit fails. Cron sends mail to MAILTO if the job outputs to stdout or stderr, but many jobs are setup to only output on error. First you need two files: an executable for sending the mail and a .service for starting the executable. For this example, the executable is just a shell script using sendmail, which is in packages that provide smtp-forwarder.

Whatever executable you use, it should probably take at least two arguments as this shell script does: the address to send to and the unit file to get the status of. The .service we create will pass these arguments:

Where user is the user being emailed and address is that user's email address. Although the recipient is hard-coded, the unit file to report on is passed as an instance parameter, so this one service can send email for many other units. At this point you can start status_email_user@dbus.service to verify that you can receive the emails.

Then simply edit the service you want emails for and add OnFailure=status_email_user@%n.service to the [Unit] section. %n passes the unit's name to the template.

See udisks#Automatically turn off an external HDD at shutdown.

To find the systemd services which failed to start:

To find out why they failed, examine their log output. See systemd/Journal#Filtering output for details.

systemd has several options for diagnosing problems with the boot process. See boot debugging for more general instructions and options to capture boot messages before systemd takes over the boot process. Also see systemd debugging documentation.

If some systemd service misbehaves or you want to get more information about what is happening, set the SYSTEMD_LOG_LEVEL environment variable to debug. For example, to run the systemd-networkd daemon in debug mode:

Add a drop-in file for the service adding the two lines:

Or equivalently, set the environment variable manually:

then restart systemd-networkd and watch the journal for the service with the -f/--follow option.

If the shutdown process takes a very long time (or seems to freeze), most likely a service not exiting is to blame. systemd waits some time for each service to exit before trying to kill it. To find out whether you are affected, see Shutdown completes eventually in the systemd documentation.

A common problem is a stalled shutdown or suspend process. To verify whether that is the case, you could run either of these commands and check the outputs

The solution to this would be to cancel these jobs by running

and then trying shutdown or reboot again.

If running journalctl -u foounit as root does not show any output for a short lived service, look at the PID instead. For example, if systemd-modules-load.service fails, and systemctl status systemd-modules-load shows that it ran as PID 123, then you might be able to see output in the journal for that PID, i.e. by running journalctl -b _PID=123 as root. Metadata fields for the journal such as _SYSTEMD_UNIT and _COMM are collected asynchronously and rely on the /proc directory for the process existing. Fixing this requires fixing the kernel to provide this data via a socket connection, similar to SCM_CREDENTIALS. In short, it is a bug. Keep in mind that immediately failed services might not print anything to the journal as per design of systemd.

The factual accuracy of this article or section is disputed.

After using systemd-analyze a number of users have noticed that their boot time has increased significantly in comparison with what it used to be. After using systemd-analyze blame NetworkManager is being reported as taking an unusually large amount of time to start.

The problem for some users has been due to /var/log/journal becoming too large. This may have other impacts on performance, such as for systemctl status or journalctl. As such the solution is to remove every file within the folder (ideally making a backup of it somewhere, at least temporarily) and then setting a journal file size limit as described in systemd/Journal#Journal size limit.

Starting with systemd 219, /usr/lib/tmpfiles.d/systemd.conf specifies ACL attributes for directories under /var/log/journal and, therefore, requires ACL support to be enabled for the filesystem the journal resides on.

See Access Control Lists#Enable ACL for instructions on how to enable ACL on the filesystem that houses /var/log/journal.

You may want to disable emergency mode on a remote machine, for example, a virtual machine hosted at Azure or Google Cloud. It is because if emergency mode is triggered, the machine will be blocked from connecting to network.

To disable it, mask emergency.service and emergency.target.

You may be trying to start or enable a user unit as a system unit. systemd.unit(5) indicates, which units reside where. By default systemctl operates on system services.

See systemd/User for more details.

Some bootloaders only set the LoaderDevicePartUUID variable when it is empty. Consequently, even if the UUID of the EFI partition changes, the bootloader will not update LoaderDevicePartUUID. By deleting the EFI variable with the commands below, the bootloader will then repopulate it with the new UUID.

**Examples:**

Example 1 (unknown):
```unknown
-H user@host
```

Example 2 (unknown):
```unknown
sshd.socket
```

Example 3 (unknown):
```unknown
netctl.service
```

Example 4 (unknown):
```unknown
dev-sda2.device
```

---

## JFS

**URL:** https://wiki.archlinux.org/title/JFS

**Contents:**
- Background
  - GNU/Linux development team
  - Technical features
- Installation
- Optimizations
  - Defragmenting JFS
  - Deadline I/O scheduler
  - External journal
  - noatime fstab attribute
  - Journal modes

The Journaled File System (JFS) is a journaling file system that was open-sourced by IBM in 1999 and support for which has been available in the Linux kernel since 2002.

This article introduces the reader to the JFS file system. In particular, procedures for implementation, maintenance and optimization will be presented along with background information on the file system itself and some cautionary notes on precarious implementations.

While it is difficult to make general comparisons between JFS and other file systems available on UNIX and UNIX-like operating systems, it is claimed that JFS uses less CPU resources than other GNU/Linux file systems [2]. With certain optimizations, JFS has also been claimed to be faster for certain file operations, as compared to other GNU/Linux file systems (see [3],Benchmarks).

The development of the GNU/Linux JFS port is headed by

JFS is a modern file system supporting many features, a few of which are listed here.

A more comprehensive (and technical) overview of the features in JFS can be found in the JFS Overview authored by developer Steve Best.

The JFS driver is built as a module in the standard Arch kernel packages.

The jfsutils package must be installed to perform all file system related tasks.

Creation of a JFS file system can be done with the either:

Both commands are equivalent.

The factual accuracy of this article or section is disputed.

There are several concepts that can be implemented with a JFS filesystem to boost its performance:

JFS, like all file systems, will degrade in performance over time due to file fragmentation [5]. While there is in-place defragmentation code in the JFS utilities, this is code held over from the OS/2 port and has yet to be implemented [6]. For file systems that can be taken off-line for a time, one can execute a script like the following to defragment their JFS file system

In this example, /dev/hdc1 is the device with the data that needs backing up and /dev/hdj1 is the device that holds the backup.

Basically, this script copies the data off the JFS file system to a backup drive, formats the original JFS file system and finally writes back the data from the backup to the freshly formatted drive in a way that JFS will write its allocation trees in a defragmentated way.

JFS seems to perform better when the kernel has been configured to use the Deadline I/O Scheduler. Indeed, JFS's performance seems to exceed that of other GNU/Linux file systems with this particular scheduler being employed [7].

As with any journaled file system, a journal is constantly accessed in accordance with disk activity. Having the journal log on the same device as the its corresponding file system thus can cause a degradation in I/O throughput. This degradation can be alleviated by putting the journal log on a separate device all together.

To make a journal device, first create a partition that is 128MB. Using a partition that is bigger than 128MB results in the excess being ignored, according to mkfs.jfs. You can either create an external log for an already-existing JFS file system by executing the following:

or a command can be issued to create both a new external journal and its corresponding JFS file system:

This last command formats BOTH the external journal and the JFS file system.

This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.

Every time a file is accessed (read or write) the default for most file systems is to append the metadata associated with that file with an updated access time. Thus, even read operations incur an overhead associated with a write to the file system. This can lead to a significant degradation in performance in some usage scenarios. Appending noatime to the fstab line for a JFS file system stops this action from happening. As access time is of little importance in most scenarios, this alteration has been widely touted as a fast and easy way to get a performance boost out of one's hardware. Even Linus Torvalds seems to be a proponent of this optimization [8].

Here is an example /etc/fstab entry with the noatime tag:

One may also mount a file system with the noatime attribute by invoking something similar to the following:

JFS does not support various journal modes like ext3. Thus, passing the mount option data=writeback with mount or in /etc/fstab will have no effect on a JFS file system. JFS's current journaling mode is similar to Ext3's default journaling mode: ordered [10].

While the OS/2 port of JFS supports block sizes of 512, 1024, 2048, and 4096 bytes, the Linux port of JFS is only able to use 4k blocks. Even though code exists in JFS utilities that correspond to file systems using variable size blocks, this has yet to be implemented [11]. As larger block sizes tend to favor performance (smaller ones favor efficient space usage), implementing smaller block sizes for JFS in Linux has been given a low priority for implementation by JFS developers.

In the event that the file system does not get properly unmounted before being powered down, one will usually have to run fsck on a JFS file system in order to be able to remount it. This procedure usually only takes a few seconds, unless the log has been damaged. If running fsck returns an unrecognized file system error, try running fsck.jfs on the target device. Normally, fsck is all that is needed.

While JFS is very stable in its current stage of development, there are some cautionary notes on using this file system.

Occasionally, a JFS root partition will be unable to mount in normal read-write mode. This is usually due to the fact that the JFS root file system fails its fsck after an unclean shutdown. It is rare that JFS fails out of fsck, and it is usually due to the JFS log itself being corrupted.

All that is required in this scenario is to boot your machine with a relatively recent Arch Linux LiveCD. Booting an Arch Linux livecd will give you access to all the JFS utilities and will load a kernel that is able to recognize JFS file systems. After booting the CD simply run fsck (or possibly fsck.jfs) on your JFS root and it should recover just fine (even though the fsck will probably take longer than normal due to the log probably being damaged). Once the fsck finishes, you should be able to boot your machine like normal.

The effectiveness of deleting files by overwriting their corresponding file system blocks with random data (i.e. using utilities like shred) can not be assured [12]. Given the design of journaled file systems, maintenance issues, and performance liabilities; reliable shredding of files as a deletion method does not sit highly on the priority list for implementation on any journaled file system.

One may force a fsck (file system check) on the root file system by following fsck#Forcing the check and rebooting.

On Arch linux systems with a JFS root on a partition under control of device-mapper (i.e. the root device is a lvm or a LUKS encrypted one), forcing an fsck can sometimes remove the /usr/man/man3/ directory. The reason for this issue is not clear, but the problem has been replicated [13].

It is suggested to get a list of Arch Packages that use /usr/man/man3/ by issuing a command similar to

before attempting a forced fsck on a JFS root partition [14]. If /usr/man/man3 does indeed disappear, simply reinstall all the packages listed in man3_pkg_list.

As stated above, the reason for this issue is not clear at the moment; but it may have something to do with the fact that a forced fsck runs through higher phases of file system checks that only happen when a JFS log gets damaged in an improper dismounting of the partition.

In JFS; journal writes are indefinitely postponed until there is another trigger such as memory pressure or an unmount operation. This infinite write delay limits reliability, as a crash can result in data loss even for data that was written minutes or hours before.[16]

As benchmarks measuring file system performance tend to be focused at specific types of disk usage, it is difficult to decipher good general comparisons rating how well JFS performs against other files systems. As mentioned before, it has been noted that JFS has a tendency to use less CPU resources than other GNU/Linux file systems and (with the right optimizations) is faster than other GNU/Linux file systems for certain types of file operations. It has been noted that JFS slows down when working with many files, however[17][18]. In the references are some links to benchmarks; but as always, it is best to test and see what works best for your own system and work load.

JFS is a stable, feature-rich file system that has not been publicized as much as some of the other Linux file systems. With optimizations, JFS is stable, CPU efficient and fast. In particular, VMWare sessions stand to benefit enormously from a properly optimized and defragmented, underlying JFS file system.

**Examples:**

Example 1 (unknown):
```unknown
# mkfs.jfs /dev/target_dev
```

Example 2 (unknown):
```unknown
# jfs_mkfs /dev/target_dev
```

Example 3 (unknown):
```unknown
umount /dev/hdc1
dd bs=4k if=/dev/hdc1 of=/dev/hdj1
jfs_fsck /dev/hdj1
mount -o ro /dev/hdj1 /fs/hdj1
jfs_mkfs /dev/hdc1
mount -o rw /dev/hdc1 /fs/hdc1
(cd /fs/hdj1 && tar -cS -b8 --one-file-system -f - .) | (cd /fs/hdc1 && tar -xS -b8 -p -f -)
umount /dev/hdj1
```

Example 4 (unknown):
```unknown
# mkfs.jfs -J journal_dev /dev/external_journal             # creates a journal on device /dev/external_journal
# mkfs.jfs -J device=/dev/external_journal /dev/jfs_device  # attaches the external journal to the existing file 
                                                          #   system on /dev/jfs_device
```

---

## General troubleshooting

**URL:** https://wiki.archlinux.org/title/Boot_debugging

**Contents:**
- General procedures
  - Additional support
- Boot problems
  - Console messages
    - Flow control
    - Printing more kernel messages
    - Producing debug kernel messages
      - Dynamic debugging
      - Subsystem-specific debugging
    - netconsole

This article explains some methods for general troubleshooting. For application specific issues, please reference the particular wiki page for that program.

This article or section needs expansion.

It is crucial to always read any error messages that appear. Sometimes it may be hard, e.g. with graphical applications, to get a proper error message.

If you require any additional support, you may ask on the forums or on IRC.

When asking for support post the complete output/logs, not just what you think are the significant sections. Sources of information include:

One of the better ways to post this information is to use a pastebin.

A link will then be output that you can paste to the forum or IRC.

Additionally, you may wish to review how to properly report issues before asking.

This article or section needs expansion.

When diagnosing boot problems, it is very important to know in which stage the boot fails.

If the debugging tools provided by any stage are not enough to fix the broken component, try using a e.g. USB stick with the latest Arch Linux ISO on it.

After the boot process, the screen is cleared and the login prompt appears, leaving users unable to read init output and error messages. This default behavior may be modified using methods outlined in the sections below.

Note that regardless of the chosen option, kernel messages can be displayed for inspection after booting by using journalctl -k or dmesg. To display all logs from the current boot use journalctl -b.

This is basic management that applies to most terminal emulators, including virtual consoles (VC):

This pauses not only the output, but also programs which try to print to the terminal, as they will block on the write() calls for as long as the output is paused. If your init appears frozen, make sure the system console is not paused.

To see error messages which are already displayed, see Getty#Have boot messages stay on tty1.

Most kernel messages are hidden during boot. You can see more of these messages by adding different kernel parameters. The simplest ones are:

Other parameters you can add that might be useful in certain situations are:

#Printing more kernel messages indicates how to print of the kernel log buffer to the console, but that buffer itself won't contain any messages it didn't already (aside from the debug systemd output). This heading discusses methods for getting more detailed information out of the kernel log.

Messages printed with pr_debug or related functions such as dev_dbg(), drm_dbg(), and bt_dev_dbg() will not be produced unless you either:

This section will discuss how to use dynamic debug, which is useful if you have already looked at your kernel log with everything up to informational logs, and would like even more debugging information from a particular location.

Firstly, you must be running a kernel that was compiled with the CONFIG_DYNAMIC_DEBUG kernel configuration option set. This is already the case for linux, so no action is required if you are using that kernel.

Then, you need to know where you want to see debug messages from. A couple of options are:

Using that "source" of messages, you have to come up with a dynamic debug query that indicates which debug messages to enable, of the format:

Some examples of queries are:

Finally, to actually enact the query, you can either:

This is a greatly simplified overview of dynamic debug's capabilities; see the documentation for further details.

There are also a number of separate debug parameters for enabling debugging in specific subsystems e.g. bootmem_debug, sched_debug. Also, initcall_debug can be useful to investigate boot freezes. (Look for calls that did not return.) Check the kernel parameter documentation for specific information.

netconsole is a kernel module that sends all kernel log messages (i.e. dmesg) over the network to another computer, without involving user space (e.g. syslogd). Name "netconsole" is a misnomer because it is not really a "console", more like a remote logging service.

It can be used either built-in or as a module. Built-in netconsole initializes immediately after NIC cards and will bring up the specified interface as soon as possible. The module is mainly used for capturing kernel panic output from a headless machine, or in other situations where the user space is no more functional.

Getting an interactive shell at some stage in the boot process can help you pinpoint exactly where and why something is failing. There are several kernel parameters for doing so, but they all launch a normal shell which you can exit to let the kernel resume what it was doing:

Another option is systemd's debug-shell which adds a root shell on tty9 (accessible with Ctrl+Alt+F9). It can be enabled by either adding systemd.debug_shell to the kernel parameters, or by enabling debug-shell.service.

See Kernel modules#Obtaining information.

Unfortunately, freezes are usually hard to debug and some of them take a lot of time to reproduce. There are some types of freezes which are easier to debug than others:

If nothing else helps, try a clean shutdown. Pressing the power button once may unfreeze the system and show the classic "shutdown screen" which displays all the units that are getting stopped. Alternatively, using the magic SysRq keys may also help to achieve a clean shutdown. This is very important because the journal may contain hints why the machine froze. The journal may not be written to disk on an unclean shutdown. Hard freezes in which the whole machine is unresponsible are harder to debug since logs can not be written to disk in time.

Remote logging may help if the freeze does not permit writing anything to disk. A crude remote logging solution, which needs to be invoked from another device, can be used for basic debugging:

Many fatal freezes in which the whole system does not respond anymore and require a forced shutdown may be related to buggy firmware, drivers or hardware. Trying a different kernel (see Kernel#Debugging regressions) or even a different Linux distribution or operating system, updating the firmware and running hardware diagnostics may help finding the problem.

If a freeze does not permit gathering any kind of logs or other information required for debugging, try reproducing the freeze in a live environment. If a graphical environment is required to reproduce the freeze or if the freeze can be reproduced on the archiso, use the live environment of a different distribution, which is preferably not based on Arch Linux to eliminate the possibility that the freeze is related to the version or patches of the kernel. Should the freeze still happen in a live environment, chances are that it may be hardware-related. If it does not happen anymore, it is necessary to be aware of the differences of both systems. Different configurations, differences in versions and kernel parameters and other, similar changes may have fixed the freeze.

However, a blinking caps lock LED may indicate a kernel panic. Some setups may not show the TTY when a kernel panic occurred, which may be confusing and can be interpreted as another kind of freeze.

If an update causes an issue but downgrading the specific package fixes it, it is likely a regression. If this happened after a normal full system upgrade, check your pacman.log to determine which package(s) may have caused the issue. The most important part of debugging regressions is checking if the issue was already fixed, as this can save much time. To do so, first ensure the application is fully updated (e.g. ensure the application is the same version as in the official repositories). If it already is or if updating it does not fix the issue, try using the development version, usually a VCS version, which may already be packaged in the AUR. If this fixes the issue and the version with the fixes is not yet in the official repositories, wait until the new version arrives in them and then switch back to it.

If the issue still persists, debug the issue and/or bisect the application and report the bug on the upstream bug tracker so it can be fixed.

This will manifest commonly (but probably not only) as:

As partially covered in System maintenance#Restart or reboot after upgrades, the kernel is not updated when you update the package but only once you reboot afterwards. Meanwhile, the kernel modules, located in /usr/lib/modules/kernelversion/ are removed by pacman when installing the new kernel. As explained in FS#16702, this approach avoids leaving files on the system not handled by the package manager but leads to the aforementioned symptoms. To fix them, reboot systematically after updating the kernel. The long-term evolution, yet to be implemented, will be to use versioned kernel packages : the main blocker being how to handle the removal of the previous kernel versions once they are not needed anymore.

Another solution is available as kernel-modules-hook, where two pacman hooks use rsync to keep the kernel modules on the file system after the kernel update and linux-modules-cleanup.service that marks the old modules for removal four weeks after once enabled.

See Pacman#Troubleshooting for general topics, and pacman/Package signing#Troubleshooting for issues with PGP keys.

If a partial upgrade was performed, try updating your whole system. A reboot may be required.

If you usually boot into a GUI and that is failing, perhaps you can press Ctrl+Alt+F1 through Ctrl+Alt+F6 and get to a working tty to run pacman through.

If the system is broken enough that you are unable to run pacman, boot using a monthly Arch ISO from a USB flash drive, an optical disc or a network with PXE. (Do not follow any of the rest of the installation guide.)

Mount your root file system:

Mount any other partitions that you created separately, adding the prefix /mnt to all of them, i.e.:

Try using your system's pacman while chrooted:

If that fails, exit the chroot, and try:

This article or section needs expansion.

fuser is a command-line utility for identifying processes using resources such as files, file systems and TCP/UDP ports.

fuser is provided by the psmisc package, which should be already installed as a dependency of the base meta package. See fuser(1) for details.

First, make sure you have a valid local session within X:

This should contain Remote=no and Active=yes in the output. If it does not, make sure that X runs on the same tty where the login occurred. This is required in order to preserve the logind session.

Basic polkit actions do not require further set-up. Some polkit actions require further authentication, even with a local session. A polkit authentication agent needs to be running for this to work. See polkit#Authentication agents for more information on this.

If, while using a program, you get an error similar to:

Use pacman or pkgfile to search for the package that owns the missing library:

In this case, the libusb-compat package needs to be installed. Alternatively, the program requesting the library may need to be rebuilt following a soname bump.

The error could also mean that the package that you used to install your program does not list the library as a dependency in its PKGBUILD: if it is an official package, report a bug; if it is an AUR package, report it to the maintainer using its page in the AUR website.

**Examples:**

Example 1 (unknown):
```unknown
$HOME/.cache
```

Example 2 (unknown):
```unknown
$HOME/.local
```

Example 3 (unknown):
```unknown
systemd.log_level=debug
```

Example 4 (unknown):
```unknown
journalctl -k
```

---

## Overlay filesystem

**URL:** https://wiki.archlinux.org/title/Overlayfs

**Contents:**
- Installation
- Usage
  - Read-only overlay
- See also

From the initial kernel commit:

Overlayfs has been in the Linux kernel since 3.18.

Overlayfs is enabled in the default kernel and the overlay kernel module is automatically loaded upon issuing a mount command.

To mount an overlay use the following mount options:

The lower directory can actually be a list of directories separated by :, all changes in the merged directory are still reflected in upper.

The above example will have the order:

To add an overlayfs entry to /etc/fstab use the following format:

The noauto and x-systemd.automount mount options are necessary to prevent systemd from hanging on boot because it failed to mount the overlay. The overlay is now mounted whenever it is first accessed and requests are buffered until it is ready. See fstab#Automount with systemd.

Sometimes, it is only desired to create a read-only view of the combination of two or more directories. In that case, it can be created in an easier manner, as the directories upper and work are not required:

When upperdir is not specified, the overlay is automatically mounted as read-only.

**Examples:**

Example 1 (unknown):
```unknown
# mount -t overlay overlay -o lowerdir=/lower,upperdir=/upper,workdir=/work /merged
```

Example 2 (unknown):
```unknown
# mount -t overlay overlay -o lowerdir=/lower1:/lower2:/lower3,upperdir=/upper,workdir=/work /merged
```

Example 3 (unknown):
```unknown
/upper
/lower1 
/lower2
/lower3
```

Example 4 (unknown):
```unknown
overlay /merged overlay noauto,x-systemd.automount,lowerdir=/lower,upperdir=/upper,workdir=/work 0 0
```

---

## Environment variables

**URL:** https://wiki.archlinux.org/title/Environment_variables

**Contents:**
- Utilities
- Defining variables
  - Globally
    - Using shell initialization files
    - Using pam_env
  - Per user
    - Graphical environment
      - Per desktop environment session
      - Per Xorg session
      - Per Wayland session

An environment variable is a named object that contains data used by one or more applications. In simple terms, it is a variable with a name and a value. The value of an environmental variable can for example be the location of all executable files in the file system, the default editor that should be used, or the system locale settings. Users new to Linux may often find this way of managing settings a bit unmanageable. However, environment variables provide a simple way to share configuration settings between multiple applications and processes in Linux.

The coreutils package contains the programs printenv and env. To list the current environmental variables with values:

The env utility can be used to run a command under a modified environment. The following example will launch xterm with the environment variable EDITOR set to vim. This will not affect the global environment variable EDITOR.

The shell builtin set(1p) allows you to change the values of shell options, set the positional parameters and to display the names and values of shell variables.

Each process stores their environment in the /proc/$PID/environ file. This file contains each key value pair delimited by a nul character (\x0). A more human readable format can be obtained with sed, e.g. sed 's:\x0:\n:g' /proc/$PID/environ.

To avoid needlessly polluting the environment, you should seek to restrict the scope of variables. In fact, graphical sessions and systemd services require you to set variables in certain locations for them to take effect. The scopes of environment variables are broken down into the contexts they affect:

Most Linux distributions tell you to change or add environment variable definitions in /etc/profile or other locations. Keep in mind that there are also package-specific configuration files containing variable settings such as /etc/locale.conf. Be sure to maintain and manage the environment variables and pay attention to the numerous files that can contain environment variables. In principle, any shell script can be used for initializing environmental variables, but following traditional UNIX conventions, these statements should only be present in some particular files.

The following files can be used for defining global environment variables on your system, each with different limitations:

The following Bash helper function can be used to append a number of directories to the PATH environment variable. Add the function at the top of the file where you define your environment (e.g. ~/.bashrc). The function will only add directories that actually exist on the filesystem, and it will avoid creating duplicate entries.

Most shells (including Bash, Zsh, and fish) allow adding variables to the environment using the export command. This allows defining environment variables in a common file such as ~/my-environment.sh:

This file can then be sourced from shell startup files:

The PAM module pam_env(8) loads the variables to be set in the environment from the following files in order: /etc/security/pam_env.conf and /etc/environment.

/etc/environment must consist of simple VARIABLE=value pairs on separate lines, for example:

/etc/security/pam_env.conf has the following format:

@{HOME} and @{SHELL} are special variables that expand to what is defined in /etc/passwd. The following example illustrates how to expand the HOME environment variable into another variable:

The format also allows to expand already defined variables in the values of other variables using ${VARIABLE} , like this:

VARIABLE=value pairs are also allowed, but variable expansion is not supported in those pairs. See pam_env.conf(5) for more information.

You do not always want to define an environment variable globally. For instance, you might want to add /home/my_user/bin to the PATH variable but do not want all other users on your system to have that in their PATH too. Local environment variables can be defined in many different files:

To add a directory to the PATH for local usage, put following in ~/.bash_profile:

To update the variable, re-login or source the file: $ source ~/.bash_profile.

If an environment variable only affects graphical applications, you may want to restrict the scope of it by only setting it within the graphical session. In order of decreasing scope:

Some graphical environments, (e.g. KDE Plasma) support executing shell scripts at login: they can be used to set environment variables. See KDE#Autostart for example.

The procedure for modifying the environment of the Xorg session depends on how it is started:

Though the end of the script depends on which file it is, and any advanced syntax depends on the shell used, the basic usage is universal:

Since Wayland does not initiate any Xorg related files, GDM and KDE Plasma source systemd user environment variables instead.

No other display managers supporting Wayland sessions (e.g. SDDM) provide direct support for this yet. However, LightDM and SDDM source startup scripts for login shells on Wayland sessions too.

greetd also sources /etc/profile and ~/.profile - this behavior is controlled by its source_profile setting, enabled by default.

If your display manager sources startup scripts like ~/.bash_profile and you want to use environment.d, you can source it like so:

To set environment variables only for a specific application instead of the whole session, edit the application's .desktop file. See Desktop entries#Modify environment variables for instructions.

For Steam games, you can configure a program's environment by editing its launch options; see Steam#Launch options.

Sometimes only temporary variables are required. One might wish to temporarily run executables from a specific directory without typing the absolute PATH each time, or use the PATH in a short temporary shell script.

For example, to add a session-specific directory to PATH, use:

To add only a shell-specific directory to PATH, use:

In Bash, PATH is already exported by default, so both of the above will remain visible to subprocesses unless overwritten. To better illustrate the difference between exported and non-exported variables, consider the following:

The following section lists a number of common environment variables used by a Linux system and describes their values.

**Examples:**

Example 1 (unknown):
```unknown
$ env EDITOR=vim xterm
```

Example 2 (unknown):
```unknown
/proc/$PID/environ
```

Example 3 (unknown):
```unknown
sed 's:\x0:\n:g' /proc/$PID/environ
```

Example 4 (unknown):
```unknown
/etc/profile
```

---

## System maintenance

**URL:** https://wiki.archlinux.org/title/Upgrade

**Contents:**
- Check for errors
  - Failed systemd services
  - Log files
- Backup
  - Configuration files
  - List of installed packages
  - Pacman database
  - Encryption metadata
  - System and user data
- Upgrading the system

Regular system maintenance is necessary for the proper functioning of Arch over a period of time. Timely maintenance is a practice many users get accustomed to.

Check if any systemd services have failed:

See systemd#Using units for more information.

Look for errors in the log files located in /var/log/, as well as messages logged in the systemd journal:

See Xorg#Troubleshooting for information on where and how Xorg logs errors.

Having backups of important data is a necessary measure to take, since human and machine processing errors are very likely to generate corruption as time passes, and also the physical media where the data is stored is inevitably destined to fail.

See Synchronization and backup programs for many alternative applications that may better suit your case. See Category:System recovery for other articles of interest.

It is highly encouraged to automate backups and test the recovery process to ensure everything works as intended. For automation see System backup#Automation.

Before editing any configuration files, create a backup so that you can revert to a working version in case of problems. Editors like vim and emacs can do this automatically. On a larger scale, consider using a configuration manager.

For dotfiles (configuration files in the home directory), see dotfiles#Tracking dotfiles directly with Git.

Maintain a list of all installed packages so that if a complete re-installation is inevitable, it is easier to re-create the original environment.

See pacman/Tips and tricks#List of installed packages for details.

See pacman/Tips and tricks#Back up the pacman database.

See Data-at-rest encryption#Backup for disk encryption scenarios.

It is recommended to perform full system upgrades regularly via pacman#Upgrading packages, to enjoy both the latest bug fixes and security updates, and also to avoid having to deal with too many package upgrades that require manual intervention at once. When requesting support from the community, it will usually be assumed that the system is up to date.

Make sure to have the Arch install media or another Linux "live" CD/USB available so you can easily rescue your system if there is a problem after updating. If you are running Arch in a production environment, or cannot afford downtime for any reason, test changes to configuration files, as well as updates to software packages, on a non-critical duplicate system first. Then, if no problems arise, roll out the changes to the production system.

If the system has packages from the AUR, carefully upgrade all of them.

pacman is a powerful package management tool, but it does not attempt to handle all corner cases. Users must be vigilant and take responsibility for maintaining their own system.

Before upgrading, users are expected to visit the Arch Linux home page to check the latest news, or alternatively subscribe to the RSS feed or the arch-announce mailing list. When updates require out-of-the-ordinary user intervention (more than what can be handled simply by following the instructions given by pacman), an appropriate news post will be made.

Before upgrading fundamental software (such as the kernel, xorg, systemd, or glibc) to a new version, look over the appropriate forum to see if there have been any reported problems.

Users must equally be aware that upgrading packages can raise unexpected problems that could need immediate intervention; therefore, it is discouraged to upgrade a stable system shortly before it is required for carrying out an important task. Instead, wait to upgrade until there is enough time available to resolve any post-upgrade issues.

Avoid doing partial upgrades. In other words, never run pacman -Sy; instead, always use pacman -Syu.

Generally avoid using the --overwrite option with pacman. The --overwrite option takes an argument containing a glob. When used, pacman will bypass file conflict checks for files that match the glob. In a properly maintained system, it should only be used when explicitly recommended by the Arch Developers. See the #Read before upgrading the system section.

Avoid using the -d option with pacman. pacman -Rdd package skips dependency checks during package removal. As a result, a package providing a critical dependency could be removed, resulting in a broken system.

Arch Linux is a rolling release distribution. That means when new library versions are pushed to the repositories, the Developers and Package Maintainers rebuild all the packages in the repositories that need to be rebuilt against the libraries. For example, if two packages depend on the same library, upgrading only one package might also upgrade the library (as a dependency), which might then break the other package which depends on an older version of the library.

That is why partial upgrades are not supported. Do not use:

When refreshing the package database, always do a full upgrade with pacman -Syu.

Be very careful when using IgnorePkg and IgnoreGroup for the same reason. If the system has locally built packages (such as AUR packages), users will need to rebuild them when their dependencies receive a soname bump.

If a partial upgrade scenario has been created, and binaries are broken because they cannot find the libraries they are linked against, do not "fix" the problem simply by symlinking. Libraries receive soname bumps when they are not backwards compatible. A simple pacman -Syu to a properly synced mirror will fix the problem as long as pacman is not broken.

When upgrading the system, be sure to pay attention to the alert notices provided by pacman. If any additional actions are required by the user, be sure to take care of them right away. If a pacman alert is confusing, search the forums or check the latest news on the Arch Linux homepage (see #Read before upgrading the system) for more detailed instructions.

When pacman is invoked, .pacnew and .pacsave files can be created. Pacman provides notice when this happens and users must deal with these files promptly. Users are referred to the pacman/Pacnew and Pacsave wiki page for detailed instructions.

Also, think about other configuration files you may have copied or created. If a package had an example configuration that you copied to your home directory, check to see if a new one has been created.

Upgrades are typically not applied to existing processes. You must restart processes to fully apply the upgrade.

The archlinux-contrib package provides a script called checkservices which runs pacdiff to merge .pacnew files then checks for processes running with outdated libraries and prompts the user if they want them to be restarted.

The kernel is particularly difficult to patch without a reboot. A reboot is always the most secure option, but if this is very inconvenient kernel live patching can be used to apply upgrades without a reboot.

If a package update is expected/known to cause problems, packagers will ensure that pacman displays an appropriate message when the package is updated. If experiencing trouble after an update, double-check pacman's output by looking at /var/log/pacman.log.

At this point, only after ensuring there is no information available through pacman, there is no relevant news on https://archlinux.org/, and there are no forum posts regarding the update, consider seeking help on the forum or over IRC. Downgrading the offending package to revert broken updates should be considered as a last resort.

After upgrading you may now have packages that are no longer needed or that are no longer in the official repositories.

Use pacman -Qtd to check for packages that were installed as a dependency but now, no other packages depend on them. If an orphaned package is still needed, it is recommended to change the installation reason to explicit. Otherwise, if the package is no longer needed, it can be removed. See pacman/Tips and tricks#Removing unused packages (orphans) for details.

Additionally, some packages may no longer be in the remote repositories, but they still may be on your local system. To list all foreign packages use pacman -Qm. Note that this list will include packages that have been installed manually (e.g., from the AUR). To exclude packages that are (still) available on the AUR, use the script from BBS#288205 or try the ancient-packagesAUR tool.

Pacman does a much better job than you at keeping track of files. If you install things manually you will, sooner or later, forget what you did, forget where you installed to, install conflicting software, install to the wrong locations, etc.

To clean up improperly installed files, see pacman/Tips and tricks#Identify files not owned by any package.

Always try open source drivers before resorting to proprietary drivers. Most of the time, open source drivers are more stable and reliable than proprietary drivers. Open source driver bugs are fixed more easily and quickly. While proprietary drivers can offer more features and capabilities, this can come at the cost of stability. To avoid this dilemma, try to choose hardware components known to have mature open source driver support with full features. Information about hardware with open source Linux drivers is available at linux-drivers.org.

Use precaution when using packages from the AUR or an unofficial user repository. Most are supplied by regular users and thus may not have the same standards as those in the official repositories. Always check PKGBUILDs for sanity and signs of mistake or malicious code before building and/or installing the package.

To simplify maintenance, limit the amount of unofficial packages used. Make periodic checks on which are in actual use, and remove (or replace with their official counterparts) any others. See pacman/Tips and tricks#Maintenance for useful commands. Following system upgrade, use rebuild-detector to identify any unofficial packages that may need to be rebuilt.

Update pacman's mirrorlist, as the quality of mirrors can vary over time, and some might go offline or their download rate might degrade.

See mirrors for details.

Programs that help with this can be found in List of applications/Utilities#Disk cleaning.

When looking for files to remove, it is important to find the files that take up the most disk space. Programs that help with this can be found in List of applications/Utilities#Disk usage display.

Remove unwanted .pkg files from /var/cache/pacman/pkg/ to free up disk space.

See pacman#Cleaning the package cache for more information.

Remove unused packages from the system to free up disk space and simplify maintenance.

See #Check for orphans and dropped packages.

Old configuration files may conflict with newer software versions, or corrupt over time. Remove unneeded configurations periodically, particularly in your home directory and ~/.config. For similar reasons, be careful when sharing home directories between installations.

Look for the following directories:

See XDG Base Directory support for more information about these directories.

To keep the home directory clean from temporary files created at the wrong place, it is a good idea to manage a list of unwanted files and remove them regularly, for example with rmshit.py.

rmlint-gitAUR can be used to find and optionally remove duplicate files, empty files, recursive empty directories and broken symlinks.

Old, broken symbolic links might be sitting around your system; you should remove them. Examples on achieving this can be found here and here. However, you should not blindly delete all broken symbolic links, as some of them serve a purpose [1].

To quickly list all the broken symlinks of permanent files on your system, use:

Then inspect and remove unnecessary entries from this list.

The following tips are generally not required, but certain users may find them useful.

Arch's rolling releases can be a boon for users who want to try the latest features and get upstream updates as soon as possible, but they can also make system maintenance more difficult. To simplify maintenance and improve stability, try to avoid cutting edge software and install only mature and proven software. Such packages are less likely to receive difficult upgrades such as major configuration changes or feature removals. Prefer software that has a strong and active development community, as well as a high number of competent users, in order to simplify support in the event of a problem.

Avoid any use of the testing repository, even individual packages from testing. These packages are experimental and not suitable for a stable system. Similarly, avoid packages which are built directly from upstream development sources. These are usually found in the AUR, with names including things like: "dev", "devel", "svn", "cvs", "git", etc.

The linux-lts package is an alternative Arch kernel package, and is available in the core repository. This particular kernel version has long-term support (LTS) from upstream, including security and bug fixes. It is useful if you use out-of-tree kernel modules and want to ensure their compatibility or if you want a fallback kernel in case a new kernel version causes problems.

To make it available as a boot option, you will need to update your boot loader's configuration file to use the LTS kernel and ram disk: vmlinuz-linux-lts and initramfs-linux-lts.img.

**Examples:**

Example 1 (unknown):
```unknown
$ systemctl --failed
```

Example 2 (unknown):
```unknown
# journalctl -b
```

Example 3 (unknown):
```unknown
pacman -Syu
```

Example 4 (unknown):
```unknown
--overwrite
```

---

## iwd

**URL:** https://wiki.archlinux.org/title/Iwctl

**Contents:**
- Installation
- Usage
  - iwctl
    - Connect to a network
    - Connect to a network using WPS/WSC
    - Disconnect from a network
    - Show device and connection information
    - Manage known networks
  - iwgtk
    - Indicator icon

iwd (iNet wireless daemon) is a wireless daemon for Linux written by Intel. The core goal of the project is to optimize resource utilization by not depending on any external libraries and instead utilizing features provided by the Linux Kernel to the maximum extent possible.

iwd can work in standalone mode or in combination with comprehensive network managers like ConnMan, systemd-networkd and NetworkManager.

Install the iwd package.

Optionally, third-party graphical and terminal user interface front-ends can be installed:

The iwd package provides the client program iwctl, the daemon iwd and the Wi-Fi monitoring tool iwmon.

Start/enable iwd.service so it can be controlled through the iwctl command or through your preferred iwd front-end.

To get an interactive prompt do:

The interactive prompt is then displayed with a prefix of [iwd]#.

To list all available commands:

First, if you do not know your wireless device name, list all Wi-Fi devices:

If the device or its corresponding adapter is turned off, turn it on:

Then, to initiate a scan for networks (note that this command will not output anything):

You can then list all available networks:

Finally, to connect to a network:

If network is hidden:

If a passphrase is required (and it is not already stored in one of the profiles that iwd automatically checks), you will be prompted to enter it. Alternatively, you can supply it as a command line argument:

If your network is configured such that you can connect to it by pressing a button (Wikipedia:Wi-Fi Protected Setup), check first that your network device is also capable of using this setup procedure.

Then, provided that your device appeared in the above list,

and push the button on your router. The procedure works also if the button was pushed beforehand, less than 2 minutes earlier.

If your network requires to validate a PIN number to connect that way, check the help command output to see how to provide the right options to the wsc command.

To disconnect from a network:

To display the details of a Wi-Fi device, like MAC address:

To display the connection state, including the connected network of a Wi-Fi device:

To list networks you have connected to previously:

To forget a known network:

Alternatively, iwgtkAUR provides a GUI front-end through which iwd can be controlled.

Running iwgtk without any arguments launches the application window, which can be used to toggle your adapters and devices on/off, change their operating modes, view available networks, connect to available networks, and manage known networks.

To launch iwgtk's indicator (tray) icon daemon, run:

If the indicator icon does not appear, then your system tray most likely lacks support for the StatusNotifierItem API, in which case you need to run a compatibility layer such as snixembed-gitAUR.

The following system trays support StatusNotifierItem, and therefore work out of the box:

The following trays only support XEmbed, and therefore require snixembed-gitAUR:

The most common use case for iwgtk is to start the indicator daemon every time you log into your desktop. If your desktop environment supports the XDG Autostart standard, this should happen automatically due to the iwgtk-indicator.desktop file which is placed in /etc/xdg/autostart/ by the AUR package.

Alternatively, a systemd unit file to start the indicator daemon is provided by the AUR package. If your desktop environment supports systemd's graphical-session.target unit, then iwgtk can be autostarted via systemd by enabling the iwgtk.service user unit.

By default, iwd stores the network configuration in the directory /var/lib/iwd. The configuration file is named as network.type, where network is the network SSID and .type is the network type, either .open, .psk or .8021x. The file is used to store the encrypted PreSharedKey and optionally the cleartext Passphrase and can also be created by the user without invoking iwctl. The file can be used for other configuration pertaining to that network SSID as well. For more settings, see iwd.network(5).

A minimal example file to connect to a WPA-PSK or WPA2-PSK secured network with SSID "spaceship" and passphrase "test1234":

To calculate the pre-shared key from the passphrase, one of these two methods can be used:

For connecting to a EAP-PWD protected enterprise access point you need to create a file called: essid.8021x in the /var/lib/iwd directory with the following content:

If you do not want autoconnect to the AP you can set the option to False and connect manually to the access point via iwctl. The same applies to the password, if you do not want to store it plaintext leave the option out of the file and just connect to the enterprise AP.

Like EAP-PWD, you also need to create a essid.8021x file in the directory. Before you proceed to write the configuration file, this is also a good time to find out which CA certificate your organization uses. This is an example configuration file that uses MSCHAPv2 password authentication:

MsCHAPv2 passwords can also be stored as an encrypted hash. The correct md4 hash can be calculated with:

Insert an EOF after your password by pressing Ctrl+d, do not hit Enter. The resulting hash needs to be stored inside the EAP-PEAP-Phase2-Password-Hash key.

Like EAP-PWD, you also need to create a essid.8021x file in the directory. Before you proceed to write the configuration file, this is also a good time to find out which CA certificate your organization uses. This is an example configuration file that uses PAP password authentication:

EAP-TLS uses x509 client certificates to authenticate you. Like ssh keys, these use public-key cryptography, so the Wi-Fi authentication server never needs to be sent a secret, and you do not need to copy and reuse a password between devices. Usually each device will use a distinct cert, one that can, in theory at least, be revoked without forcing you to change a password or disrupt your other devices.

As with the other enterprise methods you need to know the CA cert your organization uses (cacert.pem), which is used to prove to your device it is connecting to the right place. You also need to have the client certificate, which represents you and will be uploaded on each connection (client-cert.pem), and the private key that goes with it (client-key.pem), which is used to prove you own that client certificate.

You can either provide a path to the required certificate or you can embed them inside your configuration.

When you have collected the credentials, put this in your /var/lib/iwd/essid.8021x file:

This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.

One possible method of connecting to eduroam via iwd is provided here. Create the following file, filling in the necessary values:

The factual accuracy of this article or section is disputed.

If that does not work, eduroam also offers a configuration assistant tool (CAT). If your organisation has a profile within the CAT, getting connected to eduroam can be done by downloading the Linux script and running it using python. If your organisation does not support CAT, you will have to create the configuration file manually using parameters provided to you by the administrators (the below table can be helpful in doing so). It is possible to extract the necessary configuration options from the generated configuration, including the certificate and server domain mask. Additionally, some institutions are upgrading to EAP-TLS, and outsourcing the generation of client-cert.pem to SecureW2, in which case you will need to use their tool as well to generate a client cert.

The following table contains a mapping of iwd configuration options to eduroam CAT install script variables.

where method is the content of EAP-Method and should be either TLS, TTLS or PEAP. Once you have extracted all necessary information and converted them to their iwd configuration equivalent you can put them in a configuration file called essid.8021x as explained in the preceding methods.

More example tests can be found in the test cases of the upstream repository.

Instead of including an absolute path to a PEM file (for certificates and keys), the PEM itself can be included inside the network configuration file.

An embedded PEM can appear anywhere in the settings file using the following format:

where my_ca_cert is any name you can use to identify the certificate inside the configuration file.

Then the embedded certificate can be used anywhere in the settings file a certificate path is required by prefixing the value with embed:

This is not limited to CA certificates either. Client certificates, client keys (encrypted or not), and certificate chains can be included.

For WPA on a wired ethernet connection create a config as above, but place it in the /var/lib/ead directory instead.

Afterwards Start/enable ead.service.

File /etc/iwd/main.conf can be used for main configuration. See iwd.config(5).

Create or edit the file /var/lib/iwd/network.type. Add the following section to it:

By default when iwd is in disconnected state, it periodically scans for available networks. To disable periodic scan (so as to always scan manually), create / edit file /etc/iwd/main.conf and add the following section to it:

Since version 0.19, iwd can assign IP address(es) and set up routes using a built-in DHCP client or with static configuration. It is a good alternative to standalone DHCP clients.

To activate iwd's network configuration feature, create/edit /etc/iwd/main.conf and add the following section to it:

There is also ability to set route metric with RoutePriorityOffset:

Since version 1.10, iwd supports IPv6, but it is disabled by default in versions below 2.0. Since version 2.0, it is enabled by default.

To disable it, add the following to the configuration file:

To enable it in version below 2.0 and higher than 1.10:

This setting is required to be enabled whether you want to use DHCPv6 or static IPv6 configuration. It can also be set on a per-network basis.

Add the following section to /var/lib/iwd/network.type file. For example:

At the moment, iwd supports two DNS managers—systemd-resolved and resolvconf.

Add the following section to /etc/iwd/main.conf for systemd-resolved:

If you want to allow any user to read the status information, but not modify the settings, you can create the following D-Bus configuration file:

This article or section needs expansion.

By default, iwd stores network credentials to the system unencrypted. Since iwd version 1.25, iwd provides experimental support for creating encrypted profiles for systems using systemd.

First, create an encrypted credential. The following example uses systemd-creds and creates an encrypted credential called iwd-secret that is bound to the system's Trusted Platform Module which will be used to create encrypted profiles:

Next, add the LoadCredentialEncrypted option by creating a drop-in file for the iwd service.

Finally, add the SystemdEncrypt option with the value being the named credential to the iwd configuration file, reload the systemd manager, and restart the iwd service.

This can be useful, if you have trouble setting up MSCHAPv2 or TTLS. You can set the following environment variable via a drop-in snippet:

Check the iwd logs afterwards by running journalctl -u iwd.service as root.

On some machines, it is reported that iwd.service has to be restarted to work after boot. See FS#63912 and thread 251432. This probably occurs because iwd starts before wireless network card powers on.

As a workaround, find the unit needed to wait for by systemctl list-units --type=device | grep wlan0 and extend the unit accordingly:

Then reload the systemd manager configuration.

If it does not work, try also

Since version 1.0, iwd disables network interface renaming to predictable network interface names. It installs the following systemd.link(5) configuration file which prevents udev from renaming the interface to a predictable, stable name (e.g. wlp#s#):

As a result the wireless link name wlan# is kept after boot. This resolved a race condition between iwd and udev on interface renaming as explained in iwd udev interface renaming.

If this results in issues try masking it with:

Clients may not receive an IP address via DHCP when connecting to iwd in AP mode. It is therefore necessary to enable network configuration by iwd on managed interfaces:

The mentioned file has to be created if it does not already exist.

Some users experience disconnections with Wi-Fi, re-connecting continuously but stabilizing eventually and managing to connect.

Users report crashes ([1]) of iwd.service in their journal.

The core issue is having multiple conflicting services for managing their network connections. Check that you do not have enabled them at the same time to fix this issue.

To load key files iwd requires the pkcs8_key_parser kernel module. While on boot it gets loaded by systemd-modules-load.service(8) using /usr/lib/modules-load.d/pkcs8.conf, that will not be the case if iwd has just been installed.

If messages such as Error loading client private key /path/to/key show up in the journal when trying to connect to WPA Enterprise networks, manually load the module:

iwd will roam to other known APs if the connection is too bad.

This will show up in the system log as wlan0: deauthenticating from xx:xx:xx:xx:xx:xx by local choice (Reason: 3=DEAUTH_LEAVING)

You can see the connection signal strength with

You can increase the threshold to allow a worse connection. RoamThreshold defaults to -70 and RoamThreshold5G to -76.

Set SendHostname in the network's configuration file, not in /etc/iwd/main.conf.

When using resolvconf as DNS resolution method, it may have trouble writing to /etc/resolv.conf complaining about read-only file system:

To fix this problem, extend the configuration of the iwd.service systemd unit by adding drop-in file:

This will allow the iwd.service system unit to update /etc/resolv.conf. Restart iwd.service to make the change effective.

**Examples:**

Example 1 (unknown):
```unknown
iwd.service
```

Example 2 (unknown):
```unknown
iwctl device wlan0 show
```

Example 3 (unknown):
```unknown
[iwd]# help
```

Example 4 (unknown):
```unknown
[iwd]# device list
```

---

## Btrfs

**URL:** https://wiki.archlinux.org/title/Btrfs

**Contents:**
- Preparation
- File system creation
  - File system on a single device
  - Multi-device file system
    - Profiles
- Configuring the file system
  - Copy-on-Write (CoW)
    - Disabling CoW
      - Cases where CoW is still triggered
        - Effect on copying

From the Btrfs Documentation:

For user space utilities, install the btrfs-progs package that is required for basic operations.

If you need to boot from a Btrfs file system (i.e., your kernel and initramfs reside on a Btrfs partition), check if your boot loader supports Btrfs.

The following shows how to create a new Btrfs file system. To convert an Ext3/4 partition to Btrfs, see #Ext3/4 to Btrfs conversion. To use a partitionless setup, see #Partitionless Btrfs disk.

See mkfs.btrfs(8) for more information.

To create a Btrfs file system on partition /dev/partition:

The Btrfs default nodesize for metadata is 16 KiB, while the default sectorsize for data is equal to page size and autodetected. To use a larger nodesize for metadata (must be a multiple of sectorsize, up to 64 KiB is allowed), specify a value for the nodesize via the -n switch as shown in this example using 32 KiB blocks:

Multiple devices can be used to create a RAID. Supported RAID levels include RAID 0, RAID 1, RAID 10, RAID 5 and RAID 6. Starting from kernel 5.5 RAID1c3 and RAID1c4 for 3- and 4- copies of RAID 1 level. The RAID levels can be configured separately for data and metadata using the -d and -m options respectively. By default, the data has one copy (single) and the metadata is mirrored (raid1). This is similar to creating a JBOD configuration, where disks are seen as one file system, but files are not duplicated. See Using Btrfs with Multiple Devices for more information about how to create a Btrfs RAID volume.

You must include either the udev hook, systemd hook or the btrfs hook in /etc/mkinitcpio.conf in order to use multiple Btrfs devices in a pool. See the Mkinitcpio#Common hooks article for more information.

Once the file system is created, it is advised to run the following command to scan for multi-device Btrfs file systems and register them, allowing to mount the multi-device file system by specifying only one member:

See #RAID for advice on maintenance specific to multi-device Btrfs file systems.

Btrfs uses the concept of profiles to configure mirroring, parity, and striping. In standard RAID terminology, this is called RAID level. Profiles for metadata (the -m option of mkfs.btrfs(8)) and data (the -d option of mkfs.btrfs(8)) may be different for the same Btrfs file system.

Some notable profiles:

By default, Btrfs uses copy-on-write for all files all the time. Writes do not overwrite data in place; instead, a modified copy of the block is written to a new location, and metadata is updated to point at the new location. See the Btrfs Sysadmin Guide section for implementation details, as well as advantages and disadvantages.

To disable copy-on-write for newly created files in a mounted subvolume, use the nodatacow mount option. This will only affect newly created files. Copy-on-write will still happen for existing files. The nodatacow option also disables compression. See btrfs(5) for details.

To disable copy-on-write for single files/directories, do:

Copy-on-Write will still be triggered if a file with the NOCOW attribute (+C) has more than one reference (for example, a copy created via cp using reflink or a snapshot made by Btrfs).

This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.

When using cp to copy files, the NOCOW attribute is not determined by the source file, but by the destination path—either inherited from the directory or determined by whether the subvolume is mounted with the nodatacow option. Reflink copying is possible only if both the source and destination files either have or do not have the NOCOW attribute. The actual behavior depends on the --reflink option, with three possible outcomes:

For more information on the --reflink option, refer to cp(1).

If a file has copy-on-write disabled (NOCOW) and a snapshot is taken, the first write to a file block after the snapshot will be a COW operation because the snapshot locks the old file blocks in place. However, the file will retain the NOCOW attribute and any subsequent writes to the same file block will be in-place until the next snapshot.

Frequent snapshots can reduce the effectiveness of NOCOW, as COW is still required on the first write. To avoid copy-on-write for such files altogether, put them in a separate subvolume and do not take snapshots of that subvolume.

Btrfs supports transparent and automatic compression. This reduces the size of files as well as significantly increases the lifespan of flash-based media by reducing write amplification. See Fedora:Changes/BtrfsByDefault#Compression, [2], and [3]. It can also improve performance, in some cases (e.g. single thread with heavy file I/O), while obviously harming performance in other cases (e.g. multi-threaded and/or CPU intensive tasks with large file I/O). Better performance is generally achieved with the fastest compress algorithms, zstd and lzo, and some benchmarks provide detailed comparisons.

Btrfs supports the compression algorithms zlib (DEFLATE provided by zlib), lzo (LZO), and zstd (Zstandard implemented by zstd). LZO has no compression levels, whereas zlib and zstd have adjustable compression levels (zlib: 1-9; zstd: -15 to -1, 1-15, must be integers). Higher levels provide better compression but require more processing time. Changing the levels will affect CPU and I/O throughput differently, so they should be checked / benchmarked before and after changing. For more information about compression types, see btrfs(5) § COMPRESSION.

The compress=alg[:level] mount option enables automatically considering compression for every write to the filesystem, where alg is either zlib, lzo, zstd, or no (for no compression), and the optional level specifies the compression level (defaults to 3 when not specified or set to 0; not applicable for lzo). Using this option, Btrfs will check if compressing the first data block of a file write shrinks it. If it does, the entire write to that file will be compressed. If it does not, the file is marked as NOCOMPRESS, and both the current write and all subsequent writes to that file will not be compressed [4]. This is done to prevent making the disk wait to start writing until all of the data to be written is fully given to Btrfs and compression is attempted.

This article or section needs expansion.

The compress-force=alg[:level] mount option can be used instead, which makes Btrfs check each data block of every write and decide whether to compress them individually. Empirical testing on multiple mixed-use systems showed that using compress-force=zstd can significantly improve space savings (from 10% to 20%) compared to compress=zstd, but this causes (slightly) higher CPU usage and longer write times without purpose. However, forcing compression is against official Btrfs guidelines.

Only files created or modified after adding either of the above mount options will be automatically compressed.

To apply compression to existing files, use the btrfs filesystem defragment -calg command, where alg is either zlib, lzo or zstd. To specify the compression level, use the -L option, otherwise the default level is used (not applicable for lzo). For example, to re-compress all files with zstd and compression level 1, run the following command:

Using the above method to compress files is not persistent; other write operations will apply the original compression settings.

The following two methods can persistently enable compression for individual files:

The first command uses the legacy interface of file attributes inherited from the ext2 filesystem and is not flexible, defaulting to zlib compression algorithm. The second command can specify the compression algorithm, but specifying compression levels is not yet implemented, so the default level 3 is used (not applicable for lzo).

compsize takes a list of files or directories (or an entire Btrfs file system) and outputs the compression types used and actual compression ratios (compressed size/uncompressed size). Uncompressed size may not match the number given by other programs such as du(1), because every extent is counted once and only once—even if it has multiple reflinks, or if parts of it are no longer used anywhere but have not been garbage collected.

The -x option keeps it on a single file system, which is useful in situations like compsize -x / to avoid it from attempting to access non-Btrfs subdirectories and failing the entire run.

"A Btrfs subvolume is not a block device (and cannot be treated as one) instead, a Btrfs subvolume can be thought of as a POSIX file namespace. This namespace can be accessed via the top-level subvolume of the file system, or it can be mounted in its own right." [6]

Each Btrfs file system has a top-level subvolume with ID 5. This subvolume cannot be removed or replaced by another subvolume. The top-level subvolume has path / on the file system and other subvolumes are nested below the top-level subvolume. However, subvolumes can be moved around in the file system and their path may change, whereas their ID cannot.

By default, the top-level subvolume is mounted when mounting the file system. Options allow to mount a specific subvolume instead.

A major use case for subvolumes are snapshots.

See the following links for more details:

To create a subvolume, the Btrfs file system must be mounted.

To see a list of all subvolumes of the filesystem that path belongs to:

-t triggers a more readable table view.

To delete a subvolume:

If the subvolume contains other subvolumes that should be deleted, add the -R/--recursive option. Alternatively, a subvolume can be deleted like a regular directory (rm -r, rmdir).

Subvolumes can be mounted using the subvol=/path/to/subvolume or subvolid=objectid mount options. One can mimic traditional file system partitions by creating various subvolumes under the top level of the file system and then mounting them at the appropriate mount points.

For example, the following mounts the top-level subvolume to /mnt/ and creates two subvolumes named subvol_root and subvol_home. When the top-level subvolume is unmounted, the new subvolumes are mounted to /mnt/ and /mnt/home/:

Referencing a subvolume with the subvol mount option requires using the path relative to the top-level subvolume. This can be different from the path where the top-level subvolume is mounted.

See Snapper#Suggested filesystem layout, Btrfs SysadminGuide#Managing Snapshots, and SysadminGuide#Layout for example file system layouts using subvolumes.

To use a subvolume as the root mountpoint, either make it the default subvolume, or specify the subvolume via a kernel parameter using rootflags=subvol=/path/to/subvolume. Edit the root mountpoint in /etc/fstab and specify the mount option subvol=. Alternatively, the subvolume can be specified with its id, rootflags=subvolid=objectid as kernel parameter and subvolid=objectid as mount option in /etc/fstab. It is preferable to mount using subvol=/path/to/subvolume, rather than the subvolid, as the subvolid may change when restoring #Snapshots, requiring a change of mount configuration, or else the system will not boot.

The default sub-volume is mounted if no subvol= mount option is provided. To change the default subvolume, do:

where subvolume-id can be found by listing.

Changing the default subvolume with btrfs subvolume set-default will make the top level of the file system inaccessible, except by use of the subvol=/ or subvolid=5 mount options [7].

The concept of disk quota has a long tradition in the Unix world. Traditional quotas are based on file ownership, controlling space usage by limiting the total size of all files owned by users. While this approach is straightforward for file management, it has limitations in directory-level restrictions, typically requiring partitioning at installation time and lacking dynamic adjustment capabilities. Btrfs adopts a different quota design philosophy from traditional Unix systems and does not support traditional user- or group-based quota functionality. This is because its modern storage features (such as #Snapshots, #Copy-on-Write (CoW), #Deduplication, and #Compression) make traditional quota space calculations extremely complex and inaccurate [8]. As an alternative, Btrfs uses a subvolume-based quota mechanism, currently providing two implementation approaches — traditional #Quota groups (qgroups) and the newer #Simple quotas (squotas) (introduced in kernel 6.7 released in late 2023) to achieve similar space management effects. While this increases management complexity, it provides more flexible and precise storage control capabilities, allowing directory size limitations without partitioning and enabling dynamic quota adjustments.

Recent kernel improvements (5.15 and later versions) have fixed many historical issues, but some limitations remain, particularly in advanced application scenarios such as deeply nested subvolumes or complex qgroup hierarchical structures. The suggestion written here since 2016 — to carefully evaluate specific use cases before enabling qgroups — still applies today. Please be sure to consult the latest Btrfs documentation for current detailed information. It is worth noting that to address the performance issues of qgroups, Btrfs introduced the #Simple quotas (squotas) functionality, providing an alternative solution with slightly less flexibility but better performance.

Btrfs's precise quota support is implemented through qgroups above subvolumes. Qgroups are represented using qgroupids in the format level/ID. Subvolumes have level 0, where 0/ can be omitted, and the corresponding ID is the subvolume ID. For example, 0/5 represents the qgroup of the top-level subvolume. Subvolume paths can also be used instead of the 0/ID representation. Qgroups form a tree hierarchy, but a qgroup can have multiple parent qgroups [9]. Leaf qgroups have level 0, corresponding directly to subvolumes. The ID of all higher-level qgroups can be freely specified. Higher-level qgroups contain lower-level qgroups, and same-level groups cannot be nested, but level does not indicate nesting depth. For example, child qgroup 0/5 can be assigned to qgroup 5/100.

Each qgroup primarily tracks two values: the amount of total referenced space and the amount of exclusively referenced space. Total referenced space refers to the storage space occupied by all data accessible from within the qgroup, while exclusively referenced space refers to the storage space occupied by data that is only referenced by subvolumes belonging to the qgroup (and not referenced by subvolumes outside the qgroup).

Use the following command to enable qgroups for the filesystem containing path:

Change enable to disable to disable qgroups.

Level 0 qgroups corresponding to subvolumes are always automatically created. Even if subvolumes are created before enabling quotas, the corresponding qgroups will be automatically created when quotas are enabled. Similarly, when deleting subvolumes, the corresponding qgroups will be deleted and automatically disassociate from higher-level qgroups that originally contained these qgroups.

Use the following command to create qgroup qgroupid in the filesystem containing path:

Change create to destroy to destroy the qgroup. Destroying a qgroup is similar to using rm -d to delete a directory — if the qgroup has child qgroups, relationships must be removed first; if only parent qgroups exist, relationships will be automatically removed.

Use the following command to assign src as the child qgroup of dst in the filesystem containing path, note that dst must be at a higher level than src:

Change assign to remove to remove this relationship.

You can limit the total referenced space or exclusively referenced space of qgroups, or limit both simultaneously. For example, use the following command to limit the total referenced space of qgroupid to no more than 1GiB in the filesystem containing path:

If qgroupid is omitted, it attempts to interpret path as a subvolume path. Add the -e option (after limit) to limit exclusively referenced space instead. Change size to none to remove the limit.

Use the following command to list all qgroups in the filesystem containing path:

The "Path" column in the output may display some special values with the following meanings:

According to your needs, you can add some display options (after show). Common options include:

For more options, please refer to btrfs-qgroup(8) § show.

Quota rescan reads the metadata of all extents in the filesystem and updates the statistics of each qgroup accordingly.

Btrfs qgroups can handle many complex extent sharing and unsharing scenarios (including deletion of subvolumes and child qgroups) while maintaining accurate counts of total referenced space and exclusively referenced space. However, when qgroup relationships change (manual assignment/removal of child qgroups), due to insufficient recorded data (only recording total referenced space and exclusively referenced space, without explicitly recording which extents belong to shared or exclusive), quota rescan across the entire filesystem is usually required. At this time, qgroups will be marked as "inconsistent", indicating a state that requires quota rescan.

The only exception in the current implementation is when a qgroup's total referenced space equals its exclusively referenced space, meaning all data is exclusive. In this case, when performing assign/remove operations on this subvolume, it only needs to add/subtract this qgroup's referenced space to/from the parent qgroup's total referenced space and exclusively referenced space simultaneously.

Starting from kernel version 4.19, manual assignment/removal of child qgroups automatically triggers quota rescan (when necessary). However, since quota rescan has high overhead, can only be performed once at a time, and this method cannot wait for the rescan to complete before returning, you can add the --no-rescan option (after assign/remove) to avoid automatic triggering and manually trigger it later.

Use the following command to manually trigger quota rescan for the filesystem containing path:

The command returns immediately. Add the -w (--wait) option (after rescan) to wait for the rescan to complete (even if already started) before returning. Use the -W (--wait-norescan) option to only wait for the currently running rescan to complete. Use the -s (--status) option to return current progress.

Qgroups can handle many complex extent sharing and unsharing scenarios (including deletion of subvolumes and child qgroups) while maintaining accurate counts of total referenced space and exclusively referenced space. However, this flexibility comes at a cost: many computational operations are global in nature, meaning that when extent reference relationships change, they affect the statistics of all qgroups that reference the extent. This can lead to slower transaction commit speeds and unacceptable delays, especially when the number of snapshots increases dramatically.

To address this limitation of qgroups, Btrfs supports a second set of quota semantics: simple quotas (squotas). Squotas reuse the qgroup API and hierarchical structure model but do not track shared and exclusive usage. Instead, squotas attribute all extents to the subvolume that first allocated them. By introducing a small amount of new metadata records, this allows all accounting decisions to be completed locally in operations involving the allocation or release of data blocks themselves, completely avoiding complex and time-consuming reverse reference resolution processes.

Squotas need to be enabled at the filesystem level by simply adding the -s (--simple) option (after enable) to the enabling qgroups command. Note that if qgroups are already enabled, you need to first disable qgroups and then enable squotas, otherwise it will not take effect [10]. Due to its simple calculation method, squota mode does not need (and cannot) perform #Quota rescan. Apart from these two points, squotas operate exactly the same as qgroups, but the so-called "total referenced space" and "exclusively referenced space" displayed and limited are all values calculated through the method described above (attributing all extents to the subvolume that first allocated them). Since squotas need to record the subvolume that first allocated extents, they only take effect for extents written after enabling. If squotas are enabled on a non-empty filesystem, the statistics will be 0 when first enabled, so only by enabling squota limits on empty subvolumes and then writing data can you ensure accurate statistics.

The resolution at which data are written to the file system is dictated by Btrfs itself and by system-wide settings. Btrfs defaults to a 30 seconds checkpoint interval in which new data are committed to the file system. This can be changed by appending the commit mount option in /etc/fstab for the Btrfs partition.

System-wide settings also affect commit intervals. They include the files under /proc/sys/vm/* and are out-of-scope of this wiki article. The kernel documentation for them is available at https://docs.kernel.org/admin-guide/sysctl/vm.html.

A Btrfs file system is able to free unused blocks from an SSD drive supporting the TRIM command. Asynchronous discard support is available with mount option discard=async, and is enabled by default as of linux 6.2. Freed extents are not discarded immediately, but grouped together and trimmed later by a separate worker thread, improving commit latency.

Asynchronous discard can safely be used alongside periodic trim [11].

More information about enabling and using TRIM can be found in Solid state drive#TRIM.

There are some limitations of the implementation in Btrfs and Linux swap subsystem [12]:

The last two limitations for swap files are automatically handled by the btrfs filesystem mkswapfile command.

For general information about swap files, see Swap#Swap file.

To properly create a swap file on Btrfs, first create a subvolume to store the swap file, as snapshots cannot be created for that subvolume afterwards. For example, create the /swap subvolume:

Create a 4 GiB swap file swapfile in /swap/:

The --size option specifies the swap file size, with a default of 2 GiB and minimum of 40 KiB.

Activate the swap file:

Finally, edit the fstab configuration to add an entry for the swap file:

For additional information, see fstab#Usage.

For more information about removing swap files and other details, see Swap#Swap file.

General linux userspace tools such as df(1) will inaccurately report free space on a Btrfs partition as they do not account for both file and metadata usage. It is recommended to use btrfs filesystem usage to query Btrfs partitions. For example, for a full breakdown of device allocation and usage stats:

Alternatively, btrfs filesystem df allows a quick check on usage of allocated space without the requirement to run as root:

See [14] for more information.

The same limitations apply to tools which analyze space usage for some subset of the file system, such as du(1) or ncdu(1), as they do not consider #Copy-on-Write (CoW) or transparent #Compression. Instead, see btduAUR and compsize for Btrfs-aware alternatives.

Btrfs supports online defragmentation through the mount option autodefrag; see btrfs(5) § MOUNT OPTIONS. To manually defragment your root (not descending to subvolumes, mount points and directory symlinks), use:

Btrfs offers native "RAID" for #Multi-device file systems. Notable features which set Btrfs RAID apart from mdadm are self-healing redundant arrays and online balancing. See the Btrfs wiki page for more information. The Btrfs sysadmin page also has a section with some more technical background.

This article or section needs expansion.

The Btrfs Wiki Glossary says that Btrfs scrub is "[a]n online file system checking tool. Reads all the data and metadata on the file system and uses checksums and the duplicate copies from RAID storage to identify and repair any corrupt data."

To start a (background) scrub on the file system which contains /:

To check the status of a running scrub:

The btrfs-progs package brings the btrfs-scrub@.timer unit for monthly scrubbing the specified mountpoint. Enable the timer with an escaped path, e.g. btrfs-scrub@-.timer for / and btrfs-scrub@home.timer for /home. You can use systemd-escape -p /path/to/mountpoint to escape the path; see systemd-escape(1) for details.

You can also run the scrub by starting btrfs-scrub@.service (with the same encoded path). The advantage of this over btrfs scrub (as the root user) is that the results of the scrub will be logged in the systemd journal.

On large NVMe drives with insufficient cooling (e.g. in a laptop), scrubbing can read the drive fast enough and long enough to get it very hot. If you are running scrubs with systemd, you can easily limit the rate of scrubbing with the IOReadBandwidthMax option described in systemd.resource-control(5) by using a drop-in file.

"A balance passes all data in the file system through the allocator again. It is primarily intended to rebalance the data in the file system across the devices when a device is added or removed. A balance will regenerate missing copies for the redundant RAID levels, if a device has failed." [16] See Upstream FAQ page.

On a single-device filesystem, a balance may be also useful for (temporarily) reducing the amount of allocated but unused (meta)data chunks. Sometimes this is needed for fixing "file system full" issues.

"A snapshot is simply a subvolume that shares its data (and metadata) with some other subvolume, using Btrfs's COW capabilities." See Btrfs Wiki SysadminGuide#Snapshots and the Btrfs documentation for details.

To create a snapshot:

To create a readonly snapshot, add the -r flag. To create a writable version of a readonly snapshot, simply create a snapshot of it.

A subvolume can be sent to stdout or a file using the send command. This is usually most useful when piped to a Btrfs receive command. For example, to send a snapshot named /root_backup (perhaps of a snapshot you made of / earlier) to /backup, you would do the following:

The snapshot that is sent must be readonly. The above command is useful for copying a subvolume to an external device (e.g. a USB disk mounted at /backup above).

The subvolume will be created on the receiving end. It does not need to be created manually.

Another example which creates: /mnt/arch-v2/subvolumes/@var:

The parameters --proto 2 and --compressed-data used in the example might be useful for more efficient sending (assumes compressed data).

You can also send only the difference between two snapshots. For example, if you have already sent a copy of root_backup above and have made a new readonly snapshot on your system named root_backup_new, then to send only the incremental difference to /backup, do:

Now, a new subvolume named root_backup_new will be present in /backup.

See Btrfs Wiki's Incremental Backup page and #Incremental backup to external drive on how to use this for incremental backups and for tools that automate the process.

Using copy-on-write, Btrfs is able to copy files or whole subvolumes without actually copying the data. However, whenever a file is altered, a new proper copy is created. Deduplication takes this a step further by actively identifying blocks of data which share common sequences and combining them into an extent with the same copy-on-write semantics.

Tools dedicated to deduplicate a Btrfs formatted partition include duperemove and bees. One may also want to merely deduplicate data on a file based level instead using e.g. rmlint-gitAUR, rdfind, jdupesAUR or dduper-gitAUR. For an overview of available features of those programs and additional information, have a look at the upstream Wiki entry.

You can grow a file system to the maximum space available on the device, or specify an exact size. Ensure that you grow the size of the device or logical volume before you attempt to increase the size of the file system. When specifying an exact size for the file system on a device, either increasing or decreasing, ensure that the new size satisfies the following conditions:

To extend the file system size to the maximum available size of the device:

To extend the file system to a specific size:

Replace size with the desired size in bytes. You can also specify units on the value, such as K (kibibytes), M (mebibytes), or G (gibibytes). Alternatively, you can specify an increase or decrease to the current size by prefixing the value with a plus (+) or a minus (-) sign, respectively:

A few limitations should be known before trying.

Btrfs has no built-in encryption support, but there is an ongoing effort [17] to incorporate encryption based on Fscrypt.

Users can however encrypt the partition before running mkfs.btrfs, see dm-crypt/Encrypting an entire system. Another approach is stacked filesystem encryption.

The tool btrfs check has known issues and should not be run without further reading; see section #btrfs check.

Btrfs can occupy an entire data storage device, replacing the MBR or GPT partitioning schemes, using subvolumes to simulate partitions. However, using a partitionless setup is not required to simply create a Btrfs file system on an existing partition that was created using another method. There are some limitations to partitionless single disk setups:

To create a partitionless Btrfs disk, run the following command:

For example, use /dev/sda rather than /dev/sda1. The latter would format an existing partition instead of replacing the entire partitioning scheme. Because the root partition is Btrfs, make sure btrfs is compiled into the kernel, or put btrfs into mkinitcpio.conf#MODULES and regenerate the initramfs.

Install the boot loader like you would for a data storage device with a Master Boot Record. See Syslinux#Manually or GRUB/Tips and tricks#Install to partition or partitionless disk. If your kernel does not boot due to Failed to mount /sysroot., please add GRUB_PRELOAD_MODULES="btrfs" in /etc/default/grub and generate the grub configuration.

Boot from an install CD, then convert by doing:

Mount the partition and test the conversion by checking the files. Be sure to change the /etc/fstab to reflect the change (type to btrfs and fs_passno −the last field− to 0 as Btrfs does not do a file system check on boot). Also note that the UUID of the partition will have changed, so update fstab accordingly when using UUIDs. chroot into the system and rebuild your boot loader's menu list (see Install from existing Linux). If converting a root file system, while still chrooted, regenerate the initramfs or the system will not successfully boot.

After confirming that there are no problems, complete the conversion by deleting the backup ext2_saved sub-volume. Note that you cannot revert back to ext3/4 without it.

Remember that some applications which were installed prior have to be adapted to Btrfs.

NTFS can be converted using ntfs2btrfsAUR or ntfs2btrfs-gitAUR.

Unmount the target filesystem and run the conversion, the exact compression and checksum types can be specified

If everything works well after the conversion you can delete the image/ntfs.img image otherwise used for restoring the original filesystem.

There are some steps you can take after converting an existing filesystem into Btrfs.

Make file data more contiguous

#Balance to make Btrfs metadata more compact

btrfs-check cannot be used on a mounted file system. To be able to use btrfs-check without booting from a live USB, add it to the initial ramdisk:

Regenerate the initramfs.

Then if there is a problem booting, the utility is available for repair.

See btrfs-check(8) for more information.

In order to boot into a snapshot, the same procedure applies as for mounting a subvolume as your root partition, as given in section mounting a subvolume as your root partition, because snapshots can be mounted like subvolumes.

See the systemd-nspawn#Use Btrfs subvolume as container root and systemd-nspawn#Use temporary Btrfs snapshot of container articles.

Because of the copy-on-write nature of Btrfs, simply accessing files can trigger the metadata copy and writing. Reducing the frequency of access time updates may eliminate this unexpected disk usage and increase performance. See fstab#atime options for the available options.

The following packages use btrfs send and btrfs receive to send backups incrementally to an external drive. Refer to their documentation to see differences in implementation, features, and requirements.

The following package allows backing up snapper snapshots to non-Btrfs file systems:

For the managing and automatic creation of Btrfs snapshot, one may use a snapshot manager such as Snapper, Timeshift or Yabsnap.

Desktop notifications help you notice serious Btrfs issues immediately, offering better awareness compared to having no notifications at all.

btrfs-desktop-notificationAUR provides desktop notifications for the following events:

See https://gitlab.com/Zesko/btrfs-desktop-notification for more information and configuration.

See the Btrfs Troubleshooting pages and Btrfs Problem FAQ for general troubleshooting.

The offset problem may happen when you try to embed core.img into a partitioned disk. It means that it is OK to embed GRUB's core.img into a Btrfs pool on a partitionless disk (e.g. /dev/sdX) directly.

GRUB can boot Btrfs partitions, however the module may be larger than other file systems. And the core.img file made by grub-install may not fit in the first 63 sectors (31.5KiB) of the drive between the MBR and the first partition. Up-to-date partitioning tools such as fdisk and gdisk avoid this issue by offsetting the first partition by roughly 1MiB or 2MiB.

The factual accuracy of this article or section is disputed.

Users experiencing the following: error no such device: root when booting from a RAID style setup then edit /usr/share/grub/grub-mkconfig_lib and remove both quotes from the line echo " search --no-floppy --fs-uuid --set=root ${hints} ${fs_uuid}". Regenerate the config for grub and the system should boot without an error.

Sometimes a large (> 4TB) Btrfs volume can take a long time to mount, slowing down boots. Changing the group tree to block group tree can help with that

Sometimes, especially with large RAID1 arrays, mounting might time out during boot with a journal message such as:

This can easily be worked around by providing a longer timeout via the systemd-specific mount option x-systemd.mount-timeout in fstab. For example:

The factual accuracy of this article or section is disputed.

As of November 2014, there seems to be a bug in systemd or mkinitcpio causing the following error on systems with multi-device Btrfs file system using the btrfs hook in mkinitcpio.conf:

A workaround is to remove btrfs from the HOOKS array in /etc/mkinitcpio.conf and instead add btrfs to the MODULES array. Then regenerate the initramfs and reboot.

You will get the same error if you try to mount a raid array without one of the devices. In that case, you must add the degraded mount option to /etc/fstab. If your root resides on the array, you must also add rootflags=degraded to your kernel parameters.

As of August 2016, a potential workaround for this bug is to mount the array by a single drive only in /etc/fstab, and allow Btrfs to discover and append the other drives automatically. Group-based identifiers such as UUID and LABEL appear to contribute to the failure. For example, a two-device RAID1 array consisting of 'disk1' and disk2' will have a UUID allocated to it, but instead of using the UUID, use only /dev/mapper/disk1 in /etc/fstab. For a more detailed explanation, see the following blog post.

Another possible workaround is to remove the udev hook in mkinitcpio.conf and replace it with the systemd hook. In this case, btrfs should not be in the HOOKS or MODULES arrays.

See the original forums thread and FS#42884 for further information and discussion.

This article or section is out of date.

The btrfs-check(8) command can be used to check or repair an unmounted Btrfs file system. However, this repair tool is still immature and not able to repair certain file system errors even those that do not render the file system unmountable.

Since the kernel version 6.2, discard=async mount(8) option is set by default. This has been reported to cause constant drive activity on some drives even while idle, as the discard queue is filled faster than it is processed. This can cause increased power usage, especially on NVMe-based drives.

As of kernel version 6.3, the default discard iops_limit has been changed from 100 to 1000 to address this issue. You can manually set it to a desired value on an old kernel version, e.g.

Where uuid is the UUID of the Btrfs file system. The limit of 1000 will need to be tuned experimentally.

To set the parameter on boot, systemd-tmpfiles may be used, e.g. by creating the following file:

Alternatively, one can disable asynchronous discard altogether by mounting using the nodiscard mount option in fstab, and instead use Periodic TRIM.

If a drive has been moved from another computer or the order of devices has changed, and the difference in sizes reported is tiny (at most megabytes), it is possible HPA (Host Protected Area) is in effect.

To verify if HPA is enabled, use hdparm:

The output provides two numbers: the number of visible sectors and the actual number of sectors. If they differ, HPA is enabled.

If motherboard sets this forcefully and the firmware provides no option to turn that off, the only option is to shrink the affected file system. This is most easily done on the original computer, or on any machine that does not apply HPA.

The blog post Fixing Btrfs Filesystem Full Problems suggests and explains the following checks/steps:

For more up-to-date instructions regarding ENOSPC ("Out of disk space"), see ENOSPC - No available disk space | Forza's Ramblings.

**Examples:**

Example 1 (unknown):
```unknown
/dev/partition
```

Example 2 (unknown):
```unknown
# mkfs.btrfs -L mylabel /dev/partition
```

Example 3 (unknown):
```unknown
# mkfs.btrfs -L mylabel -n 32k /dev/partition
```

Example 4 (unknown):
```unknown
/var/log/journal
```

---

## Xorg

**URL:** https://wiki.archlinux.org/title/X

**Contents:**
- Installation
  - Driver installation
    - AMD
- Running
- Configuration
  - Using .conf files
  - Using xorg.conf
- Input devices
  - Input identification
  - Mouse acceleration

X.Org Server — commonly referred to as simply X — is the X.Org Foundation implementation of the X Window System (X11) display server, and it is the most popular display server among Linux users. Its ubiquity has led to making it an ever-present requisite for GUI applications, resulting in massive adoption from most distributions.

For the alternative and successor, see Wayland.

Xorg can be installed with the xorg-server package.

Additionally, some packages from the xorg-apps group are necessary for certain configuration tasks. They are pointed out in the relevant sections.

Finally, an xorg group is also available, which includes Xorg server packages, packages from the xorg-apps group and fonts.

This article or section is a candidate for moving to Graphics processing unit.

The Linux kernel includes open-source video drivers and support for hardware accelerated framebuffers. However, userland support is required for OpenGL, Vulkan and 2D acceleration in X11.

First, identify the graphics card (the Subsystem output shows the specific model):

Then, install an appropriate driver. You can search the package database for a complete list of open-source Device Dependent X (DDX) drivers:

However, hardware-specific DDX is considered legacy nowadays. There is a generic modesetting(4) DDX driver in xorg-server, which uses kernel mode setting and works well on modern hardware. The modesetting DDX driver uses Glamor[1] for 2D acceleration, which requires OpenGL.

If you want to install another DDX driver, note that Xorg searches for installed DDX drivers automatically:

In order for video acceleration to work, and often to expose all the modes that the GPU can set, a proper video driver is required:

Other DDX drivers can be found in the xorg-drivers group.

Xorg should run smoothly without closed source drivers, which are typically needed only for advanced features such as fast 3D-accelerated rendering for games. The exceptions to this rule are recent GPUs (especially NVIDIA GPUs) not supported by open source drivers.

For a translation of model names (e.g. Radeon RX 6800) to GPU architectures (e.g. RDNA 2), see Wikipedia:List of AMD graphics processing units#Features overview.

The Xorg(1) command is usually not run directly. Instead, the X server is started with either a display manager or xinit.

Xorg uses a configuration file called xorg.conf and files ending in the suffix .conf for its initial setup: the complete list of the folders where these files are searched can be found in xorg.conf(5), together with a detailed explanation of all the available options.

The /etc/X11/xorg.conf.d/ directory stores host-specific configuration. You are free to add configuration files there, but they must have a .conf suffix: the files are read in ASCII order, and by convention their names start with XX- (two digits and a hyphen, so that for example 10 is read before 20). These files are parsed by the X server upon startup and are treated like part of the traditional xorg.conf configuration file. Note that on conflicting configuration, the file read last will be processed. For this reason, the most generic configuration files should be ordered first by name. The configuration entries in the xorg.conf file are processed at the end.

For option examples to set, see Fedora:Input device configuration#xorg.conf.d.

Xorg can also be configured via /etc/X11/xorg.conf or /etc/xorg.conf. You can also generate a skeleton for xorg.conf with:

This should create a xorg.conf.new file in /root/ that you can copy over to /etc/X11/xorg.conf.

Alternatively, your proprietary video card drivers may come with a tool to automatically configure Xorg: see the article of your video driver, NVIDIA or AMDGPU PRO, for more details.

For input devices the X server defaults to the libinput driver (xf86-input-libinput), but xf86-input-evdev and related drivers are available as alternative.[2]

Udev, which is provided as a systemd dependency, will detect hardware and both drivers will act as hotplugging input driver for almost all devices, as defined in the default configuration files 10-quirks.conf and 40-libinput.conf in the /usr/share/X11/xorg.conf.d/ directory.

After starting X server, the log file will show which driver hotplugged for the individual devices (note the most recent log file name may vary):

If both do not support a particular device, install the needed driver from the xorg-drivers group. The same applies, if you want to use another driver.

To influence hotplugging, see #Configuration.

For specific instructions, see also the libinput article, the following pages below, or Fedora:Input device configuration for more examples.

See Keyboard input#Identifying keycodes in Xorg.

See Mouse acceleration.

See libinput or Synaptics.

See Keyboard configuration in Xorg.

For a headless configuration, the xf86-video-dummy driver is necessary; install it and create a configuration file, such as the following:

See main article Multihead for general information.

You must define the correct driver to use and put the bus ID of your graphic cards (in decimal notation).

To get your bus IDs (in hexadecimal):

The bus IDs here are 0:2:0 and 1:0:0.

By default, Xorg always sets DPI to 96 since 2009-01-30. A change was made with version 21.1 to provide proper DPI auto-detection, but reverted.

The DPI of the X server can be set with the -dpi command line option.

Having the correct DPI is helpful where fine detail is required (like font rendering). Previously, manufacturers tried to create a standard for 96 DPI (a 10.3" diagonal monitor would be 800x600, a 13.2" monitor 1024x768). These days, screen DPIs vary and may not be equal horizontally and vertically. For example, a 19" widescreen LCD at 1440x900 may have a DPI of 89x87.

To see if your display size and DPI are correct:

Check that the dimensions match your display size.

If you have specifications on the physical size of the screen, they can be entered in the Xorg configuration file so that the proper DPI is calculated (adjust identifier to your xrandr output):

If you only want to enter the specification of your monitor without creating a full xorg.conf, create a new configuration file. For example (/etc/X11/xorg.conf.d/90-monitor.conf):

If you do not have specifications for physical screen width and height (most specifications these days only list by diagonal size), you can use the monitor's native resolution (or aspect ratio) and diagonal length to calculate the horizontal and vertical physical dimensions. Using the Pythagorean theorem on a 13.3" diagonal length screen with a 1280x800 native resolution (or 16:10 aspect ratio):

This will give the pixel diagonal length, and with this value you can discover the physical horizontal and vertical lengths (and convert them to millimeters):

For RandR compliant drivers (for example the open source ATI driver), you can set it by:

To make it permanent, see Autostarting#On Xorg startup.

You can manually set the DPI by adding the option under the Device or Screen section:

GTK very often overrides the server's DPI via the optional X resource Xft.dpi. To find out whether this is happening to you, check with:

With GTK library versions since 3.16, when this variable is not otherwise explicitly set, GTK sets it to 96. To have GTK apps obey the server DPI you may need to explicitly set Xft.dpi to the same value as the server. The Xft.dpi resource is the method by which some desktop environments optionally force DPI to a particular value in personal settings. Among these are KDE and TDE.

DPMS is a technology that allows power saving behaviour of monitors when the computer is not in use. This will allow you to have your monitors automatically go into standby after a predefined period of time.

The Composite extension for X causes an entire sub-tree of the window hierarchy to be rendered to an off-screen buffer. Applications can then take the contents of that buffer and do whatever they like. The off-screen buffer can be automatically merged into the parent window, or merged by external programs called compositing managers. For more information, see Wikipedia:Compositing window manager.

Some window managers (e.g. Compiz, Enlightenment, KWin, marco, metacity, muffin, mutter, Xfwm) do compositing on their own. For other window managers, a standalone composite manager can be used.

This section lists utilities for automating keyboard / mouse input and window operations (like moving, resizing or raising).

See also Clipboard#Tools and an overview of X automation tools.

This article or section is out of date.

To run a nested session of another desktop environment:

This will launch a Window Maker session in a 1024 by 768 window within your current X session.

This needs the package xorg-server-xnest to be installed.

A more modern way of doing a nested X session is with Xephyr.

See xinit#Starting applications without a window manager.

See main article: OpenSSH#X11 forwarding.

With the help of xinput you can temporarily disable or enable input sources. This might be useful, for example, on systems that have more than one mouse, such as the ThinkPads and you would rather use just one to avoid unwanted mouse clicks.

Install the xorg-xinput package.

Find the name or ID of the device you want to disable:

For example in a Lenovo ThinkPad T500, the output looks like this:

Disable the device with xinput --disable device, where device is the device ID or name of the device you want to disable. In this example we will disable the Synaptics Touchpad, with the ID 10:

To re-enable the device, just issue the opposite command:

Alternatively using the device name, the command to disable the touchpad would be:

You can disable a particular input source using a configuration snippet:

device is an arbitrary name, and driver_name is the name of the input driver, e.g. libinput. device_name is what is actually used to match the proper device. For alternate methods of targeting the correct device, such as libinput's MatchIsTouchscreen, consult your input driver's documentation. Though this example uses libinput, this is a driver-agnostic method which simply prevents the device from being propagated to the driver.

Run script on hotkey:

Dependencies: xorg-xprop, xdotool

See also #Killing an application visually.

To block tty access when in an X add the following to xorg.conf:

This can be used to help restrict command line access on a system accessible to non-trusted users.

To prevent a user from killing X when it is running add the following to xorg.conf:

When an application is misbehaving or stuck, instead of using kill or killall from a terminal and having to find the process ID or name, xorg-xkill allows to click on said application to close its connection to the X server. Many existing applications do indeed abort when their connection to the X server is closed, but some can choose to continue.

Xorg may run with standard user privileges instead of root (so-called "rootless" Xorg). This is a significant security improvement over running as root. Note that some popular display managers do not support rootless Xorg (e.g. LightDM or XDM).

You can verify which user Xorg is running as with ps -o user= -C Xorg.

See also Xorg.wrap(1), systemd-logind(8), Systemd/User#Xorg as a systemd user service, Fedora:Changes/XorgWithoutRootRights and FS#41257.

To configure rootless Xorg using xinitrc:

Note that executing startx directly without exec leaves the shell open in the case of a xorg crash. Since some lock screens are executed inside xorg, this can lead to full access to the executing user.

GDM will run Xorg without root privileges by default when kernel mode setting is used.

When Xorg is run in rootless mode, Xorg logs are saved to ~/.local/share/xorg/Xorg.log. However, the stdout and stderr output from the Xorg session is not redirected to this log. To re-enable redirection, start Xorg with the -keeptty flag and redirect the stdout and stderr output to a file:

Alternatively, copy /etc/X11/xinit/xserverrc to ~/.xserverrc, and append -keeptty. See [5].

As explained above, there are circumstances in which rootless Xorg is defaulted to. If this is the case for your configuration, and you have a need to run Xorg as root, you can configure Xorg.wrap(1) to require root:

Wayback is an X11 compatibility layer which allows for running full X11 desktop environments (and window managers) using Wayland components. It's available from the AUR as wayback-x11AUR package.

If a problem occurs, view the log stored in either /var/log/ or, for the rootless X default since v1.16, in ~/.local/share/xorg/. GDM users should check the systemd journal. [6]

The logfiles are of the form Xorg.n.log with n being the display number. For a single user machine with default configuration the applicable log is frequently Xorg.0.log, but otherwise it may vary. To make sure to pick the right file it may help to look at the timestamp of the X server session start and from which console it was started. For example:

X creates configuration and temporary files in current user's home directory. Make sure there is free disk space available on the partition your home directory resides in. Unfortunately, X server does not provide any more obvious information about lack of disk space in this case.

If you use a Matrox card and DRI stopped working after upgrading to Xorg, try adding the line:

to the Device section that references the video card in xorg.conf.

X fails to start with the following log messages:

To correct, uninstall the xf86-video-fbdev package.

Error message: unable to load font `(null)'.

Some programs only work with bitmap fonts. Two major packages with bitmap fonts are available, xorg-fonts-75dpi and xorg-fonts-100dpi. You do not need both; one should be enough. To find out which one would be better in your case, try xdpyinfo from xorg-xdpyinfo, like this:

and use what is closer to the shown value.

If Xorg is set to boot up automatically and for some reason you need to prevent it from starting up before the login/display manager appears (if the system is wrongly configured and Xorg does not recognize your mouse or keyboard input, for instance), you can accomplish this task with two methods.

Depending on setup, you will need to do one or more of these steps:

If you are getting Client is not authorized to connect to server, try adding the line:

to /etc/pam.d/su and /etc/pam.d/su-l. pam_xauth will then properly set environment variables and handle xauth keys.

If the filesystem (specifically /tmp) is full, startx will fail. The log file will contain:

Make some free space on the relevant filesystem and X will start.

Your color depth is set wrong. It may need to be 24 instead of 16, for example.

If X terminates with error message SocketCreateListener() failed, you may need to delete socket files in /tmp/.X11-unix. This may happen if you have previously run Xorg as root (e.g. to generate an xorg.conf).

That error means that only the current user has access to the X server. The solution is to give access to root:

That line can also be used to give access to X to a different user than root.

**Examples:**

Example 1 (unknown):
```unknown
$ lspci -v -nn -d ::03xx
```

Example 2 (unknown):
```unknown
$ pacman -Ss xf86-video
```

Example 3 (unknown):
```unknown
/usr/share/X11/xorg.conf.d/
```

Example 4 (unknown):
```unknown
/etc/X11/xorg.conf.d/
```

---

## Diskless system

**URL:** https://wiki.archlinux.org/title/Diskless_network_boot_NFS_root

**Contents:**
- Server configuration
  - DHCP
  - TFTP
  - Network storage
    - NFS
    - NBD
    - SKUF
- Client installation
  - Directory setup
  - Bootstrapping installation

From Wikipedia:Diskless node:

First of all, we must install the following components:

Install ISC dhcp and configure it:

RFC:4578 defines the "Client System Architecture Type" dhcp option. In the above configuration, if the PXE client requests an x86_64-efi binary (type 0x7), we appropriately give them one, otherwise falling back to the legacy binary. This allows both UEFI and legacy BIOS clients to boot simultaneously on the same network segment.

Start ISC DHCP systemd service.

The TFTP server will be used to transfer the boot loader, kernel, and initramfs to the client.

Set the TFTP root to /srv/arch/boot. See TFTP for installation and configuration.

The primary difference between using NFS and NBD is while with both you can in fact have multiple clients using the same installation, with NBD (by the nature of manipulating a filesystem directly) you will need to use the copyonwrite mode to do so, which ends up discarding all writes on client disconnect. In some situations however, this might be highly desirable.

Install nfs-utils on the server.

You will need to add the root of your Arch installation to your NFS exports:

Next, start NFS services: nfs-idmapd nfs-mountd.

Install nbd and configure it.

You can boot Arch Linux using the SKUF Network Boot System project, where the root of the file system will be a sparse file located on Samba server.

To get started, install samba and create a configuration file:

Start smb systemd service

Then, create a skuf group and users who will be members of it and through whom SAMBA mounting on the client machine will happen.

Next we will create a full Arch Linux installation in a subdirectory on the server. During boot, the diskless client will get an IP address from the DHCP server, then boot from the host using PXE and mount this installation as its root.

Create a sparse file of at least 2 gibibytes, and create a btrfs filesystem on it (you can of course also use a real block device or LVM if you want).

Install devtools and arch-install-scripts, and run pacstrap to install the essential packages for the client:

Now the initramfs needs to be constructed.

Trivial modifications to the net hook are required in order for NFSv4 mounting to work (not supported by nfsmount – the default for the net hook).

The copy of net is unfortunately needed so it does not get overwritten when mkinitcpio-nfs-utils is updated on the client installation.

Edit $root/etc/mkinitcpio.conf and add nfsv4 to MODULES, netnfs4 to HOOKS, and /usr/bin/mount.nfs4 to BINARIES.

Next, we chroot our installation and run mkinitcpio:

The mkinitcpio-nbdAUR package needs to be installed on the client. Build it with makepkg and install it:

You will then need to append nbd to your HOOKS array after net; net will configure your networking for you, but not attempt a NFS mount if nfsroot is not specified in the kernel line.

To install Arch Linux on sparse file using SKUF Network Boot System, clone the git repository:

Then, build the skuf package and ISO image which will later be used as a "kickstart" to start the main system using kexec

First of all, you need to tune the method of encrypting your passwords for SAMBA (see [1] for more details):

Install required packages:

And finally, build skuf package:

And sparse file with Arch Linux:

Then, move arch.ext4 in /srv/samba.

In addition to the setup mentioned here, you should also set up your hostname, timezone, locale, and keymap, and follow any other relevant parts of the Installation guide.

This article or section is a candidate for merging with GRUB.

Though poorly documented, GRUB supports being loaded via PXE.

Create a grub prefix on the target installation for both architectures using grub-mknetdir.

Luckily for us, grub-mknetdir creates prefixes for all currently compiled/installed targets, and the grub maintainers were nice enough to give us both in the same package, thus grub-mknetdir only needs to be run once.

Now we create a trivial GRUB configuration:

GRUB will set root=(tftp,10.0.0.1) automatically, so that the kernel and initramfs are transferred via TFTP without any additional configuration, though you might want to set it explicitly if you have any other non-tftp menuentries.

PXELINUX is provided by syslinux, see PXELINUX for details.

In late boot, you will want to switch your root filesystem mount to both rw, and enable compress=lzo, for much improved disk performance in comparison to NFS.

The factual accuracy of this article or section is disputed.

If you are using NBD, you will need to umount the arch.img before/while you boot your client.

This makes things particularly interesting when it comes to kernel updates. You cannot have your client filesystem mounted while you are booting a client, but that also means you need to use a kernel separate from your client filesystem in order to build it.

You will need to first copy $root/boot from the client installation to your tftp root (i.e. /srv/boot).

You will then need to umount $root before you start the client.

Write skuflinux-smth.iso to your USB drive, plug it in client computer and select in UEFI/BIOS settings as a boot device.

**Examples:**

Example 1 (unknown):
```unknown
/etc/dhcpd.conf
```

Example 2 (unknown):
```unknown
allow booting;
allow bootp;

authoritative;

option domain-name-servers 10.0.0.1;

option architecture code 93 = unsigned integer 16;

group {
    next-server 10.0.0.1;

    if option architecture = 00:07 {
        filename "/grub/x86_64-efi/core.efi";
    } else {
        filename "/grub/i386-pc/core.0";
    }

    subnet 10.0.0.0 netmask 255.255.255.0 {
        option routers 10.0.0.1;
        range 10.0.0.128 10.0.0.254;
    }
}
```

Example 3 (unknown):
```unknown
next-server
```

Example 4 (unknown):
```unknown
/srv/arch/boot
```

---

## Users and groups

**URL:** https://wiki.archlinux.org/title/Passwd

**Contents:**
- Overview
- Permissions and ownership
- Shadow
- File list
- User management
  - Example adding a user
    - Changing user defaults
  - Example adding a system user
  - Change a user's login name or home directory
  - Other examples of user management

Users and groups are used on GNU/Linux for access control—that is, to control access to the system's files, directories, and peripherals. Linux offers relatively simple/coarse access control mechanisms by default. For more advanced options, see ACL, Capabilities and PAM#Configuration How-Tos.

A user is anyone who uses a computer. In this case, we are describing the names which represent those users. It may be Mary or Bill, and they may use the names Dragonlady or Pirate in place of their real name. All that matters is that the computer has a name for each account it creates, and it is this name by which a person gains access to use the computer. Some system services also run using restricted or privileged user accounts.

Managing users is done for the purpose of security by limiting access in certain specific ways. The superuser (root) has complete access to the operating system and its configuration; it is intended for administrative use only. Unprivileged users can use several programs for controlled privilege elevation.

Any individual may have more than one account as long as they use a different name for each account they create. Further, there are some reserved names which may not be used such as "root".

Users may be grouped together into a "group", and users may be added to an existing group to utilize the privileged access it grants.

From In UNIX Everything is a File:

From Extending UNIX File Abstraction for General-Purpose Networking:

Every file on a GNU/Linux system is owned by a user and a group. In addition, there are three types of access permissions: read, write, and execute. Different access permissions can be applied to a file's owning user, owning group, and others (those without ownership). One can determine a file's owners and permissions by viewing the long listing format of the ls command:

The first column displays the file's permissions (for example, the file initramfs-linux.img has permissions -rw-r--r--). The third and fourth columns display the file's owning user and group, respectively. In this example, all files are owned by the root user and the root group.

In this example, the sf_Shared directory is owned by the root user and the vboxsf group. It is also possible to determine a file's owners and permissions using the stat command:

Access permissions are displayed in three groups of characters, representing the permissions of the owning user, owning group, and others, respectively. For example, the characters -rw-r--r-- indicate that the file's owner has read and write permission, but not execute (rw-), whilst users belonging to the owning group and other users have only read permission (r-- and r--). Meanwhile, the characters drwxrwx--- indicate that the file's owner and users belonging to the owning group all have read, write, and execute permissions (rwx and rwx), whilst other users are denied access (---). The first character represents the file's type.

List files owned by a user or group with the find utility:

A file's owning user and group can be changed with the chown (change owner) command. A file's access permissions can be changed with the chmod (change mode) command.

See chown(1), chmod(1), and Linux file permissions for additional detail.

The user, group and password management tools on Arch Linux come from the shadow package, which is a dependency of the base meta package.

To list users currently logged on the system, the who command can be used. To list all existing user accounts including their properties stored in the user database, run passwd -Sa as root. See passwd(1) for the description of the output format.

To add a new user, use the useradd command:

If an initial login group is specified by name or number, it must refer to an already existing group. If not specified, the behaviour of useradd will depend on the USERGROUPS_ENAB variable contained in /etc/login.defs. The default behaviour (USERGROUPS_ENAB yes) is to create a group with the same name as the username.

When the login shell is intended to be non-functional, for example when the user account is created for a specific service, /usr/bin/nologin may be specified in place of a regular shell to politely refuse a login (see nologin(8)).

See useradd(8) for other supported options.

To add a new user named archie, creating its home directory and otherwise using all the defaults in terms of groups, directory names, shell used and various other parameters:

Although it is not required to protect the newly created user archie with a password, it is highly recommended to do so:

The above useradd command will also automatically create a group called archie and makes this the default group for the user archie. Making each user have their own group (with the group name same as the user name) is the preferred way to add users.

You could also make the default group something else using the -g option, but note that, in multi-user systems, using a single default group (e.g. users) for every user is not recommended. The reason is that typically, the method for facilitating shared write access for specific groups of users is setting user umask value to 002, which means that the default group will by default always have write access to any file you create. See also User Private Groups. If a user must be a member of a specific group specify that group as a supplementary group when creating the user.

In the recommended scenario, where the default group has the same name as the user name, all files are by default writeable only for the user who created them. To allow write access to a specific group, shared files/directories can be made writeable by default for everyone in this group and the owning group can be automatically fixed to the group which owns the parent directory by setting the setgid bit on this directory:

Otherwise the file creator's default group (usually the same as the user name) is used.

If a GID change is required temporarily you can also use the newgrp command to change the user's default GID to another GID at runtime. For example, after executing newgrp groupname files created by the user will be associated with the groupname GID, without requiring a re-login. To change back to the default GID, execute newgrp without a groupname.

The default values used for creating new accounts are set in /etc/default/useradd and can be displayed using useradd --defaults. For example, to change the default shell globally, set SHELL=/usr/bin/shell. A different shell can also be specified individually with the -s/--shell option. Use chsh -l to list valid login shells.

Files can also be specified to be added to newly created user home directories in /etc/skel. This is useful for minimalist window managers where config files need manual configuration to reach DE-familiar behavior. For example, to set up default shortcuts for all newly created users:

See also: Display manager#Run ~/.xinitrc as a session to add xinitrc as an option to all users on the display manager.

System users can be used to run processes/daemons under a different user, protecting (e.g. with chown) files and/or directories and more examples of computer hardening.

With the following command a system user without shell access and without a home directory is created (optionally append the -U parameter to create a group with the same name as the user, and add the user to this group):

If the system user requires a specific user and group ID, specify them with the -u/--uid and -g/--gid options when creating the user:

To change a user's home directory:

The -m option also automatically creates the new directory and moves the content there.

Make sure there is no trailing / on /my/old/home.

To change a user's login name:

Changing a username is safe and easy when done properly, just use the usermod command. If the user is associated to a group with the same name, you can rename this with the groupmod command.

Alternatively, the /etc/passwd file can be edited directly, see #User database for an introduction to its format.

Also keep in mind the following notes:

To enter user information for the GECOS comment (e.g. the full user name), type:

(this way chfn runs in interactive mode).

Alternatively the GECOS comment can be set more liberally with:

To mark a user's password as expired, requiring them to create a new password the first time they log in, type:

User accounts may be deleted with the userdel command:

The -r option specifies that the user's home directory and mail spool should also be deleted.

To change the user's login shell:

Local user information is stored in the plain-text /etc/passwd file: each of its lines represents a user account, and has seven fields delimited by colons.

Broken down, this means: user archie, whose password is in /etc/shadow, whose UID is 1001 and whose primary group is 1003. Archie is their full name and there is a comment associated to their account; their home directory is /home/archie and they are using Bash.

The pwck command can be used to verify the integrity of the user database. It can sort the user list by UID at the same time, which can be helpful for comparison:

Instead of running pwck/grpck manually, the systemd timer shadow.timer, which is part of, and is enabled by, installation of the shadow package, will start shadow.service daily. shadow.service will run pwck(8) and grpck(8) to verify the integrity of both password and group files.

If discrepancies are reported, group can be edited with the vigr(8) command and users with vipw(8). This provides an extra margin of protection in that these commands lock the databases for editing. Note that the default text editor is vi, but an alternative editor will be used if the EDITOR environment variable is set, then that editor will be used instead.

As packages adopt the change-sysusers-to-fully-locked-system-accounts, package created system users generated in the past will not inherit new package defaults for the increased security of locked/expired status. These users need to be modified manually for this change. user-analysis.sh does just that.

In addition, the script also identifies orphaned users (those created by a package no longer on the system) and can automatically delete them.

/etc/group is the file that defines the groups on the system (see group(5) for details). There is also its companion gshadow which is rarely used. Its details are at gshadow(5).

Display group membership with the groups command:

If user is omitted, the current user's group names are displayed.

The id command provides additional detail, such as the user's UID and associated GIDs:

To list all groups on the system:

Create new groups with the groupadd command:

Add users to a group with the gpasswd command (see FS#58262 regarding errors):

Alternatively, add a user to additional groups with usermod (replace additional_groups with a comma-separated list):

Modify an existing group with the groupmod command, e.g. to rename the old_group group to new_group:

To delete existing groups:

To remove users from a group:

The grpck command can be used to verify the integrity of the system's group files.

This section explains the purpose of the essential groups from the filesystem package. There are many other groups, which will be created with correct GID when the relevant package is installed. See the main page for the software for details.

Non-root workstation/desktop users often need to be added to some of following groups to allow access to hardware peripherals and facilitate system administration:

The following groups are used for system purposes, an assignment to users is only required for dedicated purposes:

Before arch migrated to systemd, users had to be manually added to these groups in order to be able to access the corresponding devices. This way has been deprecated in favour of udev marking the devices with a uaccess tag and logind assigning the permissions to users dynamically via ACLs according to which session is currently active. Note that the session must not be broken for this to work (see General troubleshooting#Session permissions to check it).

There are some notable exceptions which require adding a user to some of these groups: for example if you want to allow users to access the device even when they are not logged in. However, note that adding users to the groups can even cause some functionality to break (for example, the audio group will break fast user switching and allows applications to block software mixing).

Now solely for direct access to tapes if no custom udev rules is involved.[5][6][7].

Also required for manipulating some devices via udisks/udisksctl.

The following groups are currently not used for any purpose:

This article or section is a candidate for merging with #Shadow.

The factual accuracy of this article or section is disputed.

getent(1) can be used to read a particular record.

As warned in #User database, using specific utilities such as passwd and chfn, is a better way to change the databases. Nevertheless, there are times when editing them directly is looked after. For those times, vipw, vigr are provided. It is strongly recommended to use these tailored editors over using a general text editor as they lock the databases against concurrent editing. They also help prevent invalid entries and/or syntax errors. Note that Arch Linux prefers usage of specific tools, such as chage, for modifying the shadow database over using vipw -s and vigr -s from util-linux. See also FS#31414.

lslogins(1) and lastlog2(1) are some of the tools for viewing utmp related files.

**Examples:**

Example 1 (unknown):
```unknown
cat /dev/audio > myfile
```

Example 2 (unknown):
```unknown
cat myfile > /dev/audio
```

Example 3 (unknown):
```unknown
$ ls -l /boot/
```

Example 4 (unknown):
```unknown
total 13740
drwxr-xr-x 2 root root    4096 Jan 12 00:33 grub
-rw-r--r-- 1 root root 8570335 Jan 12 00:33 initramfs-linux-fallback.img
-rw-r--r-- 1 root root 1821573 Jan 12 00:31 initramfs-linux.img
-rw-r--r-- 1 root root 1457315 Jan  8 08:19 System.map26
-rw-r--r-- 1 root root 2209920 Jan  8 08:19 vmlinuz-linux
```

---

## systemd-creds

**URL:** https://wiki.archlinux.org/title/Systemd-creds

**Contents:**
- Credential encryption
- User credential encryption

systemd-creds is a utility and service provided by systemd, designed to securely store and retrieve credentials used by systemd units. It aims to simplify the management of sensitive information, such as usernames, passwords, API keys, and other authentication data, required by various services and applications.

See https://systemd.io/CREDENTIALS/ and systemd-creds(1) for more information.

Credentials may optionally be encrypted and authenticated, either with a key derived from a local TPM2 chip, or one stored in /var/, or both (the default) if available. To check if your system uses TPM2 encryption or only the key stored in /var/, run

To encrypt a secret stored in plaintext.txt, run

Encrypted credentials carry the intended name in them, so they cannot be mispurposed. If you omit --name, then the filename will be used as credential name.

You can check that encryption was successful with

Without using --name above, the correct command would be

Alternatively, you can use systemd-ask-password to provide the secret without first writing it to disk. The -p switch allows outputting a credential in a format ready to be included in a service file:

Edit the service file to include the relevant credential

The systemd-creds utility can also be used by a regular user.

As mentioned in systemd-creds(1), the user's scope and the /etc/machine-id are used to encrypt the credential. For example, a root user may decrypt it, but only matching the user scope:

**Examples:**

Example 1 (unknown):
```unknown
$ systemd-analyze has-tpm2
```

Example 2 (unknown):
```unknown
yes
+firmware
+driver
+system
+subsystem
```

Example 3 (unknown):
```unknown
plaintext.txt
```

Example 4 (unknown):
```unknown
# systemd-creds --name=foobar encrypt plaintext.txt ciphertext.cred
```

---

## CPU frequency scaling

**URL:** https://wiki.archlinux.org/title/CPU_frequency_scaling

**Contents:**
- Userspace tools
  - i7z
  - turbostat
  - cpupower
  - thermald
  - power-profiles-daemon
  - tuned
  - cpupower-gui
  - gnome-shell-extension-cpupower
  - auto-cpufreq

CPU performance scaling enables the operating system to scale the CPU frequency up or down in order to save power or improve performance. Scaling can be done automatically in response to system load, adjust itself in response to ACPI events, or be manually changed by user space programs.

The Linux kernel offers CPU performance scaling via the CPUFreq subsystem, which defines two layers of abstraction:

A default scaling driver and governor are selected automatically, but userspace tools like cpupower, acpid, Laptop Mode Tools, or GUI tools provided for your desktop environment, may still be used for advanced configuration.

i7zAUR is an i7 (and now i3, i5, i7, i9) CPU reporting tool for Linux. It can be launched from a Terminal with the command i7z or as GUI with i7z-gui.

turbostat can display the frequency, power consumption, idle status and other statistics of the modern Intel and AMD CPUs.

cpupower is a set of userspace utilities designed to assist with CPU frequency scaling. The package is not required to use scaling, but is highly recommended because it provides useful command-line utilities and a systemd service to change the governor at boot.

The configuration file for cpupower is located in /etc/default/cpupower. This configuration file is read by a bash script in /usr/lib/systemd/scripts/cpupower which is activated by systemd with cpupower.service. You may want to enable cpupower.service to start at boot.

thermald is a Linux daemon used to prevent the overheating of Intel CPUs. This daemon proactively controls thermal parameters using P-states, T-states, and the Intel power clamp driver. thermald can also be used for older Intel CPUs. If the latest drivers are not available, then the daemon will revert to x86 model specific registers and the Linux "cpufreq subsystem" to control system cooling.

By default, it monitors CPU temperature using available CPU digital temperature sensors and maintains CPU temperature under control, before hardware takes aggressive correction action. If there is a skin temperature sensor in thermal sysfs, then it tries to keep skin temperature under 45C.

On Tiger Lake laptops (e.g. Dell Latitude 3420), this daemon has been reported as unlocking more performance than what would be otherwise available.

The associated systemd unit is thermald.service, which should be started and enabled. See thermald(8) for more information.

The powerprofilesctl command-line tool from power-profiles-daemon handles power profiles (e.g. balanced, power-saver, performance) through the power-profiles-daemon service. GNOME and KDE also provide graphical interfaces for profile switching; see the following:

See the project's README for more information on usage, use cases, and comparisons with similar projects.

Start/enable the power-profiles-daemon service. Note that when powerprofilesctl is launched, it also attempts to start the service (see the unit status of dbus.service).

tuned is a daemon for monitoring and adaptive tuning of system devices. It can configure GPU power modes, PCIe power management, set sysctl settings, adjust kernel scheduling and more; a daemon that also configures out aspects of power management in the system.

As of release 2.23.0, the project ships with tuned-ppd, a compatibility layer for programs written for power-profiles-daemon, such as the following:

For reasons why tuned should be used instead of power-profiles-daemon, see Fedora's proposal to replace it with tuned. For opposite arguments, see [3].

Start/enable the tuned daemon service. For power-profiles-daemon compatibility, also start/enable the tuned-ppd service. To control tuned from the command line, use tuned-adm to view, set, and recommend profiles.

cpupower-gui-gitAUR is a graphical utility designed to assist with CPU frequency scaling. The GUI is based on GTK and is meant to provide the same options as cpupower. cpupower-gui can enable or disable cores and change the maximum/minimum CPU frequency and governor for each core. The application handles privilege granting through polkit and allows any logged-in user in the wheel user group to change the frequency and governor. See cpupower-gui systemd units for more information on cpupower-gui.service and cpupower-gui-user.service.

gnome-shell-extension-cpupower-gitAUR is a GNOME shell extension that can alter minimum/maximum CPU frequencies and enable/disable frequency boosting.

auto-cpufreqAUR is an automatic CPU speed and power optimizer for Linux based on active monitoring of laptop's battery state, CPU usage, CPU temperature and system load.

The nvidia-powerd daemon provides support for NVIDIA's Dynamic Boost technology on supported laptop platforms. It acts as a system-wide power controller that dynamically redistributes power between the GPU and CPU based on workload demands, while maintaining the system's total thermal budget. [4]

Start/enable the nvidia-powerd service, which is provided by the nvidia-utils package.

Scaling drivers implement the CPU-specific details of setting frequencies specified by the governor. Strictly speaking, the ACPI standard requires power-performance states (P-states) that start at P0, and becoming decreasingly performant. This functionality is called SpeedStep on Intel, and PowerNow! on AMD.

In practice, though, processors provide methods for specifying specific frequencies rather than being restricted to fixed P-states, which the scaling drivers handle.

cpupower requires drivers to know the limits of the native CPU:

The factual accuracy of this article or section is disputed.

To see a full list of available modules, run:

Load the appropriate module (see Kernel modules for details). Once the appropriate cpufreq driver is loaded, detailed information about the CPU(s) can be displayed by running

In some cases, it may be necessary to manually set maximum and minimum frequencies.

To set the maximum clock frequency (clock_freq is a clock frequency with units: GHz, MHz):

To set the minimum clock frequency:

To set the CPU to run at a specified frequency:

Alternatively, you can set the frequency manually:

The available values can be found in /sys/devices/system/cpu/cpu*/cpufreq/scaling_available_frequencies or similar. [5]

Some processors support raising their frequency above the normal maximum for a short burst of time, under appropriate thermal conditions. On Intel processors, this is called Turbo Boost, and on AMD processors this is called Turbo-Core.

intel_pstate has a driver-specific interface for prohibiting the processor from entering turbo P-States:

For scaling drivers other than intel_pstate, if the driver supports boosting, the /sys/devices/system/cpu/cpufreq/boost attribute will be present, and can be used to disable/enable boosting.

To disable boosting, run:

To enable boosting, run:

On Intel processors, x86_energy_perf_policy can also be used to configure Turbo Boost:

amd_pstate has three operation modes: CPPC autonomous (active) mode, CPPC non-autonomous (passive) mode and CPPC guided autonomous (guided) mode. Officially supported kernels are built with CONFIG_X86_AMD_PSTATE_DEFAULT_MODE=3 which means the default for them is the active mode. This can be changed with the kernel parameter amd_pstate=active, amd_pstate=passive or amd_pstate=guided. To revert to the acpi_cpufreq driver, set amd_pstate=disable instead.

Scaling governors are power schemes determining the desired frequency for the CPU. Some request a constant frequency, others implement algorithms to dynamically adjust according to the system load. The governors included in the kernel are:

Depending on the scaling driver, one of these governors will be loaded by default:

To activate a particular governor, run:

Alternatively, you can activate a governor on every available CPU manually:

See the kernel documentation for details.

To set the threshold for stepping up to another frequency:

To set the threshold for stepping down to another frequency:

The sampling rate determines how frequently the governor checks to tune the CPU. sampling_down_factor is a tunable that multiplies the sampling rate when the CPU is at its highest clock frequency, thereby delaying load evaluation and improving performance. Allowed values for sampling_down_factor are 1 to 100000. This tunable has no effect on behavior at lower CPU frequencies/loads.

To read the value (default = 1), run:

To set the value, run:

Since Linux 5.9, it is possible to set the cpufreq.default_governor kernel option.[8] To set the desired scaling parameters at boot, configure the cpupower utility and enable its systemd service. Alternatively, systemd-tmpfiles or udev rules can be used.

Both Intel and AMD define a way to have the CPU decide its own speed based on (1) a performance range from the system and (2) a performance/power hint specifying the preference. The fully-autonomous mode is activated when:

The most important feature of active governing is that only two governors appear available, powersave and performance. They do not work at all like their normal counterpart, however: these levels are translated into an Energy Performance Preference hint for the CPU's internal governor. As a result, they both provide dynamic scaling, similar to the schedutil or ondemand generic governors respectively, differing mostly in latency. The performance algorithm should give better power saving functionality than the old ondemand governor for Intel HWP.

The intel-pstate driver has, confusingly, an "active" mode that works without the CPU's active decision. This mode turns on when kernel cmdline forces an "active" mode but HWP is unavailable or disabled. It will still only provide powersave and performance, but the driver itself does the governing in a way similar to schedutil and performance (i.e. it stays at the maximum P-state). There is no real benefit to this mode compared to passive intel-pstate.

It is possible to select in-between hints with the sysfs interfaces available. The interface is identical between AMD and Intel, where the files /sys/devices/system/cpu/cpu*/cpufreq/energy_performance_preference describe the current preference and /sys/devices/system/cpu/cpu*/cpufreq/energy_performance_available_preferences providing a list of available preferences. One can also pass a number between 0 (favor performance) and 255 (favor power). A fallback implementation is provided for Intel CPUs without EPP, translating strings to EPB levels (described in next section) but failing on numbers.

x86_energy_perf_policy supports configuration of EPP hints via the --hwp-epp switch on Intel CPUs only. It works via direct access of machine-specific registers (MSRs) which differ between Intel and AMD. The program can also restrict the range of HWP frequencies using a range of frequency multipliers.

To enable hardware P-States with x86_energy_perf_policy(8):

The power consumption of modern CPUs is no longer simply dependent on the frequency or voltage setting, as there are modules that can be switched on as needed. Collaborative processor performance control (CPPC) is the P-state replacement provided by ACPI 5.0. Instead of defining a table of static frequency levels, the processor provides many abstract performance levels and the operating system selects from these levels. There are two advantages:

On the other hand, the flexible frequency breaks frequency-invariant utilization tracking, which is important for fast frequency changes by schedutil. A number of vendor-specific methods have been used to make the frequency static under CPPC, with most successes coming from arm64.

cppc_cpufreq is the generic CPPC scaling driver. amd_pstate also uses ACPI CPPC to manage the CPU frequency when the Zen 3 MSR is unavailable – this method, also called "shared memory", has higher latency than MSR.

The Intel performance and energy bias hint (EPB) is an interface provided by Intel CPUs to allow for user space to specify the desired power-performance tradeoff, on a scale of 0 (highest performance) to 15 (highest energy savings). The EPB register is another layer of performance management functioning independently from frequency scaling. It influences how aggressive P-state and C-state selection will be, and informs internal model-specific decision making that affects energy consumption.

Common values and their aliases, as recognized by sysfs and x86_energy_perf_policy(8) are:

The EPB can be set using a sysfs attribute:

With x86_energy_perf_policy:

Users may configure scaling governors to switch automatically based on different ACPI events such as connecting the AC adapter or closing a laptop lid. A quick example is given below; however, it may be worth reading full article on acpid.

Events are defined in /etc/acpi/handler.sh. If the acpid package is installed, the file should already exist and be executable. For example, to change the scaling governor from performance to conservative when the AC adapter is disconnected and change it back if reconnected:

This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.

Some CPU/BIOS configurations may have difficulties to scale to the maximum frequency or scale to higher frequencies at all. This is most likely caused by BIOS events telling the OS to limit the frequency resulting in /sys/devices/system/cpu/cpu0/cpufreq/bios_limit set to a lower value.

Either you just made a specific Setting in the BIOS Setup Utility, (Frequency, Thermal Management, etc.) you can blame a buggy/outdated BIOS or the BIOS might have a serious reason for throttling the CPU on its own.

Reasons like that can be (assuming your machine's a notebook) that the battery is removed (or near death) so you are on AC-power only. In this case, a weak AC-source might not supply enough electricity to fulfill extreme peak demands by the overall system and as there is no battery to assist this could lead to data loss, data corruption or in worst case even hardware damage!

Not all BIOS'es limit the CPU-Frequency in this case, but, for example, most IBM/Lenovo Thinkpads do. Refer to thinkwiki for more thinkpad related info on this topic.

If you checked there is not just an odd BIOS setting and you know what you are doing, you can make the Kernel ignore these BIOS-limitations.

Set the processor.ignore_ppc=1 kernel parameter. For trying this temporarily, change the value in /sys/module/processor/parameters/ignore_ppc from 0 to 1.

Some systems use another mechanism to limit the CPU frequency, e.g., when running without battery or an unofficial power adapter. See Lenovo ThinkPad T480#CPU stuck at minimum frequency for a way to manipulate the BD PROCHOT bit in Intel CPUs and Dell XPS 15 (9560)#General slowness & stuttering for alternative fixes. It does not only apply to the Lenovo ThinkPad T480, but is a common problem in Dell XPS models like the XPS15 9550 and XPS15 9560, too. The bit also is what makes at least some Intel-based MacBooks run with minimum CPU frequency when no battery is connected.

**Examples:**

Example 1 (unknown):
```unknown
/etc/default/cpupower
```

Example 2 (unknown):
```unknown
/usr/lib/systemd/scripts/cpupower
```

Example 3 (unknown):
```unknown
cpupower.service
```

Example 4 (unknown):
```unknown
cpupower.service
```

---

## Kernel/Traditional compilation

**URL:** https://wiki.archlinux.org/title/Kernel/Traditional_compilation

**Contents:**
- Preparation
  - Install the core packages
  - Create a kernel compilation directory
  - Download the kernel source
    - Semi-official kernel mirrors
  - Unpack the kernel source
- Kernel configuration
  - Default Arch configuration
  - Advanced configuration
- Compilation

This article is an introduction to building custom kernels from kernel.org sources. This method of compiling kernels is the traditional method common to all distributions. It can be, depending on your background, more complicated than using the Kernel/Arch build system. Consider the Arch build system tools are developed and maintained to make repeatable compilation tasks efficient and safe.

It is not necessary (or recommended) to use the root account or root privileges (i.e. via sudo) for kernel preparation.

Install the base-devel meta package, which pulls in necessary packages such as make and gcc. It is also recommended to install the following packages, as listed in the default Arch kernel PKGBUILD: xmlto, kmod, inetutils, bc, libelf, git, cpio, perl, tar, xz.

It is recommended to create a separate build directory for your kernel(s). In this example, the directory kernelbuild will be created in the home directory:

Download the kernel source from https://www.kernel.org. This should be the tarball (.tar.xz) file for your chosen kernel.

It can be downloaded by simply right-clicking the tar.xz link in your browser and selecting Save Link As..., or any other number of ways via alternative graphical or command-line tools that utilise HTTP, TFTP, Rsync, or Git.

In the following command-line commands, wget has been installed and is used inside the ~/kernelbuild directory to obtain kernel A.B.C:

You should also verify the correctness of the download before trusting it. First grab the signature, then use that to grab the fingerprint of the signing key, then use the fingerprint to obtain the actual signing key:

Note the signature was generated for the tar archive (i.e. extension .tar), not the compressed .tar.xz file that you have downloaded. You need to decompress the latter without untarring it. Verify that you have xz installed, then you can proceed like so:

Do not proceed if this does not result in output that includes the string "Good signature".

If wget was not used inside the build directory, it will be necessary to move the tarball into it, e.g.

This article or section needs expansion.

Semi-official mirrors of some of the kernel.org Git repositories are provided by their respective maintainers. These tend to be faster to clone from than kernel.org.

Within the build directory, unpack the kernel tarball:

To be absolutely sure that no permission errors occur, chown needs to be run to transfer ownership of the folder to the current user.

To transfer ownership of a folder with every file in it to our user, run the chown command.

This will transfer ownership of every file in the folder to you, so you do not encounter any errors related to permissions.

To finalise the preparation, ensure that the kernel tree is absolutely clean; do not rely on the source tree being clean after unpacking. To do so, first change into the new kernel source directory created, and then run the make mrproper command:

This is the most crucial step in customizing the default kernel to reflect your computer's precise specifications. Kernel configuration is set in its .config file, which includes the use of Kernel modules. By setting the options in .config properly, your kernel and computer will function most efficiently.

You can do a mixture of two things:

This method will create a .config file for the custom kernel using the default Arch kernel settings. If a stock Arch kernel is running, you can use the following command inside the custom kernel source directory:

Otherwise, the default configuration can be found online in the official Arch Linux kernel package.

There are several tools available to fine-tune the kernel configuration, which provide an alternative to otherwise spending hours manually configuring each and every one of the options available during compilation.

The chosen method should be run inside the kernel source directory, and all will either create a new .config file, or overwrite an existing one where present. All optional configurations will be automatically enabled, although any newer configuration options (i.e. with an older kernel .config) may not be automatically selected.

Once the changes have been made save the .config file. It is a good idea to make a backup copy outside the source directory. You may need to do this multiple times before you get all the options right.

If unsure, only change a few options between compilations. If you cannot boot your newly built kernel, see the list of necessary items here.

Running lspci -k # from live media lists names of kernel modules in use. Most importantly, you must maintain cgroups support. This is necessary for systemd. For more detailed information, see Gentoo:Kernel/Gentoo Kernel Configuration Guide and Gentoo:Intel#Kernel or Gentoo:Ryzen#Kernel for Intel or AMD Ryzen processors.

Compilation time will vary from as little as fifteen minutes to over an hour, depending on your kernel configuration and processor capability. Once the .config file has been set for the custom kernel, within the source directory run the following command to compile:

Once the kernel has been compiled, the modules for it must follow. First build the modules:

Then install the modules. As root or with root privileges, run the following command to do so:

This will copy the compiled modules into /lib/modules/A.B.C/. This keeps the modules for individual kernels used separated.

The kernel compilation process will generate a compressed bzImage (big zImage) of that kernel, if it does not, you may have to run

This file must be copied to the /boot directory and renamed in the process. Provided the name is prefixed with vmlinuz-, you may name the kernel as you wish. In the examples below, the installed and compiled A.B.C kernel has been copied over and renamed to vmlinuz-linuxAB:

If you do not know what making an initial RAM file system is, see Initramfs on Wikipedia and mkinitcpio.

An existing mkinitcpio preset can be copied and modified so that the custom kernel initramfs images can be generated in the same way as for an official kernel. This is useful where intending to recompile the kernel (e.g. where updated). In the example below, the preset file for the stock Arch kernel will be copied and modified for kernel A.B.C, installed above.

First, copy the existing preset file, renaming it to match the name of the custom kernel specified as a suffix to /boot/vmlinuz- when copying the bzImage:

Second, edit the file and amend for the custom kernel. Note (again) that the ALL_kver= parameter also matches the name of the custom kernel specified when copying the bzImage:

Finally, generate the initramfs images for the custom kernel in the same way as for an official kernel:

Rather than use a preset file, mkinitcpio can also be used to generate an initramfs file manually. The syntax of the command is:

For example, the command for the A.B.C custom kernel installed above would be:

The System.map file is not required for booting Linux. It is a type of "phone directory" list of functions in a particular build of a kernel. The System.map contains a list of kernel symbols (i.e function names, variable names etc) and their corresponding addresses. This "symbol-name to address mapping" is used by:

If your /boot is on a filesystem which supports symlinks (i.e. not FAT32), copy System.map to /boot, appending your kernel's name to the destination file. Then create a symlink from /boot/System.map to point to /boot/System.map-linuxAB:

After completing all steps above, you should have the following 3 files and 1 soft symlink in your /boot directory along with any other previously existing files:

Add an entry for your new kernel in your boot loader configuration file. See Arch boot process#Feature comparison for possible boot loaders, their wiki articles and other information.

**Examples:**

Example 1 (unknown):
```unknown
kernelbuild
```

Example 2 (unknown):
```unknown
$ mkdir ~/kernelbuild
```

Example 3 (unknown):
```unknown
/usr/share/doc/systemd/README
```

Example 4 (unknown):
```unknown
Save Link As...
```

---

## Kernel module

**URL:** https://wiki.archlinux.org/title/Blacklist

**Contents:**
- Obtaining information
- Automatic module loading
  - Early module loading
  - systemd
- Manual module handling
- Setting module options
  - Using modprobe
  - Using modprobe.d
  - Using kernel command line
- Aliasing

Kernel modules are pieces of code that can be loaded and unloaded into the kernel upon demand. They extend the functionality of the kernel without the need to reboot the system.

To create a kernel module, you can read The Linux Kernel Module Programming Guide. A module can be configured as built-in or loadable. To dynamically load or remove a module, it has to be configured as a loadable module in the kernel configuration (the line related to the module will therefore display the letter M).

To rebuild a kernel module automatically when a new kernel is installed, see Dynamic Kernel Module Support (DKMS).

Usually modules depend on the kernel release and are stored in the /usr/lib/modules/kernel_release/ directory.

To show what kernel modules are currently loaded:

To show information about a module:

To list the options that are set for a loaded module use systool(1) from sysfsutils:

To display the comprehensive configuration of all the modules:

To display the configuration of a particular module:

List the dependencies of a module (or alias), including the module itself:

Today, all necessary modules loading is handled automatically by udev, so if you do not need to use any out-of-tree kernel modules, there is no need to put modules that should be loaded at boot in any configuration file. However, there are cases where you might want to load an extra module during the boot process, or blacklist another one for your computer to function properly.

Early module loading depends on the initramfs generator used:

Kernel modules can be explicitly listed in files under /etc/modules-load.d/ for systemd to load them during boot. Each configuration file is named in the style of /etc/modules-load.d/program.conf. Configuration files simply contain a list of kernel modules names to load, separated by newlines. Empty lines and lines whose first non-whitespace character is # or ; are ignored.

See modules-load.d(5) for more details.

Kernel modules are handled by tools provided by the kmod package, which is installed as a dependency of a kernel package. You can use these tools manually. To load a module:

To load a module by a file name—i.e. one that is not installed in the /usr/lib/modules/kernel_release/ directory—use any of:

To unload—remove—a module, use any of:

To pass a parameter to a kernel module, you can pass them manually with modprobe or assure certain parameters are always applied using a modprobe configuration file or by using the kernel command line. If the module is built into the kernel, the kernel command line must be used and other methods will not work.

The basic way to pass parameters to a module is using the modprobe command. Parameters are specified on command line using simple key=value assignments:

Configuration files in the /etc/modprobe.d/ directory can be used to pass module settings to udev, which will use modprobe to manage the loading of the modules during system boot. Files in this directory can have any name, given that they end with the .conf extension. The file name matters, see modprobe.d(5) § CONFIGURATION DIRECTORIES AND PRECEDENCE. To show the effective configuration:

Multiple module parameters are separated by spaces, in turn a parameter can receive a list of values which is separated by commas:

You can also pass options to the module using the kernel command line. This is the only working option for modules built into the kernel. For all common boot loaders, the following syntax is correct:

Simply add this to the appropriate line in your boot loader configuration, as described in Kernel parameters#Boot loader configuration.

Aliases are alternate names for a module. For example: alias my-mod really_long_modulename means you can use modprobe my-mod instead of modprobe really_long_modulename. You can also use shell-style wildcards, so alias my-mod* really_long_modulename means that modprobe my-mod-something has the same effect. Create an alias:

Aliases can be internal—contained in the module itself. Internal aliases are usually used for #Automatic module loading when it is needed by an application, e.g. when the kernel detects a new device. To see the module internal aliases:

To see both configured and internal aliases:

Blacklisting, in the context of kernel modules, is a mechanism to prevent the kernel module from loading. This could be useful if, for example, the associated hardware is not needed, or if loading that module causes problems: for instance there may be two kernel modules that try to control the same piece of hardware, and loading them together would result in a conflict.

Some modules are loaded as part of the initramfs. mkinitcpio -M will print out all automatically detected modules: to prevent the initramfs from loading some of those modules, blacklist them in a .conf file under /etc/modprobe.d and it shall be added in by the modconf hook during image generation. Running mkinitcpio -v will list all modules pulled in by the various hooks (e.g. filesystems hook, block hook, etc.). Remember to add that .conf file to the FILES array in /etc/mkinitcpio.conf if you do not have the modconf hook in your HOOKS array (e.g. you have deviated from the default configuration), and once you have blacklisted the modules regenerate the initramfs, and reboot afterwards.

Disable an alias by overriding. For example, to prevent Bluetooth module autoloading (assuming a module named off does not exist):

To disable all internal aliases for a given module use the blacklist keyword. For example, to prevent the pcspkr module from loading on boot to avoid sounds through the PC speaker:

There is a workaround for the behaviour described in the #alias and #blacklist notes. The install configuration command instructs modprobe to run a custom command instead of inserting the module in the kernel as normal, so you can simulate the successful module loading with:

You can force the module to always fail loading with /bin/false: this will effectively prevent the module—and any other that depends on it—from loading by any means, and a log error message may be produced.

You can also blacklist modules from the boot loader boot entry configuration.

Simply add module_blacklist=module_name_1,module_name_2,module_name_3 to your kernel command line, as described in Kernel parameters#Boot loader configuration.

Another use case for a command line option is to disable hardware-specific components of a module without disabling the module entirely. For example, disabling a microphone while retaining other sound out options. See BBS#303475 for a few examples.

In case a specific module does not load and the boot log (accessible by running journalctl -b as root) says that the module is blacklisted, but the directory /etc/modprobe.d/ does not show a corresponding entry, check another modprobe source directory at /usr/lib/modprobe.d/ for blacklisting entries.

A module will not be loaded if the "vermagic" string contained within the kernel module does not match the value of the currently running kernel. If it is known that the module is compatible with the current running kernel the "vermagic" check can be ignored with modprobe --force-vermagic.

**Examples:**

Example 1 (unknown):
```unknown
/usr/lib/modules/kernel_release/
```

Example 2 (unknown):
```unknown
uname --kernel-release
```

Example 3 (unknown):
```unknown
/etc/modprobe.d/
```

Example 4 (unknown):
```unknown
$ modinfo module_name
```

---

## Kerberos

**URL:** https://wiki.archlinux.org/title/Kerberos

**Contents:**
- Installation
- Server configuration
  - Domain creation
  - Add principals
  - Firewall
  - DNS records
- Client configuration
  - Testing
- Configuring kadmin
  - Configuring kadmin ACL

Kerberos is a network authentication system. See krb5 documentation.

Install the krb5 package on your clients and server.

It is highly recommended to use a time synchronization daemon to keep client/server clocks in sync.

If hostname resolution has not been configured, you can manually add your clients and server to the hosts(5) file of each machine. Note that the FQDN (myclient.example.com) must be the first hostname after the IP address in the hosts file.

Edit /etc/krb5.conf to configure your domain:

Where $ADDRESS is IP address or domain name, where kerberos lives, like 10.0.0.120 or kerberos.example.com.

This file's format is described in the MIT Kerberos documentation

Finally, start/enable krb5-kdc.service and krb5-kadmind.service.

Start the Kerberos administration tool, using local authentication

Add a user principal to the Kerberos database:

Add the KDC principal to the Kerberos database:

Finally, Add the KDC principal to the server's keytab:

Quit the Kerberos administration tool:

You should now be able to get a Kerberos ticket:

Add ALLOW rules to your firewall for any applicable ports/protocols:

This is not necessary if you specify the kerberos and kadmin server in each machine's krb5.conf

Do not forget reverse DNS.

Edit the client's /etc/krb5.conf to match your server's configuration. You can copy this file from the server, or just set the required realm information.

You should now be able to get a Kerberos ticket on the client:

You will need /etc/krb5.conf configured on the kadmin client, and the server's firewall configured for kadmin.

Create a principal for administration:

Add the user to the kadmin ACL file:

This file's format is described in the MIT Kerberos documentation

This file's format is described in the MIT Kerberos documentation

Restart krb5-kdc.service and krb5-kadmind.service.

You can now use kadmin as your own user, authenticating with kerberos:

First, ensure you have configured krb5.conf on all involved machines.

A kerberos principal has three components, formatted as `primary/instance@REALM`. For user principals, the primary is your username and the instance is omitted or is a role (eg. "admin"): `myuser@EXAMPLE.COM` or `myuser/admin@EXAMPLE.COM`. For hosts, the primary is "host" and the instance is the server FQDN: `host/myserver.example.com@EXAMPLE.COM`. For services, the primary is the service abbreviation and the instance is the FQDN: `nfs/myserver.example.com@EXAMPLE.COM`. The realm can often be omitted, the local computer's default realm is usually assumed.

This is the easier method, but requires you to have configured kadmin.

Open kadmin as root (so we can write the keytab) on the client, authenticating with your admin principal:

Add a principal for any services you will be using, eg. "host" for SSH authentication or "nfs" for NFS:

Save each key to the local keytab:

Start kadmin on the Kerberos server, using either unix or kerberos authentication:

Add a principal for any services you will be using, eg. "host" for SSH authentication or "nfs" for NFS:

Save each key to a new keytab to be transferred to the client:

Finally, copy kbclient.keytab from the server to the client using SCP or similar, then put it in place with correct permissions:

Finally, delete kbclient.keytab from the server and client.

Set up a second server as shown above, then create the cross-realm principal on both KDCs. Cross-realm principals must be created with strong passwords, not -randkey, and the same password must be used on both KDCs. The principal must have the same key version number (kvno) in both KDCs.

To grant EXAMPLE.COM principals access to EXAMPLE.ORG resources, you would use the following principal:

The [capaths] section of krb5.conf can be used to further control cross-realm trust relationships.

Use the instructions in Service principals and keytabs to create a principal for the "host" service for both client and server, then put the client's keys in the client's keytab and the server's keys in the server's keytab.

Modify your SSH server configuration to enable GSSAPI authentication:

And modify your client configuration to send GSSAPI requests:

Get a ticket-granting ticket on the client before using ssh:

Pass the -v option to ssh to watch what's happening:

And you should now see a host ticket on the client:

To allow a different kerberos principal to authenticate to a user account, add the principal name to the target account's .k5login file. For example, to allow robert@EXAMPLE.COM to SSH to alice's account:

First, configure your NFS server. Also see NFS/Troubleshooting. Configuring a time synchronization daemon on both the clients and the server is strongly recommended. Clock drift will cause this to break, and the error message will not be helpful.

Use the instructions in #Service principals and keytabs to create a principal for the "nfs" service for both client and server, then put the client's keys in the client's keytab and the server's keys in the server's keytab.

Add a Kerberos export option. If necessary, multiple options can be specified using a colon as a delimiter with the preferred setting first, e.g. sec=krb5p:krb5i.

And reload the exports:

Mount the exported directory:

You can add -vv for verbose information, and may need -t nfs4 and -o sec=krb5p or your chosen security option.

Check that it worked with the mount command:

Some browsers have support for Kerberos protocol but disable it by default. Here are the instructions how to enable it:

Chromium needs to be run with a command line parameter that specifies a list of sites where Kerberos authentication is allowed. The easiest way is to add persistent flag to the configuration file:

To configure Firefox with trusted sites visit about:config and set network.negotiate-auth.trusted-uris property to FOO.COM (Note: for Firefox there is no "*."; for Chrome, there is).

Your realm is missing either the kadmin/admin or kadmin/changepw principal.

For clients, invalid arguments/options may happen on first setup if rpc-gssd is not loaded. Loading it is usually acomplished by enabling and starting nfs-client.target, but after first setup this target will need a restart.

If any of the following errors are encountered:

it means that package openssh is not configured with GSSAPI patch for OpenSSH. You can install openssh-gssapiAUR or follow this example[dead link 2025-04-05—HTTP 404].

**Examples:**

Example 1 (unknown):
```unknown
/etc/krb5.conf
```

Example 2 (unknown):
```unknown
/etc/krb5.conf
```

Example 3 (unknown):
```unknown
[libdefaults]
    default_realm = EXAMPLE.COM

[realms]
    EXAMPLE.COM = {
        admin_server = $ADDRESS
        # use "kdc = ..." if the kerberos SRV records aren't in DNS (see Advanced section)
        kdc = $ADDRESS
        # This breaks krb4 compatibility but increases security
        default_principal_flags = +preauth
    }

[domain_realm]
    example.com  = EXAMPLE.COM
    .example.com = EXAMPLE.COM

[logging]
    kdc          = SYSLOG:NOTICE
    admin_server = SYSLOG:NOTICE
    default      = SYSLOG:NOTICE
```

Example 4 (unknown):
```unknown
kerberos.example.com
```

---

## Microcode

**URL:** https://wiki.archlinux.org/title/Microcode

**Contents:**
- Installation
- Loading microcode
  - Early loading
    - Custom built kernels
    - Microcode initramfs packed together with the main initramfs in one file
      - mkinitcpio
      - dracut
    - Microcode in a separate initramfs file
      - GRUB
      - systemd-boot

Processor manufacturers release stability and security updates to the processor microcode. These updates provide bug fixes that can be critical to the stability of your system. Without them, you may experience spurious crashes or unexpected system halts that can be difficult to track down.

All users with an AMD or Intel CPU should install the microcode updates to ensure system stability. In virtual machines and containers, the microcode updates belongs on the host, not in the guest system.

To acquire updated microcode, depending on the processor, install one of the following packages:

mkinitcpio and dracut generate combined initramfs files by default. The microcode will be loaded automatically at boot time.

Microcode updates are usually shipped with the motherboard's firmware and applied during firmware initialization. Since OEMs might not release firmware updates in a timely fashion and old systems do not get new firmware updates at all, the ability to apply CPU microcode updates during boot was added to the Linux kernel. The Linux microcode loader supports three loading methods:

The kernel's early loader expects microcode update files in /kernel/x86/microcode/GenuineIntel.bin or /kernel/x86/microcode/AuthenticAMD.bin inside an uncompressed CPIO archive (initramfs image).

The early initramfs image can be combined with the main initramfs image into one file and passed as a single initramfs to the kernel (via the initrd= kernel command line option by your boot loader or when packed in a unified kernel image) or it can exist as a separate file in which case multiple initrd= kernel command line options need to be used. In both cases, the uncompressed CPIO archive with the microcode must be placed before the main initramfs.

Note that because of the wide variability in users' early-boot configuration, microcode updates may not be triggered automatically by Arch's default configuration.

In order for early loading to work in custom kernels, "CPU microcode loading support" needs to be compiled into the kernel, not compiled as a module. This will enable the "Early load microcode" prompt which should be set to Y.

The uncompressed microcode CPIO can be prepended into the initramfs and used as a single initramfs file. This method is preferred over #Microcode in a separate initramfs file since no additional boot parameter configuration is necessary.

For mkinitcpio to generate an initramfs file that includes microcode, make sure microcode is in the HOOKS array in /etc/mkinitcpio.conf.

If the autodetect hook precedes microcode, then only the microcode for the current CPU will be included. To include all CPU microcode files that can be found on the system, move the microcode hook before autodetect or remove the autodetect hook entirely.

When generating the initramfs, mkinitcpio will show:

You can verify the initramfs includes the microcode update files with lsinitcpio(1). E.g.:

For dracut, see dracut.conf(5) § DESCRIPTION.

In the following sections replace cpu_manufacturer with your CPU manufacturer, i.e. amd or intel.

grub-mkconfig will automatically detect the microcode update and configure GRUB appropriately. After installing the microcode package, regenerate the GRUB configuration to activate loading the microcode update by running:

Alternatively, users that manage their GRUB configuration file manually can add /boot/cpu_manufacturer-ucode.img (or /cpu_manufacturer-ucode.img if /boot is a separate partition) as follows:

Repeat it for each menu entry.

Use the initrd option to load the microcode, before the initial ramdisk, as follows:

The latest microcode cpu_manufacturer-ucode.img must be available at boot time in your EFI system partition (ESP). The ESP must be mounted as /boot in order to have the microcode updated every time amd-ucode or intel-ucode is updated. Otherwise, copy /boot/cpu_manufacturer-ucode.img to your ESP at every update of the microcode package.

Append two initrd= options:

Edit boot options in /boot/refind_linux.conf and add an initrd= option for the microcode image as the first initrd argument passed. Use either initrd=boot\cpu_manufacturer-ucode.img or initrd=cpu_manufacturer-ucode.img depending if the files in /boot are in the root of a separate partition.

The microcode is required to be the first initramfs declared for the boot options list. For example:

Users employing manual stanzas in esp/EFI/refind/refind.conf to define kernels should add the initrd= parameter with the proper path within the boot partition. This parameter is required as part of the options line, and not in the main part of the stanza. E.g.:

Multiple initramfs files can be separated by commas in /boot/syslinux/syslinux.cfg:

LILO and potentially other old boot loaders do not support multiple initramfs images. Follow the #Microcode initramfs packed together with the main initramfs in one file method instead.

For Limine you will just need to add the path to the microcode through the MODULE_PATH option in your limine.conf file. Here is an example:

Late loading of microcode updates happens after the system has booted. It uses files in /usr/lib/firmware/amd-ucode/ and /usr/lib/firmware/intel-ucode/. The microcode update files are provided by amd-ucode and intel-ucode, respectively.

Late loading requires the kernel to be built with CONFIG_MICROCODE_LATE_LOADING=y, which is not the case for Arch officially supported kernels at the moment. [1]

To manually reload the microcode, e.g. after updating the microcode files in /usr/lib/firmware/amd-ucode/ or /usr/lib/firmware/intel-ucode/, run:

This allows to apply newer microcode updates without rebooting the system.

Check the kernel messages with journalctl to see if the microcode has been updated:

One should see something similar to the following on every boot, indicating that microcode is updated very early on:

It is entirely possible, particularly with newer hardware, that there is no microcode update for the CPU.

On AMD systems using late loading the output will show the version of the old microcode before reloading the microcode and the new one once it is reloaded.

Users may consult either Intel's repository or Gentoo's wiki on AMD at the following links to see if a particular model is supported:

For Intel, it is possible to find out if the /usr/lib/firmware/intel-ucode/ contains microcode for the running CPU with iucode_tool(8).

For AMD, it can be done manually.

In case an updated CPU microcode causes issues, you may want to temporarily disable the microcode loader to allow successfully booting and downgrading the package. To disable the kernel's microcode loader, specify the dis_ucode_ldr kernel parameter.

**Examples:**

Example 1 (unknown):
```unknown
/kernel/x86/microcode/GenuineIntel.bin
```

Example 2 (unknown):
```unknown
/kernel/x86/microcode/AuthenticAMD.bin
```

Example 3 (unknown):
```unknown
CONFIG_BLK_DEV_INITRD=Y
CONFIG_MICROCODE=y
CONFIG_MICROCODE_INTEL=Y
CONFIG_MICROCODE_AMD=y
```

Example 4 (unknown):
```unknown
boot/*-ucode.img
```

---

## File systems

**URL:** https://wiki.archlinux.org/title/Unmount

**Contents:**
- Types of file systems
  - Journaling
  - FUSE-based file systems
  - Stackable file systems
  - Read-only file systems
  - Clustered file systems
  - Shared-disk file system
- Identify existing file systems
- Create a file system
- Mount a file system

Individual drive partitions can be set up using one of the many different available file systems. Each has its own advantages, disadvantages, and unique idiosyncrasies. A brief overview of supported file systems follows; the links are to Wikipedia pages that provide much more information.

See filesystems(5) for a general overview and Wikipedia:Comparison of file systems for a detailed feature comparison. File systems already loaded by the kernel or built-in are listed in /proc/filesystems, while all the installed modules can be seen with ls /lib/modules/$(uname -r)/kernel/fs.

xfs.html xfs-delayed-logging-design.html xfs-self-describing-metadata.html

The ext3/4, HFS+, JFS, NTFS, ReiserFS, and XFS file systems use journaling. Journaling provides fault-resilience by logging changes before they are committed to the file system. In the event of a system crash or power failure, such file systems are faster to bring back online and less likely to become corrupted. The logging takes place in a dedicated area of the file system.

ext3/4 offer data-mode journaling, which can optionally log data in addition to the metadata. Data-mode journaling comes with a speed penalty, because it does two write operations: first to the journal and then to the disk. Therefore, data-mode journaling is not enabled by default. The trade-off between system speed and data safety should be considered when choosing the file system type and features.

In the same vein, Reiser4 offers configurable "transaction models": a special model called wandering logs, which eliminates the need to write to the disk twice; write-anywhere, a pure copy-on-write approach; and a combined approach called hybrid which heuristically alternates between the two.

File systems based on copy-on-write (also known as write-anywhere), such as Reiser4, Btrfs, Bcachefs and ZFS, by design operate on full atomicity and also provide checksums for both metadata and inline data (operations entirely occur, or they entirely do not, and in properly functioning hardware data does not corrupt due to operations half-occurring). Therefore, these file systems are by design much less prone to data loss than other file systems and have no need to use traditional journal to protect metadata, because they are never updated in-place. Although Btrfs still has a journal-like log tree, it is only used to speed-up fdatasync/fsync.

FAT, exFAT, ext2, and HFS provide neither journaling nor atomicity, They are for temporary or legacy use and not recommended for use when reliable storage is needed.

To identify existing file systems, you can use lsblk:

An existing file system, if present, will be shown in the FSTYPE column. If mounted, it will appear in the MOUNTPOINT column.

File systems are usually created on a partition, inside logical containers such as LVM, RAID and dm-crypt, or on a regular file (see Wikipedia:Loop device). This section describes the partition case.

Before continuing, identify the device where the file system will be created and whether or not it is mounted. For example:

Mounted file systems must be unmounted before proceeding. In the above example an existing file system is on /dev/sda2 and is mounted at /mnt. It would be unmounted with:

To find just mounted file systems, see #List mounted file systems.

To create a new file system, use mkfs(8). See #Types of file systems for the exact type, as well as userspace utilities you may wish to install for a particular file system.

For example, to create a new file system of type ext4 (common for Linux data partitions) on /dev/sda1, run:

The new file system can now be mounted to a directory of choice.

To manually mount a file system located on a device (e.g., a partition) to a directory, use mount(8). This example mounts /dev/sda1 to /mnt.

This attaches the file system on /dev/sda1 at the directory /mnt, making the contents of the file system visible. Any data that existed at /mnt before this action is made invisible until the device is unmounted.

fstab contains information on how devices should be automatically mounted if present. See the fstab article for more information on how to modify this behavior.

If a device is specified in /etc/fstab and only the device or mount point is given on the command line, that information will be used in mounting. For example, if /etc/fstab contains a line indicating that /dev/sda1 should be mounted to /mnt, then the following will automatically mount the device to that location:

mount contains several options, many of which depend on the file system specified. The options can be changed, either by:

See these related articles and the article of the file system of interest for more information.

To list all mounted file systems, use findmnt(8):

findmnt takes a variety of arguments which can filter the output and show additional information. For example, it can take a device or mount point as an argument to show only information on what is specified:

findmnt gathers information from /etc/fstab, /etc/mtab, and /proc/self/mounts.

To unmount a file system use umount(8). Either the device containing the file system (e.g., /dev/sda1) or the mount point (e.g., /mnt) can be specified:

Unmount the file system and run fsck on the problematic volume.

**Examples:**

Example 1 (unknown):
```unknown
/proc/filesystems
```

Example 2 (unknown):
```unknown
ls /lib/modules/$(uname -r)/kernel/fs
```

Example 3 (unknown):
```unknown
NAME   FSTYPE LABEL     UUID                                 MOUNTPOINT
sdb
└─sdb1 vfat   Transcend 4A3C-A9E9
```

Example 4 (unknown):
```unknown
NAME   FSTYPE   LABEL       UUID                                 MOUNTPOINT
sda
├─sda1                      C4DA-2C4D
├─sda2 ext4                 5b1564b2-2e2c-452c-bcfa-d1f572ae99f2 /mnt
└─sda3                      56adc99b-a61e-46af-aab7-a6d07e504652
```

---

## Dual boot with Windows

**URL:** https://wiki.archlinux.org/title/Dual_boot_with_Windows

**Contents:**
- Important information
  - Windows UEFI vs BIOS limitations
  - Boot loader UEFI vs BIOS limitations
  - UEFI Secure Boot
  - Fast Startup and hibernation
    - Windows settings
      - Disable Fast Startup and disable hibernation
      - Disable Fast Startup and enable hibernation
      - Enable Fast Startup and enable hibernation
  - Windows filenames limitations

This is an article detailing different methods of Arch/Windows coexistence.

Microsoft imposes limitations on which firmware boot mode and partitioning style can be supported based on the version of Windows used:

In case of pre-installed systems, all systems pre-installed with Windows 8/8.1, 10 and 11 boot in UEFI/GPT mode. Up to Windows 10, the firmware bitness matches the bitness of Windows, ie. x86_64 Windows boot in x86_64 UEFI mode and 32-bit Windows boot in IA32 UEFI mode.

An easy way to detect the boot mode of Windows is to do the following[1]:

In general, Windows forces type of partitioning depending on the firmware mode used, i.e. if Windows is booted in UEFI mode, it can be installed only to a GPT disk. If Windows is booted in Legacy BIOS mode, it can be installed only to an MBR disk. This is a limitation enforced by Windows Setup, and as of April 2014 there is no officially (Microsoft) supported way of installing Windows in UEFI/MBR or BIOS/GPT configuration. Thus Windows only supports either UEFI/GPT boot or BIOS/MBR configuration.

Such a limitation is not enforced by the Linux kernel, but can depend on which boot loader is used and/or how the boot loader is configured. The Windows limitation should be considered if the user wishes to boot Windows and Linux from the same disk, since installation procedure of boot loader depends on the firmware type and disk partitioning configuration. In case where Windows and Linux dual boot from the same disk, it is advisable to follow the method used by Windows, ie. either go for UEFI/GPT boot or BIOS/MBR boot. See https://support.microsoft.com/kb/2581408 for more information.

Most of the Linux boot loaders installed for one firmware type cannot launch or chainload boot loaders of the other firmware type. That is, if Arch is installed in UEFI/GPT or UEFI/MBR mode in one disk and Windows is installed in BIOS/MBR mode in another disk, the UEFI boot loader used by Arch cannot chainload the BIOS installed Windows in the other disk. Similarly if Arch is installed in BIOS/MBR or BIOS/GPT mode in one disk and Windows is installed in UEFI/GPT in another disk, the BIOS boot loader used by Arch cannot chainload UEFI installed Windows in the other disk.

The only exceptions to this are GRUB in Apple Macs in which GRUB in UEFI mode can boot BIOS installed OS via appleloader command (does not work in non-Apple systems), and rEFInd which technically supports booting legacy BIOS OS from UEFI systems, but does not always work in non-Apple UEFI systems as per its author Rod Smith.

However if Arch is installed in BIOS/GPT in one disk and Windows is installed in BIOS/MBR mode in another disk, then the BIOS boot loader used by Arch can boot the Windows in the other disk, if the boot loader itself has the ability to chainload from another disk.

Windows Setup creates a 100 MiB EFI system partition (except for Advanced Format 4K native drives where it creates a 300 MiB ESP), so multiple kernel usage is limited. Workarounds include:

All pre-installed Windows 8/8.1, 10 and 11 systems by default boot in UEFI/GPT mode and have UEFI Secure Boot enabled by default. This is mandated by Microsoft for all OEM pre-installed systems.

Arch Linux install media does not support Secure Boot yet. See Secure Boot#Booting an installation medium.

It is advisable to disable UEFI Secure Boot in the firmware setup manually before attempting to boot Arch Linux. Windows 8/8.1, 10 and 11 SHOULD continue to boot fine even if Secure boot is disabled. The only issue with regards to disabling UEFI Secure Boot support is that it requires physical access to the system to disable secure boot option in the firmware setup, as Microsoft has explicitly forbidden presence of any method to remotely or programmatically (from within OS) disable secure boot in all Windows 8/8.1 and above pre-installed systems

The factual accuracy of this article or section is disputed.

Secure Boot changes should not affect BitLocker: the issue might be self-signing the windows boot loader and chain-loading it from a boot loader: as long as the Windows boot loader stays signed with MS keys, and the Microsoft certs enrolled it should be fine. Is the issue disabling it and booting Windows or disabling, reenabling it and then booting Windows? The first one would be understandable, the second one is something to warn about.

If you intend to use Secure Boot for Linux as well, you may need to perform changes to the Secure Boot settings. Those changes prevent unlocking the BitLocker disk without the recovery key, leading to permanent data loss. Before proceeding, check if this is the case and save your BitLocker recovery key if not already done. This is especially important if Windows was preinstalled by the vendor.

There are two OSs that can be hibernated, you can hibernate Windows and boot Linux (or another OS), or you can hibernate Linux and boot Windows, or hibernate both OSs.

For the same reason, if you share one EFI system partition between Windows and Linux, then the EFI system partition may be damaged if you hibernate (or shutdown with Fast Startup enabled) Windows and then start Linux, or hibernate Linux and then start Windows. Check the respective section in EFI system partition for mitigation strategies.

ntfs-3g added a safe-guard to prevent read-write mounting of hibernated NTFS filesystems, but the NTFS driver within the Linux kernel has no such safeguard.

Windows cannot read filesystems such as ext4 by default that are commonly used for Linux. These filesystems do not have to be considered, unless you install a Windows driver for them.

Fast Startup is a feature in Windows 8 and above that hibernates the computer rather than actually shutting it down to speed up boot times.

There are multiple options regarding the Windows settings for Fast Startup and hibernation that are covered in the next sections.

The procedure of disabling Fast Startup is described in the tutorials for Windows 8, Windows 10 and Windows 11. In any case if you disable a setting, make sure to disable the setting and then shut down Windows, before installing Linux; note that rebooting is not sufficient.

This is the safest option, and recommended if you are unsure about the issue, as it requires the least amount of user awareness when rebooting from one OS into the other. You may share the same EFI system partition between Windows and Linux.

In a Windows command-line shell with administrator privileges:

This option requires user awareness when rebooting from one OS into the other. If you want to start Linux while Windows is hibernated, which is a common use case, then

The same considerations apply as in case "Disable Fast Startup and enable hibernation", but since Windows can not be shut down fully, only hibernated, you can never read-write mount any filesystem that was mounted by Windows while Windows is hibernated.

Windows is limited to filepaths being shorter than 260 characters.

Windows also puts certain characters off limits in filenames for reasons that run all the way back to DOS:

These are limitations of Windows and not NTFS: any other OS using the NTFS partition will be fine. Windows will fail to detect these files and running chkdsk will most likely cause them to be deleted. This can lead to potential data-loss.

NTFS-3G applies Windows restrictions to new file names through the windows_names option: ntfs-3g(8) § Windows_Filename_Compatibility (see fstab).

The recommended way to set up a Linux/Windows dual booting system is to first install Windows, only using part of the disk for its partitions. When you have finished the Windows setup, boot into the Linux install environment where you can create and resize partitions for Linux while leaving the existing Windows partitions untouched. The Windows installation will create the EFI system partition which can be used by your Linux boot loader. If you are installing Windows from scratch, do note that the EFI System partition created by Windows Setup will be too small for most use cases. See #The EFI system partition created by Windows Setup is too small.

If you already have Windows installed, it will already have created some partitions on a GPT-formatted disk:

Using the Disk Management utility in Windows, check how the partitions are labelled and which type gets reported. The Reserved Partition may not be visible in the Disk Management utility in which case it can be identified using diskpart in windows cmd. This will help you understand which partitions are essential to Windows, and which others you might repurpose. The Windows Disk Management utility can also be used to shrink Windows (NTFS) partitions to free up disk space for additional partitions for Linux.

You can then proceed with partitioning, depending on your needs. The boot loader needs to support chainloading other EFI applications to dual boot Windows and Linux. An additional EFI system partition should not be created, as it may prevent Windows from booting.

Simply mount the existing partition.

Computers that come with newer versions of Windows often have Secure Boot enabled. You will need to take extra steps to either disable Secure Boot or to make your installation media compatible with secure boot (see above and in the linked page).

Even though the recommended way to set up a Linux/Windows dual booting system is to first install Windows, it can be done the other way around. In contrast to installing Windows before Linux, you will have to set aside a partition for Windows, say 40GB or larger, in advance. Or have some unpartitioned disk space, or create and resize partitions for Windows from within the Linux installation, before launching the Windows installation.

Windows will use the already existing EFI system partition. Follows an outline, assuming Secure Boot is disabled in the firmware.

The following assumes GRUB is used as a boot loader (although the process is likely similar for other boot loaders) and that Windows 10 will be installed on a GPT block device with an existing EFI system partition (see the "System partition" section in the Microsoft documentation for more information).

Create with program gdisk on the block device the following three new partitions. See [5] for more precise partition sizes.

Create NTFS file systems on the new Microsoft basic data and Windows RE (recovery) partitions using the mkntfs program from package ntfs-3g.

Reboot the system into a Windows 10 installation media. When prompted to install select the custom install option and install Windows on the Microsoft basic data partition created earlier. This should also install Microsoft EFI files in the EFI system partition.

After installation (set up of and logging into Windows not required), reboot into Linux and generate a GRUB configuration for the Windows boot manager to be available in the GRUB menu on next boot.

See #Windows UEFI vs BIOS limitations.

See Unified Extensible Firmware Interface#Windows changes boot order.

If you have a GPT-partitioned disk and erased (e.g. with mkfs.fat -F32 /dev/sdx) the EFI system partition, you will notice that Windows Boot Manager will either disappear from your boot options, or selecting it will send you back to the UEFI.

To remedy it, boot with a Windows installation media, press Shift+F10 to open the console (or click NEXT > Repair Computer > Troubleshoot... > Advanced > Command Prompt), then start the diskpart utility:

Select the appropriate hard drive by typing:

Make sure that there is a partition of type system (the EFI system partition):

Select this partition:

and assign a temporary drive letter to it:

To make sure that drive letter is correctly assigned:

Navigate to C:\ (or what your system drive letter is):

Next is the "magic" command, which recreate the BCD store (with /s for the mount point, /f for firmware type, optionally add /v for verbose):

You should now have Windows Boot Manager working as a boot option, and thus have access to Windows. Just make sure to never format your EFI system partition again!

See [6], [7] and [8].

By default, Windows Setup creates a 100 MiB EFI system partition (except for Advanced Format 4K native drives where it creates a 300 MiB ESP). This is generally too small to fit everything you need. You can replace the existing EFI system partition with a new, larger one.

If you are installing Windows from scratch, you can dictate the size of the EFI system partition during installation[9]:

Once Windows is installed, you can resize the primary partition down within Windows and then reboot and go about your usual Arch install, filling the space you just created.

Alternatively, you can use the Arch install media to create a single EFI system partition of your preferred size before you install Windows on the drive. Windows Setup will use the EFI system partition you made instead of creating its own.

On BIOS systems, Windows cumulative updates may fail with the error We couldn’t complete the updates. Undoing changes. Don’t turn off your computer. In such case, while in Windows, you need to set the Windows partition as active.

After successfully installing the Windows update, mark back your Linux partition as active, using the commands above.

When it comes to pairing Bluetooth devices with both the Linux and Windows installation, both systems have the same MAC address, but will use different link keys generated during the pairing process. This results in the device being unable to connect to one installation, after it has been paired with the other. To allow a device to connect to either installation without re-pairing, follow Bluetooth#Dual boot pairing.

**Examples:**

Example 1 (unknown):
```unknown
appleloader
```

Example 2 (unknown):
```unknown
esp/EFI/Microsoft/Boot/Fonts/
```

Example 3 (unknown):
```unknown
esp/EFI/Microsoft/Boot/
```

Example 4 (unknown):
```unknown
COMPRESSION="xz"
COMPRESSION_OPTIONS=(-9e)
MODULES_DECOMPRESS="yes"
```

---

## Category:Domain Name System

**URL:** https://wiki.archlinux.org/title/DNS_server

The main article for this category is Domain name resolution.

---

## systemd

**URL:** https://wiki.archlinux.org/title/Daemons

**Contents:**
- Basic systemctl usage
  - Using units
  - Power management
    - Soft reboot
- Writing unit files
  - Handling dependencies
  - Service types
  - Editing provided units
    - Replacement unit files
    - Drop-in files

From the project web page:

Historically, what systemd calls "service" was named daemon: any program that runs as a "background" process (without a terminal or user interface), commonly waiting for events to occur and offering services. A good example is a web server that waits for a request to deliver a page, or an ssh server waiting for someone trying to log in. While these are full featured applications, there are daemons whose work is not that visible. Daemons are for tasks like writing messages into a log file (e.g. syslog, metalog) or keeping your system time accurate (e.g. ntpd). For more information see daemon(7).

The main command used to introspect and control systemd is systemctl. Some of its uses are examining the system state and managing the system and services. See systemctl(1) for more details.

Units commonly include, but are not limited to, services (.service), mount points (.mount), devices (.device) and sockets (.socket).

When using systemctl, you generally have to specify the complete name of the unit file, including its suffix, for example sshd.socket. There are however a few short forms when specifying the unit in the following systemctl commands:

See systemd.unit(5) for details.

The commands in the below table operate on system units since --system is the implied default for systemctl. To instead operate on user units (for the calling user), use systemctl --user without root privileges. See also systemd/User#Basic setup to enable/disable user units for all users.

polkit is necessary for power management as an unprivileged user. If you are in a local systemd-logind user session and no other session is active, the following commands will work without root privileges. If not (for example, because another user is logged into a tty), systemd will automatically ask you for the root password.

Soft reboot is a special kind of a userspace-only reboot operation that does not involve the kernel. It is implemented by systemd-soft-reboot.service(8) and can be invoked through systemctl soft-reboot. As with kexec, it skips firmware re-initialization, but additionally the system does not go through kernel initialization and initramfs, and unlocked dm-crypt devices remain attached.

When /run/nextroot/ contains a valid root file system hierarchy (e.g. is the root mount of another distribution or another snapshot), soft-reboot would switch the system root into it, allowing for switching to another installation without losing states managed by kernel, e.g. networking.

The syntax of systemd's unit files (systemd.unit(5)) is inspired by XDG Desktop Entry Specification .desktop files, which are in turn inspired by Microsoft Windows .ini files. Unit files are loaded from multiple locations (to see the full list, run systemctl show --property=UnitPath), but the main ones are (listed from lowest to highest precedence):

Look at the units installed by your packages for examples, as well as systemd.service(5) § EXAMPLES.

systemd-analyze(1) can help verifying the work. See the systemd-analyze verify FILE... section of that page.

With systemd, dependencies can be resolved by designing the unit files correctly. The most typical case is when unit A requires unit B to be running before A is started. In that case add Requires=B and After=B to the [Unit] section of A. If the dependency is optional, add Wants=B and After=B instead. Note that Wants= and Requires= do not imply After=, meaning that if After= is not specified, the two units will be started in parallel.

Dependencies are typically placed on services and not on #Targets. For example, network.target is pulled in by whatever service configures your network interfaces, therefore ordering your custom unit after it is sufficient since network.target is started anyway.

There are several different start-up types to consider when writing a custom service file. This is set with the Type= parameter in the [Service] section:

See the systemd.service(5) § OPTIONS man page for a more detailed explanation of the Type values.

This article or section needs expansion.

To avoid conflicts with pacman, unit files provided by packages should not be directly edited. There are two safe ways to modify a unit without touching the original file: create a new unit file which overrides the original unit or create drop-in snippets which are applied on top of the original unit. For both methods, you must reload the unit afterwards to apply your changes. This can be done either by editing the unit with systemctl edit (which reloads the unit automatically) or by reloading all units with:

To replace the unit file /usr/lib/systemd/system/unit, create the file /etc/systemd/system/unit and reenable the unit to update the symlinks.

This opens /etc/systemd/system/unit in your editor (copying the installed version if it does not exist yet) and automatically reloads it when you finish editing.

To create drop-in files for the unit file /usr/lib/systemd/system/unit, create the directory /etc/systemd/system/unit.d/ and place .conf files there to override or add new options. systemd will parse and apply these files on top of the original unit.

The easiest way to do this is to run:

This opens the file /etc/systemd/system/unit.d/drop_in_name.conf in your text editor (creating it if necessary) and automatically reloads the unit when you are done editing. Omitting --drop-in= option will result in systemd using the default file name override.conf .

To revert any changes to a unit made using systemctl edit do:

For example, if you simply want to add an additional dependency to a unit, you may create the following file:

As another example, in order to replace the ExecStart directive, create the following file:

Note how ExecStart must be cleared before being re-assigned [2]. The same holds for every item that can be specified multiple times, e.g. OnCalendar for timers.

One more example to automatically restart a service:

For services that send logs directly to journald or syslog, you can control their verbosity by setting a numeric value between 0 and 6 for the LogLevelMax= parameter in the [Service] section using the methods described above. For example:

The standard log levels are identical to those used to filter the journal. Setting a lower number excludes all higher and less important log messages from your journal.

If a service is echoing stdout and/or stderr output, by default this will end up in the journal as well. This behavior can be suppressed by setting StandardOutput=null and/or StandardError=null in the [Service] section. Other values than null can be used to further tweak this behavior. See systemd.exec(5) § LOGGING_AND_STANDARD_INPUT/OUTPUT.

systemd uses targets to group units together via dependencies and as standardized synchronization points. They serve a similar purpose as runlevels but act a little differently. Each target is named instead of numbered and is intended to serve a specific purpose with the possibility of having multiple ones active at the same time. Some targets are implemented by inheriting all of the services of another target and adding additional services to it. There are systemd targets that mimic the common SystemVinit runlevels.

The following should be used under systemd instead of running runlevel:

The runlevels that held a defined meaning under sysvinit (i.e., 0, 1, 3, 5, and 6); have a 1:1 mapping with a specific systemd target. Unfortunately, there is no good way to do the same for the user-defined runlevels like 2 and 4. If you make use of those it is suggested that you make a new named systemd target as /etc/systemd/system/your target that takes one of the existing runlevels as a base (you can look at /usr/lib/systemd/system/graphical.target as an example), make a directory /etc/systemd/system/your target.wants, and then symlink the additional services from /usr/lib/systemd/system/ that you wish to enable.

In systemd, targets are exposed via target units. You can change them like this:

This will only change the current target, and has no effect on the next boot. This is equivalent to commands such as telinit 3 or telinit 5 in Sysvinit.

The standard target is default.target, which is a symlink to graphical.target. This roughly corresponds to the old runlevel 5.

To verify the current target with systemctl:

To change the default target to boot into, change the default.target symlink. With systemctl:

Alternatively, append one of the following kernel parameters to your boot loader:

systemd chooses the default.target according to the following order:

Some (not exhaustive) components of systemd are:

systemd is in charge of mounting the partitions and filesystems specified in /etc/fstab. The systemd-fstab-generator(8) translates all the entries in /etc/fstab into systemd units; this is performed at boot time and whenever the configuration of the system manager is reloaded.

systemd extends the usual fstab capabilities and offers additional mount options. These affect the dependencies of the mount unit. They can, for example, ensure that a mount is performed only once the network is up or only once another partition is mounted. The full list of specific systemd mount options, typically prefixed with x-systemd., is detailed in systemd.mount(5) § FSTAB.

An example of these mount options is automounting, which means mounting only when the resource is required rather than automatically at boot time. This is provided in fstab#Automount with systemd.

On UEFI-booted systems, GPT partitions such as root, home, swap, etc. can be mounted automatically following the Discoverable Partitions Specification. These partitions can thus be omitted from fstab, and if the root partition is automounted, then root= can be omitted from the kernel command line. See systemd-gpt-auto-generator(8).

The prerequisites are:

udev will create symlinks to the discovered partitions which can be used to reference the partitions and volumes in configuration files.

For /var automounting to work, the PARTUUID must match the SHA256 HMAC hash of the partition type UUID keyed by the machine ID. The required PARTUUID can be obtained using:

The primary role of systemd-sysvcompat (required by base) is to provide the traditional linux init binary. For systemd-controlled systems, init is just a symbolic link to its systemd executable.

In addition, it provides four convenience shortcuts that SysVinit users might be used to. The convenience shortcuts are halt(8), poweroff(8), reboot(8) and shutdown(8). Each one of those four commands is a symbolic link to systemctl, and is governed by systemd behavior. Therefore, the discussion at #Power management applies.

systemd-based systems can give up those System V compatibility methods by using the init= boot parameter (see, for example, /bin/init is in systemd-sysvcompat ?) and systemd native systemctl command arguments.

systemd-tmpfiles creates, deletes and cleans up volatile and temporary files and directories. It reads configuration files in /etc/tmpfiles.d/ and /usr/lib/tmpfiles.d/ to discover which actions to perform. Configuration files in the former directory take precedence over those in the latter directory.

Configuration files are usually provided together with service files, and they are named in the style of /usr/lib/tmpfiles.d/program.conf. For example, the Samba daemon expects the directory /run/samba to exist and to have the correct permissions. Therefore, the samba package ships with this configuration:

Configuration files may also be used to write values into certain files on boot. For example, if you used /etc/rc.local to disable wakeup from USB devices with echo USBE > /proc/acpi/wakeup, you may use the following tmpfile instead:

It is possible to write multiple lines to the same file, either with \n in the argument or using the w+ type on multiple lines (including the first one) for appending:

See the systemd-tmpfiles(8) and tmpfiles.d(5) man pages for details.

The factual accuracy of this article or section is disputed.

Configuration files provided by packages should not be directly edited to avoid conflicts with pacman updates. For this, many (but not all) systemd packages provide a way to modify the configuration, but without touching the original file by creation of drop-in snippets. Check the package manual to see if drop-in configuration files are supported.

To create a drop-in configuration file for the unit file /etc/systemd/unit.conf, create the directory /etc/systemd/unit.conf.d/ and place .conf files there to override or add new options. systemd will parse and apply these files on top of the original unit.

Check the overall configuration:

The applied drop-in snippets file(s) and content will be listed at the end. Restart the service for the changes to take effect.

Some packages provide a .socket unit. For example, cups provides a cups.socket unit[3]. If cups.socket is enabled (and cups.service is left disabled), systemd will not start CUPS immediately; it will just listen to the appropriate sockets. Then, whenever a program attempts to connect to one of these CUPS sockets, systemd will start cups.service and transparently hand over control of these ports to the CUPS process.

To delay a service until after the network is up, include the following dependencies in the .service file:

The network wait service of the network manager in use must also be enabled so that network-online.target properly reflects the network status.

For more detailed explanations, see the discussion in the Network configuration synchronization points.

If a service needs to perform DNS queries, it should additionally be ordered after nss-lookup.target:

See systemd.special(7) § Special Passive System Units.

For nss-lookup.target to have any effect it needs a service that pulls it in via Wants=nss-lookup.target and orders itself before it with Before=nss-lookup.target. Typically this is done by local DNS resolvers.

Check which active service, if any, is pulling in nss-lookup.target with:

This article or section needs expansion.

By default, Arch Linux does not make use of systemd presets, and does not enable most services on installation.

If you wish for systemd presets to be honored when packages are installed, you will need to create a a pacman hook in order to run systemctl preset or systemctl preset-all whenever a new systemd unit is installed. Something like

Arch Linux ships with /usr/lib/systemd/system-preset/99-default.preset containing disable *. This causes systemctl preset to disable all units by default, such that when a new package is installed, the user must manually enable the unit.

To enable units by default instead, simply create a symlink from /etc/systemd/system-preset/99-default.preset to /dev/null or replace with a file containing enable * in order to override the configuration file. This will cause systemctl preset to enable all units that get installed—regardless of unit type—unless specified in another file in one systemctl preset's configuration directories. User units are not affected. It is also possible to configure higher priority files with more specific rules on what should be enabled. See systemd.preset(5) for more information.

See systemd/Sandboxing.

In order to notify about service failures, an OnFailure= directive needs to be added to the according service file, for example by using a drop-in configuration file. Adding this directive to every service unit can be achieved with a top-level drop-in configuration file. For details about top-level drop-ins, see systemd.unit(5).

Create a top-level drop-in for services:

This adds OnFailure=failure-notification@%n.service to every service file. If some_service_unit fails, failure-notification@some_service_unit.service will be started to handle the notification delivery (or whatever task it is configured to perform).

Create the failure-notification@.service template unit:

You can create the failure-notification.sh script and define what to do or how to notify. Examples include sending e-mail, showing desktop notifications, using gotify, XMPP, etc. The %i will be the name of the failed service unit and will be passed as an argument to the script.

In order to prevent a recursion for starting instances of failure-notification@.service again and again if the start fails, create an empty drop-in configuration file with the same name as the top-level drop-in (the empty service-level drop-in configuration file takes precedence over the top-level drop-in and overrides the latter one):

You can set up systemd to send an e-mail when a unit fails. Cron sends mail to MAILTO if the job outputs to stdout or stderr, but many jobs are setup to only output on error. First you need two files: an executable for sending the mail and a .service for starting the executable. For this example, the executable is just a shell script using sendmail, which is in packages that provide smtp-forwarder.

Whatever executable you use, it should probably take at least two arguments as this shell script does: the address to send to and the unit file to get the status of. The .service we create will pass these arguments:

Where user is the user being emailed and address is that user's email address. Although the recipient is hard-coded, the unit file to report on is passed as an instance parameter, so this one service can send email for many other units. At this point you can start status_email_user@dbus.service to verify that you can receive the emails.

Then simply edit the service you want emails for and add OnFailure=status_email_user@%n.service to the [Unit] section. %n passes the unit's name to the template.

See udisks#Automatically turn off an external HDD at shutdown.

To find the systemd services which failed to start:

To find out why they failed, examine their log output. See systemd/Journal#Filtering output for details.

systemd has several options for diagnosing problems with the boot process. See boot debugging for more general instructions and options to capture boot messages before systemd takes over the boot process. Also see systemd debugging documentation.

If some systemd service misbehaves or you want to get more information about what is happening, set the SYSTEMD_LOG_LEVEL environment variable to debug. For example, to run the systemd-networkd daemon in debug mode:

Add a drop-in file for the service adding the two lines:

Or equivalently, set the environment variable manually:

then restart systemd-networkd and watch the journal for the service with the -f/--follow option.

If the shutdown process takes a very long time (or seems to freeze), most likely a service not exiting is to blame. systemd waits some time for each service to exit before trying to kill it. To find out whether you are affected, see Shutdown completes eventually in the systemd documentation.

A common problem is a stalled shutdown or suspend process. To verify whether that is the case, you could run either of these commands and check the outputs

The solution to this would be to cancel these jobs by running

and then trying shutdown or reboot again.

If running journalctl -u foounit as root does not show any output for a short lived service, look at the PID instead. For example, if systemd-modules-load.service fails, and systemctl status systemd-modules-load shows that it ran as PID 123, then you might be able to see output in the journal for that PID, i.e. by running journalctl -b _PID=123 as root. Metadata fields for the journal such as _SYSTEMD_UNIT and _COMM are collected asynchronously and rely on the /proc directory for the process existing. Fixing this requires fixing the kernel to provide this data via a socket connection, similar to SCM_CREDENTIALS. In short, it is a bug. Keep in mind that immediately failed services might not print anything to the journal as per design of systemd.

The factual accuracy of this article or section is disputed.

After using systemd-analyze a number of users have noticed that their boot time has increased significantly in comparison with what it used to be. After using systemd-analyze blame NetworkManager is being reported as taking an unusually large amount of time to start.

The problem for some users has been due to /var/log/journal becoming too large. This may have other impacts on performance, such as for systemctl status or journalctl. As such the solution is to remove every file within the folder (ideally making a backup of it somewhere, at least temporarily) and then setting a journal file size limit as described in systemd/Journal#Journal size limit.

Starting with systemd 219, /usr/lib/tmpfiles.d/systemd.conf specifies ACL attributes for directories under /var/log/journal and, therefore, requires ACL support to be enabled for the filesystem the journal resides on.

See Access Control Lists#Enable ACL for instructions on how to enable ACL on the filesystem that houses /var/log/journal.

You may want to disable emergency mode on a remote machine, for example, a virtual machine hosted at Azure or Google Cloud. It is because if emergency mode is triggered, the machine will be blocked from connecting to network.

To disable it, mask emergency.service and emergency.target.

You may be trying to start or enable a user unit as a system unit. systemd.unit(5) indicates, which units reside where. By default systemctl operates on system services.

See systemd/User for more details.

Some bootloaders only set the LoaderDevicePartUUID variable when it is empty. Consequently, even if the UUID of the EFI partition changes, the bootloader will not update LoaderDevicePartUUID. By deleting the EFI variable with the commands below, the bootloader will then repopulate it with the new UUID.

**Examples:**

Example 1 (unknown):
```unknown
-H user@host
```

Example 2 (unknown):
```unknown
sshd.socket
```

Example 3 (unknown):
```unknown
netctl.service
```

Example 4 (unknown):
```unknown
dev-sda2.device
```

---

## System maintenance

**URL:** https://wiki.archlinux.org/title/Checkupdates

**Contents:**
- Check for errors
  - Failed systemd services
  - Log files
- Backup
  - Configuration files
  - List of installed packages
  - Pacman database
  - Encryption metadata
  - System and user data
- Upgrading the system

Regular system maintenance is necessary for the proper functioning of Arch over a period of time. Timely maintenance is a practice many users get accustomed to.

Check if any systemd services have failed:

See systemd#Using units for more information.

Look for errors in the log files located in /var/log/, as well as messages logged in the systemd journal:

See Xorg#Troubleshooting for information on where and how Xorg logs errors.

Having backups of important data is a necessary measure to take, since human and machine processing errors are very likely to generate corruption as time passes, and also the physical media where the data is stored is inevitably destined to fail.

See Synchronization and backup programs for many alternative applications that may better suit your case. See Category:System recovery for other articles of interest.

It is highly encouraged to automate backups and test the recovery process to ensure everything works as intended. For automation see System backup#Automation.

Before editing any configuration files, create a backup so that you can revert to a working version in case of problems. Editors like vim and emacs can do this automatically. On a larger scale, consider using a configuration manager.

For dotfiles (configuration files in the home directory), see dotfiles#Tracking dotfiles directly with Git.

Maintain a list of all installed packages so that if a complete re-installation is inevitable, it is easier to re-create the original environment.

See pacman/Tips and tricks#List of installed packages for details.

See pacman/Tips and tricks#Back up the pacman database.

See Data-at-rest encryption#Backup for disk encryption scenarios.

It is recommended to perform full system upgrades regularly via pacman#Upgrading packages, to enjoy both the latest bug fixes and security updates, and also to avoid having to deal with too many package upgrades that require manual intervention at once. When requesting support from the community, it will usually be assumed that the system is up to date.

Make sure to have the Arch install media or another Linux "live" CD/USB available so you can easily rescue your system if there is a problem after updating. If you are running Arch in a production environment, or cannot afford downtime for any reason, test changes to configuration files, as well as updates to software packages, on a non-critical duplicate system first. Then, if no problems arise, roll out the changes to the production system.

If the system has packages from the AUR, carefully upgrade all of them.

pacman is a powerful package management tool, but it does not attempt to handle all corner cases. Users must be vigilant and take responsibility for maintaining their own system.

Before upgrading, users are expected to visit the Arch Linux home page to check the latest news, or alternatively subscribe to the RSS feed or the arch-announce mailing list. When updates require out-of-the-ordinary user intervention (more than what can be handled simply by following the instructions given by pacman), an appropriate news post will be made.

Before upgrading fundamental software (such as the kernel, xorg, systemd, or glibc) to a new version, look over the appropriate forum to see if there have been any reported problems.

Users must equally be aware that upgrading packages can raise unexpected problems that could need immediate intervention; therefore, it is discouraged to upgrade a stable system shortly before it is required for carrying out an important task. Instead, wait to upgrade until there is enough time available to resolve any post-upgrade issues.

Avoid doing partial upgrades. In other words, never run pacman -Sy; instead, always use pacman -Syu.

Generally avoid using the --overwrite option with pacman. The --overwrite option takes an argument containing a glob. When used, pacman will bypass file conflict checks for files that match the glob. In a properly maintained system, it should only be used when explicitly recommended by the Arch Developers. See the #Read before upgrading the system section.

Avoid using the -d option with pacman. pacman -Rdd package skips dependency checks during package removal. As a result, a package providing a critical dependency could be removed, resulting in a broken system.

Arch Linux is a rolling release distribution. That means when new library versions are pushed to the repositories, the Developers and Package Maintainers rebuild all the packages in the repositories that need to be rebuilt against the libraries. For example, if two packages depend on the same library, upgrading only one package might also upgrade the library (as a dependency), which might then break the other package which depends on an older version of the library.

That is why partial upgrades are not supported. Do not use:

When refreshing the package database, always do a full upgrade with pacman -Syu.

Be very careful when using IgnorePkg and IgnoreGroup for the same reason. If the system has locally built packages (such as AUR packages), users will need to rebuild them when their dependencies receive a soname bump.

If a partial upgrade scenario has been created, and binaries are broken because they cannot find the libraries they are linked against, do not "fix" the problem simply by symlinking. Libraries receive soname bumps when they are not backwards compatible. A simple pacman -Syu to a properly synced mirror will fix the problem as long as pacman is not broken.

When upgrading the system, be sure to pay attention to the alert notices provided by pacman. If any additional actions are required by the user, be sure to take care of them right away. If a pacman alert is confusing, search the forums or check the latest news on the Arch Linux homepage (see #Read before upgrading the system) for more detailed instructions.

When pacman is invoked, .pacnew and .pacsave files can be created. Pacman provides notice when this happens and users must deal with these files promptly. Users are referred to the pacman/Pacnew and Pacsave wiki page for detailed instructions.

Also, think about other configuration files you may have copied or created. If a package had an example configuration that you copied to your home directory, check to see if a new one has been created.

Upgrades are typically not applied to existing processes. You must restart processes to fully apply the upgrade.

The archlinux-contrib package provides a script called checkservices which runs pacdiff to merge .pacnew files then checks for processes running with outdated libraries and prompts the user if they want them to be restarted.

The kernel is particularly difficult to patch without a reboot. A reboot is always the most secure option, but if this is very inconvenient kernel live patching can be used to apply upgrades without a reboot.

If a package update is expected/known to cause problems, packagers will ensure that pacman displays an appropriate message when the package is updated. If experiencing trouble after an update, double-check pacman's output by looking at /var/log/pacman.log.

At this point, only after ensuring there is no information available through pacman, there is no relevant news on https://archlinux.org/, and there are no forum posts regarding the update, consider seeking help on the forum or over IRC. Downgrading the offending package to revert broken updates should be considered as a last resort.

After upgrading you may now have packages that are no longer needed or that are no longer in the official repositories.

Use pacman -Qtd to check for packages that were installed as a dependency but now, no other packages depend on them. If an orphaned package is still needed, it is recommended to change the installation reason to explicit. Otherwise, if the package is no longer needed, it can be removed. See pacman/Tips and tricks#Removing unused packages (orphans) for details.

Additionally, some packages may no longer be in the remote repositories, but they still may be on your local system. To list all foreign packages use pacman -Qm. Note that this list will include packages that have been installed manually (e.g., from the AUR). To exclude packages that are (still) available on the AUR, use the script from BBS#288205 or try the ancient-packagesAUR tool.

Pacman does a much better job than you at keeping track of files. If you install things manually you will, sooner or later, forget what you did, forget where you installed to, install conflicting software, install to the wrong locations, etc.

To clean up improperly installed files, see pacman/Tips and tricks#Identify files not owned by any package.

Always try open source drivers before resorting to proprietary drivers. Most of the time, open source drivers are more stable and reliable than proprietary drivers. Open source driver bugs are fixed more easily and quickly. While proprietary drivers can offer more features and capabilities, this can come at the cost of stability. To avoid this dilemma, try to choose hardware components known to have mature open source driver support with full features. Information about hardware with open source Linux drivers is available at linux-drivers.org.

Use precaution when using packages from the AUR or an unofficial user repository. Most are supplied by regular users and thus may not have the same standards as those in the official repositories. Always check PKGBUILDs for sanity and signs of mistake or malicious code before building and/or installing the package.

To simplify maintenance, limit the amount of unofficial packages used. Make periodic checks on which are in actual use, and remove (or replace with their official counterparts) any others. See pacman/Tips and tricks#Maintenance for useful commands. Following system upgrade, use rebuild-detector to identify any unofficial packages that may need to be rebuilt.

Update pacman's mirrorlist, as the quality of mirrors can vary over time, and some might go offline or their download rate might degrade.

See mirrors for details.

Programs that help with this can be found in List of applications/Utilities#Disk cleaning.

When looking for files to remove, it is important to find the files that take up the most disk space. Programs that help with this can be found in List of applications/Utilities#Disk usage display.

Remove unwanted .pkg files from /var/cache/pacman/pkg/ to free up disk space.

See pacman#Cleaning the package cache for more information.

Remove unused packages from the system to free up disk space and simplify maintenance.

See #Check for orphans and dropped packages.

Old configuration files may conflict with newer software versions, or corrupt over time. Remove unneeded configurations periodically, particularly in your home directory and ~/.config. For similar reasons, be careful when sharing home directories between installations.

Look for the following directories:

See XDG Base Directory support for more information about these directories.

To keep the home directory clean from temporary files created at the wrong place, it is a good idea to manage a list of unwanted files and remove them regularly, for example with rmshit.py.

rmlint-gitAUR can be used to find and optionally remove duplicate files, empty files, recursive empty directories and broken symlinks.

Old, broken symbolic links might be sitting around your system; you should remove them. Examples on achieving this can be found here and here. However, you should not blindly delete all broken symbolic links, as some of them serve a purpose [1].

To quickly list all the broken symlinks of permanent files on your system, use:

Then inspect and remove unnecessary entries from this list.

The following tips are generally not required, but certain users may find them useful.

Arch's rolling releases can be a boon for users who want to try the latest features and get upstream updates as soon as possible, but they can also make system maintenance more difficult. To simplify maintenance and improve stability, try to avoid cutting edge software and install only mature and proven software. Such packages are less likely to receive difficult upgrades such as major configuration changes or feature removals. Prefer software that has a strong and active development community, as well as a high number of competent users, in order to simplify support in the event of a problem.

Avoid any use of the testing repository, even individual packages from testing. These packages are experimental and not suitable for a stable system. Similarly, avoid packages which are built directly from upstream development sources. These are usually found in the AUR, with names including things like: "dev", "devel", "svn", "cvs", "git", etc.

The linux-lts package is an alternative Arch kernel package, and is available in the core repository. This particular kernel version has long-term support (LTS) from upstream, including security and bug fixes. It is useful if you use out-of-tree kernel modules and want to ensure their compatibility or if you want a fallback kernel in case a new kernel version causes problems.

To make it available as a boot option, you will need to update your boot loader's configuration file to use the LTS kernel and ram disk: vmlinuz-linux-lts and initramfs-linux-lts.img.

**Examples:**

Example 1 (unknown):
```unknown
$ systemctl --failed
```

Example 2 (unknown):
```unknown
# journalctl -b
```

Example 3 (unknown):
```unknown
pacman -Syu
```

Example 4 (unknown):
```unknown
--overwrite
```

---

## Ly

**URL:** https://wiki.archlinux.org/title/Ly

**Contents:**
- Installation
  - Dependencies
- Usage
- Configuration
- See also

Ly is a lightweight TUI (ncurses-like) display manager for Linux and BSD, designed with portability in mind (e.g. it does not require systemd to run).

Install the ly package.

For the full Ly experience, it is recommended to install the brightnessctl package, which is responsible for managing screen brightness settings.

Make sure to enable ly.service so Ly will be started at boot.

All the configuration can be found in the /etc/ly/config.ini file. This file is fully commented, and includes the default values.

**Examples:**

Example 1 (unknown):
```unknown
systemctl disable lightdm.service
```

Example 2 (unknown):
```unknown
/etc/ly/config.ini
```

---

## Unified Extensible Firmware Interface

**URL:** https://wiki.archlinux.org/title/Efibootmgr

**Contents:**
- UEFI firmware bitness
  - Checking the firmware bitness
    - From Linux
    - From macOS
    - From Microsoft Windows
- UEFI variables
  - UEFI variables support in Linux kernel
  - Requirements for UEFI variable support
    - Mount efivarfs
  - Userspace tools

The Unified Extensible Firmware Interface (UEFI) is an interface between operating systems and firmware. It provides a standard environment for booting an operating system and running pre-boot applications.

It is distinct from the MBR boot code method that was used by legacy BIOS systems. See Arch boot process for their differences and the boot process using UEFI. To set up UEFI boot loaders, see Arch boot process#Boot loader.

Under UEFI, every program whether it is an operating system loader or a utility (e.g. a memory testing or recovery tool), should be an EFI application corresponding to the UEFI firmware bitness/architecture.

The vast majority of x86_64 systems, including recent Apple Macs, use x64 (64-bit) UEFI firmware. The only known devices that use IA32 (32-bit) UEFI are older (pre 2008) Apple Macs, Intel Atom System-on-Chip systems (as on 2 November 2013)[1] and some older Intel server boards that are known to operate on Intel EFI 1.10 firmware.

An x64 UEFI firmware does not include support for launching 32-bit EFI applications (unlike x86_64 Linux and Windows versions which include such support). Therefore the EFI application must be compiled for that specific firmware processor bitness/architecture.

The firmware bitness can be checked from a booted operating system.

On distributions running Linux kernel 4.0 or newer, the UEFI firmware bitness can be found via the sysfs interface. Run:

It will return 64 for a 64-bit (x64) UEFI or 32 for a 32-bit (IA32) UEFI. If the file does not exist, you have not booted in UEFI mode.

Pre-2008 Macs mostly have IA32 EFI firmware while >=2008 Macs have mostly x64 EFI. All Macs capable of running Mac OS X Snow Leopard 64-bit Kernel have x64 EFI 1.x firmware.

To find out the arch of the EFI firmware in a Mac, type the following into the Mac OS X terminal:

If the command returns EFI32, it is IA32 (32-bit) EFI firmware. If it returns EFI64, it is x64 EFI firmware. Most of the Macs do not have UEFI 2.x firmware as Apple's EFI implementation is not fully compliant with UEFI 2.x specification.

64-bit versions of Windows do not support booting on a 32-bit UEFI. So, if you have a 32-bit version of Windows booted in UEFI mode, you have a 32-bit UEFI.

To check the bitness run msinfo32.exe. In the System Summary section look at the values of "System Type" and "BIOS mode".

For 64-bit Windows on a 64-bit UEFI, it will be System Type: x64-based PC and BIOS mode: UEFI. For 32-bit Windows on a 32-bit UEFI—System Type: x86-based PC and BIOS mode: UEFI. If the "BIOS mode" is not UEFI, Windows is not booted in UEFI mode.

UEFI defines variables through which an operating system can interact with the firmware. UEFI boot variables are used by the boot loader and used by the operating system only for early system start-up. UEFI runtime variables allow an operating system to manage certain settings of the firmware like the UEFI boot manager or managing the keys for UEFI Secure Boot protocol etc. You can get the list using:

Linux kernel exposes UEFI variables data to userspace via efivarfs (EFI VARiable FileSystem) interface (CONFIG_EFIVAR_FS) - mounted using efivarfs kernel module at /sys/firmware/efi/efivars - it has no maximum per-variable size limitation and supports UEFI Secure Boot variables. Introduced in kernel 3.8.

If UEFI Variables support does not work even after the above conditions are satisfied, try the below workarounds:

If efivarfs is not automatically mounted at /sys/firmware/efi/efivars by systemd during boot, you need to manually mount it to expose UEFI variables to userspace tools like efibootmgr:

See efivarfs.html for kernel documentation.

There are few tools that can access/modify the UEFI variables, namely

You will have to install the efibootmgr package.

To add a new boot option using efibootmgr, you need to know three things:

For example, if you want to add a boot option for /efi/EFI/refind/refind_x64.efi where /efi is the mount point of the ESP, run

In this example, findmnt(8) indicates that the ESP is on disk /dev/sda and has partition number 1. The path to the EFI application relative to the root of the ESP is /EFI/refind/refind_x64.efi. So you would create the boot entry as follows:

Get an overview of all boot entries and the boot order:

To set the boot order:

Where XXXX is the number that appears in the previous output of efibootmgr command.

Delete an unwanted entry:

See efibootmgr(8) or efibootmgr README for more info.

Access to the UEFI can potentially cause harm beyond the running operating system level. There are dangerous UEFI exploits like LogoFAIL which allows a malicious actor to take full control over the machine. Even hardware-level bricking is possible in some cases of poor UEFI implementation [2].

So, as the UEFI variables access is not required for daily system usage, you may want to disable it, to avoid potential security breaches or accidental harm.

Possible solutions are:

The UEFI Shell is a shell/terminal for the firmware which allows launching EFI applications which include UEFI boot loaders. Apart from that, the shell can also be used to obtain various other information about the system or the firmware like memory map (memmap), modifying boot manager variables (bcfg), running partitioning programs (diskpart), loading UEFI drivers, editing text files (edit), hexedit etc.

You can obtain a BSD licensed UEFI Shell from the TianoCore EDK2 project:

Shell v2 works best in UEFI 2.3+ systems and is recommended over Shell v1 in those systems. Shell v1 should work in all UEFI systems irrespective of the spec. version the firmware follows. More information at ShellPkg and the EDK2 mailing list thread—Inclusion of UEFI shell in Linux distro iso.

Few Asus and other AMI Aptio x64 UEFI firmware based motherboards (from Sandy Bridge onwards) provide an option called Launch EFI Shell from filesystem device. For those motherboards, copy the x64 UEFI Shell to the root of your EFI system partition, named as shellx64.efi.

Systems with Phoenix SecureCore Tiano UEFI firmware is known to have embedded UEFI Shell which can be launched using either F6, F11 or F12 key.

UEFI Shell commands usually support -b option which makes output pause after each page. Run help -b to list available internal commands. Available commands are either built into the shell or discrete EFI applications.

For more info see Intel Scripting Guide 2008[dead link 2023-07-30—HTTP 404] and Intel "Course" 2011[dead link 2023-07-30—HTTP 404].

bcfg modifies the UEFI NVRAM entries which allows the user to change the boot entries or driver options. This command is described in detail in page 96 (Section 5.3) of the UEFI Shell Specification 2.2 document.

To dump a list of current boot entries:

To add a boot menu entry for rEFInd (for example) as 4th (numbering starts from zero) option in the boot menu:

where FS0: is the mapping corresponding to the EFI system partition and FS0:\EFI\refind\refind_x64.efi is the file to be launched.

To add an entry to boot directly into your system without a boot loader, see EFI boot stub#bcfg.

To remove the 4th boot option:

To move the boot option #3 to #0 (i.e. 1st or the default entry in the UEFI Boot menu):

map displays a list of device mappings i.e. the names of available file systems (FS0) and storage devices (blk0).

Before running file system commands such as cd or ls, you need to change the shell to the appropriate file system by typing its name:

edit provides a basic text editor with an interface similar to nano, but slightly less functional. It handles UTF-8 encoding and takes care or LF vs CRLF line endings.

For example, to edit rEFInd's refind.conf in the EFI system partition (FS0: in the firmware),

Press Ctrl+e for help.

This article or section needs expansion.

UEFI drivers are pieces of software that support some functionality. For example, access to NTFS formatted partitions is usually not possible from a UEFI shell. The efifs package has drivers that support reading many more file systems from within an EFI shell. A usage example is to copy such driver to a partition that can be accessed from an UEFI shell. Then, from the UEFI shell, issuing commands such as:

After the map command has been executed, the user should be able to access NTFS formatted partitions from within a UEFI shell.

Most of the 32-bit EFI Macs and some 64-bit EFI Macs refuse to boot from a UEFI(X64)+BIOS bootable CD/DVD. If one wishes to proceed with the installation using optical media, it might be necessary to remove UEFI support first.

Extract the ISO skipping the UEFI-specific directories:

Then rebuild the ISO, excluding the UEFI optical media booting support, using xorriso(1) from libisoburn. Be sure to set the correct volume label, e.g. ARCH_202103; it can be acquired using file(1) on the original ISO.

Burn archlinux-version-x86_64-noUEFI.iso to optical media and proceed with installation normally.

OVMF is a TianoCore project to enable UEFI support for Virtual Machines. OVMF contains a sample UEFI firmware and a separate non-volatile variable store for QEMU.

You can install edk2-ovmf from the extra repository.

It is advised to make a local copy of the non-volatile variable store for your virtual machine:

To use the OVMF firmware and this variable store, add following to your QEMU command:

DUET was a TianoCore project that enabled chainloading a full UEFI environment from a BIOS system, in a way similar to BIOS operating system booting. This method is being discussed extensively. Pre-build DUET images can be downloaded from one of the repos[dead link 2023-04-07—404 Page Not Found]. Read specific instructions[dead link 2023-04-07—404 Page Not Found] for setting up DUET. However, as of November 2018, the DUET code has been removed from TianoCore git repository.

You can also try Clover which provides modified DUET images that may contain some system specific fixes and is more frequently updated compared to the gitlab repos.

To boot back into Arch Linux when you are stuck with Windows, reach Advanced startup in Windows by the Windows PowerShell command shutdown /r /o, or via Settings > Update & Security > Recovery > Advanced startup and select Restart now. When you have reached the Advanced startup menu, choose Use a device, which actually contains your UEFI boot options (not limited to USB or CD, but can also boot operating system in hard drive), and choose "Arch Linux".

On some laptops, like Lenovo XiaoXin 15are 2020, using keys like F2 or F12 does not do anything. This can possibly be fixed by returning laptops to OEM to repair mainboard information, but sometimes this is not possible or not desired. There are however other means to enter firmware setup:

If any userspace tool is unable to modify UEFI variable data, check for existence of /sys/firmware/efi/efivars/dump-* files. If they exist, delete them, reboot and retry again. If the above step does not fix the issue, try booting with efi_no_storage_paranoia kernel parameter to disable kernel UEFI variable storage space check that may prevent writing/modification of UEFI variables.

Some kernel and efibootmgr version combinations might refuse to create new boot entries. This could be due to lack of free space in the NVRAM. You can try the solution at #Userspace tools are unable to modify UEFI variable data.

You can also try to downgrade your efibootmgr install to version 0.11.0. This version works with Linux version 4.0.6. See the bug discussion FS#34641, in particular the closing comment, for more information.

If you dual boot with Windows and your motherboard just boots Windows immediately instead of your chosen EFI application, there are several possible causes and workarounds.

This issue can occur due to KMS issue. Try disabling KMS while booting the USB.

Some firmware do not support custom boot entries. They will instead only boot from hardcoded boot entries.

A typical workaround is to not rely on boot entries in the NVRAM and install the boot loader to one of the common fallback paths on the EFI system partition.

The following sections describe the fallback paths.

The UEFI specification defines default file paths for EFI binaries for booting from removable media. The relevant ones are:

While the specification defines these for removable drives only, most firmware support booting these from any drive.

See the appropriate boot loader article on how to install or migrate the boot loader to the default/fallback boot path.

On certain UEFI motherboards like some boards with an Intel Z77 chipset, adding entries with efibootmgr or bcfg from the UEFI Shell will not work because they do not show up on the boot menu list after being added to NVRAM.

This issue is caused because the motherboards can only load Microsoft Windows. To solve this you have to place the .efi file in the location that Windows uses.

Copy the BOOTx64.EFI file from the Arch Linux installation medium (FSO:) to the Microsoft directory your ESP partition on your hard drive (FS1:). Do this by booting into EFI shell and typing:

After reboot, any entries added to NVRAM should show up in the boot menu.

This is a recurring problem with Acer laptops, which occurs if .efi files have not been manually authorized. See Laptop/Acer#Firmware Setup became inaccessible after Linux installation.

efibootmgr can fail to detect EDD 3.0 and as a result create unusable boot entries in NVRAM. See efibootmgr issue 86 for the details.

To work around this, when creating boot entries manually, add the -e 3 option to the efibootmgr command. E.g.

To fix boot loader installers, like grub-install and refind-install, create a wrapper script /usr/local/bin/efibootmgr and make it executable:

Some firmware will remove boot entries referencing drives that are not present during boot. This could be an issue when frequently detaching/attaching drives or when booting from a removable drive.

The solution is to install the boot loader to the default/fallback boot path.

Some motherboards may remove boot entries due to lack of free space in the NVRAM instead of giving an error at creation. To prevent this from occurring, reduce the amount of boot entries being added by minimizing your entry creation process, as well as reducing the amount of automatic drive boot entries by the Compatibility Support Module (CSM) by disabling it from your UEFI settings. See BBS#1608838.

Another reason why boot entries might have been removed is the fact that UEFI specification allows OEMs to do "NVRAM maintenance" during boot process. Those manufacturers do it simply: they just look up for EFI applications in predefined, hardcoded paths on the device. If they fail to find any, they conclude there is no operating system on the device and wipe all boot entries from NVRAM associated with it, because they assume the NVRAM contains some corrupted or outdated data. If you do not plan to install Windows and still want to load the Linux kernel directly from the firmware, one possible workaround is to create an empty file esp/EFI/BOOT/BOOTx64.EFI:

And restore the deleted boot entry. Now after reboot the motherboard will see the "Fake OS" and should not wipe other boot entries from NVRAM. You can change the fake operating system loader with an actual EFI application if you want, of course, as long as you keep the standard fallback name.

This article or section is a candidate for merging with Lenovo.

This article or section needs expansion.

On recent Lenovo ThinkPad laptops (e.g. T16 Gen 2 AMD models), users report that custom UEFI boot entries (created with efibootmgr or bootctl) are automatically deleted at each boot, with only Windows Boot Manager and Lenovo’s own entries (PXE, Recovery, Diagnostics) restored.

This is caused by the BIOS option "Restart / OS Optimized Defaults", which resets the UEFI boot variables at each reboot to defaults optimized for Windows.

Solution: Disable "OS Optimized Defaults" in the BIOS/UEFI setup. After doing so, manually created boot entries persist correctly, allowing systemd-boot or other custom boot managers to work as intended.

**Examples:**

Example 1 (unknown):
```unknown
$ cat /sys/firmware/efi/fw_platform_size
```

Example 2 (unknown):
```unknown
$ ioreg -l -p IODeviceTree | grep firmware-abi
```

Example 3 (unknown):
```unknown
msinfo32.exe
```

Example 4 (unknown):
```unknown
System Type: x64-based PC
```

---

## systemd

**URL:** https://wiki.archlinux.org/title/Drop-in_file

**Contents:**
- Basic systemctl usage
  - Using units
  - Power management
    - Soft reboot
- Writing unit files
  - Handling dependencies
  - Service types
  - Editing provided units
    - Replacement unit files
    - Drop-in files

From the project web page:

Historically, what systemd calls "service" was named daemon: any program that runs as a "background" process (without a terminal or user interface), commonly waiting for events to occur and offering services. A good example is a web server that waits for a request to deliver a page, or an ssh server waiting for someone trying to log in. While these are full featured applications, there are daemons whose work is not that visible. Daemons are for tasks like writing messages into a log file (e.g. syslog, metalog) or keeping your system time accurate (e.g. ntpd). For more information see daemon(7).

The main command used to introspect and control systemd is systemctl. Some of its uses are examining the system state and managing the system and services. See systemctl(1) for more details.

Units commonly include, but are not limited to, services (.service), mount points (.mount), devices (.device) and sockets (.socket).

When using systemctl, you generally have to specify the complete name of the unit file, including its suffix, for example sshd.socket. There are however a few short forms when specifying the unit in the following systemctl commands:

See systemd.unit(5) for details.

The commands in the below table operate on system units since --system is the implied default for systemctl. To instead operate on user units (for the calling user), use systemctl --user without root privileges. See also systemd/User#Basic setup to enable/disable user units for all users.

polkit is necessary for power management as an unprivileged user. If you are in a local systemd-logind user session and no other session is active, the following commands will work without root privileges. If not (for example, because another user is logged into a tty), systemd will automatically ask you for the root password.

Soft reboot is a special kind of a userspace-only reboot operation that does not involve the kernel. It is implemented by systemd-soft-reboot.service(8) and can be invoked through systemctl soft-reboot. As with kexec, it skips firmware re-initialization, but additionally the system does not go through kernel initialization and initramfs, and unlocked dm-crypt devices remain attached.

When /run/nextroot/ contains a valid root file system hierarchy (e.g. is the root mount of another distribution or another snapshot), soft-reboot would switch the system root into it, allowing for switching to another installation without losing states managed by kernel, e.g. networking.

The syntax of systemd's unit files (systemd.unit(5)) is inspired by XDG Desktop Entry Specification .desktop files, which are in turn inspired by Microsoft Windows .ini files. Unit files are loaded from multiple locations (to see the full list, run systemctl show --property=UnitPath), but the main ones are (listed from lowest to highest precedence):

Look at the units installed by your packages for examples, as well as systemd.service(5) § EXAMPLES.

systemd-analyze(1) can help verifying the work. See the systemd-analyze verify FILE... section of that page.

With systemd, dependencies can be resolved by designing the unit files correctly. The most typical case is when unit A requires unit B to be running before A is started. In that case add Requires=B and After=B to the [Unit] section of A. If the dependency is optional, add Wants=B and After=B instead. Note that Wants= and Requires= do not imply After=, meaning that if After= is not specified, the two units will be started in parallel.

Dependencies are typically placed on services and not on #Targets. For example, network.target is pulled in by whatever service configures your network interfaces, therefore ordering your custom unit after it is sufficient since network.target is started anyway.

There are several different start-up types to consider when writing a custom service file. This is set with the Type= parameter in the [Service] section:

See the systemd.service(5) § OPTIONS man page for a more detailed explanation of the Type values.

This article or section needs expansion.

To avoid conflicts with pacman, unit files provided by packages should not be directly edited. There are two safe ways to modify a unit without touching the original file: create a new unit file which overrides the original unit or create drop-in snippets which are applied on top of the original unit. For both methods, you must reload the unit afterwards to apply your changes. This can be done either by editing the unit with systemctl edit (which reloads the unit automatically) or by reloading all units with:

To replace the unit file /usr/lib/systemd/system/unit, create the file /etc/systemd/system/unit and reenable the unit to update the symlinks.

This opens /etc/systemd/system/unit in your editor (copying the installed version if it does not exist yet) and automatically reloads it when you finish editing.

To create drop-in files for the unit file /usr/lib/systemd/system/unit, create the directory /etc/systemd/system/unit.d/ and place .conf files there to override or add new options. systemd will parse and apply these files on top of the original unit.

The easiest way to do this is to run:

This opens the file /etc/systemd/system/unit.d/drop_in_name.conf in your text editor (creating it if necessary) and automatically reloads the unit when you are done editing. Omitting --drop-in= option will result in systemd using the default file name override.conf .

To revert any changes to a unit made using systemctl edit do:

For example, if you simply want to add an additional dependency to a unit, you may create the following file:

As another example, in order to replace the ExecStart directive, create the following file:

Note how ExecStart must be cleared before being re-assigned [2]. The same holds for every item that can be specified multiple times, e.g. OnCalendar for timers.

One more example to automatically restart a service:

For services that send logs directly to journald or syslog, you can control their verbosity by setting a numeric value between 0 and 6 for the LogLevelMax= parameter in the [Service] section using the methods described above. For example:

The standard log levels are identical to those used to filter the journal. Setting a lower number excludes all higher and less important log messages from your journal.

If a service is echoing stdout and/or stderr output, by default this will end up in the journal as well. This behavior can be suppressed by setting StandardOutput=null and/or StandardError=null in the [Service] section. Other values than null can be used to further tweak this behavior. See systemd.exec(5) § LOGGING_AND_STANDARD_INPUT/OUTPUT.

systemd uses targets to group units together via dependencies and as standardized synchronization points. They serve a similar purpose as runlevels but act a little differently. Each target is named instead of numbered and is intended to serve a specific purpose with the possibility of having multiple ones active at the same time. Some targets are implemented by inheriting all of the services of another target and adding additional services to it. There are systemd targets that mimic the common SystemVinit runlevels.

The following should be used under systemd instead of running runlevel:

The runlevels that held a defined meaning under sysvinit (i.e., 0, 1, 3, 5, and 6); have a 1:1 mapping with a specific systemd target. Unfortunately, there is no good way to do the same for the user-defined runlevels like 2 and 4. If you make use of those it is suggested that you make a new named systemd target as /etc/systemd/system/your target that takes one of the existing runlevels as a base (you can look at /usr/lib/systemd/system/graphical.target as an example), make a directory /etc/systemd/system/your target.wants, and then symlink the additional services from /usr/lib/systemd/system/ that you wish to enable.

In systemd, targets are exposed via target units. You can change them like this:

This will only change the current target, and has no effect on the next boot. This is equivalent to commands such as telinit 3 or telinit 5 in Sysvinit.

The standard target is default.target, which is a symlink to graphical.target. This roughly corresponds to the old runlevel 5.

To verify the current target with systemctl:

To change the default target to boot into, change the default.target symlink. With systemctl:

Alternatively, append one of the following kernel parameters to your boot loader:

systemd chooses the default.target according to the following order:

Some (not exhaustive) components of systemd are:

systemd is in charge of mounting the partitions and filesystems specified in /etc/fstab. The systemd-fstab-generator(8) translates all the entries in /etc/fstab into systemd units; this is performed at boot time and whenever the configuration of the system manager is reloaded.

systemd extends the usual fstab capabilities and offers additional mount options. These affect the dependencies of the mount unit. They can, for example, ensure that a mount is performed only once the network is up or only once another partition is mounted. The full list of specific systemd mount options, typically prefixed with x-systemd., is detailed in systemd.mount(5) § FSTAB.

An example of these mount options is automounting, which means mounting only when the resource is required rather than automatically at boot time. This is provided in fstab#Automount with systemd.

On UEFI-booted systems, GPT partitions such as root, home, swap, etc. can be mounted automatically following the Discoverable Partitions Specification. These partitions can thus be omitted from fstab, and if the root partition is automounted, then root= can be omitted from the kernel command line. See systemd-gpt-auto-generator(8).

The prerequisites are:

udev will create symlinks to the discovered partitions which can be used to reference the partitions and volumes in configuration files.

For /var automounting to work, the PARTUUID must match the SHA256 HMAC hash of the partition type UUID keyed by the machine ID. The required PARTUUID can be obtained using:

The primary role of systemd-sysvcompat (required by base) is to provide the traditional linux init binary. For systemd-controlled systems, init is just a symbolic link to its systemd executable.

In addition, it provides four convenience shortcuts that SysVinit users might be used to. The convenience shortcuts are halt(8), poweroff(8), reboot(8) and shutdown(8). Each one of those four commands is a symbolic link to systemctl, and is governed by systemd behavior. Therefore, the discussion at #Power management applies.

systemd-based systems can give up those System V compatibility methods by using the init= boot parameter (see, for example, /bin/init is in systemd-sysvcompat ?) and systemd native systemctl command arguments.

systemd-tmpfiles creates, deletes and cleans up volatile and temporary files and directories. It reads configuration files in /etc/tmpfiles.d/ and /usr/lib/tmpfiles.d/ to discover which actions to perform. Configuration files in the former directory take precedence over those in the latter directory.

Configuration files are usually provided together with service files, and they are named in the style of /usr/lib/tmpfiles.d/program.conf. For example, the Samba daemon expects the directory /run/samba to exist and to have the correct permissions. Therefore, the samba package ships with this configuration:

Configuration files may also be used to write values into certain files on boot. For example, if you used /etc/rc.local to disable wakeup from USB devices with echo USBE > /proc/acpi/wakeup, you may use the following tmpfile instead:

It is possible to write multiple lines to the same file, either with \n in the argument or using the w+ type on multiple lines (including the first one) for appending:

See the systemd-tmpfiles(8) and tmpfiles.d(5) man pages for details.

The factual accuracy of this article or section is disputed.

Configuration files provided by packages should not be directly edited to avoid conflicts with pacman updates. For this, many (but not all) systemd packages provide a way to modify the configuration, but without touching the original file by creation of drop-in snippets. Check the package manual to see if drop-in configuration files are supported.

To create a drop-in configuration file for the unit file /etc/systemd/unit.conf, create the directory /etc/systemd/unit.conf.d/ and place .conf files there to override or add new options. systemd will parse and apply these files on top of the original unit.

Check the overall configuration:

The applied drop-in snippets file(s) and content will be listed at the end. Restart the service for the changes to take effect.

Some packages provide a .socket unit. For example, cups provides a cups.socket unit[3]. If cups.socket is enabled (and cups.service is left disabled), systemd will not start CUPS immediately; it will just listen to the appropriate sockets. Then, whenever a program attempts to connect to one of these CUPS sockets, systemd will start cups.service and transparently hand over control of these ports to the CUPS process.

To delay a service until after the network is up, include the following dependencies in the .service file:

The network wait service of the network manager in use must also be enabled so that network-online.target properly reflects the network status.

For more detailed explanations, see the discussion in the Network configuration synchronization points.

If a service needs to perform DNS queries, it should additionally be ordered after nss-lookup.target:

See systemd.special(7) § Special Passive System Units.

For nss-lookup.target to have any effect it needs a service that pulls it in via Wants=nss-lookup.target and orders itself before it with Before=nss-lookup.target. Typically this is done by local DNS resolvers.

Check which active service, if any, is pulling in nss-lookup.target with:

This article or section needs expansion.

By default, Arch Linux does not make use of systemd presets, and does not enable most services on installation.

If you wish for systemd presets to be honored when packages are installed, you will need to create a a pacman hook in order to run systemctl preset or systemctl preset-all whenever a new systemd unit is installed. Something like

Arch Linux ships with /usr/lib/systemd/system-preset/99-default.preset containing disable *. This causes systemctl preset to disable all units by default, such that when a new package is installed, the user must manually enable the unit.

To enable units by default instead, simply create a symlink from /etc/systemd/system-preset/99-default.preset to /dev/null or replace with a file containing enable * in order to override the configuration file. This will cause systemctl preset to enable all units that get installed—regardless of unit type—unless specified in another file in one systemctl preset's configuration directories. User units are not affected. It is also possible to configure higher priority files with more specific rules on what should be enabled. See systemd.preset(5) for more information.

See systemd/Sandboxing.

In order to notify about service failures, an OnFailure= directive needs to be added to the according service file, for example by using a drop-in configuration file. Adding this directive to every service unit can be achieved with a top-level drop-in configuration file. For details about top-level drop-ins, see systemd.unit(5).

Create a top-level drop-in for services:

This adds OnFailure=failure-notification@%n.service to every service file. If some_service_unit fails, failure-notification@some_service_unit.service will be started to handle the notification delivery (or whatever task it is configured to perform).

Create the failure-notification@.service template unit:

You can create the failure-notification.sh script and define what to do or how to notify. Examples include sending e-mail, showing desktop notifications, using gotify, XMPP, etc. The %i will be the name of the failed service unit and will be passed as an argument to the script.

In order to prevent a recursion for starting instances of failure-notification@.service again and again if the start fails, create an empty drop-in configuration file with the same name as the top-level drop-in (the empty service-level drop-in configuration file takes precedence over the top-level drop-in and overrides the latter one):

You can set up systemd to send an e-mail when a unit fails. Cron sends mail to MAILTO if the job outputs to stdout or stderr, but many jobs are setup to only output on error. First you need two files: an executable for sending the mail and a .service for starting the executable. For this example, the executable is just a shell script using sendmail, which is in packages that provide smtp-forwarder.

Whatever executable you use, it should probably take at least two arguments as this shell script does: the address to send to and the unit file to get the status of. The .service we create will pass these arguments:

Where user is the user being emailed and address is that user's email address. Although the recipient is hard-coded, the unit file to report on is passed as an instance parameter, so this one service can send email for many other units. At this point you can start status_email_user@dbus.service to verify that you can receive the emails.

Then simply edit the service you want emails for and add OnFailure=status_email_user@%n.service to the [Unit] section. %n passes the unit's name to the template.

See udisks#Automatically turn off an external HDD at shutdown.

To find the systemd services which failed to start:

To find out why they failed, examine their log output. See systemd/Journal#Filtering output for details.

systemd has several options for diagnosing problems with the boot process. See boot debugging for more general instructions and options to capture boot messages before systemd takes over the boot process. Also see systemd debugging documentation.

If some systemd service misbehaves or you want to get more information about what is happening, set the SYSTEMD_LOG_LEVEL environment variable to debug. For example, to run the systemd-networkd daemon in debug mode:

Add a drop-in file for the service adding the two lines:

Or equivalently, set the environment variable manually:

then restart systemd-networkd and watch the journal for the service with the -f/--follow option.

If the shutdown process takes a very long time (or seems to freeze), most likely a service not exiting is to blame. systemd waits some time for each service to exit before trying to kill it. To find out whether you are affected, see Shutdown completes eventually in the systemd documentation.

A common problem is a stalled shutdown or suspend process. To verify whether that is the case, you could run either of these commands and check the outputs

The solution to this would be to cancel these jobs by running

and then trying shutdown or reboot again.

If running journalctl -u foounit as root does not show any output for a short lived service, look at the PID instead. For example, if systemd-modules-load.service fails, and systemctl status systemd-modules-load shows that it ran as PID 123, then you might be able to see output in the journal for that PID, i.e. by running journalctl -b _PID=123 as root. Metadata fields for the journal such as _SYSTEMD_UNIT and _COMM are collected asynchronously and rely on the /proc directory for the process existing. Fixing this requires fixing the kernel to provide this data via a socket connection, similar to SCM_CREDENTIALS. In short, it is a bug. Keep in mind that immediately failed services might not print anything to the journal as per design of systemd.

The factual accuracy of this article or section is disputed.

After using systemd-analyze a number of users have noticed that their boot time has increased significantly in comparison with what it used to be. After using systemd-analyze blame NetworkManager is being reported as taking an unusually large amount of time to start.

The problem for some users has been due to /var/log/journal becoming too large. This may have other impacts on performance, such as for systemctl status or journalctl. As such the solution is to remove every file within the folder (ideally making a backup of it somewhere, at least temporarily) and then setting a journal file size limit as described in systemd/Journal#Journal size limit.

Starting with systemd 219, /usr/lib/tmpfiles.d/systemd.conf specifies ACL attributes for directories under /var/log/journal and, therefore, requires ACL support to be enabled for the filesystem the journal resides on.

See Access Control Lists#Enable ACL for instructions on how to enable ACL on the filesystem that houses /var/log/journal.

You may want to disable emergency mode on a remote machine, for example, a virtual machine hosted at Azure or Google Cloud. It is because if emergency mode is triggered, the machine will be blocked from connecting to network.

To disable it, mask emergency.service and emergency.target.

You may be trying to start or enable a user unit as a system unit. systemd.unit(5) indicates, which units reside where. By default systemctl operates on system services.

See systemd/User for more details.

Some bootloaders only set the LoaderDevicePartUUID variable when it is empty. Consequently, even if the UUID of the EFI partition changes, the bootloader will not update LoaderDevicePartUUID. By deleting the EFI variable with the commands below, the bootloader will then repopulate it with the new UUID.

**Examples:**

Example 1 (unknown):
```unknown
-H user@host
```

Example 2 (unknown):
```unknown
sshd.socket
```

Example 3 (unknown):
```unknown
netctl.service
```

Example 4 (unknown):
```unknown
dev-sda2.device
```

---

## mkinitcpio

**URL:** https://wiki.archlinux.org/title/Mkinitcpio.conf

**Contents:**
- Installation
- Image creation and activation
  - Automated generation
  - Manual generation
  - Customized generation
  - Unified kernel images
- Configuration
  - MODULES
  - BINARIES and FILES
  - HOOKS

mkinitcpio is a Bash script used to create initramfs images. See Arch boot process#initramfs for a general introduction.

It is important to note that there are two distinct approaches how the various tasks during initial ramdisk phase are performed:

The concrete variant is determined by the absence or presence of the systemd hook in the HOOKS array of /etc/mkinitcpio.conf. See #Common hooks for more details.

mkinitcpio has been developed by the Arch Linux developers and from community contributions. See the public Git repository.

Install the mkinitcpio package, which is a dependency of the linux package, so most users will already have it installed.

Advanced users may wish to install the latest development version of mkinitcpio from Git with the mkinitcpio-gitAUR package.

Every time a kernel is installed or upgraded, a pacman hook automatically generates a .preset file saved in /etc/mkinitcpio.d/. For example linux.preset for the official stable linux kernel package. A preset is simply a list of information required to create initial ramdisk images, instead of manually specifying the various parameters and the location of the output files. By default, it contains the instructions to create two images:

After creating the preset, the pacman hook calls the mkinitcpio script which generates the two images, using the information provided in the preset.

To run the script manually, refer to the mkinitcpio(8) manual page for instructions. In particular, to (re-)generate an initramfs image based on the preset provided by a kernel package, use the -p/--preset option followed by the preset to utilize. For example, for the linux package, use the command:

To (re-)generate initramfs images based on all existing presets, use the -P/--allpresets switch. This is typically used to regenerate all the initramfs images after a change of the global #Configuration:

Users may create any number of initramfs images with a variety of different configurations. The desired image must be specified in the respective boot loader configuration file.

Users can generate an image using an alternative configuration file. For example, the following will generate an initial ramdisk image according to the directions in /etc/mkinitcpio-custom.conf and save it as /boot/initramfs-custom.img.

If generating an image for a kernel other than the one currently running, add the kernel release version to the command line. The installed kernel releases can be found in /usr/lib/modules/, the syntax is consistent with the output of the command uname -r for each kernel.

mkinitcpio can create unified kernel images (UKIs) either by itself or via systemd-ukify. If systemd-ukify is absent or explicitly disabled using --no-ukify, the UKI will be assembled by mkinitcpio itself. Advanced features of ukify will not be available then.

See unified kernel image for details about UKI generation.

The primary configuration file for mkinitcpio is /etc/mkinitcpio.conf. Drop-in configuration files are also supported, e.g. /etc/mkinitcpio.conf.d/myhooks.conf (they aren't taken into account if mkinitcpio is called with -c option and/or use a preset containing ALL_config). Additionally, preset definitions are provided by kernel packages in the /etc/mkinitcpio.d directory (e.g. /etc/mkinitcpio.d/linux.preset).

Users can modify seven variables within the configuration file, see mkinitcpio.conf(5) § VARIABLES for more details:

The MODULES array is used to specify modules to load before anything else is done.

Modules suffixed with a ? will not throw errors if they are not found. This might be useful for custom kernels that compile in modules which are listed explicitly in a hook or configuration file.

These options allow users to add files to the image. Both BINARIES and FILES are added before hooks are run, and may be used to override files used or provided by a hook. BINARIES are auto-located within a standard PATH and are dependency-parsed, meaning any required libraries will also be added. FILES are added as-is. For example:

Note that as both BINARIES and FILES are Bash arrays, multiple entries can be added delimited with spaces.

The HOOKS array is the most important setting in the file. Hooks are small scripts which describe what will be added to the image. For some hooks, they will also contain a runtime component which provides additional behavior, such as starting a daemon, or assembling a stacked block device. Hooks are referred to by their name, and executed in the order they exist in the HOOKS array of the configuration file.

The default HOOKS setting should be sufficient for most simple, single disk setups. For root devices which are stacked or multi-block devices such as LVM, RAID, or dm-crypt, see the respective wiki pages for further necessary configuration.

Build hooks are found in /usr/lib/initcpio/install/, custom build hooks can be placed in /etc/initcpio/install/. These files are sourced by the bash shell during runtime of mkinitcpio and should contain two functions: build and help. The build function describes the modules, files, and binaries which will be added to the image. An API, documented by mkinitcpio(8), serves to facilitate the addition of these items. The help function outputs a description of what the hook accomplishes.

For a list of all available hooks:

Use mkinitcpio's -H/--hookhelp option to output help for a specific hook, for example:

Runtime hooks are found in /usr/lib/initcpio/hooks/, custom runtime hooks can be placed in /etc/initcpio/hooks/. For any runtime hook, there should always be a build hook of the same name, which calls add_runscript to add the runtime hook to the image. These files are sourced by the busybox ash shell during early userspace. With the exception of cleanup hooks, they will always be run in the order listed in the HOOKS setting. Runtime hooks may contain several functions:

run_earlyhook: Functions of this name will be run once the API file systems have been mounted and the kernel command line has been parsed. This is generally where additional daemons, such as udev, which are needed for the early boot process are started from.

run_hook: Functions of this name are run shortly after the early hooks. This is the most common hook point, and operations such as assembly of stacked block devices should take place here.

run_latehook: Functions of this name are run after the root device has been mounted. This should be used, sparingly, for further setup of the root device, or for mounting other file systems, such as /usr.

run_cleanuphook: Functions of this name are run as late as possible, and in the reverse order of how they are listed in the HOOKS array in the configuration file. These hooks should be used for any last minute cleanup, such as shutting down any daemons started by an early hook.

Post hooks are executables or shell scripts located in /usr/lib/initcpio/post/ (hooks provided by packages) and /etc/initcpio/post/ (custom hooks). These files are executed after an image is (re)generated in order to perform additional tasks like signing.

To each executable the following arguments are passed in this order:

Additionally, the following environment variables are set—KERNELVERSION the full kernel version, KERNELDESTINATION the default location where the kernel should be located on order to be booted.

A table of common hooks and how they affect image creation and runtime follows. Note that this table is not complete, as packages can provide custom hooks.

This article or section needs expansion.

Optional when using the systemd hook as it only provides a busybox recovery shell. In addition to enabling base, you'll need to temporarily add SYSTEMD_SULOGIN_FORCE=1 to your kernel parameters to use the shell.

If the autodetect hook runs before this hook, it will only add early microcode update files for the processor of the system the image is built on.

The use of this hook replaces the now deprecated --microcode flag, and the microcode option in the preset files. This also allows you to drop the microcode initrd lines from your boot configuration as they are now packed together with the main initramfs image. || class="archwiki-table-cell" data-sort-value=5 |–

For sd-encrypt see dm-crypt/System configuration#Using systemd-cryptsetup-generator.

The use of this hook requires the rw parameter to be set on the kernel command line (discussion). See fsck#Boot time checking for more details. || class="archwiki-table-cell" data-sort-value=5 |–

The kernel supports several formats for compression of the initramfs: gzip, bzip2, lzma (xz), xz, lzo (lzop), lz4 and zstd. By default mkinitcpio uses zstd compression for kernel version 5.9 and newer and gzip for kernel versions older than 5.9.

The provided mkinitcpio.conf has the various COMPRESSION options commented out. Uncomment one if you wish to switch to another compression method and make sure you have the corresponding compression utility installed. If none is specified, the default method is used. If you wish to create an uncompressed image, specify COMPRESSION=cat in the configuration file or use -z cat on the command line.

These are additional flags passed to the program specified by COMPRESSION, such as:

This option can be left empty; mkinitcpio will ensure that any supported compression method has the necessary flags to produce a working image.

With the default zstd compression, to save space for custom kernels (especially with a dual boot setup when using the EFI system partition as /boot), the --long option is very effective. However, systems with limited RAM may not be able to decompress initramfs using this option. The -v option may also be desired to see details during the initramfs generation. For example:

Highest, but slowest, compression can be achieved by using xz with the -9e compression level and also decompressing the loadable kernel modules and firmware:

MODULES_DECOMPRESS controls whether the kernel module and firmware files are decompressed during initramfs creation. The default value is no.

Arch compresses its kernel modules and linux-firmware with zstd at level 19. When using a higher compression than that for the initramfs, setting MODULES_DECOMPRESS="yes" will allow to reduce the initramfs size even further. This comes at the expense of increased RAM and CPU usage at early boot which negatively affects systems with limited RAM or weak CPUs since the kernel will spend more time to decompress the whole initramfs image than it would take to decompress the individual modules and firmware upon loading them.

This article or section needs expansion.

Runtime configuration options can be passed to init and certain hooks via the kernel command line. Kernel command-line parameters are often supplied by the boot loader. The options discussed below can be appended to the kernel command line to alter default behavior. See Kernel parameters and Arch boot process for more information.

See Boot debugging and mkinitcpio(8) for other parameters.

See RAID#Configure mkinitcpio.

net requires the mkinitcpio-nfs-utils package.

Comprehensive and up-to-date information can be found in the official kernel documentation.

This parameter tells the kernel how to configure IP addresses of devices and also how to set up the IP routing table. It can take up to nine arguments separated by colons: ip=<client-ip>:<server-ip>:<gw-ip>:<netmask>:<hostname>:<device>:<autoconf>:<dns0-ip>:<dns1-ip>:<ntp0-ip>.

If this parameter is missing from the kernel command line, all fields are assumed to be empty, and the defaults mentioned in the kernel documentation apply. In general this means that the kernel tries to configure everything using autoconfiguration.

The <autoconf> parameter can appear alone as the value to the ip parameter (without all the : characters before). If the value is ip=off or ip=none, no autoconfiguration will take place, otherwise autoconfiguration will take place. The most common way to use this is ip=dhcp.

For parameters explanation, see the kernel documentation.

If you have multiple network cards, this parameter can include the MAC address of the interface you are booting from. This is often useful as interface numbering may change, or in conjunction with pxelinux IPAPPEND 2 or IPAPPEND 3 option. If not given, eth0 will be used.

If the nfsroot parameter is NOT given on the command line, the default /tftpboot/%s will be used.

Run mkinitcpio -H net for parameter explanation.

If your root device is on LVM, see Install Arch Linux on LVM#Adding mkinitcpio hooks.

If using an encrypted root see dm-crypt/System configuration#mkinitcpio for detailed information on which hooks to include.

If you keep /usr as a separate partition, you must adhere to the following requirements:

The generation of fallback images can be disabled:

If you are curious about what is inside the initramfs image, you can extract it and poke at the files inside of it.

The initramfs image is an SVR4 CPIO archive, generated via the find and bsdcpio commands, optionally compressed with a compression scheme understood by the kernel. For more information on the compression schemes, see #COMPRESSION.

mkinitcpio includes a utility called lsinitcpio(1) which will list and/or extract the contents of initramfs images.

You can list the files in the image with:

And to extract them all in the current directory:

You can also get a more human-friendly listing of the important parts in the image:

Invoke the build_image function of the /usr/bin/mkinitcpio script with parameters

It can be done by creating a new script with the contents of the build_image function and running it with the above parameters. This will compress the contents present in the current directory in a file named outfile.

The test used by mkinitcpio to determine if /dev is mounted is to see if /dev/fd/ is there. If everything else looks fine, it can be "created" manually by:

(Obviously, /proc must be mounted as well. mkinitcpio requires that anyway, and that is the next thing it will check.)

When initramfs are being rebuilt after a kernel update, you might get warnings:

If these messages appear when generating a default initramfs image, then, as the warning says, installing additional firmware may be required. Most common firmware files can be acquired by installing the linux-firmware package. For other packages providing firmware see the table below or try searching for the module name in the official repositories or AUR.

Otherwise, if the messages only appear when generating the fallback initramfs image you have the following options:

For unavailable firmware, you can suppress the warnings by creating dummy files, e.g.:

On some motherboards (mostly ancient ones, but also a few new ones), the i8042 controller cannot be automatically detected. It is rare, but some people will surely be without keyboard. You can detect this situation in advance. If you have a PS/2 port and get i8042: PNP: No PS/2 controller found. Probing ports directly message, add atkbd to the MODULES array.[2]

With an improper initial ram-disk a system often is unbootable. So follow a system rescue procedure like below:

mkinitcpio's autodetect hook filters unneeded kernel modules in the primary initramfs scanning /sys and the modules loaded at the time it is run. If you transfer your /boot directory to another machine and the boot sequence fails during early userspace, it may be because the new hardware is not detected due to missing kernel modules. Note that USB 2.0 and 3.0 need different kernel modules.

To fix, first try choosing the fallback image from your boot loader, as it is not filtered by autodetect. Once booted, run mkinitcpio on the new machine to rebuild the primary image with the correct modules. If the fallback image fails, try booting into an Arch Linux live CD/USB, chroot into the installation, and run mkinitcpio on the new machine. As a last resort, try manually adding modules to the initramfs.

The systemd hook disables the root account. To enable the emergency shell, temporarily add SYSTEMD_SULOGIN_FORCE=1 to the kernel parameters.

Alternatively, you can use initcpio-hook-shadowcopyAUR, by installing it and adding the shadowcopy hook after systemd in /etc/mkinitcpio.conf, and regenerating initramfs with mkinitcpio -P. More documentation is available in its GitHub repo.

**Examples:**

Example 1 (unknown):
```unknown
/etc/crypttab.initramfs
```

Example 2 (unknown):
```unknown
/etc/mkinitcpio.conf
```

Example 3 (unknown):
```unknown
/etc/mkinitcpio.d/
```

Example 4 (unknown):
```unknown
linux.preset
```

---

## Users and groups

**URL:** https://wiki.archlinux.org/title/Users_and_groups

**Contents:**
- Overview
- Permissions and ownership
- Shadow
- File list
- User management
  - Example adding a user
    - Changing user defaults
  - Example adding a system user
  - Change a user's login name or home directory
  - Other examples of user management

Users and groups are used on GNU/Linux for access control—that is, to control access to the system's files, directories, and peripherals. Linux offers relatively simple/coarse access control mechanisms by default. For more advanced options, see ACL, Capabilities and PAM#Configuration How-Tos.

A user is anyone who uses a computer. In this case, we are describing the names which represent those users. It may be Mary or Bill, and they may use the names Dragonlady or Pirate in place of their real name. All that matters is that the computer has a name for each account it creates, and it is this name by which a person gains access to use the computer. Some system services also run using restricted or privileged user accounts.

Managing users is done for the purpose of security by limiting access in certain specific ways. The superuser (root) has complete access to the operating system and its configuration; it is intended for administrative use only. Unprivileged users can use several programs for controlled privilege elevation.

Any individual may have more than one account as long as they use a different name for each account they create. Further, there are some reserved names which may not be used such as "root".

Users may be grouped together into a "group", and users may be added to an existing group to utilize the privileged access it grants.

From In UNIX Everything is a File:

From Extending UNIX File Abstraction for General-Purpose Networking:

Every file on a GNU/Linux system is owned by a user and a group. In addition, there are three types of access permissions: read, write, and execute. Different access permissions can be applied to a file's owning user, owning group, and others (those without ownership). One can determine a file's owners and permissions by viewing the long listing format of the ls command:

The first column displays the file's permissions (for example, the file initramfs-linux.img has permissions -rw-r--r--). The third and fourth columns display the file's owning user and group, respectively. In this example, all files are owned by the root user and the root group.

In this example, the sf_Shared directory is owned by the root user and the vboxsf group. It is also possible to determine a file's owners and permissions using the stat command:

Access permissions are displayed in three groups of characters, representing the permissions of the owning user, owning group, and others, respectively. For example, the characters -rw-r--r-- indicate that the file's owner has read and write permission, but not execute (rw-), whilst users belonging to the owning group and other users have only read permission (r-- and r--). Meanwhile, the characters drwxrwx--- indicate that the file's owner and users belonging to the owning group all have read, write, and execute permissions (rwx and rwx), whilst other users are denied access (---). The first character represents the file's type.

List files owned by a user or group with the find utility:

A file's owning user and group can be changed with the chown (change owner) command. A file's access permissions can be changed with the chmod (change mode) command.

See chown(1), chmod(1), and Linux file permissions for additional detail.

The user, group and password management tools on Arch Linux come from the shadow package, which is a dependency of the base meta package.

To list users currently logged on the system, the who command can be used. To list all existing user accounts including their properties stored in the user database, run passwd -Sa as root. See passwd(1) for the description of the output format.

To add a new user, use the useradd command:

If an initial login group is specified by name or number, it must refer to an already existing group. If not specified, the behaviour of useradd will depend on the USERGROUPS_ENAB variable contained in /etc/login.defs. The default behaviour (USERGROUPS_ENAB yes) is to create a group with the same name as the username.

When the login shell is intended to be non-functional, for example when the user account is created for a specific service, /usr/bin/nologin may be specified in place of a regular shell to politely refuse a login (see nologin(8)).

See useradd(8) for other supported options.

To add a new user named archie, creating its home directory and otherwise using all the defaults in terms of groups, directory names, shell used and various other parameters:

Although it is not required to protect the newly created user archie with a password, it is highly recommended to do so:

The above useradd command will also automatically create a group called archie and makes this the default group for the user archie. Making each user have their own group (with the group name same as the user name) is the preferred way to add users.

You could also make the default group something else using the -g option, but note that, in multi-user systems, using a single default group (e.g. users) for every user is not recommended. The reason is that typically, the method for facilitating shared write access for specific groups of users is setting user umask value to 002, which means that the default group will by default always have write access to any file you create. See also User Private Groups. If a user must be a member of a specific group specify that group as a supplementary group when creating the user.

In the recommended scenario, where the default group has the same name as the user name, all files are by default writeable only for the user who created them. To allow write access to a specific group, shared files/directories can be made writeable by default for everyone in this group and the owning group can be automatically fixed to the group which owns the parent directory by setting the setgid bit on this directory:

Otherwise the file creator's default group (usually the same as the user name) is used.

If a GID change is required temporarily you can also use the newgrp command to change the user's default GID to another GID at runtime. For example, after executing newgrp groupname files created by the user will be associated with the groupname GID, without requiring a re-login. To change back to the default GID, execute newgrp without a groupname.

The default values used for creating new accounts are set in /etc/default/useradd and can be displayed using useradd --defaults. For example, to change the default shell globally, set SHELL=/usr/bin/shell. A different shell can also be specified individually with the -s/--shell option. Use chsh -l to list valid login shells.

Files can also be specified to be added to newly created user home directories in /etc/skel. This is useful for minimalist window managers where config files need manual configuration to reach DE-familiar behavior. For example, to set up default shortcuts for all newly created users:

See also: Display manager#Run ~/.xinitrc as a session to add xinitrc as an option to all users on the display manager.

System users can be used to run processes/daemons under a different user, protecting (e.g. with chown) files and/or directories and more examples of computer hardening.

With the following command a system user without shell access and without a home directory is created (optionally append the -U parameter to create a group with the same name as the user, and add the user to this group):

If the system user requires a specific user and group ID, specify them with the -u/--uid and -g/--gid options when creating the user:

To change a user's home directory:

The -m option also automatically creates the new directory and moves the content there.

Make sure there is no trailing / on /my/old/home.

To change a user's login name:

Changing a username is safe and easy when done properly, just use the usermod command. If the user is associated to a group with the same name, you can rename this with the groupmod command.

Alternatively, the /etc/passwd file can be edited directly, see #User database for an introduction to its format.

Also keep in mind the following notes:

To enter user information for the GECOS comment (e.g. the full user name), type:

(this way chfn runs in interactive mode).

Alternatively the GECOS comment can be set more liberally with:

To mark a user's password as expired, requiring them to create a new password the first time they log in, type:

User accounts may be deleted with the userdel command:

The -r option specifies that the user's home directory and mail spool should also be deleted.

To change the user's login shell:

Local user information is stored in the plain-text /etc/passwd file: each of its lines represents a user account, and has seven fields delimited by colons.

Broken down, this means: user archie, whose password is in /etc/shadow, whose UID is 1001 and whose primary group is 1003. Archie is their full name and there is a comment associated to their account; their home directory is /home/archie and they are using Bash.

The pwck command can be used to verify the integrity of the user database. It can sort the user list by UID at the same time, which can be helpful for comparison:

Instead of running pwck/grpck manually, the systemd timer shadow.timer, which is part of, and is enabled by, installation of the shadow package, will start shadow.service daily. shadow.service will run pwck(8) and grpck(8) to verify the integrity of both password and group files.

If discrepancies are reported, group can be edited with the vigr(8) command and users with vipw(8). This provides an extra margin of protection in that these commands lock the databases for editing. Note that the default text editor is vi, but an alternative editor will be used if the EDITOR environment variable is set, then that editor will be used instead.

As packages adopt the change-sysusers-to-fully-locked-system-accounts, package created system users generated in the past will not inherit new package defaults for the increased security of locked/expired status. These users need to be modified manually for this change. user-analysis.sh does just that.

In addition, the script also identifies orphaned users (those created by a package no longer on the system) and can automatically delete them.

/etc/group is the file that defines the groups on the system (see group(5) for details). There is also its companion gshadow which is rarely used. Its details are at gshadow(5).

Display group membership with the groups command:

If user is omitted, the current user's group names are displayed.

The id command provides additional detail, such as the user's UID and associated GIDs:

To list all groups on the system:

Create new groups with the groupadd command:

Add users to a group with the gpasswd command (see FS#58262 regarding errors):

Alternatively, add a user to additional groups with usermod (replace additional_groups with a comma-separated list):

Modify an existing group with the groupmod command, e.g. to rename the old_group group to new_group:

To delete existing groups:

To remove users from a group:

The grpck command can be used to verify the integrity of the system's group files.

This section explains the purpose of the essential groups from the filesystem package. There are many other groups, which will be created with correct GID when the relevant package is installed. See the main page for the software for details.

Non-root workstation/desktop users often need to be added to some of following groups to allow access to hardware peripherals and facilitate system administration:

The following groups are used for system purposes, an assignment to users is only required for dedicated purposes:

Before arch migrated to systemd, users had to be manually added to these groups in order to be able to access the corresponding devices. This way has been deprecated in favour of udev marking the devices with a uaccess tag and logind assigning the permissions to users dynamically via ACLs according to which session is currently active. Note that the session must not be broken for this to work (see General troubleshooting#Session permissions to check it).

There are some notable exceptions which require adding a user to some of these groups: for example if you want to allow users to access the device even when they are not logged in. However, note that adding users to the groups can even cause some functionality to break (for example, the audio group will break fast user switching and allows applications to block software mixing).

Now solely for direct access to tapes if no custom udev rules is involved.[5][6][7].

Also required for manipulating some devices via udisks/udisksctl.

The following groups are currently not used for any purpose:

This article or section is a candidate for merging with #Shadow.

The factual accuracy of this article or section is disputed.

getent(1) can be used to read a particular record.

As warned in #User database, using specific utilities such as passwd and chfn, is a better way to change the databases. Nevertheless, there are times when editing them directly is looked after. For those times, vipw, vigr are provided. It is strongly recommended to use these tailored editors over using a general text editor as they lock the databases against concurrent editing. They also help prevent invalid entries and/or syntax errors. Note that Arch Linux prefers usage of specific tools, such as chage, for modifying the shadow database over using vipw -s and vigr -s from util-linux. See also FS#31414.

lslogins(1) and lastlog2(1) are some of the tools for viewing utmp related files.

**Examples:**

Example 1 (unknown):
```unknown
cat /dev/audio > myfile
```

Example 2 (unknown):
```unknown
cat myfile > /dev/audio
```

Example 3 (unknown):
```unknown
$ ls -l /boot/
```

Example 4 (unknown):
```unknown
total 13740
drwxr-xr-x 2 root root    4096 Jan 12 00:33 grub
-rw-r--r-- 1 root root 8570335 Jan 12 00:33 initramfs-linux-fallback.img
-rw-r--r-- 1 root root 1821573 Jan 12 00:31 initramfs-linux.img
-rw-r--r-- 1 root root 1457315 Jan  8 08:19 System.map26
-rw-r--r-- 1 root root 2209920 Jan  8 08:19 vmlinuz-linux
```

---

## systemd

**URL:** https://wiki.archlinux.org/title/Systemd

**Contents:**
- Basic systemctl usage
  - Using units
  - Power management
    - Soft reboot
- Writing unit files
  - Handling dependencies
  - Service types
  - Editing provided units
    - Replacement unit files
    - Drop-in files

From the project web page:

Historically, what systemd calls "service" was named daemon: any program that runs as a "background" process (without a terminal or user interface), commonly waiting for events to occur and offering services. A good example is a web server that waits for a request to deliver a page, or an ssh server waiting for someone trying to log in. While these are full featured applications, there are daemons whose work is not that visible. Daemons are for tasks like writing messages into a log file (e.g. syslog, metalog) or keeping your system time accurate (e.g. ntpd). For more information see daemon(7).

The main command used to introspect and control systemd is systemctl. Some of its uses are examining the system state and managing the system and services. See systemctl(1) for more details.

Units commonly include, but are not limited to, services (.service), mount points (.mount), devices (.device) and sockets (.socket).

When using systemctl, you generally have to specify the complete name of the unit file, including its suffix, for example sshd.socket. There are however a few short forms when specifying the unit in the following systemctl commands:

See systemd.unit(5) for details.

The commands in the below table operate on system units since --system is the implied default for systemctl. To instead operate on user units (for the calling user), use systemctl --user without root privileges. See also systemd/User#Basic setup to enable/disable user units for all users.

polkit is necessary for power management as an unprivileged user. If you are in a local systemd-logind user session and no other session is active, the following commands will work without root privileges. If not (for example, because another user is logged into a tty), systemd will automatically ask you for the root password.

Soft reboot is a special kind of a userspace-only reboot operation that does not involve the kernel. It is implemented by systemd-soft-reboot.service(8) and can be invoked through systemctl soft-reboot. As with kexec, it skips firmware re-initialization, but additionally the system does not go through kernel initialization and initramfs, and unlocked dm-crypt devices remain attached.

When /run/nextroot/ contains a valid root file system hierarchy (e.g. is the root mount of another distribution or another snapshot), soft-reboot would switch the system root into it, allowing for switching to another installation without losing states managed by kernel, e.g. networking.

The syntax of systemd's unit files (systemd.unit(5)) is inspired by XDG Desktop Entry Specification .desktop files, which are in turn inspired by Microsoft Windows .ini files. Unit files are loaded from multiple locations (to see the full list, run systemctl show --property=UnitPath), but the main ones are (listed from lowest to highest precedence):

Look at the units installed by your packages for examples, as well as systemd.service(5) § EXAMPLES.

systemd-analyze(1) can help verifying the work. See the systemd-analyze verify FILE... section of that page.

With systemd, dependencies can be resolved by designing the unit files correctly. The most typical case is when unit A requires unit B to be running before A is started. In that case add Requires=B and After=B to the [Unit] section of A. If the dependency is optional, add Wants=B and After=B instead. Note that Wants= and Requires= do not imply After=, meaning that if After= is not specified, the two units will be started in parallel.

Dependencies are typically placed on services and not on #Targets. For example, network.target is pulled in by whatever service configures your network interfaces, therefore ordering your custom unit after it is sufficient since network.target is started anyway.

There are several different start-up types to consider when writing a custom service file. This is set with the Type= parameter in the [Service] section:

See the systemd.service(5) § OPTIONS man page for a more detailed explanation of the Type values.

This article or section needs expansion.

To avoid conflicts with pacman, unit files provided by packages should not be directly edited. There are two safe ways to modify a unit without touching the original file: create a new unit file which overrides the original unit or create drop-in snippets which are applied on top of the original unit. For both methods, you must reload the unit afterwards to apply your changes. This can be done either by editing the unit with systemctl edit (which reloads the unit automatically) or by reloading all units with:

To replace the unit file /usr/lib/systemd/system/unit, create the file /etc/systemd/system/unit and reenable the unit to update the symlinks.

This opens /etc/systemd/system/unit in your editor (copying the installed version if it does not exist yet) and automatically reloads it when you finish editing.

To create drop-in files for the unit file /usr/lib/systemd/system/unit, create the directory /etc/systemd/system/unit.d/ and place .conf files there to override or add new options. systemd will parse and apply these files on top of the original unit.

The easiest way to do this is to run:

This opens the file /etc/systemd/system/unit.d/drop_in_name.conf in your text editor (creating it if necessary) and automatically reloads the unit when you are done editing. Omitting --drop-in= option will result in systemd using the default file name override.conf .

To revert any changes to a unit made using systemctl edit do:

For example, if you simply want to add an additional dependency to a unit, you may create the following file:

As another example, in order to replace the ExecStart directive, create the following file:

Note how ExecStart must be cleared before being re-assigned [2]. The same holds for every item that can be specified multiple times, e.g. OnCalendar for timers.

One more example to automatically restart a service:

For services that send logs directly to journald or syslog, you can control their verbosity by setting a numeric value between 0 and 6 for the LogLevelMax= parameter in the [Service] section using the methods described above. For example:

The standard log levels are identical to those used to filter the journal. Setting a lower number excludes all higher and less important log messages from your journal.

If a service is echoing stdout and/or stderr output, by default this will end up in the journal as well. This behavior can be suppressed by setting StandardOutput=null and/or StandardError=null in the [Service] section. Other values than null can be used to further tweak this behavior. See systemd.exec(5) § LOGGING_AND_STANDARD_INPUT/OUTPUT.

systemd uses targets to group units together via dependencies and as standardized synchronization points. They serve a similar purpose as runlevels but act a little differently. Each target is named instead of numbered and is intended to serve a specific purpose with the possibility of having multiple ones active at the same time. Some targets are implemented by inheriting all of the services of another target and adding additional services to it. There are systemd targets that mimic the common SystemVinit runlevels.

The following should be used under systemd instead of running runlevel:

The runlevels that held a defined meaning under sysvinit (i.e., 0, 1, 3, 5, and 6); have a 1:1 mapping with a specific systemd target. Unfortunately, there is no good way to do the same for the user-defined runlevels like 2 and 4. If you make use of those it is suggested that you make a new named systemd target as /etc/systemd/system/your target that takes one of the existing runlevels as a base (you can look at /usr/lib/systemd/system/graphical.target as an example), make a directory /etc/systemd/system/your target.wants, and then symlink the additional services from /usr/lib/systemd/system/ that you wish to enable.

In systemd, targets are exposed via target units. You can change them like this:

This will only change the current target, and has no effect on the next boot. This is equivalent to commands such as telinit 3 or telinit 5 in Sysvinit.

The standard target is default.target, which is a symlink to graphical.target. This roughly corresponds to the old runlevel 5.

To verify the current target with systemctl:

To change the default target to boot into, change the default.target symlink. With systemctl:

Alternatively, append one of the following kernel parameters to your boot loader:

systemd chooses the default.target according to the following order:

Some (not exhaustive) components of systemd are:

systemd is in charge of mounting the partitions and filesystems specified in /etc/fstab. The systemd-fstab-generator(8) translates all the entries in /etc/fstab into systemd units; this is performed at boot time and whenever the configuration of the system manager is reloaded.

systemd extends the usual fstab capabilities and offers additional mount options. These affect the dependencies of the mount unit. They can, for example, ensure that a mount is performed only once the network is up or only once another partition is mounted. The full list of specific systemd mount options, typically prefixed with x-systemd., is detailed in systemd.mount(5) § FSTAB.

An example of these mount options is automounting, which means mounting only when the resource is required rather than automatically at boot time. This is provided in fstab#Automount with systemd.

On UEFI-booted systems, GPT partitions such as root, home, swap, etc. can be mounted automatically following the Discoverable Partitions Specification. These partitions can thus be omitted from fstab, and if the root partition is automounted, then root= can be omitted from the kernel command line. See systemd-gpt-auto-generator(8).

The prerequisites are:

udev will create symlinks to the discovered partitions which can be used to reference the partitions and volumes in configuration files.

For /var automounting to work, the PARTUUID must match the SHA256 HMAC hash of the partition type UUID keyed by the machine ID. The required PARTUUID can be obtained using:

The primary role of systemd-sysvcompat (required by base) is to provide the traditional linux init binary. For systemd-controlled systems, init is just a symbolic link to its systemd executable.

In addition, it provides four convenience shortcuts that SysVinit users might be used to. The convenience shortcuts are halt(8), poweroff(8), reboot(8) and shutdown(8). Each one of those four commands is a symbolic link to systemctl, and is governed by systemd behavior. Therefore, the discussion at #Power management applies.

systemd-based systems can give up those System V compatibility methods by using the init= boot parameter (see, for example, /bin/init is in systemd-sysvcompat ?) and systemd native systemctl command arguments.

systemd-tmpfiles creates, deletes and cleans up volatile and temporary files and directories. It reads configuration files in /etc/tmpfiles.d/ and /usr/lib/tmpfiles.d/ to discover which actions to perform. Configuration files in the former directory take precedence over those in the latter directory.

Configuration files are usually provided together with service files, and they are named in the style of /usr/lib/tmpfiles.d/program.conf. For example, the Samba daemon expects the directory /run/samba to exist and to have the correct permissions. Therefore, the samba package ships with this configuration:

Configuration files may also be used to write values into certain files on boot. For example, if you used /etc/rc.local to disable wakeup from USB devices with echo USBE > /proc/acpi/wakeup, you may use the following tmpfile instead:

It is possible to write multiple lines to the same file, either with \n in the argument or using the w+ type on multiple lines (including the first one) for appending:

See the systemd-tmpfiles(8) and tmpfiles.d(5) man pages for details.

The factual accuracy of this article or section is disputed.

Configuration files provided by packages should not be directly edited to avoid conflicts with pacman updates. For this, many (but not all) systemd packages provide a way to modify the configuration, but without touching the original file by creation of drop-in snippets. Check the package manual to see if drop-in configuration files are supported.

To create a drop-in configuration file for the unit file /etc/systemd/unit.conf, create the directory /etc/systemd/unit.conf.d/ and place .conf files there to override or add new options. systemd will parse and apply these files on top of the original unit.

Check the overall configuration:

The applied drop-in snippets file(s) and content will be listed at the end. Restart the service for the changes to take effect.

Some packages provide a .socket unit. For example, cups provides a cups.socket unit[3]. If cups.socket is enabled (and cups.service is left disabled), systemd will not start CUPS immediately; it will just listen to the appropriate sockets. Then, whenever a program attempts to connect to one of these CUPS sockets, systemd will start cups.service and transparently hand over control of these ports to the CUPS process.

To delay a service until after the network is up, include the following dependencies in the .service file:

The network wait service of the network manager in use must also be enabled so that network-online.target properly reflects the network status.

For more detailed explanations, see the discussion in the Network configuration synchronization points.

If a service needs to perform DNS queries, it should additionally be ordered after nss-lookup.target:

See systemd.special(7) § Special Passive System Units.

For nss-lookup.target to have any effect it needs a service that pulls it in via Wants=nss-lookup.target and orders itself before it with Before=nss-lookup.target. Typically this is done by local DNS resolvers.

Check which active service, if any, is pulling in nss-lookup.target with:

This article or section needs expansion.

By default, Arch Linux does not make use of systemd presets, and does not enable most services on installation.

If you wish for systemd presets to be honored when packages are installed, you will need to create a a pacman hook in order to run systemctl preset or systemctl preset-all whenever a new systemd unit is installed. Something like

Arch Linux ships with /usr/lib/systemd/system-preset/99-default.preset containing disable *. This causes systemctl preset to disable all units by default, such that when a new package is installed, the user must manually enable the unit.

To enable units by default instead, simply create a symlink from /etc/systemd/system-preset/99-default.preset to /dev/null or replace with a file containing enable * in order to override the configuration file. This will cause systemctl preset to enable all units that get installed—regardless of unit type—unless specified in another file in one systemctl preset's configuration directories. User units are not affected. It is also possible to configure higher priority files with more specific rules on what should be enabled. See systemd.preset(5) for more information.

See systemd/Sandboxing.

In order to notify about service failures, an OnFailure= directive needs to be added to the according service file, for example by using a drop-in configuration file. Adding this directive to every service unit can be achieved with a top-level drop-in configuration file. For details about top-level drop-ins, see systemd.unit(5).

Create a top-level drop-in for services:

This adds OnFailure=failure-notification@%n.service to every service file. If some_service_unit fails, failure-notification@some_service_unit.service will be started to handle the notification delivery (or whatever task it is configured to perform).

Create the failure-notification@.service template unit:

You can create the failure-notification.sh script and define what to do or how to notify. Examples include sending e-mail, showing desktop notifications, using gotify, XMPP, etc. The %i will be the name of the failed service unit and will be passed as an argument to the script.

In order to prevent a recursion for starting instances of failure-notification@.service again and again if the start fails, create an empty drop-in configuration file with the same name as the top-level drop-in (the empty service-level drop-in configuration file takes precedence over the top-level drop-in and overrides the latter one):

You can set up systemd to send an e-mail when a unit fails. Cron sends mail to MAILTO if the job outputs to stdout or stderr, but many jobs are setup to only output on error. First you need two files: an executable for sending the mail and a .service for starting the executable. For this example, the executable is just a shell script using sendmail, which is in packages that provide smtp-forwarder.

Whatever executable you use, it should probably take at least two arguments as this shell script does: the address to send to and the unit file to get the status of. The .service we create will pass these arguments:

Where user is the user being emailed and address is that user's email address. Although the recipient is hard-coded, the unit file to report on is passed as an instance parameter, so this one service can send email for many other units. At this point you can start status_email_user@dbus.service to verify that you can receive the emails.

Then simply edit the service you want emails for and add OnFailure=status_email_user@%n.service to the [Unit] section. %n passes the unit's name to the template.

See udisks#Automatically turn off an external HDD at shutdown.

To find the systemd services which failed to start:

To find out why they failed, examine their log output. See systemd/Journal#Filtering output for details.

systemd has several options for diagnosing problems with the boot process. See boot debugging for more general instructions and options to capture boot messages before systemd takes over the boot process. Also see systemd debugging documentation.

If some systemd service misbehaves or you want to get more information about what is happening, set the SYSTEMD_LOG_LEVEL environment variable to debug. For example, to run the systemd-networkd daemon in debug mode:

Add a drop-in file for the service adding the two lines:

Or equivalently, set the environment variable manually:

then restart systemd-networkd and watch the journal for the service with the -f/--follow option.

If the shutdown process takes a very long time (or seems to freeze), most likely a service not exiting is to blame. systemd waits some time for each service to exit before trying to kill it. To find out whether you are affected, see Shutdown completes eventually in the systemd documentation.

A common problem is a stalled shutdown or suspend process. To verify whether that is the case, you could run either of these commands and check the outputs

The solution to this would be to cancel these jobs by running

and then trying shutdown or reboot again.

If running journalctl -u foounit as root does not show any output for a short lived service, look at the PID instead. For example, if systemd-modules-load.service fails, and systemctl status systemd-modules-load shows that it ran as PID 123, then you might be able to see output in the journal for that PID, i.e. by running journalctl -b _PID=123 as root. Metadata fields for the journal such as _SYSTEMD_UNIT and _COMM are collected asynchronously and rely on the /proc directory for the process existing. Fixing this requires fixing the kernel to provide this data via a socket connection, similar to SCM_CREDENTIALS. In short, it is a bug. Keep in mind that immediately failed services might not print anything to the journal as per design of systemd.

The factual accuracy of this article or section is disputed.

After using systemd-analyze a number of users have noticed that their boot time has increased significantly in comparison with what it used to be. After using systemd-analyze blame NetworkManager is being reported as taking an unusually large amount of time to start.

The problem for some users has been due to /var/log/journal becoming too large. This may have other impacts on performance, such as for systemctl status or journalctl. As such the solution is to remove every file within the folder (ideally making a backup of it somewhere, at least temporarily) and then setting a journal file size limit as described in systemd/Journal#Journal size limit.

Starting with systemd 219, /usr/lib/tmpfiles.d/systemd.conf specifies ACL attributes for directories under /var/log/journal and, therefore, requires ACL support to be enabled for the filesystem the journal resides on.

See Access Control Lists#Enable ACL for instructions on how to enable ACL on the filesystem that houses /var/log/journal.

You may want to disable emergency mode on a remote machine, for example, a virtual machine hosted at Azure or Google Cloud. It is because if emergency mode is triggered, the machine will be blocked from connecting to network.

To disable it, mask emergency.service and emergency.target.

You may be trying to start or enable a user unit as a system unit. systemd.unit(5) indicates, which units reside where. By default systemctl operates on system services.

See systemd/User for more details.

Some bootloaders only set the LoaderDevicePartUUID variable when it is empty. Consequently, even if the UUID of the EFI partition changes, the bootloader will not update LoaderDevicePartUUID. By deleting the EFI variable with the commands below, the bootloader will then repopulate it with the new UUID.

**Examples:**

Example 1 (unknown):
```unknown
-H user@host
```

Example 2 (unknown):
```unknown
sshd.socket
```

Example 3 (unknown):
```unknown
netctl.service
```

Example 4 (unknown):
```unknown
dev-sda2.device
```

---

## Partitioning

**URL:** https://wiki.archlinux.org/title/Master_Boot_Record

**Contents:**
- Partition table
  - Master Boot Record
    - Master Boot Record (bootstrap code)
    - Master Boot Record (partition table)
  - GUID Partition Table
  - Choosing between GPT and MBR
  - Partitionless disk
    - Btrfs partitioning
- Partition scheme
  - Single root partition

An entire disk may be allocated to a single partition, or multiple ones for cases such as dual-booting, maintaining a swap partition, or to logically separate data such as audio and video files. The partitioning scheme is stored in a partition table such as Master Boot Record (MBR) or GUID Partition Table (GPT).

Partition tables are created and modified using one of many partitioning tools. The tools available for Arch Linux are listed in the #Partitioning tools section.

Partitions usually contain a file system directly which is accomplished by creating a file system on (a.k.a. formatting) the partition. Alternatively, partitions can contain LVM, block device encryption or RAID, which ultimately provide device files on which a file system can be placed (or the devices can be stacked further).

Any block device (e.g. disk, partition, LUKS device, LVM logical volume or RAID array) that directly contains a mountable file system is called a volume.

There are two main types of partition table available. These are described below in the #Master Boot Record (MBR) and #GUID Partition Table (GPT) sections along with a discussion on how to choose between the two. A third, less common alternative is using a partitionless disk, which is also discussed.

Use a partitioning tool to view the partition table of a block device.

The Master Boot Record (MBR) is the first 512 bytes of a storage device. It contains an operating system boot loader and the storage device's partition table. It plays an important role in the boot process under BIOS systems. See Wikipedia:Master boot record#Disk partitioning for the MBR structure.

The first 440 bytes of MBR are the bootstrap code area. On BIOS systems it usually contains the first stage of the boot loader. The bootstrap code can be backed up, restored from backup or erased using dd.

In the MBR partition table (also known as DOS or MS-DOS partition table) there are 3 types of partitions:

Primary partitions can be bootable and are limited to four partitions per disk or RAID volume. If the MBR partition table requires more than four partitions, then one of the primary partitions needs to be replaced by an extended partition containing logical partitions within it.

Extended partitions can be thought of as containers for logical partitions. A hard disk can contain no more than one extended partition. The extended partition is also counted as a primary partition so if the disk has an extended partition, only three additional primary partitions are possible (i.e. three primary partitions and one extended partition). The number of logical partitions residing in an extended partition is unlimited. A system that dual boots with Windows will require for Windows to reside in a primary partition.

The customary numbering scheme is to create primary partitions sda1 through sda3 followed by an extended partition sda4. The logical partitions on sda4 are numbered sda5, sda6, etc.

GUID Partition Table (GPT) is a partitioning scheme that is part of the Unified Extensible Firmware Interface specification; it uses globally unique identifiers (GUIDs), or UUIDs in the Linux world, to define partitions and partition types. It is designed to succeed the Master Boot Record partitioning scheme method.

At the start of a GUID Partition Table disk there is a protective Master Boot Record (PMBR) to protect against GPT-unaware software. This protective MBR just like an ordinary MBR has a bootstrap code area which can be used for BIOS/GPT booting with boot loaders that support it.

GUID Partition Table (GPT) is an alternative, contemporary, partitioning style; it is intended to replace the old Master Boot Record (MBR) system. GPT has several advantages over MBR which has quirks dating back to MS-DOS times. With the recent developments to the formatting tools, it is equally easy to get good dependability and performance for GPT or MBR.

Some points to consider when choosing:

Some advantages of GPT over MBR are:

The section on #Partitioning tools contains a table indicating which tools are available for creating and modifying GPT and MBR tables.

This article or section needs expansion.

Partitionless disk a.k.a. superfloppy refers to a storage device without a partition table, having one file system occupying the whole storage device. The boot sector present on a partitionless device is called a volume boot record (VBR).

Btrfs can occupy an entire data storage device and replace the MBR or GPT partitioning schemes. See the Btrfs#Partitionless Btrfs disk instructions for details.

This article or section needs expansion.

There are no strict rules for partitioning a hard drive, although one may follow the general guidance given below. A disk partitioning scheme is determined by various issues such as desired flexibility, speed, security, as well as the limitations imposed by available disk space. It is essentially personal preference. If you would like to dual boot Arch Linux and a Windows operating system please see Dual boot with Windows.

This scheme is the simplest, most flexible and should be enough for most use cases given the increase in storage size of consumer grade devices. A swap file can be created and easily resized as needed. It usually makes sense to start by considering a single / partition and then separate out others based on specific use cases like RAID, encryption, a shared media partition, etc… See #Discrete partitions for a description of some common to uncommon dedicated partitions.

The suggested minimum size is 23–32 GiB for a single root partition. More space may be needed for user files and when using a swap file. A bare minimal installation requires about 2 GiB. As examples, a simple server can fit under 4 GiB while a full KDE Plasma installation uses 10 GiB. Both examples require frequent purges of the package cache.

A GPT partition should have the "Linux root (x86-64)" type GUID 4F68BCE3-E8CD-4DB1-96E7-FBCAF984B709 (8304 type for gdisk). An MBR partition should have the default "Linux" type ID 83.

Separating out a path as a partition allows for the choice of a different filesystem and mount options. In some cases like a media partition, they can also be shared between operating systems.

Below are some example layouts that can be used when partitioning, and the following subsections detail a few of the directories which can be placed on their own separate partition and then mounted at mount points under /. See file-hierarchy(7) for a full description of the contents of these directories.

The root directory is the top of the hierarchy, the point where the primary filesystem is mounted and from which all other filesystems stem. All files and directories appear under the root directory /, even if they are stored on different physical devices. The contents of the root filesystem must be adequate to boot, restore, recover, and/or repair the system. Therefore, certain directories under / are not candidates for separate partitions.

The / partition or root partition is necessary and it is the most important. The other partitions can be replaced by it.

/ traditionally contains the /usr directory, which can grow significantly depending upon how much software is installed. 15–20 GiB should be sufficient for most users with modern hard disks. If you plan to store a swap file here and do not plan on using a separate /var, you might need a larger partition size (i.e. adding the size of your RAM to be able to hibernate and an additional 8–12 GiB for /var).

A GPT partition should have the "Linux root (x86-64)" type GUID 4F68BCE3-E8CD-4DB1-96E7-FBCAF984B709 (8304 for gdisk). An MBR partition should have the default "Linux" type ID 83.

The /boot directory contains the vmlinuz and initramfs images as well as the boot loader configuration file and boot loader stages. It also stores data that is used before the kernel begins executing user-space programs. /boot is not required for normal system operation, but only during boot and kernel upgrades (when regenerating the initial ramdisk).

See Arch boot process#Boot loader for more information on boot loader requirements and capabilities.

When using an EFI system partition as /boot, the requirements are as described in the EFI system partition article—the correct partition type must be set.

In other cases, it is recommended to set the partition type to Extended Boot Loader (XBOOTLDR) Partition which is GPT partition type GUID BC13C2FF-59E6-4262-A352-B275FD6F7172 (ea00 type for gdisk, xbootldr type for fdisk) or MBR partition type ID ea.

In both cases the suggested size for the partition is 1 GiB, which should give enough space to house multiple kernels. If still in doubt, 4 GiB ought to be enough for anybody.

The /home directory contains user-specific configuration files, caches, application data and media files.

Separating out /home allows / to be re-partitioned separately, but note that you can still reinstall Arch with /home untouched even if it is not separate—the other top-level directories just need to be removed, and then pacstrap can be run.

You should not share home directories between users on different distributions, because they use incompatible software versions and patches. Instead, consider sharing a media partition or at least using different home directories on the same /home partition. The size of this partition varies.

A GPT partition should have the "Linux home" type GUID 933AC7E1-2EB4-4F13-B844-0E14E2AEF915 (8302 type for gdisk, home type for fdisk). An MBR partition should have the default "Linux" type ID 83.

A swap is a file or partition that provides disk space used as virtual memory. Swap files and swap partitions are equally performant, but swap files are much easier to resize as needed. A swap partition can potentially be shared between operating systems, but not if hibernation is used.

Since computers have gained memory capacities superior to a gibibit, the previous "twice the amount of physical RAM" rule has become outdated. A sane default size is 4 GiB.

To use hibernation (a.k.a. suspend to disk) it is advised to create the swap partition at the size of RAM. Although the kernel will try to compress the suspend-to-disk image to fit the swap space there is no guarantee it will succeed if the used swap space is significantly smaller than RAM. See Power management/Suspend and hibernate#Hibernation for more information.

A GPT partition should have the "Linux swap" type with GUID 0657FD6D-A4AB-43C4-84E5-0933C84B4F4F (8200 type for gdisk, swap type for fdisk). An MBR partition should have the "Linux swap" type ID 82.

One can consider mounting a "data" partition to cover various files to be shared by all users. Using the /home partition for this purpose is fine as well. The size of this partition varies.

A GPT partition should have the default "Linux filesystem" type GUID 0FC63DAF-8483-4772-8E79-3D69D8477DE4. An MBR partition should have the default "Linux" type ID 83.

The /var directory stores variable data such as spool directories and files, administrative and logging data, pacman's cache, etc. It is used, for example, for caching and logging, and hence frequently read or written. Keeping it in a separate partition avoids running out of disk space due to flunky logs, etc.

It exists to make it possible to mount /usr as read-only. Everything that historically went into /usr that is written to during system operation (as opposed to installation and software maintenance) must reside under /var.

/var will contain, among other data, the pacman cache. Retaining these packages is helpful in case a package upgrade causes instability, requiring a downgrade to an older, archived package. The pacman cache will grow as the system is expanded and updated, but it can be safely cleared if space becomes an issue.

8–12 GiB on a desktop system should be sufficient for /var, depending on how much software will be installed. For users of NVIDIA, Wayland and GDM, consider adding to this partition size as to have enough free space to fit your whole video memory.

A GPT partition should have the "Linux variable data" a.k.a. "Linux /var" type GUID 4D21B016-B534-45C2-A9FB-5C16E091FD2D (8310 type for gdisk). An MBR partition should have the default "Linux" type ID 83.

This article or section needs expansion.

The following examples use /dev/sda as the example disk with /dev/sda1 as the first partition. The block device naming scheme will differ if you are partitioning a NVMe disk (e.g. /dev/nvme0n1 with partitions starting from /dev/nvme0n1p1) or an SD card or eMMC disk (e.g. /dev/mmcblk0 with partitions starting from /dev/mmcblk0p1). See Device file#Block device names for more information.

The following programs are used to create and/or manipulate device partition tables and partitions. See the linked articles for the exact commands to be used.

This table will help you to choose utility for your needs:

The rule of thumb is to align a partition's start and size to mebibytes. See Advanced Format#Partition alignment.

The CONFIG_EFI_PARTITION option in the kernel config enables GPT support in the kernel (despite the name, EFI PARTITION which looks close to EFI system partition). This option must be built in the kernel and not compiled as a loadable module. This option is required even if GPT disks are used only for data storage and not for booting. This option is enabled by default in all Arch's officially supported kernels. In case of a custom kernel, enable this option by doing CONFIG_EFI_PARTITION=y.

Some old BIOSes (from before year 2010) attempt to parse the boot sector and refuse to boot it if it does not contain a bootable MBR partition. This is a problem if one wants to use GPT on this disk, because, from the BIOS viewpoint, it contains only one, non-bootable, MBR partition of type ee (i.e., the protective MBR partition). One can mark the protective MBR entry as bootable using fdisk -t mbr /dev/sda, and it will work on some BIOSes. However, the UEFI specification prohibits the protective MBR partition entry from being bootable, and UEFI-based boards do care about this, even in the legacy boot mode. So, this matters if one wants to create a GPT-based USB flash drive that is supposed to boot both on modern UEFI-based boards and also on old BIOSes that insist on finding a bootable MBR partition. It is not possible to solve this problem using traditional tools such as fdisk or gdisk, but it is possible to create a fake MBR partition entry suitable for both kinds of BIOSes manually as a sequence of bytes.

The command below will overwrite the second MBR partition slot and add a bootable partition there of type 0 (i.e. unused), covering only the first sector of the device. It will not interfere with the GPT or with the first MBR partition entry which normally contains a protective MBR partition.

The end result will look like this:

If a SATA or NVMe drive is visible in firmware setup, but not to Linux (e.g. fdisk -l does not list it), it is possible that the controller is in firmware RAID mode.

For NVMe, the journal should show something like:

The solution is to enter firmware setup and disable NVMe RAID mode and change the SATA controller operation mode from RAID to AHCI. Mind that the setting may have a different name (e.g. "Intel Rapid Storage Technology", "Intel RST", "Intel VMD controller" or "VMD") and it could also be per-controller or per-port.

**Examples:**

Example 1 (unknown):
```unknown
parted /dev/sdX print
```

Example 2 (unknown):
```unknown
fdisk -l /dev/sdX
```

Example 3 (unknown):
```unknown
/dev/nvme0n1
```

Example 4 (unknown):
```unknown
/dev/mmcblk0
```

---

## Unified kernel image

**URL:** https://wiki.archlinux.org/title/Unified_kernel_image

**Contents:**
- Preparing a unified kernel image
  - mkinitcpio
    - Kernel command line
    - .preset file
    - pacman hook
    - Building the UKIs
  - kernel-install
  - dracut
  - ukify
  - Manually

A unified kernel image (UKI) is a single executable which can be booted directly from UEFI firmware, or automatically sourced by boot loaders with little or no configuration. It is the combination of a UEFI boot stub program like systemd-stub(7), a Linux kernel image, an initramfs, and further resources in a single UEFI PE file.

This file, and therefore all these elements can then easily be signed for use with Secure Boot.

There are several ways to generate a UKI image and install it to the proper place (the esp/Linux directory). Currently several tools compete for doing this functionality, so choose one of the following based on your needs and your likings.

mkinitcpio will assemble the UKI itself unless systemd-ukify is installed. In which case, UKI creation will be offloaded to ukify unless this is explicitly disabled with the --no-ukify option.

mkinitcpio supports reading kernel parameters from command line files in the /etc/cmdline.d directory. Mkinitcpio will concatenate the contents of all files with a .conf extension in this directory and use them to generate the kernel command line. Any lines in the command line file that start with a # character are treated as comments and ignored by mkinitcpio. Take care to remove entries pointing to microcode and initramfs.

This article or section is being considered for removal.

Alternatively, /etc/kernel/cmdline can be used to configure the kernel command line.

Next, modify /etc/mkinitcpio.d/linux.preset, or the preset that you are using, as follows, with the appropriate mount point of the EFI system partition:

Here is a working example linux.preset for the linux kernel and the Arch splash screen.

Updates to systemd-stub (part of systemd), microcode (both intel-ucode and amd-ucode), and linux kernel will automatically trigger a UKI rebuild. But you may want to review other pacman hooks in the /etc/pacman.d/hooks/ directory, such as the one for the NVIDIA driver.

Finally, make sure that the directory for the UKIs exists and regenerate the initramfs. For example, for the linux preset:

Optionally, remove any leftover initramfs-*.img from /boot or /efi.

Kernel-install is another alternative that is part of systemd and requires systemd-ukify to build unified kernel images. Make sure kernel-install is properly set up.

To generate UKIs, install systemd-ukify and set the kernel-install layout to uki:

Any configuration for #ukify must be done in /etc/kernel/uki.conf in order to be used by kernel-install, e.g.

Alternatively, for mkinitcpio to generate the UKI, set it as the default uki_generator:

In that case, systemd-ukify is not necessary. You can also set a different initrd_generator, see kernel-install(8).

Reinstall the kernel packages that you use in order for the change to take effect.

See dracut#Unified kernel image and dracut#Generate a new initramfs on kernel upgrade.

Install the systemd-ukify package. Since ukify cannot generate an initramfs on its own, if required, it must be generated using, e.g., dracut, mkinitcpio or booster.

A minimal working example can look something like this:

For further information, see ukify(1).

Put the kernel command line you want to use in a file, and create the bundle file using objcopy(1).

For microcode, first concatenate the microcode file and your initramfs, as follows:

When building the unified kernel image, pass in /tmp/combined_initramfs.img as the initramfs. This file can be removed afterwards.

A few things to note:

After creating the image, copy it to the EFI system partition:

sbctl provides a kernel-install script, a mkinitcpio post-hook, and pacman hooks to sign updated binaries.

By using a mkinitcpio post hook, the generated unified kernel images can be signed for Secure Boot. Create the following file and make it executable:

Replace /path/to/db.key and /path/to/db.crt with the paths to the key pair you want to use for signing the image.

To automatically sign UKIs using systemd-sbsign(1), set the following in your configuration file:

Limine does not automatically detect unified kernel images (UKIs). However, limine.conf can be manually configured to load them.

Example 1: Booting a UKI from the default EFI system partition

If a UKI file is stored in esp/EFI/Linux/, add the following configuration to limine.conf:

Example 2: Booting a UKI from another partition

If a UKI file is located on a different FAT32 partition, use guid(PARTUUID) with the PARTUUID instead:

For more details about supported paths and configuration options, see the Limine Paths documentation.

systemd-boot searches in esp/EFI/Linux/ for unified kernel images, and there is no further configuration needed. See sd-boot(7) § FILES

rEFInd will autodetect unified kernel images on your EFI system partition, and is capable of loading them. They can also be manually specified in refind.conf, by default located at:

Keep in mind that no kernel parameters from esp/EFI/refind_linux.conf will be passed when booting this way. If the UKI was generated without a .cmdline section, specify the kernel parameters in the menu entry with an options line.

GRUB can chainload UKIs as described in GRUB#Chainloading a unified kernel image.

efibootmgr can be used to create a UEFI boot entry for the .efi file:

See efibootmgr(8) for an explanation of the options.

**Examples:**

Example 1 (unknown):
```unknown
esp/EFI/BOOT/BOOTx64.EFI
```

Example 2 (unknown):
```unknown
BOOTIA32.EFI
```

Example 3 (unknown):
```unknown
/etc/cmdline.d
```

Example 4 (unknown):
```unknown
/etc/cmdline.d/root.conf
```

---

## Xorg

**URL:** https://wiki.archlinux.org/title/Composite_manager

**Contents:**
- Installation
  - Driver installation
    - AMD
- Running
- Configuration
  - Using .conf files
  - Using xorg.conf
- Input devices
  - Input identification
  - Mouse acceleration

X.Org Server — commonly referred to as simply X — is the X.Org Foundation implementation of the X Window System (X11) display server, and it is the most popular display server among Linux users. Its ubiquity has led to making it an ever-present requisite for GUI applications, resulting in massive adoption from most distributions.

For the alternative and successor, see Wayland.

Xorg can be installed with the xorg-server package.

Additionally, some packages from the xorg-apps group are necessary for certain configuration tasks. They are pointed out in the relevant sections.

Finally, an xorg group is also available, which includes Xorg server packages, packages from the xorg-apps group and fonts.

This article or section is a candidate for moving to Graphics processing unit.

The Linux kernel includes open-source video drivers and support for hardware accelerated framebuffers. However, userland support is required for OpenGL, Vulkan and 2D acceleration in X11.

First, identify the graphics card (the Subsystem output shows the specific model):

Then, install an appropriate driver. You can search the package database for a complete list of open-source Device Dependent X (DDX) drivers:

However, hardware-specific DDX is considered legacy nowadays. There is a generic modesetting(4) DDX driver in xorg-server, which uses kernel mode setting and works well on modern hardware. The modesetting DDX driver uses Glamor[1] for 2D acceleration, which requires OpenGL.

If you want to install another DDX driver, note that Xorg searches for installed DDX drivers automatically:

In order for video acceleration to work, and often to expose all the modes that the GPU can set, a proper video driver is required:

Other DDX drivers can be found in the xorg-drivers group.

Xorg should run smoothly without closed source drivers, which are typically needed only for advanced features such as fast 3D-accelerated rendering for games. The exceptions to this rule are recent GPUs (especially NVIDIA GPUs) not supported by open source drivers.

For a translation of model names (e.g. Radeon RX 6800) to GPU architectures (e.g. RDNA 2), see Wikipedia:List of AMD graphics processing units#Features overview.

The Xorg(1) command is usually not run directly. Instead, the X server is started with either a display manager or xinit.

Xorg uses a configuration file called xorg.conf and files ending in the suffix .conf for its initial setup: the complete list of the folders where these files are searched can be found in xorg.conf(5), together with a detailed explanation of all the available options.

The /etc/X11/xorg.conf.d/ directory stores host-specific configuration. You are free to add configuration files there, but they must have a .conf suffix: the files are read in ASCII order, and by convention their names start with XX- (two digits and a hyphen, so that for example 10 is read before 20). These files are parsed by the X server upon startup and are treated like part of the traditional xorg.conf configuration file. Note that on conflicting configuration, the file read last will be processed. For this reason, the most generic configuration files should be ordered first by name. The configuration entries in the xorg.conf file are processed at the end.

For option examples to set, see Fedora:Input device configuration#xorg.conf.d.

Xorg can also be configured via /etc/X11/xorg.conf or /etc/xorg.conf. You can also generate a skeleton for xorg.conf with:

This should create a xorg.conf.new file in /root/ that you can copy over to /etc/X11/xorg.conf.

Alternatively, your proprietary video card drivers may come with a tool to automatically configure Xorg: see the article of your video driver, NVIDIA or AMDGPU PRO, for more details.

For input devices the X server defaults to the libinput driver (xf86-input-libinput), but xf86-input-evdev and related drivers are available as alternative.[2]

Udev, which is provided as a systemd dependency, will detect hardware and both drivers will act as hotplugging input driver for almost all devices, as defined in the default configuration files 10-quirks.conf and 40-libinput.conf in the /usr/share/X11/xorg.conf.d/ directory.

After starting X server, the log file will show which driver hotplugged for the individual devices (note the most recent log file name may vary):

If both do not support a particular device, install the needed driver from the xorg-drivers group. The same applies, if you want to use another driver.

To influence hotplugging, see #Configuration.

For specific instructions, see also the libinput article, the following pages below, or Fedora:Input device configuration for more examples.

See Keyboard input#Identifying keycodes in Xorg.

See Mouse acceleration.

See libinput or Synaptics.

See Keyboard configuration in Xorg.

For a headless configuration, the xf86-video-dummy driver is necessary; install it and create a configuration file, such as the following:

See main article Multihead for general information.

You must define the correct driver to use and put the bus ID of your graphic cards (in decimal notation).

To get your bus IDs (in hexadecimal):

The bus IDs here are 0:2:0 and 1:0:0.

By default, Xorg always sets DPI to 96 since 2009-01-30. A change was made with version 21.1 to provide proper DPI auto-detection, but reverted.

The DPI of the X server can be set with the -dpi command line option.

Having the correct DPI is helpful where fine detail is required (like font rendering). Previously, manufacturers tried to create a standard for 96 DPI (a 10.3" diagonal monitor would be 800x600, a 13.2" monitor 1024x768). These days, screen DPIs vary and may not be equal horizontally and vertically. For example, a 19" widescreen LCD at 1440x900 may have a DPI of 89x87.

To see if your display size and DPI are correct:

Check that the dimensions match your display size.

If you have specifications on the physical size of the screen, they can be entered in the Xorg configuration file so that the proper DPI is calculated (adjust identifier to your xrandr output):

If you only want to enter the specification of your monitor without creating a full xorg.conf, create a new configuration file. For example (/etc/X11/xorg.conf.d/90-monitor.conf):

If you do not have specifications for physical screen width and height (most specifications these days only list by diagonal size), you can use the monitor's native resolution (or aspect ratio) and diagonal length to calculate the horizontal and vertical physical dimensions. Using the Pythagorean theorem on a 13.3" diagonal length screen with a 1280x800 native resolution (or 16:10 aspect ratio):

This will give the pixel diagonal length, and with this value you can discover the physical horizontal and vertical lengths (and convert them to millimeters):

For RandR compliant drivers (for example the open source ATI driver), you can set it by:

To make it permanent, see Autostarting#On Xorg startup.

You can manually set the DPI by adding the option under the Device or Screen section:

GTK very often overrides the server's DPI via the optional X resource Xft.dpi. To find out whether this is happening to you, check with:

With GTK library versions since 3.16, when this variable is not otherwise explicitly set, GTK sets it to 96. To have GTK apps obey the server DPI you may need to explicitly set Xft.dpi to the same value as the server. The Xft.dpi resource is the method by which some desktop environments optionally force DPI to a particular value in personal settings. Among these are KDE and TDE.

DPMS is a technology that allows power saving behaviour of monitors when the computer is not in use. This will allow you to have your monitors automatically go into standby after a predefined period of time.

The Composite extension for X causes an entire sub-tree of the window hierarchy to be rendered to an off-screen buffer. Applications can then take the contents of that buffer and do whatever they like. The off-screen buffer can be automatically merged into the parent window, or merged by external programs called compositing managers. For more information, see Wikipedia:Compositing window manager.

Some window managers (e.g. Compiz, Enlightenment, KWin, marco, metacity, muffin, mutter, Xfwm) do compositing on their own. For other window managers, a standalone composite manager can be used.

This section lists utilities for automating keyboard / mouse input and window operations (like moving, resizing or raising).

See also Clipboard#Tools and an overview of X automation tools.

This article or section is out of date.

To run a nested session of another desktop environment:

This will launch a Window Maker session in a 1024 by 768 window within your current X session.

This needs the package xorg-server-xnest to be installed.

A more modern way of doing a nested X session is with Xephyr.

See xinit#Starting applications without a window manager.

See main article: OpenSSH#X11 forwarding.

With the help of xinput you can temporarily disable or enable input sources. This might be useful, for example, on systems that have more than one mouse, such as the ThinkPads and you would rather use just one to avoid unwanted mouse clicks.

Install the xorg-xinput package.

Find the name or ID of the device you want to disable:

For example in a Lenovo ThinkPad T500, the output looks like this:

Disable the device with xinput --disable device, where device is the device ID or name of the device you want to disable. In this example we will disable the Synaptics Touchpad, with the ID 10:

To re-enable the device, just issue the opposite command:

Alternatively using the device name, the command to disable the touchpad would be:

You can disable a particular input source using a configuration snippet:

device is an arbitrary name, and driver_name is the name of the input driver, e.g. libinput. device_name is what is actually used to match the proper device. For alternate methods of targeting the correct device, such as libinput's MatchIsTouchscreen, consult your input driver's documentation. Though this example uses libinput, this is a driver-agnostic method which simply prevents the device from being propagated to the driver.

Run script on hotkey:

Dependencies: xorg-xprop, xdotool

See also #Killing an application visually.

To block tty access when in an X add the following to xorg.conf:

This can be used to help restrict command line access on a system accessible to non-trusted users.

To prevent a user from killing X when it is running add the following to xorg.conf:

When an application is misbehaving or stuck, instead of using kill or killall from a terminal and having to find the process ID or name, xorg-xkill allows to click on said application to close its connection to the X server. Many existing applications do indeed abort when their connection to the X server is closed, but some can choose to continue.

Xorg may run with standard user privileges instead of root (so-called "rootless" Xorg). This is a significant security improvement over running as root. Note that some popular display managers do not support rootless Xorg (e.g. LightDM or XDM).

You can verify which user Xorg is running as with ps -o user= -C Xorg.

See also Xorg.wrap(1), systemd-logind(8), Systemd/User#Xorg as a systemd user service, Fedora:Changes/XorgWithoutRootRights and FS#41257.

To configure rootless Xorg using xinitrc:

Note that executing startx directly without exec leaves the shell open in the case of a xorg crash. Since some lock screens are executed inside xorg, this can lead to full access to the executing user.

GDM will run Xorg without root privileges by default when kernel mode setting is used.

When Xorg is run in rootless mode, Xorg logs are saved to ~/.local/share/xorg/Xorg.log. However, the stdout and stderr output from the Xorg session is not redirected to this log. To re-enable redirection, start Xorg with the -keeptty flag and redirect the stdout and stderr output to a file:

Alternatively, copy /etc/X11/xinit/xserverrc to ~/.xserverrc, and append -keeptty. See [5].

As explained above, there are circumstances in which rootless Xorg is defaulted to. If this is the case for your configuration, and you have a need to run Xorg as root, you can configure Xorg.wrap(1) to require root:

Wayback is an X11 compatibility layer which allows for running full X11 desktop environments (and window managers) using Wayland components. It's available from the AUR as wayback-x11AUR package.

If a problem occurs, view the log stored in either /var/log/ or, for the rootless X default since v1.16, in ~/.local/share/xorg/. GDM users should check the systemd journal. [6]

The logfiles are of the form Xorg.n.log with n being the display number. For a single user machine with default configuration the applicable log is frequently Xorg.0.log, but otherwise it may vary. To make sure to pick the right file it may help to look at the timestamp of the X server session start and from which console it was started. For example:

X creates configuration and temporary files in current user's home directory. Make sure there is free disk space available on the partition your home directory resides in. Unfortunately, X server does not provide any more obvious information about lack of disk space in this case.

If you use a Matrox card and DRI stopped working after upgrading to Xorg, try adding the line:

to the Device section that references the video card in xorg.conf.

X fails to start with the following log messages:

To correct, uninstall the xf86-video-fbdev package.

Error message: unable to load font `(null)'.

Some programs only work with bitmap fonts. Two major packages with bitmap fonts are available, xorg-fonts-75dpi and xorg-fonts-100dpi. You do not need both; one should be enough. To find out which one would be better in your case, try xdpyinfo from xorg-xdpyinfo, like this:

and use what is closer to the shown value.

If Xorg is set to boot up automatically and for some reason you need to prevent it from starting up before the login/display manager appears (if the system is wrongly configured and Xorg does not recognize your mouse or keyboard input, for instance), you can accomplish this task with two methods.

Depending on setup, you will need to do one or more of these steps:

If you are getting Client is not authorized to connect to server, try adding the line:

to /etc/pam.d/su and /etc/pam.d/su-l. pam_xauth will then properly set environment variables and handle xauth keys.

If the filesystem (specifically /tmp) is full, startx will fail. The log file will contain:

Make some free space on the relevant filesystem and X will start.

Your color depth is set wrong. It may need to be 24 instead of 16, for example.

If X terminates with error message SocketCreateListener() failed, you may need to delete socket files in /tmp/.X11-unix. This may happen if you have previously run Xorg as root (e.g. to generate an xorg.conf).

That error means that only the current user has access to the X server. The solution is to give access to root:

That line can also be used to give access to X to a different user than root.

**Examples:**

Example 1 (unknown):
```unknown
$ lspci -v -nn -d ::03xx
```

Example 2 (unknown):
```unknown
$ pacman -Ss xf86-video
```

Example 3 (unknown):
```unknown
/usr/share/X11/xorg.conf.d/
```

Example 4 (unknown):
```unknown
/etc/X11/xorg.conf.d/
```

---

## Unified Extensible Firmware Interface

**URL:** https://wiki.archlinux.org/title/UEFI

**Contents:**
- UEFI firmware bitness
  - Checking the firmware bitness
    - From Linux
    - From macOS
    - From Microsoft Windows
- UEFI variables
  - UEFI variables support in Linux kernel
  - Requirements for UEFI variable support
    - Mount efivarfs
  - Userspace tools

The Unified Extensible Firmware Interface (UEFI) is an interface between operating systems and firmware. It provides a standard environment for booting an operating system and running pre-boot applications.

It is distinct from the MBR boot code method that was used by legacy BIOS systems. See Arch boot process for their differences and the boot process using UEFI. To set up UEFI boot loaders, see Arch boot process#Boot loader.

Under UEFI, every program whether it is an operating system loader or a utility (e.g. a memory testing or recovery tool), should be an EFI application corresponding to the UEFI firmware bitness/architecture.

The vast majority of x86_64 systems, including recent Apple Macs, use x64 (64-bit) UEFI firmware. The only known devices that use IA32 (32-bit) UEFI are older (pre 2008) Apple Macs, Intel Atom System-on-Chip systems (as on 2 November 2013)[1] and some older Intel server boards that are known to operate on Intel EFI 1.10 firmware.

An x64 UEFI firmware does not include support for launching 32-bit EFI applications (unlike x86_64 Linux and Windows versions which include such support). Therefore the EFI application must be compiled for that specific firmware processor bitness/architecture.

The firmware bitness can be checked from a booted operating system.

On distributions running Linux kernel 4.0 or newer, the UEFI firmware bitness can be found via the sysfs interface. Run:

It will return 64 for a 64-bit (x64) UEFI or 32 for a 32-bit (IA32) UEFI. If the file does not exist, you have not booted in UEFI mode.

Pre-2008 Macs mostly have IA32 EFI firmware while >=2008 Macs have mostly x64 EFI. All Macs capable of running Mac OS X Snow Leopard 64-bit Kernel have x64 EFI 1.x firmware.

To find out the arch of the EFI firmware in a Mac, type the following into the Mac OS X terminal:

If the command returns EFI32, it is IA32 (32-bit) EFI firmware. If it returns EFI64, it is x64 EFI firmware. Most of the Macs do not have UEFI 2.x firmware as Apple's EFI implementation is not fully compliant with UEFI 2.x specification.

64-bit versions of Windows do not support booting on a 32-bit UEFI. So, if you have a 32-bit version of Windows booted in UEFI mode, you have a 32-bit UEFI.

To check the bitness run msinfo32.exe. In the System Summary section look at the values of "System Type" and "BIOS mode".

For 64-bit Windows on a 64-bit UEFI, it will be System Type: x64-based PC and BIOS mode: UEFI. For 32-bit Windows on a 32-bit UEFI—System Type: x86-based PC and BIOS mode: UEFI. If the "BIOS mode" is not UEFI, Windows is not booted in UEFI mode.

UEFI defines variables through which an operating system can interact with the firmware. UEFI boot variables are used by the boot loader and used by the operating system only for early system start-up. UEFI runtime variables allow an operating system to manage certain settings of the firmware like the UEFI boot manager or managing the keys for UEFI Secure Boot protocol etc. You can get the list using:

Linux kernel exposes UEFI variables data to userspace via efivarfs (EFI VARiable FileSystem) interface (CONFIG_EFIVAR_FS) - mounted using efivarfs kernel module at /sys/firmware/efi/efivars - it has no maximum per-variable size limitation and supports UEFI Secure Boot variables. Introduced in kernel 3.8.

If UEFI Variables support does not work even after the above conditions are satisfied, try the below workarounds:

If efivarfs is not automatically mounted at /sys/firmware/efi/efivars by systemd during boot, you need to manually mount it to expose UEFI variables to userspace tools like efibootmgr:

See efivarfs.html for kernel documentation.

There are few tools that can access/modify the UEFI variables, namely

You will have to install the efibootmgr package.

To add a new boot option using efibootmgr, you need to know three things:

For example, if you want to add a boot option for /efi/EFI/refind/refind_x64.efi where /efi is the mount point of the ESP, run

In this example, findmnt(8) indicates that the ESP is on disk /dev/sda and has partition number 1. The path to the EFI application relative to the root of the ESP is /EFI/refind/refind_x64.efi. So you would create the boot entry as follows:

Get an overview of all boot entries and the boot order:

To set the boot order:

Where XXXX is the number that appears in the previous output of efibootmgr command.

Delete an unwanted entry:

See efibootmgr(8) or efibootmgr README for more info.

Access to the UEFI can potentially cause harm beyond the running operating system level. There are dangerous UEFI exploits like LogoFAIL which allows a malicious actor to take full control over the machine. Even hardware-level bricking is possible in some cases of poor UEFI implementation [2].

So, as the UEFI variables access is not required for daily system usage, you may want to disable it, to avoid potential security breaches or accidental harm.

Possible solutions are:

The UEFI Shell is a shell/terminal for the firmware which allows launching EFI applications which include UEFI boot loaders. Apart from that, the shell can also be used to obtain various other information about the system or the firmware like memory map (memmap), modifying boot manager variables (bcfg), running partitioning programs (diskpart), loading UEFI drivers, editing text files (edit), hexedit etc.

You can obtain a BSD licensed UEFI Shell from the TianoCore EDK2 project:

Shell v2 works best in UEFI 2.3+ systems and is recommended over Shell v1 in those systems. Shell v1 should work in all UEFI systems irrespective of the spec. version the firmware follows. More information at ShellPkg and the EDK2 mailing list thread—Inclusion of UEFI shell in Linux distro iso.

Few Asus and other AMI Aptio x64 UEFI firmware based motherboards (from Sandy Bridge onwards) provide an option called Launch EFI Shell from filesystem device. For those motherboards, copy the x64 UEFI Shell to the root of your EFI system partition, named as shellx64.efi.

Systems with Phoenix SecureCore Tiano UEFI firmware is known to have embedded UEFI Shell which can be launched using either F6, F11 or F12 key.

UEFI Shell commands usually support -b option which makes output pause after each page. Run help -b to list available internal commands. Available commands are either built into the shell or discrete EFI applications.

For more info see Intel Scripting Guide 2008[dead link 2023-07-30—HTTP 404] and Intel "Course" 2011[dead link 2023-07-30—HTTP 404].

bcfg modifies the UEFI NVRAM entries which allows the user to change the boot entries or driver options. This command is described in detail in page 96 (Section 5.3) of the UEFI Shell Specification 2.2 document.

To dump a list of current boot entries:

To add a boot menu entry for rEFInd (for example) as 4th (numbering starts from zero) option in the boot menu:

where FS0: is the mapping corresponding to the EFI system partition and FS0:\EFI\refind\refind_x64.efi is the file to be launched.

To add an entry to boot directly into your system without a boot loader, see EFI boot stub#bcfg.

To remove the 4th boot option:

To move the boot option #3 to #0 (i.e. 1st or the default entry in the UEFI Boot menu):

map displays a list of device mappings i.e. the names of available file systems (FS0) and storage devices (blk0).

Before running file system commands such as cd or ls, you need to change the shell to the appropriate file system by typing its name:

edit provides a basic text editor with an interface similar to nano, but slightly less functional. It handles UTF-8 encoding and takes care or LF vs CRLF line endings.

For example, to edit rEFInd's refind.conf in the EFI system partition (FS0: in the firmware),

Press Ctrl+e for help.

This article or section needs expansion.

UEFI drivers are pieces of software that support some functionality. For example, access to NTFS formatted partitions is usually not possible from a UEFI shell. The efifs package has drivers that support reading many more file systems from within an EFI shell. A usage example is to copy such driver to a partition that can be accessed from an UEFI shell. Then, from the UEFI shell, issuing commands such as:

After the map command has been executed, the user should be able to access NTFS formatted partitions from within a UEFI shell.

Most of the 32-bit EFI Macs and some 64-bit EFI Macs refuse to boot from a UEFI(X64)+BIOS bootable CD/DVD. If one wishes to proceed with the installation using optical media, it might be necessary to remove UEFI support first.

Extract the ISO skipping the UEFI-specific directories:

Then rebuild the ISO, excluding the UEFI optical media booting support, using xorriso(1) from libisoburn. Be sure to set the correct volume label, e.g. ARCH_202103; it can be acquired using file(1) on the original ISO.

Burn archlinux-version-x86_64-noUEFI.iso to optical media and proceed with installation normally.

OVMF is a TianoCore project to enable UEFI support for Virtual Machines. OVMF contains a sample UEFI firmware and a separate non-volatile variable store for QEMU.

You can install edk2-ovmf from the extra repository.

It is advised to make a local copy of the non-volatile variable store for your virtual machine:

To use the OVMF firmware and this variable store, add following to your QEMU command:

DUET was a TianoCore project that enabled chainloading a full UEFI environment from a BIOS system, in a way similar to BIOS operating system booting. This method is being discussed extensively. Pre-build DUET images can be downloaded from one of the repos[dead link 2023-04-07—404 Page Not Found]. Read specific instructions[dead link 2023-04-07—404 Page Not Found] for setting up DUET. However, as of November 2018, the DUET code has been removed from TianoCore git repository.

You can also try Clover which provides modified DUET images that may contain some system specific fixes and is more frequently updated compared to the gitlab repos.

To boot back into Arch Linux when you are stuck with Windows, reach Advanced startup in Windows by the Windows PowerShell command shutdown /r /o, or via Settings > Update & Security > Recovery > Advanced startup and select Restart now. When you have reached the Advanced startup menu, choose Use a device, which actually contains your UEFI boot options (not limited to USB or CD, but can also boot operating system in hard drive), and choose "Arch Linux".

On some laptops, like Lenovo XiaoXin 15are 2020, using keys like F2 or F12 does not do anything. This can possibly be fixed by returning laptops to OEM to repair mainboard information, but sometimes this is not possible or not desired. There are however other means to enter firmware setup:

If any userspace tool is unable to modify UEFI variable data, check for existence of /sys/firmware/efi/efivars/dump-* files. If they exist, delete them, reboot and retry again. If the above step does not fix the issue, try booting with efi_no_storage_paranoia kernel parameter to disable kernel UEFI variable storage space check that may prevent writing/modification of UEFI variables.

Some kernel and efibootmgr version combinations might refuse to create new boot entries. This could be due to lack of free space in the NVRAM. You can try the solution at #Userspace tools are unable to modify UEFI variable data.

You can also try to downgrade your efibootmgr install to version 0.11.0. This version works with Linux version 4.0.6. See the bug discussion FS#34641, in particular the closing comment, for more information.

If you dual boot with Windows and your motherboard just boots Windows immediately instead of your chosen EFI application, there are several possible causes and workarounds.

This issue can occur due to KMS issue. Try disabling KMS while booting the USB.

Some firmware do not support custom boot entries. They will instead only boot from hardcoded boot entries.

A typical workaround is to not rely on boot entries in the NVRAM and install the boot loader to one of the common fallback paths on the EFI system partition.

The following sections describe the fallback paths.

The UEFI specification defines default file paths for EFI binaries for booting from removable media. The relevant ones are:

While the specification defines these for removable drives only, most firmware support booting these from any drive.

See the appropriate boot loader article on how to install or migrate the boot loader to the default/fallback boot path.

On certain UEFI motherboards like some boards with an Intel Z77 chipset, adding entries with efibootmgr or bcfg from the UEFI Shell will not work because they do not show up on the boot menu list after being added to NVRAM.

This issue is caused because the motherboards can only load Microsoft Windows. To solve this you have to place the .efi file in the location that Windows uses.

Copy the BOOTx64.EFI file from the Arch Linux installation medium (FSO:) to the Microsoft directory your ESP partition on your hard drive (FS1:). Do this by booting into EFI shell and typing:

After reboot, any entries added to NVRAM should show up in the boot menu.

This is a recurring problem with Acer laptops, which occurs if .efi files have not been manually authorized. See Laptop/Acer#Firmware Setup became inaccessible after Linux installation.

efibootmgr can fail to detect EDD 3.0 and as a result create unusable boot entries in NVRAM. See efibootmgr issue 86 for the details.

To work around this, when creating boot entries manually, add the -e 3 option to the efibootmgr command. E.g.

To fix boot loader installers, like grub-install and refind-install, create a wrapper script /usr/local/bin/efibootmgr and make it executable:

Some firmware will remove boot entries referencing drives that are not present during boot. This could be an issue when frequently detaching/attaching drives or when booting from a removable drive.

The solution is to install the boot loader to the default/fallback boot path.

Some motherboards may remove boot entries due to lack of free space in the NVRAM instead of giving an error at creation. To prevent this from occurring, reduce the amount of boot entries being added by minimizing your entry creation process, as well as reducing the amount of automatic drive boot entries by the Compatibility Support Module (CSM) by disabling it from your UEFI settings. See BBS#1608838.

Another reason why boot entries might have been removed is the fact that UEFI specification allows OEMs to do "NVRAM maintenance" during boot process. Those manufacturers do it simply: they just look up for EFI applications in predefined, hardcoded paths on the device. If they fail to find any, they conclude there is no operating system on the device and wipe all boot entries from NVRAM associated with it, because they assume the NVRAM contains some corrupted or outdated data. If you do not plan to install Windows and still want to load the Linux kernel directly from the firmware, one possible workaround is to create an empty file esp/EFI/BOOT/BOOTx64.EFI:

And restore the deleted boot entry. Now after reboot the motherboard will see the "Fake OS" and should not wipe other boot entries from NVRAM. You can change the fake operating system loader with an actual EFI application if you want, of course, as long as you keep the standard fallback name.

This article or section is a candidate for merging with Lenovo.

This article or section needs expansion.

On recent Lenovo ThinkPad laptops (e.g. T16 Gen 2 AMD models), users report that custom UEFI boot entries (created with efibootmgr or bootctl) are automatically deleted at each boot, with only Windows Boot Manager and Lenovo’s own entries (PXE, Recovery, Diagnostics) restored.

This is caused by the BIOS option "Restart / OS Optimized Defaults", which resets the UEFI boot variables at each reboot to defaults optimized for Windows.

Solution: Disable "OS Optimized Defaults" in the BIOS/UEFI setup. After doing so, manually created boot entries persist correctly, allowing systemd-boot or other custom boot managers to work as intended.

**Examples:**

Example 1 (unknown):
```unknown
$ cat /sys/firmware/efi/fw_platform_size
```

Example 2 (unknown):
```unknown
$ ioreg -l -p IODeviceTree | grep firmware-abi
```

Example 3 (unknown):
```unknown
msinfo32.exe
```

Example 4 (unknown):
```unknown
System Type: x64-based PC
```

---

## EFI boot stub

**URL:** https://wiki.archlinux.org/title/EFI_boot_stub

**Contents:**
- Booting an EFI boot stub
  - Using UEFI directly
    - efibootmgr
    - bcfg
  - Using UEFI Shell
    - Using a startup.nsh script
- Tips and tricks
  - Boot entry with fallback ramdisk
  - Archiso on ESP
- Troubleshooting

An EFI boot stub (aka EFI stub) is a kernel that is an EFI executable, i.e. that can directly be booted from the UEFI.

Historically, this article and the Debian Wiki used the terms as one word (EFISTUB or EFIStub).

By default Arch Linux kernels are EFI boot stubs. If compiling the kernel, activate it by setting CONFIG_EFI_STUB=y. See The EFI Boot Stub for more information.

Before continuing, you need an EFI system partition and choose how it is mounted.

UEFI is designed to remove the need for an intermediate boot loader such as GRUB. If your motherboard has a good UEFI implementation, it is possible to embed the kernel parameters within a UEFI boot entry and for the motherboard to boot Arch directly. You can use efibootmgr or UEFI Shell v2 to modify your motherboard's boot entries.

To create a boot entry with efibootmgr that will load the kernel:

Where /dev/sdX and Y are the drive and partition number where the ESP is located and root= parameters with your Linux root partitions.

If omitted, then the first partition on /dev/sda is used as the ESP.

Note that the -u/--unicode argument in quotes is just the list of kernel parameters, so you may need to add additional parameters (e.g. for suspend to disk).

An example with LTS Linux kernel, NVME storage, BTRFS filesystem with specific subvolume and hibernation on swap partition:

For getting a list with the boot entries, setting the boot order or removing them, see efibootmgr.

Some UEFI implementations make it difficult to modify the NVRAM successfully using efibootmgr. If efibootmgr cannot successfully create an entry, you can use the bcfg command in UEFI Shell v2 (i.e., from the Arch Linux live iso).

First, find out the device number where your ESP resides with:

In this example, 1 is used as the device number. To list the contents of the ESP:

To view the current boot entries:

To add an entry for your kernel, use:

Where N is the location where the entry will be added in the boot menu. 0 is the first menu item. Menu items already existing will be shifted in the menu without being discarded.

You can add kernel options directly:

Or by creating a file on your ESP:

In the file, add the boot line. For example:

Press F2 to save and then F3 to exit.

Add these options to your previous entry:

Repeat this process for any additional entries.

To remove a previously added item do:

If you do not want to create a permanent boot entry it is possible to launch the kernel from UEFI Shell since it is a normal UEFI application:

In this case, the kernel parameters are passed as normal parameters to the EFI boot stub kernel.

To avoid needing to remember all of your kernel parameters every time, you can save the executable command to a shell script such as archlinux.nsh on your EFI system partition, then run it with:

Some UEFI implementations do not retain EFI variables between cold boots (e.g. VirtualBox before version 6.1) and anything set through the UEFI is lost on poweroff.

The UEFI Shell Specification 2.0 establishes that a script called startup.nsh at the root of the ESP partition will always be interpreted and can contain arbitrary instructions; among those you can set a bootloading line. Make sure you mount the ESP partition on /boot and create a startup.nsh script that contains a kernel bootloading line. For example:

This method will work with almost all UEFI versions you may encounter in real hardware, you can use it as last resort. The script must be a single long line. Sections in brackets are optional and given only as a guide. Shell style linebreaks are for visual clarification only. FAT filesystems use the backslash as path separator and in this case, the backslash declares the initramfs is located in the root of the ESP partition.

Without a boot manager, the kernel command line is not changeable at boot time. To have at least some sort of fallback possibility, e.g. to use the initramfs-linux-fallback.img and/or start without Intel microcode, simply create a further boot entry with efibootmgr, e.g. labeled "Arch Linux fallback" and the desired fallback options.

It is possible to put the Arch Linux ISO on the ESP to have a recovery system. As of 2025.08.01 release the size required is 1.2G.

First download archlinux-YYYY.MM.DD-x86_64.iso.

Next, create a directory for archiso in your ESP:

Extract the contents of the arch directory in there:

In the last step, create a boot entry using efibootmgr. Replace the following:

diskpath is the device path (eg. /dev/sda)

diskpart is the partition number

You can now choose the rescue system from the UEFI boot loader.

Some motherboards, such as Haswell-era ASUS boards (as encountered on the french forum), will not notice changes to boot entries unless the system is booted with another pre-existing one.

**Examples:**

Example 1 (unknown):
```unknown
CONFIG_EFI_STUB=y
```

Example 2 (unknown):
```unknown
esp/EFI/arch/initramfs-linux.img
```

Example 3 (unknown):
```unknown
initrd=\EFI\arch\initramfs-linux.img
```

Example 4 (unknown):
```unknown
# efibootmgr --create --disk /dev/sdX --part Y --label "Arch Linux" --loader /vmlinuz-linux --unicode 'root=block_device_identifier rw initrd=\initramfs-linux.img'
```

---

## Unified Extensible Firmware Interface/Secure Boot

**URL:** https://wiki.archlinux.org/title/Secure_Boot

**Contents:**
- Checking Secure Boot status
  - Before booting the OS
  - After booting the OS
- Booting an installation medium
  - Disabling Secure Boot
  - Repacking the installation image
  - Editing the installation medium
- Implementing Secure Boot
  - Using your own keys
    - Backing up current variables

Secure Boot is a security feature found in the UEFI standard, designed to add a layer of protection to the pre-boot process: by maintaining a cryptographically signed list of binaries authorized or forbidden to run at boot, it helps in improving the confidence that the machine core boot components (boot manager, kernel, initramfs) have not been tampered with.

As such it can be seen as a continuation or complement to the efforts in securing one's computing environment, reducing the attack surface that other software security solutions such as system encryption cannot easily cover, while being totally distinct and not dependent on them. Secure Boot just stands on its own as a component of current security practices, with its own set of pros and cons.

At this point, one has to look at the firmware setup. If the machine was booted and is running, in most cases it will have to be rebooted.

You may access the firmware configuration by pressing a special key during the boot process. The key to use depends on the firmware. It is usually one of Esc, F2, Del or possibly another Fn key. Sometimes the right key is displayed for a short while at the beginning of the boot process. The motherboard manual usually records it. You might want to press the key, and keep pressing it, immediately following powering on the machine, even before the screen actually displays anything.

After entering the firmware setup, be careful not to change any settings without prior intention. Usually there are navigation instructions, and short help for the settings, at the bottom of each setup screen. The setup itself might be composed of several pages. You will have to navigate to the correct place. The interesting setting might be simply denoted by secure boot, which can be set on or off.

An easy way to check Secure Boot status on systems using systemd is to use bootctl(1):

Here we see that Secure Boot is enabled and enforced (in user mode); other values are disabled (setup) for Setup Mode, disabled (disabled) if Secure Boot is disabled and disabled (unsupported) if the firmware does not feature Secure Boot.

Another way to check whether the machine was booted with Secure Boot is to use this command:

If Secure Boot is enabled, this command returns 1 as the final integer in a list of five, for example:

Note, however, that the kernel may be unaware of Secure Boot (even if it is enabled in the firmware) if an insufficiently capable boot loader is used. This can be verified by checking the kernel messages shortly after the system starts up:

The kernel messages will otherwise read Secure boot enabled.

The official installation image does not support Secure Boot (archlinux/archiso#69). Secure Boot support was initially added in archlinux-2013.07.01-dual.iso and later removed in archlinux-2016.06.01-dual.iso. At that time prebootloader was replaced with efitools, even though the latter uses unsigned EFI binaries. There has been no support for Secure Boot in the official installation medium ever since.

In order to boot an installation medium in a Secure Boot system, you will need to either disable Secure Boot or modify the image in order to add a signed boot loader.

Archboot images provide a way to use secure boot on installation media.

The Secure Boot feature can be disabled via the UEFI firmware interface. How to access the firmware configuration is described in #Before booting the OS.

If using a hotkey did not work and you can boot Windows, you can force a reboot into the firmware configuration in the following way (for Windows 10): Settings > Update & Security > Recovery > Advanced startup (Restart now) > Troubleshoot > Advanced options > UEFI Firmware settings > restart.

Note that some motherboards (this is the case in a Packard Bell laptop and recent Xiaomi laptops) only allow to disable secure boot if you have set an administrator password (that can be removed afterwards). See also Rod Smith's Disabling Secure Boot.

If you are using a USB flash installation medium, then it is possible to manually edit the EFI system partition on the medium to add support for Secure Boot.

Plug in the USB drive, then mount the partition:

Then follow the instructions in #Using a signed boot loader to install any signed boot loader. For example, to install #PreLoader:

There are certain conditions making for an ideal setup of Secure boot:

A simple and fully self-reliant setup is described in #Using your own keys, while #Using a signed boot loader makes use of intermediate tools signed by a third-party.

Using the GRUB boot loader requires extra steps before enabling secure boot, see GRUB#Secure Boot support for details.

Secure Boot implementations use these keys:

See The Meaning of all the UEFI Keys for a more detailed explanation.

To use Secure Boot you need at least PK, KEK and db keys. While you can add multiple KEK, db and dbx certificates, only one Platform Key is allowed.

Once Secure Boot is in "User Mode" keys can only be updated by signing the update (using sign-efi-sig-list) with a higher level key. Platform key can be signed by itself.

Before creating new keys and modifying EFI variables, it is advisable to backup the current variables, so that they may be restored in case of error.

Install the efitools package, then run the following commands to backup all four of the principal Secure Boot variables:

If you perform this command on a new computer or motherboard, the variables you extract will most likely be the ones provided by Microsoft.

Secure Boot is in Setup Mode when the Platform Key is removed. To put firmware in Setup Mode, enter firmware setup utility and find an option to delete or clear certificates. How to enter the setup utility is described in #Before booting the OS.

As of v257, you can easily set up Secure Boot with systemd and systemd-boot. Install systemd-ukify.

First set up your desired configuration as in Unified kernel image#ukify (you can use the template from /usr/lib/kernel/uki.conf), then generate your signing keys:

Sign the boot loader with the newly created keys (see systemd-boot#Signing for Secure Boot for automation):

Next, configure the ESP for auto-enrollment:

This installs (or updates) the signed systemd-boot boot loader to the ESP, and creates three files PK.auth, KEK.auth and db.auth in /boot/loader/keys/auto/. No separate platform or key exchange keys are generated, instead the signing key is enrolled at all three levels.

Finally, set secure-boot-enroll force in /boot/loader/loader.conf, and reboot to enroll the keys in the firmware. See loader.conf(5).

sbctl is a user-friendly way of setting up secure boot and signing files.

To use it, install sbctl. See also the upstream README and sbctl(8).

Before starting, go to your firmware settings and set secure boot mode to Setup mode. This is different for each device: see sbctl(8) § USAGE.

Once you log back in, check the secure boot status:

You should see that sbctl is not installed and secure boot is disabled.

Then create your custom secure boot keys:

Enroll your keys, with Microsoft's keys, to the UEFI:

Check the secure boot status again:

sbctl should be installed now, but secure boot will not work until the boot files have been signed with the keys you just created.

Check what files need to be signed for secure boot to work:

Now sign all the unsigned files. Usually the kernel and the boot loader need to be signed. For example:

The files that need to be signed will depend on your system's layout, kernel and boot loader.

This example assumes that the outputted file paths are relative to /boot. Alternatively, you can use:

This is agnostic of the filepath, and doesn't need the '✗' character.

Now you are done! Reboot your system and turn secure boot back on in the firmware settings. If the boot loader and OS load, secure boot should be working. Check with:

sbctl comes with a pacman hook that automatically signs all new files whenever the Linux kernel, systemd or the boot loader is updated.

Nearly all of the following sections require you to install the efitools package.

You will need private keys and certificates in various formats:

Create a GUID for owner identification:

Sign an empty file to allow removing Platform Key when in "User Mode":

Signature Database key:

A helper/convenience script is offered by the author of the reference page on this topic[3] (requires python). A mildly edited version is also packaged as sbkeysAUR.

In order to use it, simply create a folder in a secure location (e.g. /etc/efi-keys/ if later use of sbupdate-gitAUR to automate unified kernel image creation and signing is planned) and run it:

This will produce the required files in different formats.

Use one of the following methods to enroll db, KEK and PK certificates.

Install sbsigntools. Create a directory /etc/secureboot/keys with the following directory structure -

Then copy each of the .auth files that were generated earlier into their respective locations (for example, PK.auth into /etc/secureboot/keys/PK and so on).

If you want to verify the changes sbkeysync will make to the system's UEFI keystore, use:

Finally, use sbkeysync to enroll your keys.

On next boot the UEFI should be back in User Mode and enforcing Secure Boot policy.

Copy all *.cer, *.esl, *.auth files (except the noPK.auth file!) to a FAT formatted file system (you can use EFI system partition).

The EFI system partition of your PC must not be encrypted according to the UEFI specifications and can be mounted and read on another PC (if your PC is stolen and if the hard drive is taken out and connected to another PC). Copying the noPK.auth file to the ESP of your PC and deleting it afterwards is also not advisable, because deleted files on the FAT32 EFI system partition can be recovered with tools like PhotoRec.

Launch firmware setup utility and enroll db, KEK and PK certificates. Firmwares have various different interfaces, see Replacing Keys Using Your Firmware's Setup Utility for example how to enroll keys.

If the used tool supports it prefer using .auth and .esl over .cer.

This article or section is out of date.

KeyTool.efi is in efitools package, copy it to ESP. To use it after enrolling keys, sign it with sbsign.

Launch KeyTool-signed.efi using firmware setup utility, boot loader or UEFI Shell and enroll keys.

See Replacing Keys Using KeyTool for explanation of KeyTool menu options.

Since systemd version 252, systemd-boot can be used to enroll keys. To enroll keys, copy the db.auth, KEK.auth and PK.auth to the special folder on the ESP:

Where NAME can be any unique name you assign, such as MYKEYS.

After copying the keys and enabling the secure boot setup mode a new entry would appear in the boot menu that would read Enroll Secure Boot keys: MYKEYS. Activating this entry would enroll the secure boot keys.

When Secure Boot is active (i.e. in "User Mode"), only signed EFI binaries (e.g. applications, drivers, unified kernel images) can be launched.

Install sbsigntools to sign EFI binaries with sbsign(1).

To sign your kernel and boot manager use sbsign, e.g.:

You can also use a mkinitcpio post hook to sign the kernel when the initramfs gets generated.

Create the following file an make it executable:

Replace /path/to/db.key and /path/to/db.crt with the paths to the key pair you want to use for signing the kernel.

If you are using systemd-boot, there is a dedicated pacman hook doing this task semi-automatically.

See Unified kernel image#Signing the UKIs for Secure Boot.

This article or section is out of date.

sbupdate is a tool made specifically to automate unified kernel image generation and signing on Arch Linux. It handles installation, removal and updates of kernels through pacman hooks.

Install sbupdate-gitAUR and configure it following the instructions given on the project's homepage.[4]

Once configured, simply run sbupdate as root for first-time image generation.

Once Secure Boot is in "User Mode" any changes to KEK, db and dbx need to be signed with a higher level key.

For example, if you wanted to replace your db key with a new one:

If instead of replacing your db key, you want to add another one to the Signature Database, you need to use the option -a (see sign-efi-sig-list(1)):

When new_db.auth is created, enroll it.

This article or section needs expansion.

It is usually not possible to boot Windows by signing its boot loader (EFI/Microsoft/Boot/bootmgfw.efi) with a custom, personal key with Secure Boot Mode enabled, without enrolling the "Microsoft Windows Production PCA 2011" key in the UEFI Secure Boot variables:

So to dual boot with Windows,

Create EFI Signature Lists from Microsoft's DER format db certificates using Microsoft's GUID (77fa9abd-0359-4d32-bd60-28f4e78f784b) and combine them in one file for simplicity:

Optional (for strict conformity with Microsoft UEFI Secure Boot requirements): Create an EFI Signature List from Microsoft's DER format KEK certificates using Microsoft's GUID (77fa9abd-0359-4d32-bd60-28f4e78f784b):

Sign a db variable update with your KEK. Use sign-efi-sig-list with option -a to add not replace a db certificate:

Optional (for strict conformity with Microsoft UEFI Secure Boot requirements): Sign a KEK variable update with your PK. Use sign-efi-sig-list with option -a to add not replace a KEK certificate:

Follow #Enrolling keys in firmware to enroll add_MS_db.auth and for strict conformity with Microsoft UEFI Secure Boot requirements add_MS_Win_KEK.auth into the UEFI Secure Boot Database variables.

Using a signed boot loader means using a boot loader signed with Microsoft's key. There are two known signed boot loaders: PreLoader and shim. Their purpose is to chainload other EFI binaries (usually boot loaders). Since Microsoft would never sign a boot loader that automatically launches any unsigned binary, PreLoader and shim use an allowlist called Machine Owner Key list, abbreviated MokList. If the SHA256 hash of the binary (Preloader and shim) or key the binary is signed with (shim) is in the MokList they execute it, if not they launch a key management utility which allows enrolling the hash or key.

The enrollment of the Microsoft 3rd Party UEFI CA certificate needs to be enabled in firmware settings to launch EFI binaries and OpROMs signed with this certificate.

When run, PreLoader tries to launch loader.efi. If the hash of loader.efi is not in MokList, PreLoader will launch HashTool.efi. In HashTool you must enroll the hash of the EFI binaries you want to launch, that means your boot loader (loader.efi) and kernel.

Install preloader-signedAUR and copy PreLoader.efi and HashTool.efi to the boot loader directory; for systemd-boot use:

Now copy over the boot loader binary and rename it to loader.efi; for systemd-boot use:

Finally, create a new NVRAM entry to boot PreLoader.efi:

Replace X with the drive letter and replace Y with the partition number of the EFI system partition.

This entry should be added to the list as the first to boot; check with the efibootmgr command and adjust the boot-order if necessary.

If there are problems booting the custom NVRAM entry, copy HashTool.efi and loader.efi to the default loader location booted automatically by UEFI systems:

Copy over PreLoader.efi and rename it:

For particularly intransigent UEFI implementations, copy PreLoader.efi to the default loader location used by Windows systems:

As before, copy HashTool.efi and loader.efi to esp/EFI/Microsoft/Boot/.

When the system starts with Secure Boot enabled, follow the steps above to enroll loader.efi and /vmlinuz-linux (or whichever kernel image is being used).

A message will show up that says Failed to Start loader... I will now execute HashTool. To use HashTool for enrolling the hash of loader.efi and vmlinuz.efi, follow these steps. These steps assume titles for a remastered archiso installation media. The exact titles you will get depends on your boot loader setup.

Every entry of hashes enrolled in the MOK database eats up a little piece of space of NVRAM. You may want to delete useless hashes to free the space and to prevent outdated programs from booting.

Install efitools and copy KeyTool.efi:

Manage to boot to Preloader and you will see the KeyTool entry. You can then edit hashes in the MOK database.

Uninstall preloader-signedAUR and simply remove the copied files and revert configuration; for systemd-boot use:

Where N is the NVRAM boot entry created for booting PreLoader.efi. Check with the efibootmgr command and adjust the boot-order if necessary.

When run, shim tries to launch grubx64.efi. If MokList does not contain the hash of grubx64.efi or the key it is signed with, shim will launch MokManager (mmx64.efi). In MokManager you must enroll the hash of the EFI binaries you want to launch (your boot loader (grubx64.efi) and kernel) or enroll the key they are signed with.

Install shim-signedAUR.

Rename your current boot loader to grubx64.efi, because, by default, Shim will try load and run a file named grubx64.efi. While this default can be overridden by passing a file path to a different EFI binary as a command line argument to shim, since some firmware have issues with UEFI boot entries that have command line arguments, it is more foolproof to rely on the default.

Copy shim and MokManager to your boot loader directory on ESP; use previous filename of your boot loader as as the filename for shimx64.efi:

Finally, create a new NVRAM entry to boot BOOTx64.EFI:

shim can authenticate binaries by Machine Owner Key or hash stored in MokList.

Using hash is simpler, but each time you update your boot loader or kernel you will need to add their hashes in MokManager. With MOK you only need to add the key once, but you will have to sign the boot loader and kernel each time it updates.

If shim does not find the SHA256 hash of grubx64.efi in MokList it will launch MokManager (mmx64.efi).

In MokManager select Enroll hash from disk, find grubx64.efi and add it to MokList. Repeat the steps and add your kernel vmlinuz-linux. When done select Continue boot and your boot loader will launch and it will be capable launching the kernel.

Create a Machine Owner Key:

Sign your boot loader (named grubx64.efi) and kernel:

You will need to do this each time they are updated. You can automate the kernel signing with a mkinitcpio post hook. Create the following file and make it executable:

Copy MOK.cer to a FAT formatted file system (you can use EFI system partition).

Reboot and enable Secure Boot. If shim does not find the certificate grubx64.efi is signed with in MokList it will launch MokManager (mmx64.efi).

In MokManager select Enroll key from disk, find MOK.cer and add it to MokList. When done select Continue boot and your boot loader will launch and it will be capable launching any binary signed with your Machine Owner Key.

See GRUB#Shim-lock for instructions.

Every entry of hash/key enrolled in the MOK database eats up a little piece of space of NVRAM. You may want to delete useless hash/key to free the space and to prevent outdated programs from booting.

MOK database can be managed with mokutil.

List enrolled keys and hashes:

Remove hash from database. The password entered here will be asked for confirmation to delete in MOK manager.

Remove key from database:

List hashes/keys to be deleted on next reboot:

On next reboot, MOK manager will be initiated with option to Enroll/Delete hashes/keys. See mokutil(1) for more details.

Uninstall shim-signedAUR, remove the copied shim and MokManager files and rename back your boot loader.

The only way to prevent anyone with physical access from disabling Secure Boot is to protect the firmware settings with a password. Most UEFI firmwares provide such a feature, usually listed under the "Security" section in the firmware settings.

Consider enabling kernel lockdown mode. See [7].

It is possible to unpack and repack the official installation image using libisoburn and mtools. This way, you can create an image that supports Secure Boot, either with custom keys or with a signed boot loader.

This article or section needs expansion.

Support for Secure Boot using custom keys can be added to the official ISO by simply extracting the boot loader (BOOTx64.EFI and BOOTIA32.EFI), kernel, UEFI shell, signing them and then repacking the ISO with the signed files.

First extract the relevant files and El Torito boot images:

xorrisofs(1) option -rational-rock as used by mkarchiso makes the files on ISO 9660 read-only which persists after extracting them. Make the files writable so that they can be modified:

Sign the files. To do so with sbsigntools, for example:

Copy the signed EFI binaries to eltorito_img2_uefi.img. It will be used as the EFI system partition and will be listed as an El Torito UEFI boot image. The size of eltorito_img2_uefi.img is fixed, but there are 8 MiB free space added by mkarchiso (for the purposes of rounding/alignment, to account for reserved sectors, etc.), so the size increase from the signatures should not be an issue.

Repack the ISO using the modified El Torito UEFI boot image and add the signed EFI binaries to ISO 9660:

Boot the resulting archlinux-YYYY.MM.DD-x86_64-Secure_Boot.iso.

Another way to add Secure Boot support to the official ISO is by extracting the boot loader and replacing it with #PreLoader.

First, extract the boot loader and the El Torito boot image:

Replace the BOOTx64.efi file with PreLoader:

Add the new files to the boot image:

Finally, repack the ISO using the modified boot image and the new boot loader files:

Support for Secure Boot using shim with a Machine Owner Key (MOK) can be added to the official ISO by extracting the boot loader, kernel and UEFI shell, signing them and then repacking the ISO with the signed files and shim.

First extract the relevant files and El Torito boot images. The boot loader file name will need to be grubx64.efi so that shim can find it.

xorrisofs(1) option -rational-rock as used by mkarchiso makes the files on ISO 9660 read-only which persists after extracting them. Make the files writable so that they can be modified:

Sign the files with your MOK:

Acquire pre-signed shim EFI binaries, e.g. by installing shim-signedAUR. Place shim and MokManager in the current directory and change the shim EFI binary's file name to BOOTx64.EFI:

Copy shim, MokManager, the signed EFI binaries and the DER format MOK to eltorito_img2_uefi.img. It will be used as the EFI system partition and will be listed as an El Torito UEFI boot image. The size of eltorito_img2_uefi.img is fixed, but there are 8 MiB free space added by mkarchiso (for the purposes of rounding/alignment, to account for reserved sectors, etc.), so the size increase from the added files should not be an issue.

Repack the ISO using the modified El Torito UEFI boot image and add shim, MokManager, the signed EFI binaries and the DER format MOK to ISO 9660:

Boot the resulting archlinux-YYYY.MM.DD-x86_64-Secure_Boot.iso. When MokManager launches, select Enroll key from disk > ARCHISO_EFI > MOK.cer. After enrolling the key, reboot and at the next boot, the live environment will successfully boot.

Option ROMs (OpROMs), i.e. device firmware that is executed during boot, must be signed for Secure Boot otherwise the devices will not be initialized. Typically OpROMs are signed with the Microsoft 3rd Party UEFI CA certificate which may prevent implementing Secure Boot with only your own keys. To solve this, the SHA256 digests of OpROMs can be enrolled instead.

On systems with a TPM, it is possible to acquire Option ROM SHA256 digests from the TPM event log.

Install tpm2-tools and digest-to-efi-sig-listAUR.

Use tpm2_eventlog(1) to read /sys/kernel/security/tpm0/binary_bios_measurements and look for the digests of BOOT_SERVICES_DRIVER.[8]

This will print an easy to parse list of digests:

Use digest-to-efi-sig-listAUR to create an EFI signature list for each OpROM digest you find:

In case you have multiple OpROMs, combine their EFI signature lists into one so that it could be signed as a single file:

You could use a bash script like this if you have a lot of digests.

Sign the EFI signature list for appending to the Signature Database:

The final and most dangerous step is to remove Microsoft 3rd Party UEFI CA certificate from the Signature Database and see if the system still boots and all devices still work.

**Examples:**

Example 1 (unknown):
```unknown
System:
      Firmware: UEFI 2.80 (American Megatrends 5.26)
 Firmware Arch: x64
   Secure Boot: enabled (user)
  TPM2 Support: yes
  Measured UKI: yes
  Boot into FW: supported
...
```

Example 2 (unknown):
```unknown
disabled (setup)
```

Example 3 (unknown):
```unknown
disabled (disabled)
```

Example 4 (unknown):
```unknown
disabled (unsupported)
```

---

## systemd/User

**URL:** https://wiki.archlinux.org/title/User_unit

**Contents:**
- How it works
- Basic setup
  - Environment variables
    - systemd user instance
    - Service example
    - Re-using the shell login environment
    - DISPLAY and XAUTHORITY
    - PATH
    - pam_env
  - Automatic start-up of systemd user instances

systemd offers the ability to manage services under the user's control with a per-user systemd instance, enabling them to start, stop, enable, and disable their own user units. This is convenient for daemons and other services that are commonly run for a single user, such as mpd, or to perform automated tasks like fetching mail.

As per default configuration in /etc/pam.d/system-login, the pam_systemd module automatically launches a systemd --user instance when the user logs in for the first time. This process will survive as long as there is some session for that user, and will be killed as soon as the last session for the user is closed. When #Automatic start-up of systemd user instances is enabled, the instance is started on boot and will not be killed. The systemd user instance is responsible for managing user services, which can be used to run daemons or automated tasks with all the benefits of systemd, such as socket activation, timers, dependency system, and strict process control via cgroups.

Similar to system units, user units are located in the following directories (ordered by ascending precedence):

When a systemd user instance starts, it brings up the per user target default.target. Other units can be controlled manually with systemctl --user. See systemd.special(7) § UNITS MANAGED BY THE USER SERVICE MANAGER.

All the user units will be placed in ~/.config/systemd/user/. If you want to start units on first login, execute systemctl --user enable unit for any unit you want to be autostarted.

Units started by user instance of systemd do not inherit any of the environment variables set in places like .bashrc etc. There are several ways to set environment variables for them:

One variable you may want to set is PATH.

After configuration, the command systemctl --user show-environment can be used to verify that the values are correct. You may need to run systemctl --user daemon-reload for changes to take effect immediately.

The above only addresses default environment variables for user units. However, the systemd user instance itself is also affected by some environment variables. In particular, certain specifiers (see systemd.unit(5) § SPECIFIERS) are affected by XDG variables.

However, the systemd user instance will only use environment variables that are set when it is started. In particular, it will not try parsing files, see upstream bug #29414 (closed WONTFIX). Therefore, if such environment variables are needed, they should be set in a drop-in configuration file, see #Service example.

systemd does not provide introspection tools to check these values, however, something like the following service can be used to help checking that the specifiers expand as expected:

Create the drop-in directory /etc/systemd/system/user@.service.d/ and inside create a file that has the extension .conf (e.g. local.conf):

If you normally set your environment through the shell login mechanisms (i.e. in ~/.profile, ~/.bash_profile, ~/.zprofile, or similar), the shell login environment can be read into a systemd user instance using the systemd.environment-generator(7) logic (as above). Create the following script:

The script invokes your $SHELL as a login shell, and dumps the resulting environment, while removing ephemeral shell variables. This is executed only once, on manager start, and can be reloaded on demand, using systemctl --user daemon-reload.

It provides the same environment block one gets with a non-interactive login shell — the same environment one would see after loging in through Getty or SSH, but not including anything set in ~/.bashrc, ~/.zshrc, and friends — including the system-wide environment from /etc/profile and /etc/profile.d. This is similar to what e.g. gnome-shell does, which is starting a login shell, and updating systemd with the resulting environment.

DISPLAY is used by any X application to know which display to use and XAUTHORITY to provide a path to the user's .Xauthority file and thus the cookie needed to access the X server. If you plan on launching X applications from systemd units, these variables need to be set. systemd provides a script in /etc/X11/xinit/xinitrc.d/50-systemd-user.sh to import those variables into the systemd user session on X launch. [3] So unless you start X in a nonstandard way, user services should be aware of the DISPLAY and XAUTHORITY.

If you customize your PATH and plan on launching applications that make use of it from systemd units, you should make sure the modified PATH is set on the systemd environment. Assuming you set your PATH in .bash_profile, the best way to make systemd aware of your modified PATH is by adding the following to .bash_profile after the PATH variable is set:

Environment variables can be made available through use of the pam_env.so module. See Environment variables#Using pam_env for configuration details.

The systemd user instance is started after the first login of a user and killed after the last session of the user is closed. Sometimes it may be useful to start it right after boot, and keep the systemd user instance running after the last session closes, for instance to have some user process running without any open session. Lingering is used to that effect. Use the following command to enable lingering for your own user, if polkit is installed:

Without polkit or to enable lingering for a different user:

To list all users which have the permit for lingering see column "LINGER" with yes:

or inspect /var/lib/systemd/linger. To revoke lingering:

See systemd#Writing unit files for general information about writing systemd unit files.

The following is an example of a user version of the mpd service:

The factual accuracy of this article or section is disputed.

The following is a user service used by foldingathomeAUR, which takes into account variable home directories where Folding@home can find certain files:

As detailed in systemd.unit(5) § SPECIFIERS, the %h variable is replaced by the home directory of the user running the service. There are other variables that can be taken into account in the systemd manpages.

The journal for the user can be read using the analogous command:

To specify a unit, one can use

systemd-tmpfiles allows users to manage custom volatile and temporary files and directories just like in the system-wide way (see systemd#systemd-tmpfiles - temporary files). User-specific configuration files are read from ~/.config/user-tmpfiles.d/ and ~/.local/share/user-tmpfiles.d/, in that order. For this functionality to be used, it is needed to enable the necessary systemd user units for your user:

The syntax of the configuration files is the same than those used system-wide. See the systemd-tmpfiles(8) and tmpfiles.d(5) man pages for details.

This article or section needs expansion.

There are several ways to run xorg within systemd units. Below there are 3 options, either by starting a new user session with an xorg process, launching xorg from a systemd user service, or launching xinit and application as a service.

Alternatively, xorg can be run from within a systemd user service. This is nice since other X-related units can be made to depend on xorg, etc, but on the other hand, it has some drawbacks explained below.

xorg-server provides integration with systemd in two ways:

Unfortunately, to be able to run xorg in unprivileged mode, it needs to run inside a session. So, right now the handicap of running xorg as user service is that it must be run with root privileges (like before 1.16), and cannot take advantage of the unprivileged mode introduced in 1.16.

This is how to launch xorg from a user service:

1. Make xorg run with root privileges for any user, by editing /etc/X11/Xwrapper.config. This builds on Xorg#Xorg as Root by adding the stipulation that this need not be done from a physical console. That is, allowed_user's default of console is being overwritten with anybody; see Xorg.wrap(1).

2. Add the following units to ~/.config/systemd/user

where ${XDG_VTNR} is the virtual terminal where xorg will be launched, either hard-coded in the service unit, or set in the systemd environment with

3. Make sure to configure the DISPLAY environment variable as explained above.

4. Then, to enable socket activation for xorg on display 0 and tty 2 one would do:

Now running any X application will launch xorg on virtual terminal 2 automatically.

The environment variable XDG_VTNR can be set in the systemd environment from .bash_profile, and then one could start any X application, including a window manager, as a systemd unit that depends on xorg@0.socket.

The factual accuracy of this article or section is disputed.

The service below is an example to run xinit and mate-session with user privilege.

To run a window manager as a systemd service, you first need to run #Xorg as a systemd user service. In the following we will use awesome as an example:

Rather than logging you into a window manager session for your user session by default, you may want to automatically run a terminal multiplexer (such as screen or tmux) in the background.

Create the following:

Separating login from X login is most likely only useful for those who boot to a TTY instead of to a display manager (in which case you can simply bundle everything you start in mystuff.target).

The dependency cruft.target, like the mystuff.target above, allows starting anything which should run before the multiplexer starts (or which you want started at boot regardless of timing), such as a GnuPG daemon session.

You then need to create a service for your multiplexer session. Here is a sample service, using tmux as an example and sourcing a gpg-agent session which wrote its information to /tmp/gpg-agent-info. This sample session, when you start X, will also be able to run X programs, since $DISPLAY is set:

Enable tmux.service, multiplexer.target and any services you created to be run by cruft.target, start user@.service as usual and you should be done.

Arch Linux builds the systemd package with --without-kill-user-processes, setting KillUserProcesses to no by default. This setting causes user processes not to be killed when the user logs out. To change this behavior in order to have all user processes killed on the user's logout, set KillUserProcesses=yes in /etc/systemd/logind.conf.

Note that changing this setting breaks terminal multiplexers such as tmux and GNU Screen. If you change this setting, you can still use a terminal multiplexer by using systemd-run as follows:

For example, to run screen you would do:

Using systemd-run will keep the process running after logout only while the user is logged in at least once somewhere else in the system and user@.service is still running.

After the user logs out of all sessions, user@.service will be terminated too, by default, unless the user has "lingering" enabled [9]. To effectively allow users to run long-term tasks even if they are completely logged out, lingering must be enabled for them. See #Automatic start-up of systemd user instances and loginctl(1) for details.

If you see errors such as this and your login session is broken, it is possible that another system (non-user) service on your system is creating this directory. This can happen for example if you use a docker container that has a bind mount to /run/user/1000. To fix this, you can either fix the container by removing the mount, or disable/delay the docker service.

If you see this message during shutdown, usually with a 2 minute timeout, it means that one of the user services did not stop in a timely manner. This can be caused by a misbehaving application which spawned a transient service earlier. You can simply wait for the timeout to expire, but if this bothers you, you can either create an override for the misbehaving service or reduce the global timeout for all user services.

To troubleshoot this problem, start the systemd debug shell:

Then, reboot or shut down the system. When the problem occurs, switch to the debug shell using Ctrl+Alt+F9. To find out which service is preventing the shutdown, run:

For most open source applications, this problem should be reported to the respective maintainers such that an override isn't necessary. For closed source applications, however, an override can be created like so:

This will shorten the timeout of that particular service to 1 second. The --force parameter is only required for transient services which do not create a .service file on disk. The override will work regardless. Instead of the timeout, KillSignal=SIGKILL can be used. This will cause the service to be killed immediately when the user manager is stopped. Only use this if you know the service can handle it.

If you don't care which service is preventing the shutdown, you can change the global timeout for all user services in a similar manner:

After this timeout, any user services which haven't gracefully stopped will be killed, which is equivalent to a sudden power loss. Adjust this value for your particular use case. Setting the timeout too low may cause data corruption depending on the application.

**Examples:**

Example 1 (unknown):
```unknown
/etc/pam.d/system-login
```

Example 2 (unknown):
```unknown
pam_systemd
```

Example 3 (unknown):
```unknown
systemd --user
```

Example 4 (unknown):
```unknown
/usr/lib/systemd/user/
```

---

## init

**URL:** https://wiki.archlinux.org/title/Init

**Contents:**
- Inits (integrated)
- Inits
- Init scripts
- Service managers
- Configuration
  - Migrate running services
  - logind
  - Device permissions
  - Rootless X
  - Power management

Init is the first process started during system boot. It is a daemon process that continues running until the system is shut down. Init is the direct or indirect ancestor of all other processes, and automatically adopts all orphaned processes. It is started by the kernel using a hard-coded filename; if the kernel is unable to start it, panic will result. Init is typically assigned process identifier 1.

The init scripts (or rc) are launched by the init process to guarantee basic functionality on system start and shutdown. This includes (un)mounting of file systems and launching of daemons. A service manager takes this one step further by providing active control over launched processes, or process supervision. An example is to monitor for crashes and restart processes accordingly.

These components combine to the init system. Some inits include the service manager in the init process, or have init scripts in close relation to them. These inits are below referred to as integrated, though entries in different categories may explicitly depend on each other.

To run daemons under the new init, save a list of running daemons:

and configure the #Init scripts accordingly. See also [2].

logind requires systemd to be the init process. [3] As such, local sessions and other functionality is not available.

Add users to respective user groups for device access and reboot. Current group membership should first be checked with id user.

See also Users and groups#Pre-systemd groups. To create group rules for use with Polkit, see Polkit#Bypass password prompt.

The factual accuracy of this article or section is disputed.

As Xorg.wrap does not check if logind is active [4], root rights for Xorg need be enabled manually.

See pm-utilsAUR and acpid to replace Power management with systemd.

Arch uses timer files instead of cron by default. See archlinux-cronjobs[dead link 2025-08-15—HTTP 404] for basic cron jobs.

This article or section needs expansion.

User instances of dbus-daemon are launched by systemd/User [5]. When requiring IPC between desktop applications, restore 30-dbus.sh:

systemd-nspawn is a tool for systemd systems. Since Linux 2.6.19, it is possible, however, to run systemd on a non-systemd system by using PID namespace. For it, the kernel needs to be configured with CONFIG_PID_NS and CONFIG_NAMESPACES).

The PID namespace creates a new hierarchy of processes starting with PID 1. In addition to this, systemd requires a chrooted root filesystem to be mounted. Hence, you have to at least make a bind mount, because otherwise some services will fail with

as systemd tries to remount the root with private option.

To setup a chroot with a new PID namespace, you can use jchroot.[6] [7]. Make sure not to mount /proc inside the new root before chrooting, otherwise systemd will detect the chroot environment. You can mount it later once systemd is running.

**Examples:**

Example 1 (unknown):
```unknown
$ systemctl list-units --state=running "*.service" > daemons.list
```

Example 2 (unknown):
```unknown
# usermod -a -G video,audio,power,disk,storage,optical,lp,scanner,input user
```

Example 3 (unknown):
```unknown
/etc/X11/xinit/xinitrc.d/30-dbus.sh
```

Example 4 (unknown):
```unknown
#!/bin/bash

# launches a session dbus instance
if [ -z "${DBUS_SESSION_BUS_ADDRESS-}" ] && type dbus-launch >/dev/null; then
  eval $(dbus-launch --sh-syntax --exit-with-session)
fi
```

---

## Help:Reading

**URL:** https://wiki.archlinux.org/title/Daemon-reload

**Contents:**
- Organization
- Formatting
- Root, regular user or another user
- Append, add, create, edit
  - Make executable
- Source
- Installation of packages
  - Official packages
  - Arch User Repository
- Control of systemd units

Because the vast majority of the ArchWiki contains indications that may need clarification for users new to Arch Linux (or GNU/Linux in general), this rundown of basic procedures was written both to avoid confusion in the assimilation of the articles and to deter repetition in the content itself.

Most articles on the ArchWiki do not attempt to provide a holistic introduction to a single topic; they are instead written in adherence to the "Don't Repeat Yourself" principle, under the assumption that the user will seek out and read any supporting material that they do not yet understand. Where possible, such supporting material is indicated in the article via special formatting, see #Formatting.

Because of this organization, it may be necessary to examine several related sources in order to fully understand an ArchWiki article. In particular, users who are new to Arch (or GNU/Linux in general) should expect to end up reading a great number of articles even when solving simple problems. It is especially important to study the supporting material before seeking additional help from other users.

Some lines are written like so:

Others have a different prefix:

The numeral or hash sign (#) indicates that the command needs to be run as root, whereas the dollar sign ($) shows that the command should be run as a regular user.

When the commands need to run as a specific user, they will be prefixed by the username in square brackets, for example:

This means you should use a privilege elevation tool, e.g. with sudo:

A notable exception to watch out for:

In this example, the context surrounding the numeral sign communicates that this is not to be run as a command; it should be edited into a file instead. So in this case, the numeral sign denotes a comment. A comment can be explanatory text that will not be interpreted by the associated program. Bash scripts denotation for comments happens to coincide with the root PS1.

After further examination, "give away" signs include the uppercase character following the # sign. Usually, Unix commands are not written this way and most of the time they are short abbreviations instead of full-blown English words (e.g., Copy becomes cp).

Regardless, most articles make this easy to discern by notifying the reader:

Append to ~/path/to/file:

When prompted to append to, add to, create, or edit one or more files, it is implied that you should use one of the following methods.

To create or modify multiline files, it is suggested to use a text editor. For example, using the nano command to edit the file /etc/bash.bashrc is:

To create or overwrite a file from a string, it may be simpler to use output redirection. The following example creates or overwrites the contents of the file /etc/hostname with the text myhostname.

Output redirection can also be used to append a string to a file. The following example appends the text [custom-repo] to the file /etc/pacman.conf.

When prompted to create directories, use the mkdir command:

After creating a file, if it is meant to be run as a script (whether manually or called by another program), it needs to be set as executable, for example with:

See chmod. Some applications such as file managers may provide graphical interfaces to do the same.

Some applications, notably command-line shells, use scripts for their configuration: after modifying them, they must be sourced in order for the changes to be applied. In the case of bash, for example, this is done by running (you can also replace source with .):

When the wiki suggests modifying such a configuration script, it will not explicitly remind you to source the file, and only in some cases will it point to this section with a reminder link.

When an article invites you to install some packages in the conventional way, it will not indicate the detailed instructions to do so; instead, it will simply mention the names of the packages to be installed.

The subsections below give an overview of the generic installation procedures depending on the package type.

For packages from the official repositories, you will read something like:

This means that you have to run:

The pacman article contains detailed explanations to deal with package management in Arch Linux proficiently.

For packages from the Arch User Repository (AUR), you will read something like:

This means that in general you have to follow the foobarAUR link, download the PKGBUILD archive, extract it, verify the content and finally run, in the same folder:

The Arch User Repository article contains all the detailed explanations and best practices to deal with AUR packages.

When an article invites to start, enable, etc., some systemd unit (e.g. a service), it will not indicate the detailed instructions to do so, but instead you will read something like:

This means that you have to run:

A notable command that does not follow this exact pattern is systemctl daemon-reload which will be called without arguments.

The systemd#Using units section contains structured list of available actions (like start, enable, enable and start, etc.) with their corresponding systemctl commands.

It is important to remember that there are two different kinds of configurations on a GNU/Linux system. System-wide configuration affects all users. Since system-wide settings are generally located in the /etc directory, root privileges are required in order to alter them. For example, to apply a Bash setting that affects all users, /etc/bash.bashrc should be modified.

User-specific configuration affects only a single user. Dotfiles are used for user-specific configuration. For example, the file ~/.bashrc is the user-specific configuration file. The idea is that each user can define their own settings, such as aliases, functions and other interactive features like the prompt, without affecting other users' preferences.

Bash and other Bourne-compatible shells, such as Zsh, also source files depending on whether the shell is a login shell or an interactive shell. See Bash#Configuration files and Zsh#Startup/Shutdown files for details.

Some code blocks may contain so-called pseudo-variables, which, as the name says, are not actual variables used in the code. Instead they are generic placeholders and have to be manually replaced with system-specific configuration items before the code may be run or parsed. Common shells such as bash and zsh provide tab-completion to auto-complete parameters for common commands such as systemctl.

In the articles that comply with Help:Style/Formatting and punctuation, pseudo-variables are formatted in italics. For example:

In this case interface_name is used as a pseudo-variable placeholder in a systemd template unit. All systemd template units, identifiable by the @ sign, require a system-specific configuration item as argument. See systemd#Using units.

In this case the pseudo-variables are used to describe the parameters that must be substituted for them. Details on how to gather them are elaborated on in the section Securely wipe disk#Calculate blocks to wipe manually, which features the command.

This article or section needs expansion.

In case of file examples, pasting pseudo-variables in real configuration files might break the programs that use them.

In most cases, ellipses (...) are not part of the actual file content or code output, and instead represent omitted or optional text that is not relevant for the discussed subject.

For example HOOKS=(... encrypt ... filesystems ...) or:

Be aware though that, in a few instances, ellipses may be a meaningful part of the code syntax: attentive users should be able to recognize these cases by the context.

**Examples:**

Example 1 (unknown):
```unknown
# mkinitcpio -p linux
```

Example 2 (unknown):
```unknown
$ makepkg -s
```

Example 3 (unknown):
```unknown
sudo command
```

Example 4 (unknown):
```unknown
[postgres]$ initdb -D /var/lib/postgres/data
```

---

## Overlay filesystem

**URL:** https://wiki.archlinux.org/title/Overlay_filesystem

**Contents:**
- Installation
- Usage
  - Read-only overlay
- See also

From the initial kernel commit:

Overlayfs has been in the Linux kernel since 3.18.

Overlayfs is enabled in the default kernel and the overlay kernel module is automatically loaded upon issuing a mount command.

To mount an overlay use the following mount options:

The lower directory can actually be a list of directories separated by :, all changes in the merged directory are still reflected in upper.

The above example will have the order:

To add an overlayfs entry to /etc/fstab use the following format:

The noauto and x-systemd.automount mount options are necessary to prevent systemd from hanging on boot because it failed to mount the overlay. The overlay is now mounted whenever it is first accessed and requests are buffered until it is ready. See fstab#Automount with systemd.

Sometimes, it is only desired to create a read-only view of the combination of two or more directories. In that case, it can be created in an easier manner, as the directories upper and work are not required:

When upperdir is not specified, the overlay is automatically mounted as read-only.

**Examples:**

Example 1 (unknown):
```unknown
# mount -t overlay overlay -o lowerdir=/lower,upperdir=/upper,workdir=/work /merged
```

Example 2 (unknown):
```unknown
# mount -t overlay overlay -o lowerdir=/lower1:/lower2:/lower3,upperdir=/upper,workdir=/work /merged
```

Example 3 (unknown):
```unknown
/upper
/lower1 
/lower2
/lower3
```

Example 4 (unknown):
```unknown
overlay /merged overlay noauto,x-systemd.automount,lowerdir=/lower,upperdir=/upper,workdir=/work 0 0
```

---

## File systems

**URL:** https://wiki.archlinux.org/title/Mount

**Contents:**
- Types of file systems
  - Journaling
  - FUSE-based file systems
  - Stackable file systems
  - Read-only file systems
  - Clustered file systems
  - Shared-disk file system
- Identify existing file systems
- Create a file system
- Mount a file system

Individual drive partitions can be set up using one of the many different available file systems. Each has its own advantages, disadvantages, and unique idiosyncrasies. A brief overview of supported file systems follows; the links are to Wikipedia pages that provide much more information.

See filesystems(5) for a general overview and Wikipedia:Comparison of file systems for a detailed feature comparison. File systems already loaded by the kernel or built-in are listed in /proc/filesystems, while all the installed modules can be seen with ls /lib/modules/$(uname -r)/kernel/fs.

xfs.html xfs-delayed-logging-design.html xfs-self-describing-metadata.html

The ext3/4, HFS+, JFS, NTFS, ReiserFS, and XFS file systems use journaling. Journaling provides fault-resilience by logging changes before they are committed to the file system. In the event of a system crash or power failure, such file systems are faster to bring back online and less likely to become corrupted. The logging takes place in a dedicated area of the file system.

ext3/4 offer data-mode journaling, which can optionally log data in addition to the metadata. Data-mode journaling comes with a speed penalty, because it does two write operations: first to the journal and then to the disk. Therefore, data-mode journaling is not enabled by default. The trade-off between system speed and data safety should be considered when choosing the file system type and features.

In the same vein, Reiser4 offers configurable "transaction models": a special model called wandering logs, which eliminates the need to write to the disk twice; write-anywhere, a pure copy-on-write approach; and a combined approach called hybrid which heuristically alternates between the two.

File systems based on copy-on-write (also known as write-anywhere), such as Reiser4, Btrfs, Bcachefs and ZFS, by design operate on full atomicity and also provide checksums for both metadata and inline data (operations entirely occur, or they entirely do not, and in properly functioning hardware data does not corrupt due to operations half-occurring). Therefore, these file systems are by design much less prone to data loss than other file systems and have no need to use traditional journal to protect metadata, because they are never updated in-place. Although Btrfs still has a journal-like log tree, it is only used to speed-up fdatasync/fsync.

FAT, exFAT, ext2, and HFS provide neither journaling nor atomicity, They are for temporary or legacy use and not recommended for use when reliable storage is needed.

To identify existing file systems, you can use lsblk:

An existing file system, if present, will be shown in the FSTYPE column. If mounted, it will appear in the MOUNTPOINT column.

File systems are usually created on a partition, inside logical containers such as LVM, RAID and dm-crypt, or on a regular file (see Wikipedia:Loop device). This section describes the partition case.

Before continuing, identify the device where the file system will be created and whether or not it is mounted. For example:

Mounted file systems must be unmounted before proceeding. In the above example an existing file system is on /dev/sda2 and is mounted at /mnt. It would be unmounted with:

To find just mounted file systems, see #List mounted file systems.

To create a new file system, use mkfs(8). See #Types of file systems for the exact type, as well as userspace utilities you may wish to install for a particular file system.

For example, to create a new file system of type ext4 (common for Linux data partitions) on /dev/sda1, run:

The new file system can now be mounted to a directory of choice.

To manually mount a file system located on a device (e.g., a partition) to a directory, use mount(8). This example mounts /dev/sda1 to /mnt.

This attaches the file system on /dev/sda1 at the directory /mnt, making the contents of the file system visible. Any data that existed at /mnt before this action is made invisible until the device is unmounted.

fstab contains information on how devices should be automatically mounted if present. See the fstab article for more information on how to modify this behavior.

If a device is specified in /etc/fstab and only the device or mount point is given on the command line, that information will be used in mounting. For example, if /etc/fstab contains a line indicating that /dev/sda1 should be mounted to /mnt, then the following will automatically mount the device to that location:

mount contains several options, many of which depend on the file system specified. The options can be changed, either by:

See these related articles and the article of the file system of interest for more information.

To list all mounted file systems, use findmnt(8):

findmnt takes a variety of arguments which can filter the output and show additional information. For example, it can take a device or mount point as an argument to show only information on what is specified:

findmnt gathers information from /etc/fstab, /etc/mtab, and /proc/self/mounts.

To unmount a file system use umount(8). Either the device containing the file system (e.g., /dev/sda1) or the mount point (e.g., /mnt) can be specified:

Unmount the file system and run fsck on the problematic volume.

**Examples:**

Example 1 (unknown):
```unknown
/proc/filesystems
```

Example 2 (unknown):
```unknown
ls /lib/modules/$(uname -r)/kernel/fs
```

Example 3 (unknown):
```unknown
NAME   FSTYPE LABEL     UUID                                 MOUNTPOINT
sdb
└─sdb1 vfat   Transcend 4A3C-A9E9
```

Example 4 (unknown):
```unknown
NAME   FSTYPE   LABEL       UUID                                 MOUNTPOINT
sda
├─sda1                      C4DA-2C4D
├─sda2 ext4                 5b1564b2-2e2c-452c-bcfa-d1f572ae99f2 /mnt
└─sda3                      56adc99b-a61e-46af-aab7-a6d07e504652
```

---

## udev

**URL:** https://wiki.archlinux.org/title/Udev_rule

**Contents:**
- Installation
- Introduction to udev rules
  - Example
  - List the attributes of a device
  - Testing rules before loading
  - Loading new rules
- Components of udev rules
  - Action matching
    - The change action
  - Device attributes

udev (user space /dev) is a user space system that enables the operating system administrator to register user space handlers for events. The events received by udev's daemon are mainly generated by the Linux kernel in response to physical events relating to peripheral devices.

As such, udev main purpose is to act upon peripheral detection and hot-plugging, including actions that return control to the kernel, e.g. loading kernel modules or device firmware. Another component of this detection is adjusting the permissions of the device to be accessible to non-root users and groups. udev manages device nodes in the /dev directory by adding, symlinking and renaming them.

udev is part of systemd and thus installed by default. See systemd-udevd.service(8) for more information.

udev rules written by the administrator go in /etc/udev/rules.d/, their file name has to end with .rules. The udev rules shipped with various packages are found in /usr/lib/udev/rules.d/. If there are two files by the same name under /usr/lib and /etc, the ones in /etc take precedence.

To learn about udev rules, refer to the udev(7) manual. Also see Writing udev rules and some practical examples are provided within the guide: Writing udev rules - Examples.

Below is an example of a rule that creates a symlink /dev/video-cam when a webcamera is connected.

Let say this camera is currently connected and has loaded with the device name /dev/video2. The reason for writing this rule is that at the next boot, the device could show up under a different name, like /dev/video0.

To identify the webcamera, from the video4linux device we use KERNEL=="video2" and SUBSYSTEM=="video4linux", then walking up two levels above, we match the webcamera using vendor and product ID's from the usb parent SUBSYSTEMS=="usb", ATTRS{idVendor}=="05a9" and ATTRS{idProduct}=="4519". Note that this matching is case sensitive, so "05A9" can not be used to match the idVendor in this example.

We are now able to create a rule match for this device as follows:

Here we create a symlink using SYMLINK+="video-cam" but we could easily set user OWNER="john" or group using GROUP="video" or set the permissions using MODE="0660".

If you intend to write a rule to do something when a device is being removed, be aware that device attributes may not be accessible. In this case, you will have to work with preset device environment variables. To monitor those environment variables, execute the following command while unplugging your device:

In this command's output, you will see value pairs such as ID_VENDOR_ID and ID_MODEL_ID, which match the previously used attributes idVendor and idProduct. A rule that uses device environment variables instead of device attributes may look like this:

To get a list of all of the attributes of a device you can use to write rules, run this command:

Replace device_name with the device present in the system, such as /dev/sda or /dev/ttyUSB0.

If you do not know the device name you can also list all attributes of a specific system path:

To narrow down the search for a device, figure out the class and run:

You can use the symlink outright or what it points as the input to --name. For example:

To get the path of a bare USB device which does not populate any subordinate device you have to use the full USB device path. Start monitor mode and then plug in the USB device to get it:

You can just choose the deepest path and --attribute-walk will show all parent's attributes anyway:

This will not perform all actions in your new rules but it will however process symlink rules on existing devices which might come in handy if you are unable to load them otherwise. You can also directly provide the path to the device you want to test the udev rule for:

udev automatically detects changes to rules files, so changes take effect immediately without requiring udev to be restarted. However, the rules are not re-triggered automatically on already existing devices. Hot-pluggable devices, such as USB devices, will probably have to be reconnected for the new rules to take effect, or at least unloading and reloading the ohci-hcd and ehci-hcd kernel modules and thereby reloading all USB drivers.

If rules fail to reload automatically:

To manually force udev to trigger your rules:

ACTION=="" is used to only match against a device when something specific is happening, usually when it is appearing or disappearing. There are eight types of actions that can be raised for udev rules to match against:

The change action is somewhat special, because not all drivers use it the same way, if at all. A change event is only emitted as a result of the driver manually raising a userspace event (or uevent) of type KOBJ_CHANGE. This normally signals that something has happened to that device, but it takes some additional information to determine what exactly.

Since that event is only raised under a limited number of situations, here is a (best-effort, non-exhaustive) list of subsystems that do raise change events, and under what conditions.

Device attributes, listed as ATTR{my_attr}=="..." (or ATTRS{my_attr}=="..." for parent devices) in udev rules and udevadm info --attribute-walk, correspond to all the device information that is made available through sysfs, which exposes information from the "kobject" classes used by the kernel to keep track of device state. This means that there is no need for udev-specific tools to examine these, and that a simple ls /sys/class/thermal/thermal_zone0/ or cat /sys/class/input/input0/name is all that is needed to explore device attributes, and some can even be changed with something like echo 1 > "/sys/class/leds/input2::capslock/brightness". It is, however, a lot more conveinient to use udevadm info --attribute-walk /sys/class/input/mouse0, which has the advantage of showing all matchable attributes for that device all at once in the same format that udev rules expect (as well as the attributes for any parent devices, also in their readily usable format).

Most of these attributes have well-documented meanings, behaviors and accepted values based on what kernel submodule is handling the device, such as sysfs-bus-usb for USB devices and sysfs-class-typec for USB type-C ports, for instance. Other attributes are present as subtrees, such as sysfs-devices-physical_location or sysfs-devices-power, which use slashes to separate levels, just like directories, giving ATTR{power/control}=="auto".

The "environment" of an event is a set of device properties and event properties (and rarely global properties), both written as ENV{MY_PROPERTY}=="..." in udev rules and both reported as E: MY_PROPERTY=... by udevadm monitor --property and udevadm info (without --attribute-walk). Despite being called "environment", they have nothing to do with environment variables (although they do get passed as environment variables to programs started by RUN+="..."). These contain context information added to an event by the kernel module or other udev rules to make that information available to downstream rules or components. The only difference between event properties and device properties is that devices properties are stored and can be examined with udevadm info, whereas event properties are transient and can only be seen if the event is caught by udevadm monitor --property. A lot of device properties are also available as device attributes with similar names.

Unlike with attributes, udev rules are allowed to set event properties arbitrarily, and there is also no concept of "parent properties" to inspect beyond the ones that are set on the event (which is why there is no ENVS{...}=="" directive). Note that setting a property with a udev rule sets a device property, which will be stored until the device is removed and will thus appear in every event raised by that device (unless the property's name starts with a period, like ENV{.PART_SUFFIX}, which will be added to the event properties with the leading dot in the name and be usable by other rules, but not stored).

Tags (added using TAG+="foo", removed with TAG-="foo", matched with TAG=="foo") are used by userspace software interacting with udev to list and identify all devices they should be acting upon. Those can also be arbitrary, but there are a few well-known ones.

To mount removable drives, do not call mount from udev rules. This is ill-advised for two reasons:

There are some options that work:

Programs started by udev will block further events from that device, and any tasks spawned from a udev rule will be killed after event handling is completed. If you need to spawn a long-running process with udev, the intended way is to have a systemd unit which handles running the actual command, and a udev rule that merely signals that this unit should run. However, using systemctl in a udev rule is discouraged, since it is meant for user interaction and may block, among other things.

The correct way of doing this is to have the rule tag the device as needing a systemd device unit (see systemd.device(5)) using TAG+="systemd" and adding an device property of either ENV{SYSTEMD_WANTS}+= for services that would run with systemctl --system or ENV{SYSTEMD_USER_WANTS}+= for services that should run with systemctl --user. For example:

SYSTEMD_WANTS is equivalent to the Wants= directive elsewhere in systemd, meaning the device will not be affected if the service fails, does not exists, or completes successfully at any point.

When a kernel driver initializes a device, the default state of the device node is to be owned by root:root, with permissions 600. [1] This makes devices inaccessible to regular users unless the driver changes the default, or a udev rule in user space changes the permissions.

The OWNER, GROUP, and MODE udev values can be used to provide access, though one encounters the issue of how to make a device usable to all users without an overly permissive mode. Ubuntu's approach is to create a plugdev group that devices are added to, but this practice is not only discouraged by the systemd developers, [2] but considered a bug when shipped in udev rules on Arch (FS#35602). Another approach historically employed, as described in Users and groups#Pre-systemd groups, is to have different groups corresponding to categories of devices.

The modern recommended approach for systemd systems is to use a MODE of 660 to let the group use the device, and then attach a TAG named uaccess [3]. This special tag makes udev apply a dynamic user ACL to the device node, which coordinates with systemd-logind(8) to make the device usable to logged-in users. For an example of a udev rule implementing this:

Create the rule /etc/udev/rules.d/95-hdmi-plug.rules with the following content:

Create the rule /etc/udev/rules.d/95-monitor-hotplug.rules with the following content to launch arandr on plug in of a VGA monitor cable:

Some display managers store the .Xauthority outside the user home directory. You will need to update the ENV{XAUTHORITY} accordingly. As an example GNOME Display Manager looks as follows:

If your eSATA drive is not detected when you plug it in, there are a few things you can try. You can reboot with the eSATA plugged in. Or you could try:

Or you could install scsiaddAUR (from the AUR) and try:

Hopefully, your drive is now in /dev. If it is not, you could try the above commands while running:

to see if anything is actually happening.

If you connected an eSATA bay or another eSATA adapter, the system will still recognize this disk as an internal SATA drive. GNOME and KDE will ask you for your root password all the time. The following rule will mark the specified SATA-Port as an external eSATA-Port. With that, a normal GNOME user can connect their eSATA drives to that port like a USB drive, without any root password and so on.

Because udev loads all modules asynchronously, they are initialized in a different order. This can result in devices randomly switching names. A udev rule can be added to use static device names. See also Persistent block device naming for block devices and Network configuration#Change interface name for network devices.

For setting up the webcam in the first place, refer to Webcam setup.

Using multiple webcams will assign video devices as /dev/video* randomly on boot. The recommended solution is to create symlinks using a udev rule as in the #Example:

If you use multiple printers, /dev/lp[0-9] devices will be assigned randomly on boot, which will break e.g. CUPS configuration.

You can create the following rule, which will create symlinks under /dev/lp/by-id and /dev/lp/by-path, similar to Persistent block device naming scheme:

To perform some action on a specific disk device /dev/sdX identified permanently by its unique serial ID_SERIAL_SHORT as displayed with udevadm info /dev/sdX, one can use the below rule. It is passing as a parameter the device name found if any to illustrate:

A udev rule can be useful to enable the wakeup triggers of a USB device, like a mouse or a keyboard, so that it can be used to wake the system from sleep.

First, identify the vendor and product identifiers of the USB device. They will be used to recognize it in the udev rule. For example:

Then, find where the device is connected to using:

Now create the rule to change the power/wakeup attribute of both the device and the USB controller it is connected to whenever it is added:

USB devices need to be reset after exiting a suspended state (whether from the system resuming from sleep or from a port being turned off when the device was idle for power saving), which the Linux kernel handles mostly transparently in a process called reset-resume, so that USB drives do not look like they have been disconnected and reconnected every time. This is mostly desirable, but some devices, like USB TTY interfaces, do need to be manually reconfigured after being power cycled, which is not signalled by any uevent from the relevent drivers.

The one uevent that does get raised, however, is the one from losing and regaining power/wakeup, since USB devices are unusable while unconfigured and would be unable to wake up the system from sleep in this state. These two events have no unique event properties, but the first one can still be easily identified because DEVNUM is set to zero (which is not a valid device number) immediately before the device is unconfigured and loses power/wakeup, raising the uevent. When that happens, one can simply touch the bConfigurationValue sysfs attribute to force the system to reconfigure the device non-transparently, as if it had been disconnected during sleep, which unbinds all drivers and removes all children devices before adding them back when the device is ready again.

This article or section is a candidate for merging with #Testing rules before loading.

It can be useful to trigger various udev events. For example, you might want to simulate a USB device disconnect on a remote machine. In such cases, use udevadm trigger:

This command will trigger a USB remove event on all USB devices with vendor ID abcd.

To trigger a desktop notification from a udev rule, use systemd-run(1) as explained in Desktop notifications#Send notifications to another user:

To launch multiple or long commands, an executable script can be given to systemd-run:

In rare cases, udev can make mistakes and load the wrong modules. To prevent it from doing this, you can blacklist modules. Once blacklisted, udev will never load that module – not at boot-time and not even later on when a hot-plug event is received (e.g., you plug in your USB flash drive).

To get hardware debug info, use the kernel parameter udev.log-priority=debug. Alternatively you can set

This option can also be compiled into your initramfs by adding the configuration file to your FILES array:

and then regenerate the initramfs.

After migrating to LDAP or updating an LDAP-backed system, udevd can hang at boot at the message "Starting UDev Daemon". This is usually caused by udevd trying to look up a name from LDAP but failing, because the network is not up yet. The solution is to ensure that all system group names are present locally.

Extract the group names referenced in udev rules and the group names actually present on the system:

To see the differences, do a side-by-side diff:

In this case, the pcscd group is for some reason not present in the system. Add the missing groups. Also, make sure that local resources are looked up before resorting to LDAP. /etc/nsswitch.conf should contain the following line:

You need to create a custom udev rule for that particular device. To get definitive information of the device you can use either ID_SERIAL or ID_SERIAL_SHORT (remember to change /dev/sdb if needed):

Then we set UDISKS_AUTO="1" to mark the device for automounting and UDISKS_SYSTEM="0" to mark the device as "removable". See udisks(8) for details.

Remember to reload udev rules with udevadm control --reload. Next time you plug your device in, it will be treated as an external drive.

If the group ID of your optical drive is set to disk and you want to have it set to optical, you have to create a custom udev rule:

When xrandr or another X-based program tries to connect to an X server, it falls back to a TCP connection on failure. However, due to IPAddressDeny in the systemd-udev service configuration, this hangs. Eventually the program will be killed and event processing will resume.

If the rule is for a drm device and the hang causes event processing to complete once the X server has started, this can cause 3D acceleration to stop working with a failed to authenticate magic error.

**Examples:**

Example 1 (unknown):
```unknown
…/by-path/…
```

Example 2 (unknown):
```unknown
…/by-uuid/…
```

Example 3 (unknown):
```unknown
/etc/udev/rules.d/
```

Example 4 (unknown):
```unknown
/usr/lib/udev/rules.d/
```

---

## NILFS2

**URL:** https://wiki.archlinux.org/title/NILFS2

**Contents:**
- Creating a NILFS2 file system
  - Changing label
- Mounting
  - Mount options
    - Error handling
    - Discard
- Snapshots
  - Listing snapshots
  - Converting checkpoints to snapshots
  - Creating snapshots

NILFS2 (New Implementation of a Log-structured File System) is a log-structured file system supporting versioning of the entire file system and continuous snapshotting, which allows users to even restore files mistakenly overwritten or destroyed just a few seconds ago.

It was developed by Nippon Telegraph and Telephone Corporation (NTT) CyberSpace Laboratories and a community from all over the world. NILFS was released under the terms of the GNU General Public License (GPL).

NILFS2 is continously snapshoted, as such it can be considered versioning file system where file system automatically keeps past versions of every file.

This article assumes the device has partitions already setup. Install nilfs-utils. Use mkfs.nilfs2 to format the target partition referred to as /dev/sdxY:

See mkfs.nilfs2(8) for all available options.

Nilfs-tune command should be used only for unmounted file systems.

The file system can then be mounted manually or via other mechanisms:

errors=continue / remount-ro / panic Specifies what NILFS2 should do on file system error. Continue Ignores error, remount-ro remounts file system read-only and panic halts the system.

discard / nodiscard Enable/disable TRIM

See mount.nilfs2(8) for all available options.

NILFS2 has two types of snapshots, checkpoints and snapshots. They are often written as cp and ss respectivelly.

NILFS2 creates checkpoints on every write automatically. Checkpoint is snapshot that can be automatically deleted by NILFS2 garbage collector to free up space when file system is filled up or when user manually prompts it for cleaning.

Snapshots have to be manually converted from checkpoints and are never deleted automatically. They can be mounted to recover data from them.

Checkpoints and snapshots can be listed with:

To list only snapshots(ss) use -s flag with lscp. See lscp(1) for all available options.

In order to recover data from checkpoint it first has to be converted into snapshot. To do that use chcp utility:

checkpoint-number is number of the checkpoint listed in lscp in the CNO column.

lscp will now show line like:

To convert snapshot back to checkpoint use:

Use following to create a checkpoint:

Or to create a snapshot directly:

Following command will mount snapshot checkpoint-number read-only into /mnt/snapshot:

To delete a singular checkpoint use:

To delete a range of checkpoints you can use:

Where start is number of starting checkpoint and end the number of ending checkpoint.

To delete all checkpoints older than a checkpoint use:

To delete all checkpoints newer than a checkpoint use:

See rmcp(8) for further details.

To clear obsolete checkpoints from a mounted NILFS2 file system run:

See nilfs-clean(8) and nilfs_cleanerd(8) for further details on garbage collector.

nilfs-resize tool resizes online NILFS2 file system.

Mounted NILFS2 filesystem can be resized with following command:

Where desired-size can be suffixed by unit letter s, K, M, G, or T, for 512 byte sectors, kilobytes, megabytes, gigabytes, or terabytes, respectively. If desired-size is not specified, it will default to the size of partition.

nilfs-resize does not change size of paritions themselves, they either have to be shrink afterwards when shrinking the filesystem or grown beforehand when enlarging it. This can be done with fdisk for example.

See nilfs-resize(8) for further details.

**Examples:**

Example 1 (unknown):
```unknown
mkfs.nilfs2
```

Example 2 (unknown):
```unknown
# mkfs.nilfs2 -L mylabel /dev/sdxY
```

Example 3 (unknown):
```unknown
# nilfs-tune -L newlabel /dev/sdxY
```

Example 4 (unknown):
```unknown
# mount /dev/sdxY /mnt/foo
```

---

## LXDM

**URL:** https://wiki.archlinux.org/title/LXDM

**Contents:**
- Installation
- Configuration
  - Default session
    - Globally
    - Per user
  - Autologin
  - Last used options
- Tips and tricks
  - Adding face icons
  - Simultaneous users and switching users

LXDM is a lightweight display manager for the LXDE desktop environment.

LXDM does not support the XDMCP protocol. An alternative that does is LightDM.

Install the lxdm package.

Enable the provided lxdm.service unit to start LXDM at boot.

The configuration files for LXDM are all located in /etc/lxdm/. The main configuration file is lxdm.conf. Its format is documented in its comments. Another file, Xsession, is the systemwide x session configuration file and should generally not be edited. All other files in this directory are shell scripts, which are run when certain events happen in LXDM.

The default session can be set globally, as well as set at an individual-user level. Individual user preferences take precedence over globally set preferences for the user in question.

Edit /etc/lxdm/lxdm.conf and change the session line to whatever session or DE is desired:

Example using Openbox:

This is useful for themes that have no visible session selection box, and if experiencing trouble using autologin.

To define an individual user's preferred session, simply edit their respective ~/.dmrc to define the selection.

Example: user1 wants Xfce4, user2 wants Cinnamon, and user3 wants GNOME:

The list of installed sessions can be displayed by using command:

To log in to one account automatically on startup, without providing a password, find the line in /etc/lxdm/lxdm.conf that looks like this:

Uncomment it, substituting the target user instead of dgod.

Previously used LXDM options can be found in:

A 96x96 px image (jpg or png) can optionally be displayed on a per-user basis, replacing the stock icon. Copy or symlink the target image to $HOME/.face. The gnome-control-center package supplies some default icons suitable for the lxdm screen. Look under /usr/share/pixmaps/faces after installing that package.

LXDM allows multiple users to be logged into different TTYs simultaneously. The following command is used to allow another user to log in without logging out the current user:

The LXDM themes are located in /usr/share/lxdm/themes.

There is only one theme provided with LXDM: Industrial. To display the background file wave.svg which is part of this theme, install librsvg.

lxdm-themesAUR provides 6 extra themes: Archlinux, ArchlinuxFull, ArchlinuxTop, Arch-Dark, Arch-Stripes, and IndustrialArch.

Choice of theme is configurable in /etc/lxdm/lxdm.conf:

You can also configure LXDM to use a GTK theme (stored in /usr/share/themes) in /etc/lxdm/lxdm.conf:

After a user logs on, LXDM sources all of the following files, in the below order:

These files can be used to set session environment variables and to start services which must set certain environment variables in order for clients in the session to be able to use the service, like ssh-agent. See Xprofile for details.

Note that LXDM does not source ~/.xinitrc, so those migrating from a DM that does use this file, like SLiM, will have to move their settings elsewhere — probably ~/.xprofile. Also note LXDM does not source ~/.bash_profile.

If you still want to use your ~/.xinitrc file, you can add a line to the /etc/lxdm/PostLogin event file:

LXDM also makes use of .Xresources, .Xkbmap, and .Xmodmap. See /etc/lxdm/Xsession for details on how LXDM uses system-wide and per-user configuration files to configure the session.[1]

When using the default LXDM theme=Industrial and a dark background image (e.g. bg=/usr/share/backgrounds/img.png) there may be a short bright flash before LXDM starts. This is caused by the bg_color: property of the selected GTK theme. To avoid this change gtk_theme=Adwaita to gtk_theme=Adwaita-dark or to another dark theme.

If you had trouble logging out when using lxdm (e.g. stuck, display freeze, etc..) try uncomment the reset=1 option in /etc/lxdm/lxdm.conf to refresh xserver on every logout.

**Examples:**

Example 1 (unknown):
```unknown
lxdm.service
```

Example 2 (unknown):
```unknown
PreShutdown
```

Example 3 (unknown):
```unknown
/etc/lxdm/lxdm.conf
```

Example 4 (unknown):
```unknown
session=/usr/bin/startlxde
```

---

## Domain name resolution

**URL:** https://wiki.archlinux.org/title/DNS

**Contents:**
- Name Service Switch
  - Resolve a domain name using NSS
- Glibc resolver
  - Overwriting of /etc/resolv.conf
    - Alternative using nmcli
  - Limit lookup time
  - Hostname lookup delayed with IPv6
  - Local domain names
- Lookup utilities
- Resolver performance

In general, a domain name represents an IP address and is associated to it in the Domain Name System (DNS). This article explains how to configure domain name resolution and resolve domain names.

This article or section needs expansion.

The Name Service Switch (NSS) facility is part of the GNU C Library (glibc) and backs the getaddrinfo(3) API, used to resolve domain names. NSS allows system databases to be provided by separate services, whose search order can be configured by the administrator in nsswitch.conf(5). The database responsible for domain name resolution is the hosts database, for which glibc offers the following services:

systemd provides three NSS services for hostname resolution:

NSS databases can be queried with getent(1). A domain name can be resolved through NSS using:

The glibc resolver reads /etc/resolv.conf for every resolution to determine the nameservers and options to use.

resolv.conf(5) lists nameservers together with some configuration options. Nameservers listed first are tried first, up to three nameservers may be listed. Lines starting with a number sign (#) are ignored.

Network managers tend to overwrite /etc/resolv.conf, for specifics see the corresponding section:

To prevent programs from overwriting /etc/resolv.conf, it is also possible to write-protect it by setting the immutable file attribute:

This article or section is a candidate for merging with NetworkManager#/etc/resolv.conf.

If you use NetworkManager, nmcli(1) can be used to set persistent options for /etc/resolv.conf. Change "Wired" to the name of your connection. Example:

For more options have a look at the man pages of nmcli(1), nm-settings-nmcli(5) and resolv.conf(5).

If you are confronted with a very long hostname lookup (may it be in pacman or while browsing), it often helps to define a small timeout after which an alternative nameserver is used. To do so, put the following in /etc/resolv.conf.

If you experience a 5 second delay when resolving hostnames it might be due to a DNS-server/Firewall misbehaving and only giving one reply to a parallel A and AAAA request.[1] You can fix that by setting the following option in /etc/resolv.conf:

To be able to use the hostname of local machine names without the fully qualified domain name, add a line to /etc/resolv.conf with the local domain such as:

That way you can refer to local hosts such as mainmachine1.example.org as simply mainmachine1 when using the ssh command, but the drill command still requires the fully qualified domain names in order to perform lookups.

To query specific DNS servers and DNS/DNSSEC records you can use dedicated DNS lookup utilities or those shipped with DNS servers. These tools implement DNS themselves and do not use NSS.

Some DNS server packages ship with DNS lookup utilities that can be used without running the DNS server:

The Glibc resolver does not cache queries. To implement local caching, use systemd-resolved or set up a local caching DNS server and use it as the name server by setting 127.0.0.1 and ::1 as the name servers in /etc/resolv.conf or in /etc/resolvconf.conf if using openresolv.

This article or section needs expansion.

The DNS protocol (Do53) is unencrypted and does not account for confidentiality, integrity or authentication, so if you use an untrusted network or a malicious ISP, your DNS queries can be eavesdropped and the responses manipulated. Furthermore, DNS servers can conduct DNS hijacking.

You need to trust your DNS server to treat your queries confidentially. DNS servers are provided by ISPs and third-parties. Alternatively you can run your own recursive name server (a.k.a. recursive resolver, a.k.a. DNS recursor), which however takes more effort. If you use a DHCP client in untrusted networks, be sure to set static name servers to avoid using and being subject to arbitrary DNS servers, or alternatively, use a VPN to connect to a secure network and use its DNS servers. To secure your communication with a remote DNS server you can use an encrypted protocol, provided that both the upstream server and your local resolver support the protocol. Common encrypted DNS protocols are:

To verify that responses are actually from authoritative name servers, you can validate DNSSEC, provided that both the upstream server(s) and your local resolver support it.

Although one may use an encrypted DNS resolver, a TLS connection still leaks the domain names in the Server Name Indication (SNI) when requesting the domain certificate. This leak can be checked using the Wireshark filter tls.handshake.extensions_server_name_len > 0, or using the following tshark command:

A proposed solution is to use the Encrypted Client Hello (ECH), a TLS 1.3 protocol extension.

Be aware that some client software, such as major web browsers[2][3], are starting to implement DNS over HTTPS. While the encryption of queries may often be seen as a bonus, it also means the software sidetracks queries around the system resolver configuration.[4]

Firefox provides configuration options to enable or disable DNS over HTTPS and select a DNS server. Mozilla has setup a Trusted Recursive Resolver (TRR) programme with transparency information on their default providers. It is notable that Firefox supports and automatically enables the Encrypted Client Hello (ECH) for TRR providers, see Firefox/Privacy#Encrypted Client Hello.

Chromium will examine the user's system resolver and enable DNS over HTTPS if the system resolver addresses are known to also provide DNS over HTTPS. See this blog post for more information and how DNS over HTTPS can be disabled.

Mozilla has proposed universally disabling application-level DNS if the system resolver cannot resolve the domain use-application-dns.net. Currently, this is only implemented in Firefox.

Oblivious DNS over HTTPS (ODoH)—RFC 9230—is a system which addresses a number of DNS privacy concerns. See Cloudflare's article for more information. It added DNS over HTTPS to the academic Oblivious DNS design. See the Improving the privacy of DNS and DoH with oblivion article for a discussion of the differences.

This article or section needs expansion.

Communication between recursive resolvers and root servers is not encrypted and the root server operators are against implementing it. For encrypted communication with authoritative servers there is the experimental RFC 9539 which allows the opportunistic use of DNS over TLS and DNS over QUIC.

There are various third-party DNS services. Wikipedia has a list of "notable" public DNS service operators while the curl project's wiki has a more extensive list of publicly available DNS over HTTPS servers (a lot of which also support DNS over TLS). The systemd package configures fallback DNS for systemd-resolved when no DNS servers are configured (manually or via DHCP/RA).

You can use dnsperftest to test the performance of the most popular DNS resolvers from your location. dnsperf.com provides global benchmarks between providers.

Some DNS services also provide dedicated software:

DNS servers can be authoritative and recursive. If they are neither, they are called stub resolvers and simply forward all queries to another recursive name server. Stub resolvers are typically used to introduce DNS caching on the local host or network. Note that the same can also be achieved with a fully-fledged name server. This section compares the available DNS servers, for a more detailed comparison, refer to Wikipedia:Comparison of DNS server software.

It is possible to use specific DNS resolvers when querying specific domain names. This is particularly useful when connecting to a VPN, so that queries to the VPN network are resolved by the VPN's DNS, while queries to the internet will still be resolved by your standard DNS resolver. It can also be used on local networks.

To implement it, you need to use a local resolver because glibc does not support it.

In a dynamic environment (laptops and to some extents desktops), you need to configure your resolver based on the network(s) you are connected to. The best way to do that is to use openresolv because it supports multiple subscribers. Some network managers support it, either through openresolv, or by configuring the resolver directly. NetworkManager supports conditional forwarding without openresolv.

**Examples:**

Example 1 (unknown):
```unknown
/etc/resolv.conf
```

Example 2 (unknown):
```unknown
$ getent ahosts domain_name
```

Example 3 (unknown):
```unknown
/etc/resolv.conf
```

Example 4 (unknown):
```unknown
/etc/resolv.conf
```

---

## dm-crypt/Swap encryption

**URL:** https://wiki.archlinux.org/title/Dm-crypt/Swap_encryption

**Contents:**
- Without suspend-to-disk support
  - By-id naming
  - UUID and LABEL
  - Disabling hibernation in desktop environments
- With suspend-to-disk support
  - Using a swap file
  - Using a swap partition
    - Using a TPM
    - Using an additional passphrase or keyfile
    - Unlocking the partition in the initramfs

Depending on requirements, different methods may be used to encrypt the swap partition which are described in the following. A setup where the swap encryption is re-initialised on reboot (with a new encryption) provides higher data protection, because it avoids sensitive file fragments which may have been swapped out a long time ago without being overwritten. However, re-encrypting swap also forbids using a suspend-to-disk feature generally.

In systems where suspend-to-disk (hibernation) is not a desired feature, /etc/crypttab can be set up to decrypt the swap partition with a random password with plain dm-crypt at boot-time. The random password is discarded on shutdown, leaving behind only encrypted, inaccessible data in the swap device.

To enable this feature, simply uncomment the line beginning with swap in /etc/crypttab. Change the device parameter to the name of your swap device. For example, it will look something like this:

This will map /dev/sdX# to /dev/mapper/swap as a swap partition that can be added in /etc/fstab like a normal swap. If you had a non-encrypted swap partition before, do not forget to disable it - or re-use its fstab entry by changing the device to /dev/mapper/swap. The default options should be sufficient for most usage. For other options and an explanation of each column, see crypttab(5) as well as point cryptsetup FAQ 2.3.

To use a by-id persistent device naming instead of kernel simple naming, first identify the swap device:

Then use as a persistent reference for the /dev/sdX# example partition (if two results are returned as above, choose either one of them):

After a reboot to activate the encrypted swap, you will note that running swapon -s shows an arbitrary device mapper entry (e.g. /dev/dm-1) for it, while the lsblk command shows crypt in the FSTYPE column. Due to fresh encryption each boot, the UUID for /dev/mapper/swap will change every time.

It is dangerous to use crypttab swap with simple kernel device names like /dev/sdX# or even /dev/disk/by-id/ata-SERIAL-partX. A small change in your device names or partitioning layout and /etc/crypttab will see your valuable data formatted on the next boot. Using a PARTLABEL or PARTUUID is safer, but it does not protect you if you then decide to use that partition for something else without removing the crypttab entry first.

It is more reliable to identify the correct partition by giving it a genuine UUID or LABEL. By default that does not work because dm-crypt and mkswap would simply overwrite any content on that partition which would remove the UUID and LABEL too; however, it is possible to specify a swap offset. This allows you to create a very small, empty, bogus filesystem with no other purpose than providing a persistent UUID or LABEL for the swap encryption.

Create a filesystem with label of your choice:

The unusual parameter after the device name limits the filesystem size to 1 MiB, leaving room for encrypted swap behind it.

With this, /dev/sdX# now can easily be identified either by UUID or LABEL, regardless of how its device name or even partition number might change in the future. All that is left are the /etc/crypttab and /etc/fstab entries. For example, using different encryption options:

Note the offset: it is 2048 sectors of 512 bytes (it is not affected by the dm-crypt sector size), thus 1 MiB. This way the encrypted swap will not affect the filesystem LABEL/UUID, and data alignment works out as well.

Using this setup, the cryptswap will only try to use the partition with the corresponding LABEL, regardless of what its device name may be. Should you decide to use the partition for something else, by formatting it the cryptswap LABEL would also be gone, so /etc/crypttab will not overwrite it on your next boot.

Desktop environments may not automatically detect that a swap partition is randomly encrypted and cannot be used for suspend-to-disk.

Xfce can be configured to hide its Hibernate and Hybrid Sleep buttons by running these commands:

The following three methods are alternatives for setting up an encrypted swap for suspend-to-disk (hibernation). If you apply any of them, be aware that critical data swapped out by the system may potentially stay in the swap over a long period (i.e. until it is overwritten). To reduce this risk consider setting up a system job which re-encrypts swap, e.g. each time the system is going into a regular shut-down, along with the method of your choice.

A swap file can be placed in a file system within an encrypted device. Follow the swap file creation instructions in Swap#Swap file and set up hibernation according to Power management/Suspend and hibernate#Configure the initramfs.

When used with a systemd-based initramfs and an encrypted root filesystem, this can be a very simple and flexible method for encrypting swap. After setting up an encrypted root filesystem, the swap file can be made, activated, and added to /etc/fstab and work for hibernation without any further setup.

Use cryptsetup-luksFormat(8) to create the encrypted container for the swap partition:

Open the container to /dev/mapper/swap:

Create a swap filesystem inside the mapped partition:

If not using systemd#GPT partition automounting, add the mapped partition to /etc/fstab by adding the following line:

To set up your system to resume from hibernation, use the resume=/dev/mapper/swap kernel parameter. See Power management/Suspend and hibernate#Pass hibernate location to initramfs for details.

The following provides unattended swap decryption with a key stored in the TPM.

You can use systemd-cryptenroll to enroll the key to the Luks container and TPM, and wipe the previously created keyslot containing the password:

Check the result with

This article or section needs expansion.

This article or section is being considered for removal.

The basic setup above has the disadvantage of having to insert an additional passphrase for the swap partition manually on every boot.

To resume from an encrypted swap partition, the encrypted partition must be unlocked in the initramfs.

When using the systemd-based initramfs with the sd-encrypt mkinitcpio hook, either

For example, for a TPM backed encrypted swap device:

When using the default busybox-based initramfs with the encrypt hook, follow the instructions below.

If the swap device is on a different device from that of the root file system, it will not be opened by the encrypt hook, i.e. the resume will take place before /etc/crypttab can be used, therefore it is required to create a hook in /etc/mkinitcpio.conf to open the swap LUKS device before resuming.

This article or section is a candidate for merging with dm-crypt/Specialties#Multiple non-root partitions.

Now you have to create a hook to open the swap at boot time. You can either install and configure mkinitcpio-openswapAUR, or follow the following instructions. Create a hook file containing the open command:

This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.

This article or section needs expansion.

for opening the swap device by typing your password or

for opening the swap device by loading a keyfile from a crypted root device or

for opening the swap device by extracting a keyfile directly from an encrypted root device without mounting it.

On ext4, the block number can be determined as follows:

The relevant block number will appear under the Physical column in the output.

In this case, the key should be no larger than a single block (4 KiB by default on ext4); otherwise, the file may become fragmented and unlocking may fail.

On some computers race conditions may occur when mkinitcpio tries to mount the device before the decryption process and device enumeration is completed. The commented Optional block will delay the boot process up to 2 seconds until the root device is ready to mount.

Then create and edit the hook setup file:

Add the hook openswap in the HOOKS array in /etc/mkinitcpio.conf, before filesystem but after encrypt. Do not forget to add the resume hook after openswap.

Regenerate the initramfs.

At boot time, the openswap hook will open the swap partition so the kernel resume may use it. If you use special hooks for resuming from hibernation, make sure they are placed after openswap in the HOOKS array. Please note that because of initramfs opening swap, there is no entry for swap in /etc/crypttab needed in this case.

Add the keyfile to LUKS:

Configure dracut to include the resume module and add the swap.key file to the initramfs (See also dracut#Hibernation):

Regenerate the initramfs.

Add the rd.luks.name and rd.luks.key (replace the swap's partition UUID) entries to your kernel command line. Your kernel command might look like this now:

If the swap volume is in a volume group that gets activated in initramfs, simply follow the instructions in Power management/Suspend and hibernate#Hibernation.

This article or section is being considered for removal.

The above examples use AES-256-XTS, which is the same as cryptsetup's default algorithm for ordinary disk encryption. This algorithm is unlikely to be a performance bottleneck due to widespread AES acceleration in computers. That said, if a user really does have a very fast disk for swapping, one may want to use AES-128-XTS which is usually 40% faster due to a reduction in round count: replace size=512 with size=256. The key size and round count of AES-128 provides ample safety for non-quantum purposes, as Aumasson argued in Too Much Crypto. (A reduced-round-count AES-256-XTS would be the perfect combination of speed and quantum-safety, but it is not standard and hence not available.)

**Examples:**

Example 1 (unknown):
```unknown
/etc/crypttab
```

Example 2 (unknown):
```unknown
/etc/crypttab
```

Example 3 (unknown):
```unknown
/etc/crypttab
```

Example 4 (unknown):
```unknown
# <name>  <device>     <password>     <options>
swap      /dev/sdX#    /dev/urandom   swap,cipher=aes-xts-plain64,size=512,sector-size=4096
```

---

## Unified Extensible Firmware Interface/Secure Boot

**URL:** https://wiki.archlinux.org/title/Unified_Extensible_Firmware_Interface/Secure_Boot

**Contents:**
- Checking Secure Boot status
  - Before booting the OS
  - After booting the OS
- Booting an installation medium
  - Disabling Secure Boot
  - Repacking the installation image
  - Editing the installation medium
- Implementing Secure Boot
  - Using your own keys
    - Backing up current variables

Secure Boot is a security feature found in the UEFI standard, designed to add a layer of protection to the pre-boot process: by maintaining a cryptographically signed list of binaries authorized or forbidden to run at boot, it helps in improving the confidence that the machine core boot components (boot manager, kernel, initramfs) have not been tampered with.

As such it can be seen as a continuation or complement to the efforts in securing one's computing environment, reducing the attack surface that other software security solutions such as system encryption cannot easily cover, while being totally distinct and not dependent on them. Secure Boot just stands on its own as a component of current security practices, with its own set of pros and cons.

At this point, one has to look at the firmware setup. If the machine was booted and is running, in most cases it will have to be rebooted.

You may access the firmware configuration by pressing a special key during the boot process. The key to use depends on the firmware. It is usually one of Esc, F2, Del or possibly another Fn key. Sometimes the right key is displayed for a short while at the beginning of the boot process. The motherboard manual usually records it. You might want to press the key, and keep pressing it, immediately following powering on the machine, even before the screen actually displays anything.

After entering the firmware setup, be careful not to change any settings without prior intention. Usually there are navigation instructions, and short help for the settings, at the bottom of each setup screen. The setup itself might be composed of several pages. You will have to navigate to the correct place. The interesting setting might be simply denoted by secure boot, which can be set on or off.

An easy way to check Secure Boot status on systems using systemd is to use bootctl(1):

Here we see that Secure Boot is enabled and enforced (in user mode); other values are disabled (setup) for Setup Mode, disabled (disabled) if Secure Boot is disabled and disabled (unsupported) if the firmware does not feature Secure Boot.

Another way to check whether the machine was booted with Secure Boot is to use this command:

If Secure Boot is enabled, this command returns 1 as the final integer in a list of five, for example:

Note, however, that the kernel may be unaware of Secure Boot (even if it is enabled in the firmware) if an insufficiently capable boot loader is used. This can be verified by checking the kernel messages shortly after the system starts up:

The kernel messages will otherwise read Secure boot enabled.

The official installation image does not support Secure Boot (archlinux/archiso#69). Secure Boot support was initially added in archlinux-2013.07.01-dual.iso and later removed in archlinux-2016.06.01-dual.iso. At that time prebootloader was replaced with efitools, even though the latter uses unsigned EFI binaries. There has been no support for Secure Boot in the official installation medium ever since.

In order to boot an installation medium in a Secure Boot system, you will need to either disable Secure Boot or modify the image in order to add a signed boot loader.

Archboot images provide a way to use secure boot on installation media.

The Secure Boot feature can be disabled via the UEFI firmware interface. How to access the firmware configuration is described in #Before booting the OS.

If using a hotkey did not work and you can boot Windows, you can force a reboot into the firmware configuration in the following way (for Windows 10): Settings > Update & Security > Recovery > Advanced startup (Restart now) > Troubleshoot > Advanced options > UEFI Firmware settings > restart.

Note that some motherboards (this is the case in a Packard Bell laptop and recent Xiaomi laptops) only allow to disable secure boot if you have set an administrator password (that can be removed afterwards). See also Rod Smith's Disabling Secure Boot.

If you are using a USB flash installation medium, then it is possible to manually edit the EFI system partition on the medium to add support for Secure Boot.

Plug in the USB drive, then mount the partition:

Then follow the instructions in #Using a signed boot loader to install any signed boot loader. For example, to install #PreLoader:

There are certain conditions making for an ideal setup of Secure boot:

A simple and fully self-reliant setup is described in #Using your own keys, while #Using a signed boot loader makes use of intermediate tools signed by a third-party.

Using the GRUB boot loader requires extra steps before enabling secure boot, see GRUB#Secure Boot support for details.

Secure Boot implementations use these keys:

See The Meaning of all the UEFI Keys for a more detailed explanation.

To use Secure Boot you need at least PK, KEK and db keys. While you can add multiple KEK, db and dbx certificates, only one Platform Key is allowed.

Once Secure Boot is in "User Mode" keys can only be updated by signing the update (using sign-efi-sig-list) with a higher level key. Platform key can be signed by itself.

Before creating new keys and modifying EFI variables, it is advisable to backup the current variables, so that they may be restored in case of error.

Install the efitools package, then run the following commands to backup all four of the principal Secure Boot variables:

If you perform this command on a new computer or motherboard, the variables you extract will most likely be the ones provided by Microsoft.

Secure Boot is in Setup Mode when the Platform Key is removed. To put firmware in Setup Mode, enter firmware setup utility and find an option to delete or clear certificates. How to enter the setup utility is described in #Before booting the OS.

As of v257, you can easily set up Secure Boot with systemd and systemd-boot. Install systemd-ukify.

First set up your desired configuration as in Unified kernel image#ukify (you can use the template from /usr/lib/kernel/uki.conf), then generate your signing keys:

Sign the boot loader with the newly created keys (see systemd-boot#Signing for Secure Boot for automation):

Next, configure the ESP for auto-enrollment:

This installs (or updates) the signed systemd-boot boot loader to the ESP, and creates three files PK.auth, KEK.auth and db.auth in /boot/loader/keys/auto/. No separate platform or key exchange keys are generated, instead the signing key is enrolled at all three levels.

Finally, set secure-boot-enroll force in /boot/loader/loader.conf, and reboot to enroll the keys in the firmware. See loader.conf(5).

sbctl is a user-friendly way of setting up secure boot and signing files.

To use it, install sbctl. See also the upstream README and sbctl(8).

Before starting, go to your firmware settings and set secure boot mode to Setup mode. This is different for each device: see sbctl(8) § USAGE.

Once you log back in, check the secure boot status:

You should see that sbctl is not installed and secure boot is disabled.

Then create your custom secure boot keys:

Enroll your keys, with Microsoft's keys, to the UEFI:

Check the secure boot status again:

sbctl should be installed now, but secure boot will not work until the boot files have been signed with the keys you just created.

Check what files need to be signed for secure boot to work:

Now sign all the unsigned files. Usually the kernel and the boot loader need to be signed. For example:

The files that need to be signed will depend on your system's layout, kernel and boot loader.

This example assumes that the outputted file paths are relative to /boot. Alternatively, you can use:

This is agnostic of the filepath, and doesn't need the '✗' character.

Now you are done! Reboot your system and turn secure boot back on in the firmware settings. If the boot loader and OS load, secure boot should be working. Check with:

sbctl comes with a pacman hook that automatically signs all new files whenever the Linux kernel, systemd or the boot loader is updated.

Nearly all of the following sections require you to install the efitools package.

You will need private keys and certificates in various formats:

Create a GUID for owner identification:

Sign an empty file to allow removing Platform Key when in "User Mode":

Signature Database key:

A helper/convenience script is offered by the author of the reference page on this topic[3] (requires python). A mildly edited version is also packaged as sbkeysAUR.

In order to use it, simply create a folder in a secure location (e.g. /etc/efi-keys/ if later use of sbupdate-gitAUR to automate unified kernel image creation and signing is planned) and run it:

This will produce the required files in different formats.

Use one of the following methods to enroll db, KEK and PK certificates.

Install sbsigntools. Create a directory /etc/secureboot/keys with the following directory structure -

Then copy each of the .auth files that were generated earlier into their respective locations (for example, PK.auth into /etc/secureboot/keys/PK and so on).

If you want to verify the changes sbkeysync will make to the system's UEFI keystore, use:

Finally, use sbkeysync to enroll your keys.

On next boot the UEFI should be back in User Mode and enforcing Secure Boot policy.

Copy all *.cer, *.esl, *.auth files (except the noPK.auth file!) to a FAT formatted file system (you can use EFI system partition).

The EFI system partition of your PC must not be encrypted according to the UEFI specifications and can be mounted and read on another PC (if your PC is stolen and if the hard drive is taken out and connected to another PC). Copying the noPK.auth file to the ESP of your PC and deleting it afterwards is also not advisable, because deleted files on the FAT32 EFI system partition can be recovered with tools like PhotoRec.

Launch firmware setup utility and enroll db, KEK and PK certificates. Firmwares have various different interfaces, see Replacing Keys Using Your Firmware's Setup Utility for example how to enroll keys.

If the used tool supports it prefer using .auth and .esl over .cer.

This article or section is out of date.

KeyTool.efi is in efitools package, copy it to ESP. To use it after enrolling keys, sign it with sbsign.

Launch KeyTool-signed.efi using firmware setup utility, boot loader or UEFI Shell and enroll keys.

See Replacing Keys Using KeyTool for explanation of KeyTool menu options.

Since systemd version 252, systemd-boot can be used to enroll keys. To enroll keys, copy the db.auth, KEK.auth and PK.auth to the special folder on the ESP:

Where NAME can be any unique name you assign, such as MYKEYS.

After copying the keys and enabling the secure boot setup mode a new entry would appear in the boot menu that would read Enroll Secure Boot keys: MYKEYS. Activating this entry would enroll the secure boot keys.

When Secure Boot is active (i.e. in "User Mode"), only signed EFI binaries (e.g. applications, drivers, unified kernel images) can be launched.

Install sbsigntools to sign EFI binaries with sbsign(1).

To sign your kernel and boot manager use sbsign, e.g.:

You can also use a mkinitcpio post hook to sign the kernel when the initramfs gets generated.

Create the following file an make it executable:

Replace /path/to/db.key and /path/to/db.crt with the paths to the key pair you want to use for signing the kernel.

If you are using systemd-boot, there is a dedicated pacman hook doing this task semi-automatically.

See Unified kernel image#Signing the UKIs for Secure Boot.

This article or section is out of date.

sbupdate is a tool made specifically to automate unified kernel image generation and signing on Arch Linux. It handles installation, removal and updates of kernels through pacman hooks.

Install sbupdate-gitAUR and configure it following the instructions given on the project's homepage.[4]

Once configured, simply run sbupdate as root for first-time image generation.

Once Secure Boot is in "User Mode" any changes to KEK, db and dbx need to be signed with a higher level key.

For example, if you wanted to replace your db key with a new one:

If instead of replacing your db key, you want to add another one to the Signature Database, you need to use the option -a (see sign-efi-sig-list(1)):

When new_db.auth is created, enroll it.

This article or section needs expansion.

It is usually not possible to boot Windows by signing its boot loader (EFI/Microsoft/Boot/bootmgfw.efi) with a custom, personal key with Secure Boot Mode enabled, without enrolling the "Microsoft Windows Production PCA 2011" key in the UEFI Secure Boot variables:

So to dual boot with Windows,

Create EFI Signature Lists from Microsoft's DER format db certificates using Microsoft's GUID (77fa9abd-0359-4d32-bd60-28f4e78f784b) and combine them in one file for simplicity:

Optional (for strict conformity with Microsoft UEFI Secure Boot requirements): Create an EFI Signature List from Microsoft's DER format KEK certificates using Microsoft's GUID (77fa9abd-0359-4d32-bd60-28f4e78f784b):

Sign a db variable update with your KEK. Use sign-efi-sig-list with option -a to add not replace a db certificate:

Optional (for strict conformity with Microsoft UEFI Secure Boot requirements): Sign a KEK variable update with your PK. Use sign-efi-sig-list with option -a to add not replace a KEK certificate:

Follow #Enrolling keys in firmware to enroll add_MS_db.auth and for strict conformity with Microsoft UEFI Secure Boot requirements add_MS_Win_KEK.auth into the UEFI Secure Boot Database variables.

Using a signed boot loader means using a boot loader signed with Microsoft's key. There are two known signed boot loaders: PreLoader and shim. Their purpose is to chainload other EFI binaries (usually boot loaders). Since Microsoft would never sign a boot loader that automatically launches any unsigned binary, PreLoader and shim use an allowlist called Machine Owner Key list, abbreviated MokList. If the SHA256 hash of the binary (Preloader and shim) or key the binary is signed with (shim) is in the MokList they execute it, if not they launch a key management utility which allows enrolling the hash or key.

The enrollment of the Microsoft 3rd Party UEFI CA certificate needs to be enabled in firmware settings to launch EFI binaries and OpROMs signed with this certificate.

When run, PreLoader tries to launch loader.efi. If the hash of loader.efi is not in MokList, PreLoader will launch HashTool.efi. In HashTool you must enroll the hash of the EFI binaries you want to launch, that means your boot loader (loader.efi) and kernel.

Install preloader-signedAUR and copy PreLoader.efi and HashTool.efi to the boot loader directory; for systemd-boot use:

Now copy over the boot loader binary and rename it to loader.efi; for systemd-boot use:

Finally, create a new NVRAM entry to boot PreLoader.efi:

Replace X with the drive letter and replace Y with the partition number of the EFI system partition.

This entry should be added to the list as the first to boot; check with the efibootmgr command and adjust the boot-order if necessary.

If there are problems booting the custom NVRAM entry, copy HashTool.efi and loader.efi to the default loader location booted automatically by UEFI systems:

Copy over PreLoader.efi and rename it:

For particularly intransigent UEFI implementations, copy PreLoader.efi to the default loader location used by Windows systems:

As before, copy HashTool.efi and loader.efi to esp/EFI/Microsoft/Boot/.

When the system starts with Secure Boot enabled, follow the steps above to enroll loader.efi and /vmlinuz-linux (or whichever kernel image is being used).

A message will show up that says Failed to Start loader... I will now execute HashTool. To use HashTool for enrolling the hash of loader.efi and vmlinuz.efi, follow these steps. These steps assume titles for a remastered archiso installation media. The exact titles you will get depends on your boot loader setup.

Every entry of hashes enrolled in the MOK database eats up a little piece of space of NVRAM. You may want to delete useless hashes to free the space and to prevent outdated programs from booting.

Install efitools and copy KeyTool.efi:

Manage to boot to Preloader and you will see the KeyTool entry. You can then edit hashes in the MOK database.

Uninstall preloader-signedAUR and simply remove the copied files and revert configuration; for systemd-boot use:

Where N is the NVRAM boot entry created for booting PreLoader.efi. Check with the efibootmgr command and adjust the boot-order if necessary.

When run, shim tries to launch grubx64.efi. If MokList does not contain the hash of grubx64.efi or the key it is signed with, shim will launch MokManager (mmx64.efi). In MokManager you must enroll the hash of the EFI binaries you want to launch (your boot loader (grubx64.efi) and kernel) or enroll the key they are signed with.

Install shim-signedAUR.

Rename your current boot loader to grubx64.efi, because, by default, Shim will try load and run a file named grubx64.efi. While this default can be overridden by passing a file path to a different EFI binary as a command line argument to shim, since some firmware have issues with UEFI boot entries that have command line arguments, it is more foolproof to rely on the default.

Copy shim and MokManager to your boot loader directory on ESP; use previous filename of your boot loader as as the filename for shimx64.efi:

Finally, create a new NVRAM entry to boot BOOTx64.EFI:

shim can authenticate binaries by Machine Owner Key or hash stored in MokList.

Using hash is simpler, but each time you update your boot loader or kernel you will need to add their hashes in MokManager. With MOK you only need to add the key once, but you will have to sign the boot loader and kernel each time it updates.

If shim does not find the SHA256 hash of grubx64.efi in MokList it will launch MokManager (mmx64.efi).

In MokManager select Enroll hash from disk, find grubx64.efi and add it to MokList. Repeat the steps and add your kernel vmlinuz-linux. When done select Continue boot and your boot loader will launch and it will be capable launching the kernel.

Create a Machine Owner Key:

Sign your boot loader (named grubx64.efi) and kernel:

You will need to do this each time they are updated. You can automate the kernel signing with a mkinitcpio post hook. Create the following file and make it executable:

Copy MOK.cer to a FAT formatted file system (you can use EFI system partition).

Reboot and enable Secure Boot. If shim does not find the certificate grubx64.efi is signed with in MokList it will launch MokManager (mmx64.efi).

In MokManager select Enroll key from disk, find MOK.cer and add it to MokList. When done select Continue boot and your boot loader will launch and it will be capable launching any binary signed with your Machine Owner Key.

See GRUB#Shim-lock for instructions.

Every entry of hash/key enrolled in the MOK database eats up a little piece of space of NVRAM. You may want to delete useless hash/key to free the space and to prevent outdated programs from booting.

MOK database can be managed with mokutil.

List enrolled keys and hashes:

Remove hash from database. The password entered here will be asked for confirmation to delete in MOK manager.

Remove key from database:

List hashes/keys to be deleted on next reboot:

On next reboot, MOK manager will be initiated with option to Enroll/Delete hashes/keys. See mokutil(1) for more details.

Uninstall shim-signedAUR, remove the copied shim and MokManager files and rename back your boot loader.

The only way to prevent anyone with physical access from disabling Secure Boot is to protect the firmware settings with a password. Most UEFI firmwares provide such a feature, usually listed under the "Security" section in the firmware settings.

Consider enabling kernel lockdown mode. See [7].

It is possible to unpack and repack the official installation image using libisoburn and mtools. This way, you can create an image that supports Secure Boot, either with custom keys or with a signed boot loader.

This article or section needs expansion.

Support for Secure Boot using custom keys can be added to the official ISO by simply extracting the boot loader (BOOTx64.EFI and BOOTIA32.EFI), kernel, UEFI shell, signing them and then repacking the ISO with the signed files.

First extract the relevant files and El Torito boot images:

xorrisofs(1) option -rational-rock as used by mkarchiso makes the files on ISO 9660 read-only which persists after extracting them. Make the files writable so that they can be modified:

Sign the files. To do so with sbsigntools, for example:

Copy the signed EFI binaries to eltorito_img2_uefi.img. It will be used as the EFI system partition and will be listed as an El Torito UEFI boot image. The size of eltorito_img2_uefi.img is fixed, but there are 8 MiB free space added by mkarchiso (for the purposes of rounding/alignment, to account for reserved sectors, etc.), so the size increase from the signatures should not be an issue.

Repack the ISO using the modified El Torito UEFI boot image and add the signed EFI binaries to ISO 9660:

Boot the resulting archlinux-YYYY.MM.DD-x86_64-Secure_Boot.iso.

Another way to add Secure Boot support to the official ISO is by extracting the boot loader and replacing it with #PreLoader.

First, extract the boot loader and the El Torito boot image:

Replace the BOOTx64.efi file with PreLoader:

Add the new files to the boot image:

Finally, repack the ISO using the modified boot image and the new boot loader files:

Support for Secure Boot using shim with a Machine Owner Key (MOK) can be added to the official ISO by extracting the boot loader, kernel and UEFI shell, signing them and then repacking the ISO with the signed files and shim.

First extract the relevant files and El Torito boot images. The boot loader file name will need to be grubx64.efi so that shim can find it.

xorrisofs(1) option -rational-rock as used by mkarchiso makes the files on ISO 9660 read-only which persists after extracting them. Make the files writable so that they can be modified:

Sign the files with your MOK:

Acquire pre-signed shim EFI binaries, e.g. by installing shim-signedAUR. Place shim and MokManager in the current directory and change the shim EFI binary's file name to BOOTx64.EFI:

Copy shim, MokManager, the signed EFI binaries and the DER format MOK to eltorito_img2_uefi.img. It will be used as the EFI system partition and will be listed as an El Torito UEFI boot image. The size of eltorito_img2_uefi.img is fixed, but there are 8 MiB free space added by mkarchiso (for the purposes of rounding/alignment, to account for reserved sectors, etc.), so the size increase from the added files should not be an issue.

Repack the ISO using the modified El Torito UEFI boot image and add shim, MokManager, the signed EFI binaries and the DER format MOK to ISO 9660:

Boot the resulting archlinux-YYYY.MM.DD-x86_64-Secure_Boot.iso. When MokManager launches, select Enroll key from disk > ARCHISO_EFI > MOK.cer. After enrolling the key, reboot and at the next boot, the live environment will successfully boot.

Option ROMs (OpROMs), i.e. device firmware that is executed during boot, must be signed for Secure Boot otherwise the devices will not be initialized. Typically OpROMs are signed with the Microsoft 3rd Party UEFI CA certificate which may prevent implementing Secure Boot with only your own keys. To solve this, the SHA256 digests of OpROMs can be enrolled instead.

On systems with a TPM, it is possible to acquire Option ROM SHA256 digests from the TPM event log.

Install tpm2-tools and digest-to-efi-sig-listAUR.

Use tpm2_eventlog(1) to read /sys/kernel/security/tpm0/binary_bios_measurements and look for the digests of BOOT_SERVICES_DRIVER.[8]

This will print an easy to parse list of digests:

Use digest-to-efi-sig-listAUR to create an EFI signature list for each OpROM digest you find:

In case you have multiple OpROMs, combine their EFI signature lists into one so that it could be signed as a single file:

You could use a bash script like this if you have a lot of digests.

Sign the EFI signature list for appending to the Signature Database:

The final and most dangerous step is to remove Microsoft 3rd Party UEFI CA certificate from the Signature Database and see if the system still boots and all devices still work.

**Examples:**

Example 1 (unknown):
```unknown
System:
      Firmware: UEFI 2.80 (American Megatrends 5.26)
 Firmware Arch: x64
   Secure Boot: enabled (user)
  TPM2 Support: yes
  Measured UKI: yes
  Boot into FW: supported
...
```

Example 2 (unknown):
```unknown
disabled (setup)
```

Example 3 (unknown):
```unknown
disabled (disabled)
```

Example 4 (unknown):
```unknown
disabled (unsupported)
```

---

## Arch boot process

**URL:** https://wiki.archlinux.org/title/Initramfs

**Contents:**
- Firmware types
  - UEFI
  - BIOS
- System initialization
  - UEFI
    - Multibooting
  - BIOS
- Boot loader
  - Feature comparison
- Kernel

In order to boot Arch Linux, a Linux-capable boot loader must be set up. The boot loader is responsible for loading the kernel and initial ramdisk before initiating the boot process. The procedure is quite different for BIOS and UEFI systems.

The firmware is the very first program that is executed once the system is switched on.

The Unified Extensible Firmware Interface has support for reading both the partition table as well as file systems. UEFI does not launch any boot code from the Master Boot Record (MBR) whether it exists or not, instead booting relies on boot entries in the NVRAM.

The UEFI specification mandates support for the FAT12, FAT16, and FAT32 file systems (see UEFI specification version 2.11, section 13.3.1.1), but any conformant vendor can optionally add support for additional file systems; for example, HFS+ or APFS in some Apple's firmwares. UEFI implementations also support ISO 9660 for optical discs.

UEFI launches EFI applications, e.g. boot loaders, boot managers, UEFI shell, etc. These applications are usually stored as files in the EFI system partition. Each vendor can store its files in the EFI system partition under the /EFI/vendor_name directory. The applications can be launched by adding a boot entry to the NVRAM or from the UEFI shell.

The UEFI specification has support for legacy BIOS booting with its Compatibility Support Module (CSM). If CSM is enabled in the UEFI, the UEFI will generate CSM boot entries for all drives. If a CSM boot entry is chosen to be booted from, the UEFI's CSM will attempt to boot from the drive's MBR bootstrap code.

A BIOS or Basic Input-Output System is in most cases stored in a flash memory in the motherboard itself and independent of the system storage. Originally created for the IBM PC to handle hardware initialization and the boot process, it has been replaced progressively since 2010 by UEFI which does not suffer from the same technical limitations.

System switched on, the power-on self-test (POST) is executed. See also Modern CPUs have a backstage cast by Hugo Landau.

If Secure Boot is enabled, the boot process will verify authenticity of the EFI binary by signature.

Since each OS or vendor can maintain its own files within the EFI system partition without affecting the other, multi-booting using UEFI is just a matter of launching a different EFI application corresponding to the particular operating system's boot loader. This removes the need for relying on the chain loading mechanisms of one boot loader to load another OS.

See also Dual boot with Windows.

A boot loader is a piece of software started by the firmware—UEFI or BIOS. It is responsible for loading the kernel with the wanted kernel parameters and any external initramfs images.

A boot manager presents a menu of boot options, or provides some other way to control the boot process—i.e. it just runs other EFI executables.

In the case of UEFI, the kernel itself can be directly launched by the UEFI using the EFI boot stub. A separate boot loader or a boot manager can still be used for the purpose of editing kernel parameters before booting.

Systems with 32-bit IA32 UEFI require a boot loader that supports mixed mode booting.

Since almost no boot loader supports such stacked block devices and since file systems can introduce new features which may not yet be supported by any boot loader (e.g. archlinux/packaging/packages/grub#7, FS#79857, FS#59047, FS#58137, FS#51879, FS#46856, FS#38750, FS#21733 and fscrypt encrypted directories), using a separate /boot partition with a universally supported file system, such as FAT32, is oftentimes more feasible.

See also Wikipedia:Comparison of boot loaders.

The boot loader boots the vmlinux image containing the kernel.

The kernel functions on a low level (kernelspace) interacting between the hardware of the machine and the programs. The kernel initially performs hardware enumeration and initialization before continuing to userspace. See Wikipedia:Kernel (operating system) and Wikipedia:Linux kernel for a detailed explanation.

An initramfs (initial RAM file system) image is a cpio archive providing the necessary files for early userspace (see below) to successfully start the late userspace. This predominantly means all kernel modules, user space tools, associated libraries, supporting files like udev rules, etc. required to locate, access and mount the root file system. With the concept of initramfs it is possible to handle even more complex setups, like e.g. booting from an external drive, stacked devices (logical volumes, software RAIDs, compression, encryption) or running a tiny SSH server in early userspace for remote unlocking or maintenance tasks of the root file system.

The majority of modules will be loaded during later stages of the init process by udev after having switched root to the root file system.

The process is as follows:

Initramfs images are Arch Linux' preferred method for setting up the early userspace and can be generated with mkinitcpio, dracut or booster.

Since 6.13.8 officially supported kernels have Btrfs and Ext4 drivers built-in [4].

This makes it possible for the kernel to use a root partition with these file systems directly and load the rest of external modules needed from there. Although, there are some quirks to keep in mind:

Another thing you really need initramfs for is early microcode loading. But it is not necessary to build full image for that, Arch provides microcode in separate initramfs files, which could be used independently.

If no initramfs image is provided, the kernel always contains still an empty image to start from [8]. So there should be no issues with root partition pinning.

The early userspace stage, a.k.a. the initramfs stage, takes place in rootfs consisting of the files provided by the #initramfs. Early userspace starts by the kernel executing the /init binary as PID 1.

The function of early userspace is configurable, but its main purpose is to bootstrap the system to the point where it can access the root file system. This includes:

Note that the early userspace serves more than just setting up the root file system. There are tasks that can only be performed before the root file system is mounted, such as fsck and resuming from hibernation.

At the final stage of early userspace, the real root is mounted at /sysroot/ (in case of a systemd-based initramfs) or at /new_root/ (in case of a busybox-based one), and then switched to by using systemctl switch-root when using systemd-based initramfs or switch_root(8) when using busybox-based initramfs. The late userspace starts by executing the init program from the real root file system.

The startup of late userspace is executed by the init process. Arch officially uses systemd which is built on the concept of units and services, but the functionality described here largely overlaps with other init systems.

The init process calls getty once for each virtual terminal (typically six of them). getty initializes each terminal and protects it from unauthorized access. When the username and password are provided, getty checks them against /etc/passwd and /etc/shadow, then calls login(1).

The login program begins a session for the user by setting environment variables and starting the user's shell, based on /etc/passwd. The login program displays the contents of /etc/motd (message of the day) after a successful login, just before it executes the login shell. It is a good place to display your Terms of Service to remind users of your local policies or anything you wish to tell them.

Once the user's shell is started, it will typically run a runtime configuration file, such as bashrc, before presenting a prompt to the user. If the account is configured to start X at login, the runtime configuration file will call startx or xinit. Jump to #Graphical session (Xorg) for the end.

This article or section needs expansion.

Additionally, init can be configured to start a display manager instead of getty on a specific virtual terminal. This requires manually enabling its systemd service file. The display manager then starts a graphical session.

xinit runs the user's xinitrc runtime configuration file, which normally starts a window manager or a desktop environment. When the user is finished and exits, xinit, startx, the shell, and login will terminate in that order, returning to getty or the display manager.

**Examples:**

Example 1 (unknown):
```unknown
/EFI/vendor_name
```

Example 2 (unknown):
```unknown
\EFI\BOOT\BOOTx64.EFI
```

Example 3 (unknown):
```unknown
BOOTIA32.EFI
```

Example 4 (unknown):
```unknown
esp/EFI/Linux/
```

---

## Domain name resolution

**URL:** https://wiki.archlinux.org/title/Resolv.conf

**Contents:**
- Name Service Switch
  - Resolve a domain name using NSS
- Glibc resolver
  - Overwriting of /etc/resolv.conf
    - Alternative using nmcli
  - Limit lookup time
  - Hostname lookup delayed with IPv6
  - Local domain names
- Lookup utilities
- Resolver performance

In general, a domain name represents an IP address and is associated to it in the Domain Name System (DNS). This article explains how to configure domain name resolution and resolve domain names.

This article or section needs expansion.

The Name Service Switch (NSS) facility is part of the GNU C Library (glibc) and backs the getaddrinfo(3) API, used to resolve domain names. NSS allows system databases to be provided by separate services, whose search order can be configured by the administrator in nsswitch.conf(5). The database responsible for domain name resolution is the hosts database, for which glibc offers the following services:

systemd provides three NSS services for hostname resolution:

NSS databases can be queried with getent(1). A domain name can be resolved through NSS using:

The glibc resolver reads /etc/resolv.conf for every resolution to determine the nameservers and options to use.

resolv.conf(5) lists nameservers together with some configuration options. Nameservers listed first are tried first, up to three nameservers may be listed. Lines starting with a number sign (#) are ignored.

Network managers tend to overwrite /etc/resolv.conf, for specifics see the corresponding section:

To prevent programs from overwriting /etc/resolv.conf, it is also possible to write-protect it by setting the immutable file attribute:

This article or section is a candidate for merging with NetworkManager#/etc/resolv.conf.

If you use NetworkManager, nmcli(1) can be used to set persistent options for /etc/resolv.conf. Change "Wired" to the name of your connection. Example:

For more options have a look at the man pages of nmcli(1), nm-settings-nmcli(5) and resolv.conf(5).

If you are confronted with a very long hostname lookup (may it be in pacman or while browsing), it often helps to define a small timeout after which an alternative nameserver is used. To do so, put the following in /etc/resolv.conf.

If you experience a 5 second delay when resolving hostnames it might be due to a DNS-server/Firewall misbehaving and only giving one reply to a parallel A and AAAA request.[1] You can fix that by setting the following option in /etc/resolv.conf:

To be able to use the hostname of local machine names without the fully qualified domain name, add a line to /etc/resolv.conf with the local domain such as:

That way you can refer to local hosts such as mainmachine1.example.org as simply mainmachine1 when using the ssh command, but the drill command still requires the fully qualified domain names in order to perform lookups.

To query specific DNS servers and DNS/DNSSEC records you can use dedicated DNS lookup utilities or those shipped with DNS servers. These tools implement DNS themselves and do not use NSS.

Some DNS server packages ship with DNS lookup utilities that can be used without running the DNS server:

The Glibc resolver does not cache queries. To implement local caching, use systemd-resolved or set up a local caching DNS server and use it as the name server by setting 127.0.0.1 and ::1 as the name servers in /etc/resolv.conf or in /etc/resolvconf.conf if using openresolv.

This article or section needs expansion.

The DNS protocol (Do53) is unencrypted and does not account for confidentiality, integrity or authentication, so if you use an untrusted network or a malicious ISP, your DNS queries can be eavesdropped and the responses manipulated. Furthermore, DNS servers can conduct DNS hijacking.

You need to trust your DNS server to treat your queries confidentially. DNS servers are provided by ISPs and third-parties. Alternatively you can run your own recursive name server (a.k.a. recursive resolver, a.k.a. DNS recursor), which however takes more effort. If you use a DHCP client in untrusted networks, be sure to set static name servers to avoid using and being subject to arbitrary DNS servers, or alternatively, use a VPN to connect to a secure network and use its DNS servers. To secure your communication with a remote DNS server you can use an encrypted protocol, provided that both the upstream server and your local resolver support the protocol. Common encrypted DNS protocols are:

To verify that responses are actually from authoritative name servers, you can validate DNSSEC, provided that both the upstream server(s) and your local resolver support it.

Although one may use an encrypted DNS resolver, a TLS connection still leaks the domain names in the Server Name Indication (SNI) when requesting the domain certificate. This leak can be checked using the Wireshark filter tls.handshake.extensions_server_name_len > 0, or using the following tshark command:

A proposed solution is to use the Encrypted Client Hello (ECH), a TLS 1.3 protocol extension.

Be aware that some client software, such as major web browsers[2][3], are starting to implement DNS over HTTPS. While the encryption of queries may often be seen as a bonus, it also means the software sidetracks queries around the system resolver configuration.[4]

Firefox provides configuration options to enable or disable DNS over HTTPS and select a DNS server. Mozilla has setup a Trusted Recursive Resolver (TRR) programme with transparency information on their default providers. It is notable that Firefox supports and automatically enables the Encrypted Client Hello (ECH) for TRR providers, see Firefox/Privacy#Encrypted Client Hello.

Chromium will examine the user's system resolver and enable DNS over HTTPS if the system resolver addresses are known to also provide DNS over HTTPS. See this blog post for more information and how DNS over HTTPS can be disabled.

Mozilla has proposed universally disabling application-level DNS if the system resolver cannot resolve the domain use-application-dns.net. Currently, this is only implemented in Firefox.

Oblivious DNS over HTTPS (ODoH)—RFC 9230—is a system which addresses a number of DNS privacy concerns. See Cloudflare's article for more information. It added DNS over HTTPS to the academic Oblivious DNS design. See the Improving the privacy of DNS and DoH with oblivion article for a discussion of the differences.

This article or section needs expansion.

Communication between recursive resolvers and root servers is not encrypted and the root server operators are against implementing it. For encrypted communication with authoritative servers there is the experimental RFC 9539 which allows the opportunistic use of DNS over TLS and DNS over QUIC.

There are various third-party DNS services. Wikipedia has a list of "notable" public DNS service operators while the curl project's wiki has a more extensive list of publicly available DNS over HTTPS servers (a lot of which also support DNS over TLS). The systemd package configures fallback DNS for systemd-resolved when no DNS servers are configured (manually or via DHCP/RA).

You can use dnsperftest to test the performance of the most popular DNS resolvers from your location. dnsperf.com provides global benchmarks between providers.

Some DNS services also provide dedicated software:

DNS servers can be authoritative and recursive. If they are neither, they are called stub resolvers and simply forward all queries to another recursive name server. Stub resolvers are typically used to introduce DNS caching on the local host or network. Note that the same can also be achieved with a fully-fledged name server. This section compares the available DNS servers, for a more detailed comparison, refer to Wikipedia:Comparison of DNS server software.

It is possible to use specific DNS resolvers when querying specific domain names. This is particularly useful when connecting to a VPN, so that queries to the VPN network are resolved by the VPN's DNS, while queries to the internet will still be resolved by your standard DNS resolver. It can also be used on local networks.

To implement it, you need to use a local resolver because glibc does not support it.

In a dynamic environment (laptops and to some extents desktops), you need to configure your resolver based on the network(s) you are connected to. The best way to do that is to use openresolv because it supports multiple subscribers. Some network managers support it, either through openresolv, or by configuring the resolver directly. NetworkManager supports conditional forwarding without openresolv.

**Examples:**

Example 1 (unknown):
```unknown
/etc/resolv.conf
```

Example 2 (unknown):
```unknown
$ getent ahosts domain_name
```

Example 3 (unknown):
```unknown
/etc/resolv.conf
```

Example 4 (unknown):
```unknown
/etc/resolv.conf
```

---

## systemd-firstboot

**URL:** https://wiki.archlinux.org/title/Systemd-firstboot

**Contents:**
- Installation
- Usage
  - Interactively configure system settings during boot of a fresh Arch Linux installation
    - Delete existing settings
    - Modify and enable systemd-firstboot.service
    - Finalize installation
- See also

Starting with version 216 of systemd, the command systemd-firstboot allows for setting of basic system settings before or during the first boot of a newly created system. The tool is able of initialize the following system settings: timezone, locale, hostname, the root password, as well as automated generation of a machine ID.

As systemd-firstboot interacts with the filesystem directly and does not make use of the related systemd services (such as timedatectl, hostnamectl or localectl), it should not be executed on an already running system.

Settings can be specified non-interactively when externally used on filesystem images, or interactively if executed during the early boot process.

A default Arch Linux installation will set most variables systemd-firstboot is able to manipulate, or facilitate the creation of skeleton files which prevent its use when installing the systemd package through pacstrap.

systemd-firstboot is part of and packaged with systemd.

Allowing systemd-firstboot to manipulate a previously un-booted Arch Linux installation is particularly useful in situations where installation is undertaken by an individual other than the eventual end user, such as in the distribution of laptops with a pre-loaded install.

The following steps should be appended to the end of the Configure the system section of the Installation guide, before the target partitions are unmounted, thus taking place within the chroot of the new installation. Make sure all locales you want available have been generated, non-generated ones will not be offered as a possible setting.

If the following files are present, systemd-firstboot will not prompt for the setting they relate to.

Edit /etc/passwd and remove the root account from it, otherwise the root will be treating as configured and systemd-firstboot will not prompt for the root password.

Use a drop-in file in which --prompt makes systemd-firstboot query for all possible settings and the [Install] section specifies where in the boot process the service is to be activated.

Enable systemd-firstboot.service

Continue installing as per the Installation guide. Unless more configuration is to be undertaken, exit the chroot, unmount the partitions and shut down. Upon the next boot, systemd-firstboot will execute. Presuming no other changes to system configuration are made, removing the files above and rebooting will trigger systemd-firstboot again, in case you wish to test whether the installation worked.

**Examples:**

Example 1 (unknown):
```unknown
# rm /etc/{machine-id,localtime,hostname,shadow,locale.conf}
```

Example 2 (unknown):
```unknown
/etc/passwd
```

Example 3 (unknown):
```unknown
/etc/systemd/system/systemd-firstboot.service.d/install.conf
```

Example 4 (unknown):
```unknown
[Service]
ExecStart=
ExecStart=/usr/bin/systemd-firstboot --prompt

[Install]
WantedBy=sysinit.target
```

---

## Autostarting

**URL:** https://wiki.archlinux.org/title/Autostart

**Contents:**
- On bootup / shutdown
- On user login / logout
- On device plug in / unplug
- On time events
- On filesystem events
- On shell login / logout
- On Xorg startup
- On desktop environment startup
- On window manager startup

This article links to various methods to launch scripts or applications automatically when some particular event is taking place.

Enable the relevant systemd services. If an application does not provide a systemd service, write your own.

Enable the relevant user unit.

Periodically at certain times, dates or intervals:

Once at a date and time:

Use an inotify event watcher:

See Command-line shell#Configuration files.

Most desktop environments implement XDG Autostart.

If the desktop environments has an article, see its Autostart section.

Many window managers (and compositors) implement XDG Autostart.

If it has an article, see its Autostart section.

---

## udev

**URL:** https://wiki.archlinux.org/title/Udev_rules

**Contents:**
- Installation
- Introduction to udev rules
  - Example
  - List the attributes of a device
  - Testing rules before loading
  - Loading new rules
- Components of udev rules
  - Action matching
    - The change action
  - Device attributes

udev (user space /dev) is a user space system that enables the operating system administrator to register user space handlers for events. The events received by udev's daemon are mainly generated by the Linux kernel in response to physical events relating to peripheral devices.

As such, udev main purpose is to act upon peripheral detection and hot-plugging, including actions that return control to the kernel, e.g. loading kernel modules or device firmware. Another component of this detection is adjusting the permissions of the device to be accessible to non-root users and groups. udev manages device nodes in the /dev directory by adding, symlinking and renaming them.

udev is part of systemd and thus installed by default. See systemd-udevd.service(8) for more information.

udev rules written by the administrator go in /etc/udev/rules.d/, their file name has to end with .rules. The udev rules shipped with various packages are found in /usr/lib/udev/rules.d/. If there are two files by the same name under /usr/lib and /etc, the ones in /etc take precedence.

To learn about udev rules, refer to the udev(7) manual. Also see Writing udev rules and some practical examples are provided within the guide: Writing udev rules - Examples.

Below is an example of a rule that creates a symlink /dev/video-cam when a webcamera is connected.

Let say this camera is currently connected and has loaded with the device name /dev/video2. The reason for writing this rule is that at the next boot, the device could show up under a different name, like /dev/video0.

To identify the webcamera, from the video4linux device we use KERNEL=="video2" and SUBSYSTEM=="video4linux", then walking up two levels above, we match the webcamera using vendor and product ID's from the usb parent SUBSYSTEMS=="usb", ATTRS{idVendor}=="05a9" and ATTRS{idProduct}=="4519". Note that this matching is case sensitive, so "05A9" can not be used to match the idVendor in this example.

We are now able to create a rule match for this device as follows:

Here we create a symlink using SYMLINK+="video-cam" but we could easily set user OWNER="john" or group using GROUP="video" or set the permissions using MODE="0660".

If you intend to write a rule to do something when a device is being removed, be aware that device attributes may not be accessible. In this case, you will have to work with preset device environment variables. To monitor those environment variables, execute the following command while unplugging your device:

In this command's output, you will see value pairs such as ID_VENDOR_ID and ID_MODEL_ID, which match the previously used attributes idVendor and idProduct. A rule that uses device environment variables instead of device attributes may look like this:

To get a list of all of the attributes of a device you can use to write rules, run this command:

Replace device_name with the device present in the system, such as /dev/sda or /dev/ttyUSB0.

If you do not know the device name you can also list all attributes of a specific system path:

To narrow down the search for a device, figure out the class and run:

You can use the symlink outright or what it points as the input to --name. For example:

To get the path of a bare USB device which does not populate any subordinate device you have to use the full USB device path. Start monitor mode and then plug in the USB device to get it:

You can just choose the deepest path and --attribute-walk will show all parent's attributes anyway:

This will not perform all actions in your new rules but it will however process symlink rules on existing devices which might come in handy if you are unable to load them otherwise. You can also directly provide the path to the device you want to test the udev rule for:

udev automatically detects changes to rules files, so changes take effect immediately without requiring udev to be restarted. However, the rules are not re-triggered automatically on already existing devices. Hot-pluggable devices, such as USB devices, will probably have to be reconnected for the new rules to take effect, or at least unloading and reloading the ohci-hcd and ehci-hcd kernel modules and thereby reloading all USB drivers.

If rules fail to reload automatically:

To manually force udev to trigger your rules:

ACTION=="" is used to only match against a device when something specific is happening, usually when it is appearing or disappearing. There are eight types of actions that can be raised for udev rules to match against:

The change action is somewhat special, because not all drivers use it the same way, if at all. A change event is only emitted as a result of the driver manually raising a userspace event (or uevent) of type KOBJ_CHANGE. This normally signals that something has happened to that device, but it takes some additional information to determine what exactly.

Since that event is only raised under a limited number of situations, here is a (best-effort, non-exhaustive) list of subsystems that do raise change events, and under what conditions.

Device attributes, listed as ATTR{my_attr}=="..." (or ATTRS{my_attr}=="..." for parent devices) in udev rules and udevadm info --attribute-walk, correspond to all the device information that is made available through sysfs, which exposes information from the "kobject" classes used by the kernel to keep track of device state. This means that there is no need for udev-specific tools to examine these, and that a simple ls /sys/class/thermal/thermal_zone0/ or cat /sys/class/input/input0/name is all that is needed to explore device attributes, and some can even be changed with something like echo 1 > "/sys/class/leds/input2::capslock/brightness". It is, however, a lot more conveinient to use udevadm info --attribute-walk /sys/class/input/mouse0, which has the advantage of showing all matchable attributes for that device all at once in the same format that udev rules expect (as well as the attributes for any parent devices, also in their readily usable format).

Most of these attributes have well-documented meanings, behaviors and accepted values based on what kernel submodule is handling the device, such as sysfs-bus-usb for USB devices and sysfs-class-typec for USB type-C ports, for instance. Other attributes are present as subtrees, such as sysfs-devices-physical_location or sysfs-devices-power, which use slashes to separate levels, just like directories, giving ATTR{power/control}=="auto".

The "environment" of an event is a set of device properties and event properties (and rarely global properties), both written as ENV{MY_PROPERTY}=="..." in udev rules and both reported as E: MY_PROPERTY=... by udevadm monitor --property and udevadm info (without --attribute-walk). Despite being called "environment", they have nothing to do with environment variables (although they do get passed as environment variables to programs started by RUN+="..."). These contain context information added to an event by the kernel module or other udev rules to make that information available to downstream rules or components. The only difference between event properties and device properties is that devices properties are stored and can be examined with udevadm info, whereas event properties are transient and can only be seen if the event is caught by udevadm monitor --property. A lot of device properties are also available as device attributes with similar names.

Unlike with attributes, udev rules are allowed to set event properties arbitrarily, and there is also no concept of "parent properties" to inspect beyond the ones that are set on the event (which is why there is no ENVS{...}=="" directive). Note that setting a property with a udev rule sets a device property, which will be stored until the device is removed and will thus appear in every event raised by that device (unless the property's name starts with a period, like ENV{.PART_SUFFIX}, which will be added to the event properties with the leading dot in the name and be usable by other rules, but not stored).

Tags (added using TAG+="foo", removed with TAG-="foo", matched with TAG=="foo") are used by userspace software interacting with udev to list and identify all devices they should be acting upon. Those can also be arbitrary, but there are a few well-known ones.

To mount removable drives, do not call mount from udev rules. This is ill-advised for two reasons:

There are some options that work:

Programs started by udev will block further events from that device, and any tasks spawned from a udev rule will be killed after event handling is completed. If you need to spawn a long-running process with udev, the intended way is to have a systemd unit which handles running the actual command, and a udev rule that merely signals that this unit should run. However, using systemctl in a udev rule is discouraged, since it is meant for user interaction and may block, among other things.

The correct way of doing this is to have the rule tag the device as needing a systemd device unit (see systemd.device(5)) using TAG+="systemd" and adding an device property of either ENV{SYSTEMD_WANTS}+= for services that would run with systemctl --system or ENV{SYSTEMD_USER_WANTS}+= for services that should run with systemctl --user. For example:

SYSTEMD_WANTS is equivalent to the Wants= directive elsewhere in systemd, meaning the device will not be affected if the service fails, does not exists, or completes successfully at any point.

When a kernel driver initializes a device, the default state of the device node is to be owned by root:root, with permissions 600. [1] This makes devices inaccessible to regular users unless the driver changes the default, or a udev rule in user space changes the permissions.

The OWNER, GROUP, and MODE udev values can be used to provide access, though one encounters the issue of how to make a device usable to all users without an overly permissive mode. Ubuntu's approach is to create a plugdev group that devices are added to, but this practice is not only discouraged by the systemd developers, [2] but considered a bug when shipped in udev rules on Arch (FS#35602). Another approach historically employed, as described in Users and groups#Pre-systemd groups, is to have different groups corresponding to categories of devices.

The modern recommended approach for systemd systems is to use a MODE of 660 to let the group use the device, and then attach a TAG named uaccess [3]. This special tag makes udev apply a dynamic user ACL to the device node, which coordinates with systemd-logind(8) to make the device usable to logged-in users. For an example of a udev rule implementing this:

Create the rule /etc/udev/rules.d/95-hdmi-plug.rules with the following content:

Create the rule /etc/udev/rules.d/95-monitor-hotplug.rules with the following content to launch arandr on plug in of a VGA monitor cable:

Some display managers store the .Xauthority outside the user home directory. You will need to update the ENV{XAUTHORITY} accordingly. As an example GNOME Display Manager looks as follows:

If your eSATA drive is not detected when you plug it in, there are a few things you can try. You can reboot with the eSATA plugged in. Or you could try:

Or you could install scsiaddAUR (from the AUR) and try:

Hopefully, your drive is now in /dev. If it is not, you could try the above commands while running:

to see if anything is actually happening.

If you connected an eSATA bay or another eSATA adapter, the system will still recognize this disk as an internal SATA drive. GNOME and KDE will ask you for your root password all the time. The following rule will mark the specified SATA-Port as an external eSATA-Port. With that, a normal GNOME user can connect their eSATA drives to that port like a USB drive, without any root password and so on.

Because udev loads all modules asynchronously, they are initialized in a different order. This can result in devices randomly switching names. A udev rule can be added to use static device names. See also Persistent block device naming for block devices and Network configuration#Change interface name for network devices.

For setting up the webcam in the first place, refer to Webcam setup.

Using multiple webcams will assign video devices as /dev/video* randomly on boot. The recommended solution is to create symlinks using a udev rule as in the #Example:

If you use multiple printers, /dev/lp[0-9] devices will be assigned randomly on boot, which will break e.g. CUPS configuration.

You can create the following rule, which will create symlinks under /dev/lp/by-id and /dev/lp/by-path, similar to Persistent block device naming scheme:

To perform some action on a specific disk device /dev/sdX identified permanently by its unique serial ID_SERIAL_SHORT as displayed with udevadm info /dev/sdX, one can use the below rule. It is passing as a parameter the device name found if any to illustrate:

A udev rule can be useful to enable the wakeup triggers of a USB device, like a mouse or a keyboard, so that it can be used to wake the system from sleep.

First, identify the vendor and product identifiers of the USB device. They will be used to recognize it in the udev rule. For example:

Then, find where the device is connected to using:

Now create the rule to change the power/wakeup attribute of both the device and the USB controller it is connected to whenever it is added:

USB devices need to be reset after exiting a suspended state (whether from the system resuming from sleep or from a port being turned off when the device was idle for power saving), which the Linux kernel handles mostly transparently in a process called reset-resume, so that USB drives do not look like they have been disconnected and reconnected every time. This is mostly desirable, but some devices, like USB TTY interfaces, do need to be manually reconfigured after being power cycled, which is not signalled by any uevent from the relevent drivers.

The one uevent that does get raised, however, is the one from losing and regaining power/wakeup, since USB devices are unusable while unconfigured and would be unable to wake up the system from sleep in this state. These two events have no unique event properties, but the first one can still be easily identified because DEVNUM is set to zero (which is not a valid device number) immediately before the device is unconfigured and loses power/wakeup, raising the uevent. When that happens, one can simply touch the bConfigurationValue sysfs attribute to force the system to reconfigure the device non-transparently, as if it had been disconnected during sleep, which unbinds all drivers and removes all children devices before adding them back when the device is ready again.

This article or section is a candidate for merging with #Testing rules before loading.

It can be useful to trigger various udev events. For example, you might want to simulate a USB device disconnect on a remote machine. In such cases, use udevadm trigger:

This command will trigger a USB remove event on all USB devices with vendor ID abcd.

To trigger a desktop notification from a udev rule, use systemd-run(1) as explained in Desktop notifications#Send notifications to another user:

To launch multiple or long commands, an executable script can be given to systemd-run:

In rare cases, udev can make mistakes and load the wrong modules. To prevent it from doing this, you can blacklist modules. Once blacklisted, udev will never load that module – not at boot-time and not even later on when a hot-plug event is received (e.g., you plug in your USB flash drive).

To get hardware debug info, use the kernel parameter udev.log-priority=debug. Alternatively you can set

This option can also be compiled into your initramfs by adding the configuration file to your FILES array:

and then regenerate the initramfs.

After migrating to LDAP or updating an LDAP-backed system, udevd can hang at boot at the message "Starting UDev Daemon". This is usually caused by udevd trying to look up a name from LDAP but failing, because the network is not up yet. The solution is to ensure that all system group names are present locally.

Extract the group names referenced in udev rules and the group names actually present on the system:

To see the differences, do a side-by-side diff:

In this case, the pcscd group is for some reason not present in the system. Add the missing groups. Also, make sure that local resources are looked up before resorting to LDAP. /etc/nsswitch.conf should contain the following line:

You need to create a custom udev rule for that particular device. To get definitive information of the device you can use either ID_SERIAL or ID_SERIAL_SHORT (remember to change /dev/sdb if needed):

Then we set UDISKS_AUTO="1" to mark the device for automounting and UDISKS_SYSTEM="0" to mark the device as "removable". See udisks(8) for details.

Remember to reload udev rules with udevadm control --reload. Next time you plug your device in, it will be treated as an external drive.

If the group ID of your optical drive is set to disk and you want to have it set to optical, you have to create a custom udev rule:

When xrandr or another X-based program tries to connect to an X server, it falls back to a TCP connection on failure. However, due to IPAddressDeny in the systemd-udev service configuration, this hangs. Eventually the program will be killed and event processing will resume.

If the rule is for a drm device and the hang causes event processing to complete once the X server has started, this can cause 3D acceleration to stop working with a failed to authenticate magic error.

**Examples:**

Example 1 (unknown):
```unknown
…/by-path/…
```

Example 2 (unknown):
```unknown
…/by-uuid/…
```

Example 3 (unknown):
```unknown
/etc/udev/rules.d/
```

Example 4 (unknown):
```unknown
/usr/lib/udev/rules.d/
```

---

## systemd/Timers

**URL:** https://wiki.archlinux.org/title/Systemd_timer

**Contents:**
- Timer units
- Service units
- Management
- Examples
  - Monotonic timer
  - Realtime timer
- Transient timer units
- As a cron replacement
  - Benefits
  - Caveats

Timers are systemd unit files whose name ends in .timer that control .service files or events. Timers can be used as an alternative to cron (read #As a cron replacement). Timers have built-in support for calendar time events, monotonic time events, and can be run asynchronously.

Timers are systemd unit files with a suffix of .timer. Timers are like other unit configuration files and are loaded from the same paths but include a [Timer] section which defines when and how the timer activates. Timers are defined as one of two types:

For a full explanation of timer options, see the systemd.timer(5). The argument syntax for calendar events and time spans is defined in systemd.time(7).

For each .timer file, a matching .service file exists (e.g. foo.timer and foo.service). The .timer file activates and controls the .service file. The .service does not require an [Install] section as it is the timer units that are enabled. If necessary, it is possible to control a differently-named unit using the Unit= option in the timer's [Timer] section.

To use a timer unit enable and start it like any other unit (remember to add the .timer suffix). To view all started timers, run:

A service unit file can be scheduled with a timer out-of-the-box. The following examples schedule foo.service to be run with a corresponding timer called foo.timer.

A timer which will start 15 minutes after boot and again every week while the system is running.

A timer which starts once a week (at 12:00am on Monday). When activated, it triggers the service immediately if it missed the last start time (option Persistent=true), for example due to the system being powered off:

When more specific dates and times are required, OnCalendar events uses the following format:

An asterisk may be used to specify any value and commas may be used to list possible values. Two values separated by .. indicate a contiguous range.

In the below example the service is run the first four days of each month at 12:00 PM, but only if that day is a Monday or a Tuesday.

To run a service on the first Saturday of every month, use:

When using the DayOfWeek part, at least one weekday has to be specified. If you want something to run every day at 4am, use:

To run a service at different times, OnCalendar may be specified more than once. In the example below, the service runs at 22:30 on weekdays and at 20:00 on weekends.

You can also specify a timezone at the end of the directive (use timedatectl list-timezones to list accepted values)

More information is available in systemd.time(7).

One can use systemd-run to create transient .timer units. That is, one can set a command to run at a specified time without having a service file. For example the following command touches a file after 30 seconds:

One can also specify a pre-existing service file that does not have a timer file. For example, the following starts the systemd unit named someunit.service after 12.5 hours have elapsed:

See systemd-run(1) for more information and examples.

Although cron is arguably the most well-known job scheduler, systemd timers can be an alternative.

The main benefits of using timers come from each job having its own systemd service. Some of these benefits are:

Some things that are easy to do with cron are difficult to do with timer units alone:

Also note that user timer units will only run during an active user login session by default. However, lingering can enable services to run at boot even when the user has no active login session.

Several of the caveats can be worked around by installing a package that parses a traditional crontab to configure the timers. systemd-cron-next-gitAUR and systemd-cronAUR are two such packages. These can provide the missing MAILTO feature.

Also, like with crontabs, a unified view of all scheduled jobs can be obtained with systemctl. See #Management.

Outside of migrating from an existing crontab, using the same periodicity as cron can be desired. To avoid the tedious task of creating a timer for each service to start periodically, use a template unit, for example:

Then one only needs to enable/start monthly@unit_name.timer.

Some software will track the time elapsed since they last ran, for example blocking the update of a database if the last download ended less than 24 hours ago.

By default, timers do not track when the task they launched has ended. To work around this, we can use OnUnitInactiveSeconds:

The systemd-timer-notifyAUR provides an automatic desktop notification that helps you notice when a systemd service is triggered by a timer and is running. The notification will automatically close when the service finishes.

This can be helpful for understanding why CPU usage is high or for preventing a shutdown when a backup service hasn't finished.

For more details and configuration options, visit https://gitlab.com/Zesko/systemd-timer-notify

**Examples:**

Example 1 (unknown):
```unknown
OnCalendar=
```

Example 2 (unknown):
```unknown
OnUnitActiveSec
```

Example 3 (unknown):
```unknown
timers.target
```

Example 4 (unknown):
```unknown
WantedBy=timers.target
```

---

## polkit

**URL:** https://wiki.archlinux.org/title/Polkit

**Contents:**
- Installation
  - Authentication agents
- Configuration
  - Actions
  - Authorization rules
  - Administrator identities
- Examples
  - Allow a user to use the org.freedesktop.timedate1.set-timezone action
  - Debugging/logging
  - Disable suspend and hibernate

From polkit homepage:

Polkit is used for controlling system-wide privileges. It provides an organized way for non-privileged processes to communicate with privileged ones. In contrast to systems such as sudo, it does not grant root permission to an entire process, but rather allows a finer level of control of centralized system policy.

Polkit works by delimiting distinct actions, e.g. running GParted, and delimiting users by group or by name, e.g. members of the wheel group. It then defines how – if at all – those users are allowed those actions, e.g. by identifying as members of the group by typing in their passwords.

Install the polkit package.

An authentication agent is used to make the user of a session prove that they really are the user (by authenticating as the user) or an administrative user (by authenticating as an administrator). The polkit package contains pkttyagent, a textual authentication agent which is used as a general fallback.

If you are using a graphical environment, make sure that a graphical authentication agent is installed and autostarted on login (e.g. via xinitrc).

Cinnamon, Deepin, Hyprland, GNOME, GNOME Flashback, KDE, LXDE, LXQt, MATE, and Xfce have an authentication agent already. In other desktop environments, you have to choose one of the following implementations:

Polkit definitions can be divided into two kinds:

Polkit operates on top of the existing permissions systems in Linux – group membership, administrator status – it does not replace them. The .rules files designate a subset of users, refer to one (or more) of the actions specified in the actions files, and determine with what restrictions these actions can be taken by those users. As an example, a rules file could overrule the default requirement for all users to authenticate as an admin when using GParted, determining that some specific user does not need to. A different example: A certain user is not allowed to use GParted at all.

The actions available to you via polkit will depend on the packages you have installed. Some are used in multiple desktop environments (org.freedesktop.*), some are DE-specific (org.gnome.*) and some are specific to a single program (org.gnome.gparted.policy). The command pkaction lists all the actions defined in /usr/share/polkit-1/actions for quick reference.

To get an idea of what polkit can do, here are a few commonly used groups of actions:

Each action is defined in an <action> tag in a .policy file. The org.gnome.gparted.policy contains a single action and looks like this:

The attribute id is the actual command sent to D-Bus, the message tag is the explanation to the user when authentication is required and the icon_name is sort of obvious.

The defaults tag is where the permissions or lack thereof are located. It contains three settings: allow_any, allow_inactive, and allow_active. Both inactive and active here refer to local sessions on local consoles or displays, whereas the allow_any setting is used for all others, including remote sessions (SSH, VNC, etc.).

For each of these settings the following options are available:

These are default setting and unless overruled in later configuration will be valid for all users.

See the polkit(8) man page for a detailed explanation.

As can be seen from the GParted action, users are required to authenticate as administrators in order to use GParted, regardless of whether the session is active or inactive.

Authorization rules that overrule the default settings are laid out in a set of directories as described above. For all purposes relating to personal configuration of a single system, only /etc/polkit-1/rules.d should be used.

The addRule() method is used for adding a function that may be called whenever an authorization check for action and subject is performed. Functions are called in the order they have been added until one of the functions returns a value. Hence, to add an authorization rule that is processed before other rules, put it in a file in /etc/polkit-1/rules.d with a name that sorts before other rules files, for example 00-early-checks.rules.

The layout of the .rules files is fairly self-explanatory:

Inside the function, we check for the specified action ID (org.gnome.gparted) and for the user's group (admin), then return a value "yes".

The addAdminRule() method is used for adding a function that may be called whenever administrator authentication is required. The function is used to specify what identities may be used for administrator authentication for the authorization check identified by action and subject. Functions added are called in the order they have been added until one of the functions returns a value.

The default configuration for administrator identities is contained in the file /usr/share/polkit-1/rules.d/50-default.rules so any changes to that configuration should be made by copying the file to the /etc/polkit-1/rules.d directory and editing that file:

The only part to edit (once copied) is the return array of the function: as whom should a user authenticate when asked to authenticate as an administrative user? If they are a member of the group designated as admins, they only need enter their own password. If some other user, e.g. root, is the only admin identity, they would need to enter the root password. The format of the user identification is the same as the one used in designating authorities.

The Arch default is to make all members of the group wheel administrators. A rule like below will have polkit ask for the root password instead of the users password for Admin authentication.

To allow a user named archie to use the org.freedesktop.timedate1.set-timezone action without authentication, create the following polkit rule file as root:

After saving the rule file, the policy should take effect immediately. You can test it by setting the timezone using the timedatectl:

If the operation completes without asking for authentication, then the rule works as intended. If the action does not seem to be allowed, ensure there are no conflicting rules with higher precedence (lower number prefixes) in /etc/polkit-1/rules.d/.

To enable logging with polkit.log() function, remove the --no-debug flag from the ExecStart command of polkit.service file; either by editing the unit temporarily (with systemctl --runtime) or permanently with:

The following rule logs detailed information about any requested access:

To manually test rules, use pkcheck:[1]

The factual accuracy of this article or section is disputed.

The following rule disables suspend and hibernate for all users.

To achieve something similar to the sudo NOPASSWD option and get authorized solely based on user/group identity, you can create custom rules in /etc/polkit-1/rules.d/. This allows you to override password authentication either only for specific actions or globally. See [2] for an example rule set.

Create the following file as root:

Replace wheel by any group of your preference.

This will result in automatic authentication for any action requiring admin rights via Polkit. As such, be careful with the group you choose to give such rights to.

There is also AUTH_ADMIN_KEEP which allows to keep the authorization for 5 minutes. However, the authorization is per process, hence if a new process asks for an authorization within 5 minutes the new process will ask for the password again anyway. In particular, run0 and pkexec do not keep authorization such as sudo. The feature is supported at polkit-git (see [3]).

Create the following file as root:

The action.ids selected here are just (working) examples for GParted and Libvirt, but you can replace them by any other of your liking as long as they exist (custom made or supplied by a package), and so can you define any group instead of wheel.

The || operator is used to delimit actions (logical OR), and && means logical AND and must be kept as the last operator.

File managers may ask for a password when trying to mount a storage device, or yield a Not authorized or similar error. See Udisks#Configuration for details.

By checking for certain values passed to the polkit policy check, you can give specific users or groups the ability to manage specific units. As an example, you might want regular users to start and stop wpa_supplicant:

**Examples:**

Example 1 (unknown):
```unknown
/usr/lib/hyprpolkitagent/hyprpolkitagent
```

Example 2 (unknown):
```unknown
/usr/bin/lxqt-policykit-agent
```

Example 3 (unknown):
```unknown
/usr/bin/lxpolkit
```

Example 4 (unknown):
```unknown
/usr/lib/mate-polkit/polkit-mate-authentication-agent-1
```

---

## fwupd

**URL:** https://wiki.archlinux.org/title/Fwupd

**Contents:**
- Installation
  - Graphical front-ends
- Usage
- Configuration
  - Disable local cache server (passim)
- Setup for UEFI upgrade
  - Prepare ESP
  - Secure Boot
    - Using your own keys
      - Assisted process with sbctl

fwupd is a simple daemon to allow session software to update device firmware on your local machine. It's designed for desktops, but also usable on phones and headless servers.

Supported devices are listed here and more are to come.

Install the fwupd package.

Most peripherals can be updated directly in the OS. However, BIOS/UEFI updates generally require UEFI mode (UpdateCapsule), where the firmware applies the update safely on reboot. See #Setup for UEFI upgrade if you intend such use.

Certain desktop environments front-end solutions have built-in fwupd support:

This article or section needs expansion.

The package provides a fwupd.service which will automatically start the fwupd daemon when the first query is received. [1]

To display all devices detected by fwupd:

To download the latest metadata from the Linux Vendor firmware Service (LVFS):

To list updates available for any devices on the system:

fwupd v1.9.5 from September 2023 introduced a dependency on passim, a local cache server intended to help reduce LVFS bandwidth usage by making each machine able to serve the metadata file it downloads everyday to others[2][3].

passimd is a daemon which listens for connections on port 27500 from any IP addresses (i.e. it listens on 0.0.0.0:27500). This has led to some criticism regarding the security implications[4][5], and indeed several vulnerabilities were reported just a few weeks later[6][7].

On Arch the request from FS#79614 to make the dependency optional at compile-time was denied because it would require creating a split-package for a library.

As a consequence, if you wish to disable passimd you should follow the advice given by the author[8]: add P2pPolicy=nothing to /etc/fwupd/fwupd.conf and/or mask passim.service.

The following requirements should be met:

fwupd will copy all the necessary files over to the esp, but for this to work, a basic folder layout must be present on your esp; this constitutes the creation of an EFI directory on your esp:

Restart the fwupd.service unit afterwards. You can now fwupdmgr refresh and fwupdmgr update. You will be prompted to reboot (into the firmware updater).

Currently, fwupd relies on shim to chainload the fwupd EFI binary on systems with Secure Boot enabled; for this to work, shim has to be installed correctly.

You can sign the UEFI executable with sbctl. For an explanation of how to setup sbctl, see Unified Extensible Firmware Interface/Secure Boot with sbctl.

Then after each update of fwupd, the UEFI executable will be automatically signed, thanks to the sbctl pacman hook (/usr/share/libalpm/hooks/zz-sbctl.hook).

Finally, you have to set DisableShimForSecureBoot in /etc/fwupd/fwupd.conf and restart fwupd.service:

Alternatively, you can manually sign the UEFI executable used to perform upgrades, which is located in /usr/lib/fwupd/efi/fwupdx64.efi. The signed UEFI executable is expected in /usr/lib/fwupd/efi/fwupdx64.efi.signed. Using sbsigntools, this can be achieved by running:

To automatically sign this file when installed or upgraded, a Pacman hook can be used:

Make sure to replace keyfile and certfile with the corresponding paths of your keys.

Instead of a pacman hook, you can also create a symlink from /usr/lib/fwupd/efi/fwupdx64.efi to /usr/lib/fwupd/efi/fwupdx64.efi.signed, and add the file to the EXTRA_SIGN list in /etc/sbupdate.conf.

Finally, you have to set DisableShimForSecureBoot in /etc/fwupd/fwupd.conf and restart fwupd.service:

See https://github.com/fwupd/fwupd/issues/669 for more information.

fwupdmgr update reports no error, but the reboot it prompts stuck and holding the power button has no response. Try switching off the power, or press the reset button (on a laptop, it might be a hole on the back) to force-reboot.

Symptom: fwupdmgr update reports no error and prompts for reboot (e.g., on BIOS update). However, the system reboots normally (or stuck) and the firmware update does NOT happen.

Possible cause: In BIOS settings changing the boot order must be allowed.

Possible other solution if there are multiple updates pending: Try updating packages one at a time. Use the following to select packages:

(Where update_ID is something like f95c9218acd12697af946874bfe4239587209232.)

At least fwupdmgr 1.5.2 deduces the wrong mount point if bind is used to mount the EFI system partition to /boot. Consequently it fails to write the UEFI update file to /boot/EFI/arch/fw (fwupdmgr while it should be written to esp/EFI/arch/fw.) This results in a (misleading) file system is read-only error message. In case the update was performed by Discover (or any other fwupd-capable Update GUI), no error or misleading errors may be shown.

As a workaround, run umount /boot first if it was bind-mounted to esp/EFI/arch before, then run fwupdmgr update to write the UEFI update file to esp/EFI/arch/fw, mount /boot and reboot the system to perform the UEFI update.

If the EFI system partition (ESP) is still not detected after all requirements in #Setup for UEFI upgrade are met, the mount point can be specified manually:

See the relevant article in the fwupd wiki for additional reasons that this might occur.

Setting the ESP location prevents fwupdx64.efi from being installed in unwanted EFI system partitions located on other disks.

The MSR plugin allows querying the state of DCI, a debugging interface available for Intel CPUs that should be disabled on production machines according to fwupd's documentation.

This plugin needs the msr kernel module loaded. msr is a built-in kernel module in all the official Arch Linux kernel packages, but unofficial kernel packages might have it as a loadable kernel module. In the latter case, we need to explicitly load the module at boot.

When starts fwupd, it checks the esp location as EspLocation from /etc/fwupd/fwupd.conf. Modify it to your corresponding setup if encounter this error.

**Examples:**

Example 1 (unknown):
```unknown
fwupd.service
```

Example 2 (unknown):
```unknown
$ fwupdmgr get-devices
```

Example 3 (unknown):
```unknown
$ fwupdmgr refresh
```

Example 4 (unknown):
```unknown
fwupd-refresh.timer
```

---

## limits.conf

**URL:** https://wiki.archlinux.org/title/Limits.conf

**Contents:**
- Syntax
- Recommendations
  - core
  - nice
  - nofile
  - nproc
  - priority

This article or section is a candidate for moving to System limits.

/etc/security/limits.conf allows setting resource limits for users logged in via PAM. This is a useful way of preventing, for example, fork-bombs from using up all system resources.

The default file comes well-commented, but extra information can be gleaned by checking the limits.conf(5) man page.

Corefiles are useful for debugging, but annoying when normally using your system. You should have a soft limit of 0 and a hard limit of unlimited, and then temporarily raise your limit for the current shell with ulimit -c unlimited when you need corefiles for debugging.

You should disallow everyone except for root from having processes of minimal niceness (-20), so that root can fix an unresponsive system.

This limits the number of file descriptors any process owned by the specified domain can have open at any one time. You may need to increase this value to something as high as 8192 for certain games to work. Some database applications like MongoDB or Apache Kafka recommend setting nofile to 64000 or 128000[1].

Having an nproc limit is important, because this will limit how many times a fork-bomb can replicate. However, having it too low can make your system unstable or even unusable, as new processes will not be able to be created.

A value of 300 is too low for even the most minimal of Window-managers to run more than a few desktop applications and daemons, but is often fine for an X-less server (In fact, 300 is the value that the University of Georgia's Computer Science department used for the undergrad process limit on its Linux servers in 2017.).

Here is an example nproc limit for all users on a system:

Note that this value of 2048 is just an example, and you may need to set yours higher. On the flipside, you also may be able to do with it being lower.

Whatever you set your nproc to, make sure to allow your root user to create as many processes as it wants; else, you might make your system inoperable by setting the normal nproc limit too low. Note that this line has to come after the global hardlimit, and that the value below (65536) is arbitrary.

The default niceness should generally be 0, but you can set individual users and groups to have different default priorities using this parameter.

**Examples:**

Example 1 (unknown):
```unknown
/etc/security/limits.conf
```

Example 2 (unknown):
```unknown
/etc/systemd/system.conf
```

Example 3 (unknown):
```unknown
/etc/systemd/user.conf
```

Example 4 (unknown):
```unknown
/etc/systemd/system/unit.d/override.conf
```

---

## systemd-resolved

**URL:** https://wiki.archlinux.org/title/Systemd-resolvconf

**Contents:**
- Installation
- Configuration
  - DNS
    - Setting DNS servers
      - Automatically
      - Manually
      - Fallback
    - DNSSEC
    - DNS over TLS
      - Global DNS over TLS

systemd-resolved is a systemd service that provides network name resolution to local applications via a D-Bus interface, the resolve NSS service (nss-resolve(8)), and a local DNS stub listener on 127.0.0.53. See systemd-resolved(8) for the usage.

systemd-resolved is a part of the systemd package that is installed by default.

systemd-resolved provides resolver services for Domain Name System (DNS) (including DNSSEC and DNS over TLS), Multicast DNS (mDNS) and Link-Local Multicast Name Resolution (LLMNR).

The resolver can be configured by editing /etc/systemd/resolved.conf and/or drop-in .conf files in /etc/systemd/resolved.conf.d/. See resolved.conf(5).

To use systemd-resolved start and enable systemd-resolved.service.

Software that relies on glibc's getaddrinfo(3) (or similar) will work out of the box, since, by default, /etc/nsswitch.conf is configured to use nss-resolve(8) if it is available.

To provide domain name resolution for software that reads /etc/resolv.conf directly, such as web browsers, Go, GnuPG and QEMU when using user networking, systemd-resolved has four different modes for handling the file—stub, static, uplink and foreign. They are described in systemd-resolved(8) § /ETC/RESOLV.CONF. We will focus here only on the recommended mode, i.e. the stub mode which uses /run/systemd/resolve/stub-resolv.conf.

/run/systemd/resolve/stub-resolv.conf contains the local stub 127.0.0.53 as the only DNS server and a list of search domains. This is the recommended mode of operation that propagates the systemd-resolved managed configuration to all clients. To use it, replace /etc/resolv.conf with a symbolic link to it:

systemd-resolved will work out of the box with a network manager using /etc/resolv.conf. No particular configuration is required since systemd-resolved will be detected by following the /etc/resolv.conf symlink. This is going to be the case with systemd-networkd, NetworkManager, and iwd.

However, if the DHCP and VPN clients use the resolvconf program to set name servers and search domains (see openresolv#Users for a list of software that use resolvconf), the additional package systemd-resolvconf is needed to provide the /usr/bin/resolvconf symlink.

In stub and static modes, custom DNS server(s) can be set in the resolved.conf(5) file:

For more information on per-link configuration see systemd.network(5).

If systemd-resolved does not receive DNS server addresses from the network manager and no DNS servers are configured manually then systemd-resolved falls back to the fallback DNS addresses to ensure that DNS resolution always works.

The addresses can be changed by setting FallbackDNS in resolved.conf(5). E.g.:

To disable the fallback DNS functionality set the FallbackDNS option without specifying any addresses:

The systemd package is built with DNSSEC validation disabled by default. This can be changed with the DNSSEC setting in resolved.conf(5).

Test DNSSEC validation by querying a domain name with no signature:

Now test a domain with valid signature:

DNS over TLS is disabled by default. To enable it change the DNSOverTLS setting in the [Resolve] section in resolved.conf(5). To enable validation of your DNS provider's server certificate, include their hostname in the DNS setting in the format ip_address#hostname. For example:

ngrep can be used to test if DNS over TLS is working since DNS over TLS always uses port 853 and never port 53. The command ngrep port 53 should produce no output when a hostname is resolved with DNS over TLS and ngrep port 853 should produce encrypted output.

Wireshark can be used for more detailed packet inspection of DNS over TLS queries.

Enabling DNS over TLS for specific connections depends on the network manager:

systemd-resolved answers DNS requests to local applications via loopback interface per default. To make systemd-resolved answer DNS requests on additional interfaces or addresses than the default one, set the option DNSStubListenerExtra for every additional interface in resolved.conf(5). For example:

systemd-resolved is capable of working as a multicast DNS resolver and responder.

The resolver provides hostname resolution using a "hostname.local" naming scheme.

mDNS will only be activated for a connection if both systemd-resolved's mDNS support is enabled, and if the configuration for the currently active network manager enables mDNS for the connection.

systemd-resolved's mDNS support is enabled by default. It can be disabled by its MulticastDNS setting (see resolved.conf(5) § OPTIONS).

Enabling per-connection mDNS support depends on the network manager:

Link-Local Multicast Name Resolution is a hostname resolution protocol created by Microsoft.

LLMNR will only be activated for the connection if both the systemd-resolved's global setting (LLMNR in resolved.conf(5) § OPTIONS) and the network manager's per-connection setting is enabled. By default systemd-resolved enables LLMNR responder; systemd-networkd and NetworkManager[3] enable it for connections.

If you plan to use LLMNR and use a firewall, make sure to open UDP and TCP ports 5355.

To query DNS records, mDNS or LLMNR hosts you can use the resolvectl utility.

For example, to query a DNS record:

systemd-resolved may not search the local domain when given just the hostname, even when UseDomains=yes or Domains=[domain-list] is present in the appropriate systemd-networkd's .network file, and that file produces the expected search [domain-list] in resolv.conf. You can run networkctl status or resolvectl status to check if the search domains are actually being picked up.

Possible workarounds:

To make systemd-resolved resolve hostnames that are not fully qualified domain names, add ResolveUnicastSingleLabel=yes to /etc/systemd/resolved.conf.

This only seems to work with LLMNR disabled (LLMNR=no).

If you are using systemd-networkd, you might want the domain supplied by the DHCP server or IPv6 Router Advertisement to be used as a search domain. This is disabled by default, to enable it add to the interface's .network file:

You can check what systemd-resolved has for each interface with:

**Examples:**

Example 1 (unknown):
```unknown
/etc/systemd/resolved.conf
```

Example 2 (unknown):
```unknown
/etc/systemd/resolved.conf.d/
```

Example 3 (unknown):
```unknown
systemd-resolved.service
```

Example 4 (unknown):
```unknown
/etc/nsswitch.conf
```

---

## dm-crypt/System configuration

**URL:** https://wiki.archlinux.org/title/Dm-crypt/System_configuration

**Contents:**
- Unlocking in early userspace
  - mkinitcpio
    - Examples
  - Kernel parameters
    - root
    - resume
    - Using encrypt hook
      - cryptdevice
      - cryptkey
      - crypto

This article or section needs expansion.

Booting an encrypted root volume requires that the initramfs contains the necessary tools for early userspace to unlock the volume. The instructions on what to unlock are typically passed via kernel parameters.

The following sections describe how to configure mkinitcpio and list which kernel parameters are required.

Depending on the particular scenarios, a subset of the following mkinitcpio hooks will have to be enabled:

Other hooks needed should be clear from other manual steps followed during the installation of the system.

A typical /etc/mkinitcpio.conf configuration using encrypt hook:

A configuration with systemd-based initramfs using sd-encrypt hook:

The kernel parameters you need to specify depend on whether the encrypt hook or the sd-encrypt hook is being used. root and resume are specified the same way for both.

The root= parameter specifies the device of the actual (decrypted) root file system:

This specifies the device containing the encrypted root on a cold boot. It is parsed by the encrypt hook to identify which device contains the encrypted system:

This parameter specifies the location of a keyfile and is required by the encrypt hook for reading such a keyfile to unlock the cryptdevice (unless a key is in the default location, see below). It can have three parameter sets, depending on whether the keyfile exists as a file in a particular device, a bitstream starting on a specific location, or a file in the initramfs.

For a file in a device the format is:

Example: cryptkey=LABEL=usbstick:vfat:/secretkey

For a bitstream on a device the key's location is specified with the following:

where the offset and size are in bytes. For example, cryptkey=UUID=ZZZZZZZZ-ZZZZ-ZZZZ-ZZZZ-ZZZZZZZZZZZZ:0:512 reads a 512 byte keyfile starting at the beginning of the device.

For a file included in the initramfs the format is[1]:

Example: cryptkey=rootfs:/secretkey

Also note that if cryptkey is not specified, it defaults to /crypto_keyfile.bin (in the initramfs).[2]

See also dm-crypt/Device encryption#Keyfiles.

This parameter is specific to pass dm-crypt plain mode options to the encrypt hook.

The arguments relate directly to the cryptsetup options. See dm-crypt/Device encryption#Encryption options for plain mode.

For a disk encrypted with just plain default options, the crypto arguments must be specified, but each entry can be left blank:

A specific example of arguments is

systemd-cryptsetup-generator is a systemd unit generator that reads a subset of kernel parameters, and /etc/crypttab, for the purpose of unlocking encrypted devices. See the systemd-cryptsetup-generator(8) man page for more details about it and all options it supports.

systemd-cryptsetup-generator is run during the initramfs stage when using the sd-encrypt mkinitcpio hook or the systemd dracut module.

In what follows, we describe some of the kernel parameters that systemd-cryptsetup-generator interprets.

Specify the UUID of the device to be decrypted on boot with this flag.

By default, the mapped device will be located at /dev/mapper/luks-XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX where XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX is the UUID of the LUKS partition.

Specify the name of the mapped device after the LUKS partition is open, where XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX is the UUID of the LUKS partition. This is equivalent to the second parameter of encrypt's cryptdevice.

For example, specifying rd.luks.name=12345678-9abc-def0-1234-56789abcdef0=root causes the unlocked LUKS device with UUID 12345678-9ABC-DEF0-1234-56789ABCDEF0 to be located at /dev/mapper/root.

Specify the location of a password file used to decrypt the device specified by its UUID. There is no default location like there is with the encrypt hook parameter cryptkey.

If the keyfile is included in the initramfs:

If the keyfile is on another device:

Replace UUID=ZZZZZZZZ-ZZZZ-ZZZZ-ZZZZ-ZZZZZZZZZZZZ with the identifier of the device on which the keyfile is located.

Set options for the device specified by it UUID or, if not specified, for all UUIDs not specified elsewhere (e.g., crypttab).

This parameter is the analogue of crypttab's options field. The format is the same—options are separated by commas, options with values are specified using option=value. This is roughly equivalent to the third parameter of encrypt's cryptdevice.

There are two options that affect the timeout for entering the password during boot:

If you want to disable the timeout altogether, then set both timeouts to zero:

When the user is typing the password, systemd-cryptsetup by default outputs asterisks (*) for each typed character. This is unlike the encrypt hook, which does not output anything. To silence the output, set the password-echo=no option:

If a TPM2 chip is available in your system, or you use FIDO2-compatible security key, you can use it to automatically unlock your volume instead of using a password or a keyfile.

In addition to rd.luks.uuid or rd.luks.name, set:

Alternatively, using /etc/crypttab.initramfs:

Here the encrypted volume is mounted under the name root (appearing in /dev/mapper/root), mounted via the UUID of the storage device, with no password, and retrieving the key from the TPM2 device.

Note that none or - must be provided in the password field order for the TPM2 device to be used, otherwise the value given will be used as a password or key, and if it does not work it will ask you to type in the passkey during boot without attempting to load the key from the TPM2 device.

If specifying the device via UUID as shown above, ensure it is that of the underlying (encrypted) storage device, not the UUID of the decrypted volume that is specified elsewhere as the root filesystem.

When using a detached LUKS header, specify the block device with the encrypted data. Must be used together with rd.luks.options to specify the header file location.

See dm-crypt/Specialties#Encrypted system using a detached LUKS header for details and instructions.

If all the prerequisites of systemd#GPT partition automounting are met, you can avoid specifying any rd.luks kernel parameters. systemd-cryptsetup-generator will automatically try to unlock the LUKS-encrypted root partition.

To persistently reference discovered partitions in configuration files, use the identifiers from Persistent block device naming#by-designator and gpt-auto. If you want to specify crypttab options for the root partition, use /dev/disk/by-designator/root-luks as the device in /etc/crypttab.initramfs. For example, /etc/crypttab.initramfs without kernel options and no password:

The /etc/crypttab (encrypted device table) file is similar to the fstab file and contains a list of encrypted devices to be unlocked during system boot up. This file can be used for automatically mounting encrypted swap devices or secondary file systems.

crypttab is read before fstab, so that dm-crypt containers can be unlocked before the file system inside is mounted. Note that crypttab is read after the system has booted up, therefore it is not a replacement for unlocking encrypted partitions by using mkinitcpio hooks and configuring them by using kernel parameters as in the case of encrypting the root partition. crypttab processing at boot time is made by the systemd-cryptsetup-generator(8) automatically.

See crypttab(5) for details, read below for some examples, and the #Mounting at boot time section for instructions on how to use UUIDs to mount an encrypted device.

To test your crypttab immediately after editing it, reload the systemd manager configuration with a daemon-reload and start the newly generated systemd-cryptsetup@name.service.

For more on systemd-cryptsetup@name.service, see #Mounting on demand.

This article or section is a candidate for merging with #crypttab.

If you want to mount an encrypted drive at boot time, enter the device's UUID in /etc/crypttab. You get the UUID (partition) by using the command lsblk -f and adding it to crypttab in the form:

In case your encrypted device is a LUKS2 volume you may also refer to it by its label if it has one:

You should likely take care that the label set in the LUKS2 header is not the same as the label of the encrypted file system it contains.

The first parameter is your preferred device mapper's name for the encrypted drive. The option none will trigger a prompt during boot to type the passphrase for unlocking the partition. The timeout option defines a timeout in seconds for entering the decryption password during boot.

If the keyfile for a secondary file system is itself stored inside an encrypted root, it is safe while the system is powered off and can be sourced to automatically unlock the mount during with boot via crypttab. For example, unlock a crypt specified by UUID:

Then use the device mapper's name (defined in /etc/crypttab) to make an entry in /etc/fstab:

Since /dev/mapper/home-crypt already is the result of a unique partition mapping, there is no need to specify a UUID for it. In any case, the mapper with the filesystem will have a different UUID than the partition it is encrypted in.

The systemd generators also automatically process stacked block devices at boot.

For example, you can create a RAID setup, use cryptsetup on it and create an LVM logical volume with respective filesystem inside the encrypted block device. A resulting:

will ask for the passphrase and mount automatically at boot.

Given you specify the correct corresponding crypttab (e.g. UUID for the crypto_LUKS device) and fstab (/dev/vgraid/lvraid) entries, there is no need to add additional mkinitcpio hooks/configuration, because /etc/crypttab processing applies to non-root mounts only. One exception is when the mdadm_udev hook is used already (e.g. for the root device). In this case /etc/madadm.conf and the initramfs need updating to achieve the correct root raid is picked first.

you can start systemd-cryptsetup@externaldrive.service when you have an entry as follows in your /etc/crypttab:

That way you do not need to remember the exact crypttab options. It will prompt you for the passphrase if needed.

The corresponding unit file is generated automatically by systemd-cryptsetup-generator(8). You can list all generated unit files using:

When secondary devices are defined in crypttab without being required for the boot process, the default options will have the cryptsetup service wait for them to be unlocked before the boot process can continue. The nofail option will avoid this by removing Before=cryptsetup.target from the mount service of the specified device. This may be used when devices are unlocked using keyfiles, as without this option the boot process will be delayed by several seconds until the unlock process has finished.

If you are using Plymouth, make sure to use the correct modules (see Plymouth#mkinitcpio) or disable it. Otherwise, Plymouth will swallow the password prompt, making a system boot impossible.

If you unlock the LUKS device with a keyboard or a keyfile on a filesystem that is not present when generating the initramfs, you might need to add the corresponding modules to the MODULES array of mkinitcpio. This might also be needed when the keyboard is connected through a USB hub. See mkinitcpio#MODULES for more information on this issue and Minimal initramfs#Sorting out modules as a starting point for potential keyboard and filesystem module names to be added.

In general, for keyboards that are not connected to the PC at initramfs generation time, you need to place the keyboard hook before the autodetect hook or only the parts necessary for the currently connected hardware are kept, see mkinitcpio#Common hooks.

**Examples:**

Example 1 (unknown):
```unknown
/etc/crypttab
```

Example 2 (unknown):
```unknown
sd-vconsole
```

Example 3 (unknown):
```unknown
/etc/vconsole.conf
```

Example 4 (unknown):
```unknown
consolefont
```

---

## Xorg

**URL:** https://wiki.archlinux.org/title/Xorg

**Contents:**
- Installation
  - Driver installation
    - AMD
- Running
- Configuration
  - Using .conf files
  - Using xorg.conf
- Input devices
  - Input identification
  - Mouse acceleration

X.Org Server — commonly referred to as simply X — is the X.Org Foundation implementation of the X Window System (X11) display server, and it is the most popular display server among Linux users. Its ubiquity has led to making it an ever-present requisite for GUI applications, resulting in massive adoption from most distributions.

For the alternative and successor, see Wayland.

Xorg can be installed with the xorg-server package.

Additionally, some packages from the xorg-apps group are necessary for certain configuration tasks. They are pointed out in the relevant sections.

Finally, an xorg group is also available, which includes Xorg server packages, packages from the xorg-apps group and fonts.

This article or section is a candidate for moving to Graphics processing unit.

The Linux kernel includes open-source video drivers and support for hardware accelerated framebuffers. However, userland support is required for OpenGL, Vulkan and 2D acceleration in X11.

First, identify the graphics card (the Subsystem output shows the specific model):

Then, install an appropriate driver. You can search the package database for a complete list of open-source Device Dependent X (DDX) drivers:

However, hardware-specific DDX is considered legacy nowadays. There is a generic modesetting(4) DDX driver in xorg-server, which uses kernel mode setting and works well on modern hardware. The modesetting DDX driver uses Glamor[1] for 2D acceleration, which requires OpenGL.

If you want to install another DDX driver, note that Xorg searches for installed DDX drivers automatically:

In order for video acceleration to work, and often to expose all the modes that the GPU can set, a proper video driver is required:

Other DDX drivers can be found in the xorg-drivers group.

Xorg should run smoothly without closed source drivers, which are typically needed only for advanced features such as fast 3D-accelerated rendering for games. The exceptions to this rule are recent GPUs (especially NVIDIA GPUs) not supported by open source drivers.

For a translation of model names (e.g. Radeon RX 6800) to GPU architectures (e.g. RDNA 2), see Wikipedia:List of AMD graphics processing units#Features overview.

The Xorg(1) command is usually not run directly. Instead, the X server is started with either a display manager or xinit.

Xorg uses a configuration file called xorg.conf and files ending in the suffix .conf for its initial setup: the complete list of the folders where these files are searched can be found in xorg.conf(5), together with a detailed explanation of all the available options.

The /etc/X11/xorg.conf.d/ directory stores host-specific configuration. You are free to add configuration files there, but they must have a .conf suffix: the files are read in ASCII order, and by convention their names start with XX- (two digits and a hyphen, so that for example 10 is read before 20). These files are parsed by the X server upon startup and are treated like part of the traditional xorg.conf configuration file. Note that on conflicting configuration, the file read last will be processed. For this reason, the most generic configuration files should be ordered first by name. The configuration entries in the xorg.conf file are processed at the end.

For option examples to set, see Fedora:Input device configuration#xorg.conf.d.

Xorg can also be configured via /etc/X11/xorg.conf or /etc/xorg.conf. You can also generate a skeleton for xorg.conf with:

This should create a xorg.conf.new file in /root/ that you can copy over to /etc/X11/xorg.conf.

Alternatively, your proprietary video card drivers may come with a tool to automatically configure Xorg: see the article of your video driver, NVIDIA or AMDGPU PRO, for more details.

For input devices the X server defaults to the libinput driver (xf86-input-libinput), but xf86-input-evdev and related drivers are available as alternative.[2]

Udev, which is provided as a systemd dependency, will detect hardware and both drivers will act as hotplugging input driver for almost all devices, as defined in the default configuration files 10-quirks.conf and 40-libinput.conf in the /usr/share/X11/xorg.conf.d/ directory.

After starting X server, the log file will show which driver hotplugged for the individual devices (note the most recent log file name may vary):

If both do not support a particular device, install the needed driver from the xorg-drivers group. The same applies, if you want to use another driver.

To influence hotplugging, see #Configuration.

For specific instructions, see also the libinput article, the following pages below, or Fedora:Input device configuration for more examples.

See Keyboard input#Identifying keycodes in Xorg.

See Mouse acceleration.

See libinput or Synaptics.

See Keyboard configuration in Xorg.

For a headless configuration, the xf86-video-dummy driver is necessary; install it and create a configuration file, such as the following:

See main article Multihead for general information.

You must define the correct driver to use and put the bus ID of your graphic cards (in decimal notation).

To get your bus IDs (in hexadecimal):

The bus IDs here are 0:2:0 and 1:0:0.

By default, Xorg always sets DPI to 96 since 2009-01-30. A change was made with version 21.1 to provide proper DPI auto-detection, but reverted.

The DPI of the X server can be set with the -dpi command line option.

Having the correct DPI is helpful where fine detail is required (like font rendering). Previously, manufacturers tried to create a standard for 96 DPI (a 10.3" diagonal monitor would be 800x600, a 13.2" monitor 1024x768). These days, screen DPIs vary and may not be equal horizontally and vertically. For example, a 19" widescreen LCD at 1440x900 may have a DPI of 89x87.

To see if your display size and DPI are correct:

Check that the dimensions match your display size.

If you have specifications on the physical size of the screen, they can be entered in the Xorg configuration file so that the proper DPI is calculated (adjust identifier to your xrandr output):

If you only want to enter the specification of your monitor without creating a full xorg.conf, create a new configuration file. For example (/etc/X11/xorg.conf.d/90-monitor.conf):

If you do not have specifications for physical screen width and height (most specifications these days only list by diagonal size), you can use the monitor's native resolution (or aspect ratio) and diagonal length to calculate the horizontal and vertical physical dimensions. Using the Pythagorean theorem on a 13.3" diagonal length screen with a 1280x800 native resolution (or 16:10 aspect ratio):

This will give the pixel diagonal length, and with this value you can discover the physical horizontal and vertical lengths (and convert them to millimeters):

For RandR compliant drivers (for example the open source ATI driver), you can set it by:

To make it permanent, see Autostarting#On Xorg startup.

You can manually set the DPI by adding the option under the Device or Screen section:

GTK very often overrides the server's DPI via the optional X resource Xft.dpi. To find out whether this is happening to you, check with:

With GTK library versions since 3.16, when this variable is not otherwise explicitly set, GTK sets it to 96. To have GTK apps obey the server DPI you may need to explicitly set Xft.dpi to the same value as the server. The Xft.dpi resource is the method by which some desktop environments optionally force DPI to a particular value in personal settings. Among these are KDE and TDE.

DPMS is a technology that allows power saving behaviour of monitors when the computer is not in use. This will allow you to have your monitors automatically go into standby after a predefined period of time.

The Composite extension for X causes an entire sub-tree of the window hierarchy to be rendered to an off-screen buffer. Applications can then take the contents of that buffer and do whatever they like. The off-screen buffer can be automatically merged into the parent window, or merged by external programs called compositing managers. For more information, see Wikipedia:Compositing window manager.

Some window managers (e.g. Compiz, Enlightenment, KWin, marco, metacity, muffin, mutter, Xfwm) do compositing on their own. For other window managers, a standalone composite manager can be used.

This section lists utilities for automating keyboard / mouse input and window operations (like moving, resizing or raising).

See also Clipboard#Tools and an overview of X automation tools.

This article or section is out of date.

To run a nested session of another desktop environment:

This will launch a Window Maker session in a 1024 by 768 window within your current X session.

This needs the package xorg-server-xnest to be installed.

A more modern way of doing a nested X session is with Xephyr.

See xinit#Starting applications without a window manager.

See main article: OpenSSH#X11 forwarding.

With the help of xinput you can temporarily disable or enable input sources. This might be useful, for example, on systems that have more than one mouse, such as the ThinkPads and you would rather use just one to avoid unwanted mouse clicks.

Install the xorg-xinput package.

Find the name or ID of the device you want to disable:

For example in a Lenovo ThinkPad T500, the output looks like this:

Disable the device with xinput --disable device, where device is the device ID or name of the device you want to disable. In this example we will disable the Synaptics Touchpad, with the ID 10:

To re-enable the device, just issue the opposite command:

Alternatively using the device name, the command to disable the touchpad would be:

You can disable a particular input source using a configuration snippet:

device is an arbitrary name, and driver_name is the name of the input driver, e.g. libinput. device_name is what is actually used to match the proper device. For alternate methods of targeting the correct device, such as libinput's MatchIsTouchscreen, consult your input driver's documentation. Though this example uses libinput, this is a driver-agnostic method which simply prevents the device from being propagated to the driver.

Run script on hotkey:

Dependencies: xorg-xprop, xdotool

See also #Killing an application visually.

To block tty access when in an X add the following to xorg.conf:

This can be used to help restrict command line access on a system accessible to non-trusted users.

To prevent a user from killing X when it is running add the following to xorg.conf:

When an application is misbehaving or stuck, instead of using kill or killall from a terminal and having to find the process ID or name, xorg-xkill allows to click on said application to close its connection to the X server. Many existing applications do indeed abort when their connection to the X server is closed, but some can choose to continue.

Xorg may run with standard user privileges instead of root (so-called "rootless" Xorg). This is a significant security improvement over running as root. Note that some popular display managers do not support rootless Xorg (e.g. LightDM or XDM).

You can verify which user Xorg is running as with ps -o user= -C Xorg.

See also Xorg.wrap(1), systemd-logind(8), Systemd/User#Xorg as a systemd user service, Fedora:Changes/XorgWithoutRootRights and FS#41257.

To configure rootless Xorg using xinitrc:

Note that executing startx directly without exec leaves the shell open in the case of a xorg crash. Since some lock screens are executed inside xorg, this can lead to full access to the executing user.

GDM will run Xorg without root privileges by default when kernel mode setting is used.

When Xorg is run in rootless mode, Xorg logs are saved to ~/.local/share/xorg/Xorg.log. However, the stdout and stderr output from the Xorg session is not redirected to this log. To re-enable redirection, start Xorg with the -keeptty flag and redirect the stdout and stderr output to a file:

Alternatively, copy /etc/X11/xinit/xserverrc to ~/.xserverrc, and append -keeptty. See [5].

As explained above, there are circumstances in which rootless Xorg is defaulted to. If this is the case for your configuration, and you have a need to run Xorg as root, you can configure Xorg.wrap(1) to require root:

Wayback is an X11 compatibility layer which allows for running full X11 desktop environments (and window managers) using Wayland components. It's available from the AUR as wayback-x11AUR package.

If a problem occurs, view the log stored in either /var/log/ or, for the rootless X default since v1.16, in ~/.local/share/xorg/. GDM users should check the systemd journal. [6]

The logfiles are of the form Xorg.n.log with n being the display number. For a single user machine with default configuration the applicable log is frequently Xorg.0.log, but otherwise it may vary. To make sure to pick the right file it may help to look at the timestamp of the X server session start and from which console it was started. For example:

X creates configuration and temporary files in current user's home directory. Make sure there is free disk space available on the partition your home directory resides in. Unfortunately, X server does not provide any more obvious information about lack of disk space in this case.

If you use a Matrox card and DRI stopped working after upgrading to Xorg, try adding the line:

to the Device section that references the video card in xorg.conf.

X fails to start with the following log messages:

To correct, uninstall the xf86-video-fbdev package.

Error message: unable to load font `(null)'.

Some programs only work with bitmap fonts. Two major packages with bitmap fonts are available, xorg-fonts-75dpi and xorg-fonts-100dpi. You do not need both; one should be enough. To find out which one would be better in your case, try xdpyinfo from xorg-xdpyinfo, like this:

and use what is closer to the shown value.

If Xorg is set to boot up automatically and for some reason you need to prevent it from starting up before the login/display manager appears (if the system is wrongly configured and Xorg does not recognize your mouse or keyboard input, for instance), you can accomplish this task with two methods.

Depending on setup, you will need to do one or more of these steps:

If you are getting Client is not authorized to connect to server, try adding the line:

to /etc/pam.d/su and /etc/pam.d/su-l. pam_xauth will then properly set environment variables and handle xauth keys.

If the filesystem (specifically /tmp) is full, startx will fail. The log file will contain:

Make some free space on the relevant filesystem and X will start.

Your color depth is set wrong. It may need to be 24 instead of 16, for example.

If X terminates with error message SocketCreateListener() failed, you may need to delete socket files in /tmp/.X11-unix. This may happen if you have previously run Xorg as root (e.g. to generate an xorg.conf).

That error means that only the current user has access to the X server. The solution is to give access to root:

That line can also be used to give access to X to a different user than root.

**Examples:**

Example 1 (unknown):
```unknown
$ lspci -v -nn -d ::03xx
```

Example 2 (unknown):
```unknown
$ pacman -Ss xf86-video
```

Example 3 (unknown):
```unknown
/usr/share/X11/xorg.conf.d/
```

Example 4 (unknown):
```unknown
/etc/X11/xorg.conf.d/
```

---

## archiso

**URL:** https://wiki.archlinux.org/title/Archiso

**Contents:**
- Installation
- Prepare a custom profile
  - Profile structure
  - Selecting packages
    - Custom local repository
    - Packages from multilib
  - Adding files to image
  - Adding repositories to the image
    - archzfs example
  - Kernel

Archiso is a highly-customizable tool for building Arch Linux live CD/USB ISO images, netboot artifacts and bootstrap tarballs. The official images are built with archiso and include the following packages. It can be used as the basis for rescue systems, linux installers or other systems. This wiki article explains how to install archiso, and how to configure it to control aspects of the resulting ISO image such as included packages and files. Technical requirements and build steps can be found in the official project documentation. Archiso is implemented with a number of bash scripts. The core component of archiso is the mkarchiso command. Its options are documented in mkarchiso -h and not covered here.

Install the archiso package.

Archiso comes with two profiles, releng and baseline.

To build an unmodified version of the profiles, skip to #Build the ISO. Otherwise, if you wish to adapt or customize one of archiso's shipped profiles, copy it from /usr/share/archiso/configs/profile-name/ to a writable directory with a name of your choice. For example:

Proceed to the following sections to customize and build the custom profile.

An archiso profile contains configuration that defines the resulting ISO image. The profile structure is documented in /usr/share/doc/archiso/README.profile.rst[1].

Edit packages.x86_64 to select which packages are to be installed on the live system image, listing packages line by line.

To add packages not located in standard Arch repositories (e.g. packages from the AUR or customized with the ABS), set up a custom local repository and add your custom packages to it. Then add your repository to pacman.conf as follows:

To install packages from the multilib repository, simply uncomment that repository in pacman.conf.

The airootfs directory is used as the starting point for the root directory (/) of the live system on the image. All its contents will be copied over to the working directory before packages are installed.

Place any custom files and/or directories in the desired location under airootfs/. For example, if you have custom nftables rules on your current system you want to be used on your live image, copy them over as such:

Similarly, some care is required for special configuration files that reside somewhere down the hierarchy. Missing parts of the directory structure can be simply created with mkdir(1).

By default, permissions will be 644 for files and 755 for directories. All of them will be owned by the root user. To set different permissions or ownership for specific files and/or folders, use the file_permissions associative array in profiledef.sh. See README.profile.rst for details.

To add a repository that can be used in the live environment, create a suitably modified pacman.conf and place it in archlive/airootfs/etc/.

If the repository also uses a key, place the key in archlive/airootfs/usr/share/pacman/keyrings/. The key file name must end with .gpg. Additionally, the key must be trusted. This can be accomplished by creating a GnuPG exported trust file in the same directory. The file name must end with -trusted. The first field is the key fingerprint, and the second is the trust. You can reference /usr/share/pacman/keyrings/archlinux-trusted for an example.

The files in this example are:

Import and trust the key (during image build or post-boot setup script):

Although both archiso's included profiles only have linux, ISOs can be made to include other or even multiple kernels.

First, edit packages.x86_64 to include kernel package names that you want. When mkarchiso runs, it will include all work_dir/airootfs/boot/vmlinuz-* and work_dir/boot/initramfs-*.img files in the ISO (and additionally in the FAT image used for UEFI booting).

mkinitcpio presets by default will build fallback initramfs images. For an ISO, the main initramfs image would not typically include the autodetect hook, thus making an additional fallback image unnecessary. To prevent the creation of an fallback initramfs image, so that it does not take up space or slow down the build process, place a custom preset in archlive/airootfs/etc/mkinitcpio.d/pkgbase.preset. For example, for linux-lts:

Finally create boot loader configuration to allow booting the kernel(s).

Archiso supports syslinux for BIOS booting and GRUB or systemd-boot for UEFI booting. Refer to the articles of the boot loaders for information on their configuration syntax.

mkarchiso expects that GRUB configuration is in the grub directory, systemd-boot configuration is in the efiboot directory and syslinux configuration in the syslinux directory.

If you want to make your archiso bootable on a UEFI Secure Boot enabled environment, you must use a signed boot loader. You can follow the instructions on Secure Boot#Booting an installation medium.

To enable systemd services/sockets/timers for the live environment, you need to manually create the symbolic links just as systemctl enable does it.

For example, to enable gpm.service, which contains WantedBy=multi-user.target, run:

The required symlinks can be found out by reading the systemd unit, or if you have the service installed, by enabling it and observing the systemctl output.

Starting X at boot is done by enabling your login manager's systemd service. If you do not know which .service to enable, you can easily find out in case you are using the same program on the system you build your ISO on. Just use:

Now create the same symlink in archlive/airootfs/etc/systemd/system/. For LXDM:

This will enable LXDM at system start on your live system.

The configuration for getty's automatic login is located under airootfs/etc/systemd/system/getty@tty1.service.d/autologin.conf.

You can modify this file to change the auto login user:

Or remove autologin.conf altogether to disable auto login.

If you are using the serial console, create airootfs/etc/systemd/system/serial-getty@ttyS0.service.d/autologin.conf with the following content instead:

To create a user which will be available in the live environment, you must manually edit archlive/airootfs/etc/passwd, archlive/airootfs/etc/shadow, archlive/airootfs/etc/group and archlive/airootfs/etc/gshadow.

For example, to add a user archie. Add them to archlive/airootfs/etc/passwd following the passwd(5) syntax:

Add the user to archlive/airootfs/etc/shadow following the syntax of shadow(5). If you want to define a password for the user, generate a password hash with openssl passwd -6 and add it to the file. For example:

Otherwise, you may keep the password field empty, meaning that the user can log in with no password.

Add the user's group and the groups which they will part of to archlive/airootfs/etc/group according to group(5). For example:

Create the appropriate archlive/airootfs/etc/gshadow according to gshadow(5):

Make sure /etc/shadow and /etc/gshadow have the correct permissions:

After package installation, mkarchiso will create all specified home directories for users listed in archlive/airootfs/etc/passwd and copy work_directory/x86_64/airootfs/etc/skel/* to them. The copied files will have proper user and group ownership.

To change the live system's default keymap on the console, add a vconsole.conf with the desired keymap. Check Linux console/Keyboard configuration and vconsole.conf(5) for details and more options.

For example, to change the console's and X11's default keymap to German (de-latin1), create archlive/airootfs/etc/vconsole.conf with the following contents:

To change the live system's locale, you must first enable and generate the localization files. For this to work you must add a custom locale.gen with the required locales and run locale-gen within the live system. This then enables you to also change the system's default locale.

For example, to enable de_DE.UTF-8 UTF-8 and en_US.UTF-8 UTF-8, create archlive/airootfs/etc/locale.gen with the following contents (check Locale#Generating locales for details):

You then need to run locale-gen within the live system. This can be achieved with a Pacman hook that is later automatically removed from the image by another hook. Create archlive/airootfs/etc/pacman.d/hooks/locale-gen.hook with the following contents (check Pacman#Hooks and alpm-hooks(5) for details):

Optionally, you can then change the live system's default locale by adding a custom locale.conf. For example, to change the default locale to de_DE.UTF-8, create archlive/airootfs/etc/locale.conf with the following contents (check Locale#Variables and locale.conf(5) for details):

Start by copying the file /etc/os-release into the etc/ folder in the rootfs. Then, edit the file accordingly. You can also change the name inside of GRUB and syslinux.

Build an ISO which you can then burn to CD or USB by running:

Replace /path/to/profile/ with the path to your custom profile, or with /usr/share/archiso/configs/releng/ if you are building an unmodified profile.

When run, the script will download and install the packages you specified to work_directory/x86_64/airootfs, create the kernel and init images, apply your customizations and finally build the ISO into the output directory.

Temporary files are copied into a work directory. If mkarchiso is run with the -r option, it will delete the work directory after successfully building the ISO as long as the directory did not exist beforehand. Alternatively, you can delete the work directory manually:

See Installation guide#Prepare an installation medium for various options.

Install the optional dependencies qemu-desktop and edk2-ovmf.

Use the convenience script run_archiso to run a built image using QEMU.

The virtual machine can also be run using UEFI emulation:

When creating a custom profile, some modifications will require you to run setup commands within the live system's environment before creating the ISO. For example, if you want to enable additional locales, you must run locale-gen within the live system to generate the necessary localization files.

To run setup commands before creating the ISO, add a Pacman hook below archlive/airootfs/etc/pacman.d/hooks/. Check Pacman#Hooks and alpm-hooks(5) for details on how to create custom Pacman hooks. Also check #Locales for an example.

Since these setup hooks are often required to run just once as part of the ISO build process, the releng profile includes a Pacman hook that removes any Pacman hook that is marked with the string "remove from airootfs". So, if you don't want your Pacman hook to also apply on the running live system, add the following comment to your Pacman hook:

To install Arch Linux via SSH without any interaction with the system, an SSH public key must be placed in authorized_keys.

Adding the SSH key can either be done manually (explained here), or by cloud-init.

To add the key manually, first copy archiso's releng profile to a writable directory. The following example uses archlive.

Create a .ssh directory in the home directory of the user which will be used to log in. The following example will be using the root user.

Add the SSH public key(s), which will be used to log in, to authorized_keys:

Set correct permissions and ownership for the .ssh directory and the authorized_keys file:

Finally build the ISO. Upon booting the ISO, OpenSSH will start and it will be possible to log in using the corresponding SSH private key(s).

Create /var/lib/iwd/ inside the profile's airootfs directory and set the correct permissions:

Follow the instructions in iwd#Network configuration and iwd.network(5) to create a network configuration file for your Wi-Fi network.

Save the configuration file inside archlive/airootfs/var/lib/iwd/.

When installing packages in the live environment, for example on hardware requiring DKMS modules, the default size of the root file system might not allow the download and installation of such packages due to its size.

It will manifest as the following error message when downloading files or installing packages in the live environment:

To adjust the size on the fly:

See tmpfs(5) § size for the possible parameters of SIZE.

To adjust the size at the boot loader stage (by pressing e or Tab) use the boot option:

To adjust the size while building an image add the boot option to:

The result can be checked with:

See mkinitcpio-archiso boot parameters.

If you want to use a window manager in the Live CD, you must add the necessary and correct video drivers, or the WM may freeze on loading.

**Examples:**

Example 1 (unknown):
```unknown
mkarchiso -h
```

Example 2 (unknown):
```unknown
/usr/share/archiso/configs/profile-name/
```

Example 3 (unknown):
```unknown
$ cp -r /usr/share/archiso/configs/releng/ archlive
```

Example 4 (unknown):
```unknown
/usr/share/doc/archiso/README.profile.rst
```

---

## sysctl

**URL:** https://wiki.archlinux.org/title/Sysctl

**Contents:**
- Installation
- Configuration
- Security
- Networking
  - Improving performance
    - Increasing the size of the receive queue.
    - Increase the maximum connections
    - Increase the memory dedicated to the network interfaces
    - Enable TCP Fast Open
    - Tweak the pending connection handling

sysctl is a tool for examining and changing kernel parameters at runtime. sysctl is implemented in procfs, the virtual process file system at /proc/.

The procps-ng package should already be installed, as it is a dependency of the base meta package.

The sysctl preload/configuration file can be created at /etc/sysctl.d/99-sysctl.conf. For systemd, /etc/sysctl.d/ and /usr/lib/sysctl.d/ are drop-in directories for kernel sysctl parameters. The naming and source directory decide the order of processing, which is important since the last parameter processed may override earlier ones. For example, parameters in a /usr/lib/sysctl.d/50-default.conf will be overriden by equal parameters in /etc/sysctl.d/50-default.conf and any configuration file processed later from both directories.

To load all configuration files manually, execute:

which will also output the applied hierarchy. A single parameter file can also be loaded explicitly with:

See the new configuration files and more specifically sysctl.d(5) for more information.

The parameters available are those listed under /proc/sys/. For example, the kernel.sysrq parameter refers to the file /proc/sys/kernel/sysrq on the file system. The sysctl --all command can be used to display all currently available values.

Settings can be changed through file manipulation or using the sysctl(8) utility. For example, to temporarily enable the magic SysRq key:

See Linux kernel documentation for details about kernel.sysrq.

To preserve changes between reboots, add or modify the appropriate lines in /etc/sysctl.d/99-sysctl.conf or another applicable parameter file in /etc/sysctl.d/.

See also Security#Kernel hardening, as well as the rest of this article.

The received frames will be stored in this queue after taking them from the ring buffer on the network card.

Increasing this value for high speed cards may help prevent losing packets:

The upper limit on how many connections the kernel will accept (default 4096):

This article or section needs expansion.

The factual accuracy of this article or section is disputed.

By default the Linux network stack is not configured for high speed large file transfer across WAN links (i.e. handle more network packets) and setting the correct values may save memory resources:

It is also possible increase the default 4096 UDP limits:

See the following sources for more information and recommend values:

This article or section needs expansion.

TCP Fast Open is an extension to the transmission control protocol (TCP) that helps reduce network latency by enabling data to be exchanged during the sender’s initial TCP SYN [2]. Using the value 3 instead of the default 1 allows TCP Fast Open for both incoming and outgoing connections:

tcp_max_syn_backlog is the maximum queue length of pending connections 'Waiting Acknowledgment'.

In the event of a synflood DOS attack, this queue can fill up pretty quickly, at which point TCP SYN cookies will kick in allowing your system to continue to respond to legitimate traffic, and allowing you to gain access to block malicious IPs.

If the server suffers from overloads at peak times, you may want to increase this value a little bit:

tcp_max_tw_buckets is the maximum number of sockets in TIME_WAIT state.

After reaching this number the system will start destroying the socket that are in this state.

Increase this to prevent simple DOS attacks:

tcp_tw_reuse sets whether TCP should reuse an existing connection in the TIME-WAIT state for a new outgoing connection if the new timestamp is strictly bigger than the most recent timestamp recorded for the previous connection.

The default value is 2, means it's enabled for loopback connections only. You can set it to 1 to enable for all connections, this helps avoid from running out of available network sockets:

Specify how many seconds to wait for a final FIN packet before the socket is forcibly closed. This is strictly a violation of the TCP specification, but required to prevent denial-of-service attacks. In Linux 2.2, the default value was 180 [3]:

tcp_slow_start_after_idle sets whether TCP should start at the default window size only for new connections or also for existing connections that have been idle for too long.

This setting kills persistent single connection performance and could be turned off:

This article or section is out of date.

TCP keepalive is a mechanism for TCP connections that help to determine whether the other end has stopped responding or not. TCP will send the keepalive probe that contains null data to the network peer several times after a period of idle time. If the peer does not respond, the socket will be closed automatically. By default, TCP keepalive process waits for two hours (7200 secs) for socket activity before sending the first keepalive probe, and then resend it every 75 seconds. As long as there is TCP/IP socket communications going on and active, no keepalive packets are needed.

The longer the maximum transmission unit (MTU) the better for performance, but the worse for reliability.

This is because a lost packet means more data to be retransmitted and because many routers on the Internet cannot deliver very long packets:

See https://blog.cloudflare.com/path-mtu-discovery-in-practice/ for more information.

Disabling timestamp generation will reduce spikes and may give a performance boost on gigabit networks:

TCP Selective Acknowledgement (TCP SACK), controlled by the boolean tcp_sack, allows the receiving side to give the sender more detail about lost segments, reducing volume of retransmissions. This is useful on high latency networks, but disable this to improve throughput on high-speed LANs. Also disable tcp_dsack, if you aren't sending SACK you certainly don't want to send duplicates! Forward Acknowledgement works on top of SACK and will be disabled if SACK is. [6]

The BBR congestion control algorithm can help achieve higher bandwidths and lower latencies for internet traffic. First, load the tcp_bbr module.

The factual accuracy of this article or section is disputed.

The Wikipedia:Ephemeral port is typically used by the Transmission Control Protocol (TCP), User Datagram Protocol (UDP), or the Stream Control Transmission Protocol (SCTP) as the port assignment for the client end of a client–server communication.

The following specifies a parameter set to tighten network security options of the kernel for the IPv4 protocol and related IPv6 parameters where an equivalent exists.

For some use-cases, for example using the system as a router, other parameters may be useful or required as well.

Helps protect against SYN flood attacks. Only kicks in when net.ipv4.tcp_max_syn_backlog is reached. More details at, for example, [7]. As of linux 5.10, it is set by default.

The factual accuracy of this article or section is disputed.

Protect against tcp time-wait assassination hazards, drop RST packets for sockets in the time-wait state. Not widely supported outside of Linux, but conforms to RFC:

By enabling reverse path filtering, the kernel will do source validation of the packets received from all the interfaces on the machine. This can protect from attackers that are using IP spoofing methods to do harm.

The kernel's default value is 0 (no source validation), but systemd ships /usr/lib/sysctl.d/50-default.conf that sets net.ipv4.conf.all.rp_filter to 2 (loose mode)[9].

The following will set the reverse path filtering mechanism to value 1 (strict mode):

The relationship and behavior of net.ipv4.conf.default.*, net.ipv4.conf.interface.* and net.ipv4.conf.all.* is explained in ip-sysctl.html.

A martian packet is an IP packet which specifies a source or destination address that is reserved for special-use by Internet Assigned Numbers Authority (IANA). See Reserved IP addresses for more details.

Often martian and unroutable packet may be used for a dangerous purpose. Logging these packets for further inspection may be useful [10]:

Background is at What are ICMP redirects? Should they be blocked?

To disable ICMP redirect acceptance:

To disable ICMP redirect sending when on a non router:

To disable ICMP echo (aka ping) requests:

This article or section is out of date.

The IPPROTO_ICMP (icmp(7)) socket type adds the possibility to send ICMP_ECHO messages and receive corresponding ICMP_ECHOREPLY messages without the need to open a raw(7) socket, an operation which requires the CAP_NET_RAW capability or the SUID bit with a proper privileged owner. These ICMP_ECHO messages are sent by the ping application thus making the IPPROTO_ICMP socket also known as ping socket in addition to ICMP Echo socket.

ping_group_range determines the GID range of groups which their users are allowed to create IPPROTO_ICMP sockets. Additionally, the owner of the CAP_NET_RAW capability is also allowed to create IPPROTO_ICMP sockets. By default this range is 1 0 which means no one is allowed to create IPPROTO_ICMP sockets except root. To take advantage of this setting programs which currently uses raw sockets need to ported to use IPPROTO_ICMP sockets instead. For example, QEMU uses IPPROTO_ICMP for SLIRP aka User-mode networking, so allowing the user running QEMU to create IPPROTO_ICMP sockets means it is possible to ping from the guest.

To allow only users which are members of the group with GID 100 to create IPPROTO_ICMP sockets:

To allow all the users in the system to create IPPROTO_ICMP sockets:

There are several key parameters to tune the operation of the virtual memory subsystem of the Linux kernel and the write out of dirty data to disk. See the official Linux kernel documentation for more information. For example:

As noted in the comments for the parameters, one needs to consider the total amount of RAM when setting these values. For example, simplifying by taking the installed system RAM instead of available memory:

See https://lonesysadmin.net/2013/12/22/better-linux-disk-caching-performance-vm-dirty_ratio/ for more information.

Decreasing the virtual file system (VFS) cache parameter value may improve system responsiveness:

See RAID#Change sync speed limits.

Set dirty bytes to small enough value (for example 4 MiB):

**Examples:**

Example 1 (unknown):
```unknown
/etc/sysctl.d/99-sysctl.conf
```

Example 2 (unknown):
```unknown
/etc/sysctl.d/
```

Example 3 (unknown):
```unknown
/usr/lib/sysctl.d/
```

Example 4 (unknown):
```unknown
/usr/lib/sysctl.d/50-default.conf
```

---

## Ext3

**URL:** https://wiki.archlinux.org/title/Ext3

**Contents:**
- Using tune2fs and e2fsck
- Using directory indexing
- Enable full journaling
- Disable lengthy boot time checks
- Reclaim reserved filesystem space
- Assigning a label

The ext3 filesystem used to be a great choice back when it was added in 2001 due to its introduction of journaling. However, ext4 should be used instead unless it is required by e.g legacy software. See Wikipedia:Ext3#ext4 for improvements of ext4 over ext3.

Before we begin, we need to make sure you are comfortable with using the tune2fs utility to alter the filesystem options of an ext2 or ext3 partition (or convert ext2 to ext3). Please read tune2fs(8).

It is generally a good idea to run a filesystem check using the e2fsck utility after you have completed the alterations you wish to make on your filesystem. This will verify that your filesystem is clean and fix it if needed. You should also read the manual page for the e2fsck utility if you have not yet done so.

This feature improves file access in large directories or directories containing many files by using hashed binary trees to store the directory information. It is perfectly safe to use, and it provides a fairly substantial improvement in most cases; so it is a good idea to enable it:

This will only take effect with directories created on that filesystem after tune2fs is run. In order to apply this to currently existing directories, we must run the e2fsck utility to optimize and reindex the directories on the filesystem:

By default, ext3 partitions mount with the 'ordered' data mode. In this mode, all data is written to the main filesystem and its metadata is committed to the journal, whose blocks are logically grouped into transactions to decrease disk I/O. This tends to be a good default for most people. However, I have found a method that increases both reliability and performance (in some situations): journaling everything, including the file data itself (known as 'journal' data mode). Normally, one would think that journaling all data would decrease performance, because the data is written to disk twice: once to the journal then later committed to the main filesystem, but this does not seem to be the case. I have enabled it on all nine of my partitions and have only seen a minor performance loss in deleting large files. In fact, doing this can actually improve performance on a filesystem where much reading and writing is to be done simultaneously. See this article written by Daniel Robbins on IBM's website for more information.

There are two different ways to activate journal data mode. The first is by adding data=journal as a mount option in /etc/fstab. If you do it this way and want your root filesystem to also use it, you should also pass rootflags=data=journal as a kernel parameter in your boot loader configuration. In the second method, you will use tune2fs to modify the default mount options in the file system superblock:

Please note that the second method may not work for older kernels. Especially Linux 2.4.20 and below will likely disregard the default mount options on the superblock. If you are feeling adventurous you may also want to tweak the journal size. (I have left the journal size at the default.) A larger journal may give you better performance (at the cost of more disk space and longer recovery times). Please be sure to read the relevant section of the tune2fs manual before doing so:

It seems that our ext3 filesystems are still being checked every 30 mounts or so. This is a good default for many because it helps prevent filesystem corruption when you have hardware issues, such as bad IDE/SATA/SCSI cabling, power supply failures, etc. One of the driving forces for creating journaling filesystems was that the filesystem could easily be returned to a consistent state by recovering and replaying the needed journaled transactions. Therefore, we can safely disable these mount-count and time-dependent checks if we are certain the filesystem will be quickly checked to recover the journal if needed to restore filesystem and data consistency. Before you do this please make sure your filesystem entry in /etc/fstab has a positive integer in its 6th field (pass) so that it is checked at boot time automatically. You may do so using the following command:

If you just want to limit the checks to happen less often without totally disabling them (for peace of mind). A great method is to change from a number of count's check to a time frame check. See tune2fs(8). Here is once every month:

Ext3 partition contain a used space of 5% for special reasons by default. The main reason is to help with less fragmentation on the filesystem. The other reason for such space is so root can log in even when the filesystem becomes 100% used. Without this option, the root user might not be able to log in to "clean up" because the system could become unstable, trying to write logs to a 100% full system for example.

The issue with this is that hard drives are getting so big the 5% can add up to be quite a large amount of wasted space. (eg. 100 GB = 5 GB reserved). Now if you separate your filesystems to like /home for example it might be a good idea to adjust these and reclaim that wasted space on long-term archive partitions (see this email for more info). It is a safe bet to leave your / filesystem at 5% reserved just in case. Leave reserved space for filesystems containing /var and /tmp also or else you will end up with problems.

Now to change your reserved space to 1% of the drive, which is fair for non-root filesystems.

Once you have created and formated a partition, you can assign it a label using the e2label command. This allows you to add the partition to /etc/fstab using a label instead of using a device path (useful for an USB drive). To add a label to a partition, type the following command as root:

If the optional argument new-label is not present, e2label will simply display the current filesystem label. If the optional argument new-label is present, then e2label will set the filesystem label to be new-labelq. Ext2 and ext3 filesystem labels can be at most 16 characters long; if new-label is longer than 16 characters, e2label will truncate it and print a warning message.

**Examples:**

Example 1 (unknown):
```unknown
# tune2fs -O dir_index /dev/sdXY
```

Example 2 (unknown):
```unknown
# e2fsck -D -f /dev/sdXY
```

Example 3 (unknown):
```unknown
/etc/mke2fs.conf
```

Example 4 (unknown):
```unknown
journal=data
```

---

## systemd/Journal

**URL:** https://wiki.archlinux.org/title/Systemd/Journal

**Contents:**
- Priority level
- Facility
- Filtering output
- Tips and tricks
  - Journal size limit
    - Per unit size limit by a journal namespace
  - Clean journal files manually
  - Journald in conjunction with syslog
  - Forward journald to /dev/tty12
  - Specify a different journal to view

systemd has its own logging system called the journal; running a separate logging daemon is not required. To read the log, use journalctl(1).

In Arch Linux, the directory /var/log/journal/ is a part of the systemd package, and the journal (when Storage= is set to auto in /etc/systemd/journald.conf) will write to /var/log/journal/. If that directory is deleted, systemd will not recreate it automatically and instead will write its logs to /run/log/journal/ in a non-persistent way. However, the directory will be recreated if Storage=persistent is added to journald.conf and systemd-journald.service is restarted (or the system is rebooted).

Systemd journal classifies messages by Priority level and Facility. Logging classification corresponds to classic Syslog protocol (RFC 5424).

A syslog severity code (in systemd called priority) is used to mark the importance of a message RFC 5424 6.2.1.

These rules are recommendations, and the priority level of a given error is at the application developer's discretion. It is always possible that the error will be at a higher or lower level than expected.

A syslog facility code is used to specify the type of program that is logging the message RFC 5424 6.2.1.

Useful facilities to watch: 0, 1, 3, 4, 9, 10, 15.

journalctl allows for the filtering of output by specific fields. If there are many messages to display, or if the filtering of large time spans has to be done, the output of this command can be extensively delayed.

See journalctl(1), systemd.journal-fields(7), or Lennart Poettering's blog post for details.

If the journal is persistent (non-volatile), its size limit is set to a default value of 10% of the size of the underlying file system but capped at 4 GiB. For example, with /var/log/journal/ located on a 20 GiB partition, journal data may take up to 2 GiB. On a 50 GiB partition, it would max at 4 GiB. To confirm current limits on your system review systemd-journald unit logs:

The maximum size of the persistent journal can be controlled by uncommenting and changing the following:

It is also possible to use the drop-in snippets configuration override mechanism rather than editing the global configuration file. In this case, place the overrides under the [Journal] header:

Restart the systemd-journald.service after changing this setting to apply the new limit.

See journald.conf(5) for more info.

Edit the unit file for the service you wish to configure (for example sshd) and add LogNamespace=ssh in the [Service] section.

Then create /etc/systemd/journald@ssh.conf by copying /etc/systemd/journald.conf. After that, edit journald@ssh.conf and adjust SystemMaxUse to your liking.

Restarting the service should automatically start the new journal service systemd-journald@ssh.service. The logs from the namespaced service can be viewed with journalctl --namespace ssh.

See systemd-journald.service(8) § JOURNAL NAMESPACES for details about journal namespaces.

Journal files can be globally removed from /var/log/journal/ using e.g. rm, or can be trimmed according to various criteria using journalctl. For example:

Journal files must have been rotated out and made inactive before they can be trimmed by vacuum commands. Rotation of journal files can be done by running journalctl --rotate. The --rotate argument can also be provided alongside one or more vacuum criteria arguments to perform rotation and then trim files in a single command.

See journalctl(1) for more info.

Compatibility with a classic, non-journald aware syslog implementation can be provided by letting systemd forward all messages via the socket /run/systemd/journal/syslog. To make the syslog daemon work with the journal, it has to bind to this socket instead of /dev/log (official announcement).

The default journald.conf for forwarding to the socket is ForwardToSyslog=no to avoid system overhead, because rsyslog or syslog-ng pull the messages from the journal by itself.

See Syslog-ng#Overview and Syslog-ng#syslog-ng and systemd journal, or rsyslog respectively, for details on configuration.

Create a drop-in directory /etc/systemd/journald.conf.d and create a fw-tty12.conf file in it:

Then restart systemd-journald.service.

There may be a need to check the logs of another system that is dead in the water, like booting from a live system to recover a production system. In such case, one can mount the disk in e.g. /mnt, and specify the journal path via -D/--directory, like so:

By default, a regular user only has access to their own per-user journal. To grant read access for the system journal as a regular user, you can add that user to the systemd-journal user group. Members of the adm and wheel groups are also given read access.

See journalctl(1) § DESCRIPTION and Users and groups#User groups for more information.

Desktop notifications can help you quickly notice error messages, improving awareness compared to manually checking logs or not noticing them at all.

The journalctl-desktop-notificationAUR package provides an automatic desktop notification for each error message logged by any process. For more details and configuration options, visit the GitLab project page.

**Examples:**

Example 1 (unknown):
```unknown
/var/log/journal/
```

Example 2 (unknown):
```unknown
/etc/systemd/journald.conf
```

Example 3 (unknown):
```unknown
/var/log/journal/
```

Example 4 (unknown):
```unknown
/run/log/journal/
```

---

## acpid

**URL:** https://wiki.archlinux.org/title/Acpid

**Contents:**
- Installation
- Configuration
  - Determine the event
  - Define event action
  - Alternative configuration
- Tips and tricks
  - Example events
  - Enabling volume control
  - Enabling backlight control
  - Enabling Wi-Fi toggle

acpid2 is a flexible and extensible daemon for delivering ACPI events. When an event occurs, it executes programs to handle the event. These events are triggered by certain actions, such as:

Install the acpid package. Then start/enable acpid.service.

acpid comes with a number of predefined actions for triggered events, such as what should happen when you press the Power button on your machine. By default, these actions are defined in /etc/acpi/handler.sh, which is executed after any ACPI events are detected (as determined by /etc/acpi/events/anything).

Unfortunately, not every computer labels ACPI events in the same way. For example, the Sleep button may be identified on one machine as SLPB and on another as SBTN.

To determine how your buttons or Fn shortcuts are recognized, run the following command:

Now press the Power button and/or Sleep button (e.g. Fn+Esc) on your machine. The result should look something this:

If that does not work, run:

or with openbsd-netcat:

Then press the power button and you will see something like this:

The output of acpi_listen is sent to /etc/acpi/handler.sh as $1, $2 , $3 & $4 parameters. Example:

The following is a brief example of one such action. In this case, when the Sleep button is pressed, acpid runs the command echo -n mem >/sys/power/state which should place the computer into a sleep (suspend) state:

The event may be different on different machines. For example if Sleep button is actually recognized as SBTN, rather than the SLPB label specified in the default /etc/acpi/handler.sh. In order for Sleep function to work properly on this machine, we would need to replace SLPB) with SBTN).

Using this information as a base, you can easily customize the /etc/acpi/handler.sh file to execute a variety of commands depending on which event is triggered. See the #Tips and tricks section below for other commonly used commands.

By default, all ACPI events are passed through the /etc/acpi/handler.sh script. This is due to the ruleset outlined in /etc/acpi/events/anything:

While this works just fine as it is, some users may prefer to define event rules and actions in their own self-contained scripts. The following is an example of how to use an individual event file and corresponding action script:

As root, create the following files:

Make the script executable, and reload the acpid.service to get acpid to recognize the changes to these files.

Using this method, it is easy to create any number of individual event/action scripts.

The following are examples of events that can be used in the /etc/acpi/handler.sh script. These examples should be modified so that they apply your specific environment e.g. changing the event variable names interpreted by acpi_listen.

To set the laptop screen brightness when plugged in power or not (the numbers might need to be adjusted, see /sys/class/backlight/acpi_video0/max_brightness):

Find out the acpi identity of the volume buttons (see above) and substitute it for the acpi events in the files below.

Similar to volume control, acpid also enables you to control screen backlight. To achieve this you write some handler, like this:

The event in acpi_listen should be something like:

Connect them to ACPI events:

You can also create a simple wireless-power switch by pressing the WLAN button. Example of event:

Since b336c96 acpid generates events for some ordinary key presses, such as arrow keys. This results in event/handler spam, visible in system logs or top. Events for these buttons can be dropped in the configuration file:

To run commands depending on Xorg, defining the X display as well as the MIT magic cookie file (via XAUTHORITY) is required. Latter is a security credential providing read and write access to the X server, display, and any input devices (see xauth(1)).

See [3] for an example function when using xinitrc.

In addition to rule files, acpid accepts connections on a UNIX domain socket, by default /var/run/acpid.socket. User applications may connect to this socket.

Where handler.sh can be a script similar to /etc/acpi/handler.sh.

This example uses inhibited property of input device drivers as a replacement for xinput which does not work under Wayland.

**Examples:**

Example 1 (unknown):
```unknown
acpid.service
```

Example 2 (unknown):
```unknown
/etc/acpi/handler.sh
```

Example 3 (unknown):
```unknown
/etc/acpi/events/anything
```

Example 4 (unknown):
```unknown
# journalctl -f
```

---

## tmpfs

**URL:** https://wiki.archlinux.org/title/Tmpfs

**Contents:**
- Usage
- Examples
- Disable automatic mount
- Troubleshooting
  - Opening symlinks in tmpfs as root fails
- Tips and tricks
  - Allocate more memory to accommodate profiles in /run/user/xxxx
- See also

tmpfs is a temporary filesystem that resides in memory and/or swap partition(s). Mounting directories as tmpfs can be an effective way of speeding up accesses to their files, or to ensure that their contents are automatically cleared upon reboot.

Some directories where tmpfs(5) is commonly used are /tmp, /var/lock and /var/run. Do not use it on /var/tmp, because that directory is meant for temporary files that are preserved across reboots.

Arch uses a tmpfs /run directory, with /var/run and /var/lock simply existing as symlinks for compatibility. It is also used for /tmp by the default systemd setup and does not require an entry in fstab unless a specific configuration is needed.

glibc 2.2 and above expects tmpfs to be mounted at /dev/shm for POSIX shared memory. Mounting tmpfs at /dev/shm is handled automatically by systemd and manual configuration in fstab is not necessary.

Generally, tasks and programs that run frequent read/write operations can benefit from using a tmpfs directory. Some applications can even receive a substantial gain by offloading some (or all) of their data onto the shared memory. For example, relocating the Firefox profile into RAM shows a significant improvement in performance.

By default, a tmpfs partition has its maximum size set to half of the available RAM, however it is possible to overrule this value. To explicitly set a maximum size, in this example to override the default /tmp mount, use the size mount option:

To specify a more secure mounting, specify the following mount option:

See the tmpfs(5) man page and Security#File systems for more information.

Reboot for the changes to take effect. Note that although it may be tempting to simply run mount -a to make the changes effective immediately, this will make any files currently residing in these directories inaccessible (this is especially problematic for running programs with lockfiles, for example). However, if all of them are empty, it should be safe to run mount -a instead of rebooting (or mount them individually).

After applying changes, verify that they took effect by looking at /proc/mounts and using findmnt:

The tmpfs can also be temporarily resized without the need to reboot, for example when a large compile job needs to run soon. In this case, run:

Or resize based on RAM:

Under systemd, /tmp is automatically mounted as a tmpfs, if it is not already a dedicated mountpoint (either tmpfs or on-disk) in /etc/fstab. To disable the automatic mount, mask the tmp.mount systemd unit.

Files will no longer be stored in a tmpfs, but on the block device instead. The /tmp contents will now be preserved between reboots (they are still cleaned up after 10 days though), which might not be the desired behavior. To regain the previous behavior and clean the /tmp directory automatically when restarting, consider using tmpfiles.d(5):

Considering /tmp is using tmpfs, change the current directory to /tmp, then create a file and create a symlink to that file in the same /tmp directory. Permission denied errors are to be expected when attempting to read the symlink due to /tmp having the sticky bit set.

This behavior can be controlled via /proc/sys/fs/protected_symlinks or simply via sysctl: sysctl -w fs.protected_symlinks=0. See Sysctl#Configuration to make this permanent.

The standard way of controlling the size of tmpfs in /run/user/ is the RuntimeDirectorySize directive in /etc/systemd/logind.conf (see logind.conf(5) for more). By default, 10% of physical memory is used but one can increase it safely. Remember that tmpfs only consumes what is actually used; the number specified here is just a maximum allowed.

**Examples:**

Example 1 (unknown):
```unknown
tmpfs   /tmp         tmpfs   rw,nodev,nosuid,size=2G          0  0
```

Example 2 (unknown):
```unknown
tmpfs   /www/cache    tmpfs  rw,size=1G,nr_inodes=5k,noexec,nodev,nosuid,uid=user,gid=group,mode=1700 0 0
```

Example 3 (unknown):
```unknown
/proc/mounts
```

Example 4 (unknown):
```unknown
$ findmnt /tmp
```

---

## dm-crypt/Specialties

**URL:** https://wiki.archlinux.org/title/Dm-crypt/Specialties

**Contents:**
- Securing the unencrypted boot partition
  - Booting from a removable device
  - chkboot
    - Juergen Schmidt's script
    - AUR package
  - mkinitcpio-chkcryptoboot
    - Installation
    - Technical overview
  - AIDE
  - STARK

The /boot partition and the Master Boot Record are the two areas of the disk that are not encrypted, even in an encrypted root configuration. They cannot usually be encrypted because the boot loader and BIOS (respectively) are unable to unlock a dm-crypt container in order to continue the boot process. An exception is GRUB, which gained a feature to unlock a LUKS encrypted /boot - see dm-crypt/Encrypting an entire system#Encrypted boot partition (GRUB).

This section describes steps that can be taken to make the boot process more secure.

Using a separate device to boot a system is a fairly straightforward procedure, and offers a significant security improvement against some kinds of attacks. Two vulnerable parts of a system employing an encrypted root filesystem are

These must be stored unencrypted in order for the system to boot. In order to protect these from tampering, it is advisable to store them on a removable medium, such as a USB drive, and boot from that drive instead of the hard disk. As long as you keep the drive with you at all times, you can be certain that those components have not been tampered with, making authentication far more secure when unlocking your system.

It is assumed that you already have your system configured with a dedicated partition mounted at /boot. If you do not, please follow the steps in dm-crypt/System configuration#Kernel parameters, substituting your hard disk for a removable drive.

Prepare the removable drive (/dev/sdx).

Copy your existing /boot contents to the new one.

Mount the new partition. Do not forget to update your fstab file accordingly.

Update GRUB. grub-mkconfig should detect the new partition UUID automatically, but custom menu entries may need to be updated manually.

Reboot and test the new configuration. Remember to set your device boot order accordingly in your BIOS or UEFI. If the system fails to boot, you should still be able to boot from the hard drive in order to correct the problem.

Referring to an article from the ct-magazine (Issue 3/12, page 146, 01.16.2012, [2]) the following script checks files under /boot for changes of SHA-1 hash, inode, and occupied blocks on the hard drive. It also checks the Master Boot Record. The script cannot prevent certain type of attacks, but a lot are made harder. No configuration of the script itself is stored in unencrypted /boot. With a locked/powered-off encrypted system, this makes it harder for some attackers because it is not apparent that an automatic checksum comparison of the partition is done upon boot. However, an attacker who anticipates these precautions can manipulate the firmware to run their own code on top of your kernel and intercept file system access, e.g. to boot, and present the untampered files. Generally, no security measures below the level of the firmware are able to guarantee trust and tamper evidence.

The script with installation instructions is available (Author: Juergen Schmidt, ju at heisec.de; License: GPLv2). There is also package chkbootAUR to install. The AUR package is recommended, as it has additional helpful scripts.

As /usr/local/bin/chkboot_user.sh needs to be executed right after login, you need to add it to the autostart (e.g. under KDE -> System Settings -> Startup and Shutdown -> Autostart; GNOME 3: gnome-session-properties).

With Arch Linux, changes to /boot are pretty frequent, for example by new kernels rolling-in. Therefore it may be helpful to use the scripts with every full system update. One way to do so:

After installing, enable chkboot.service.

You may want to add chkboot to the end of your mkinitcpio hooks, so that your chkboot hashes get updated every time mkinitcpio regenerates your initramfs. You can do this by adding chkboot to the end of the HOOKS array in /etc/mkinitcpio.conf.

The AUR package also comes with an chkboot-desktopalert script, which will cause a graphical window to pop up with a warning if /boot changes are detected. You can make use of this script by adding it to the startup scripts of your graphical environment.

mkinitcpio-chkcryptobootAUR is a mkinitcpio hook that performs integrity checks during early-userspace and advises the user not to enter their root partition password if the system appears to have been compromised. Security is achieved through an encrypted boot partition, which is unlocked using GRUB's cryptodisk.mod module, and a root filesystem partition, which is encrypted with a password different from the former. This way, the initramfs and kernel are secured against offline tampering, and the root partition can remain secure even if the /boot partition password is entered on a compromised machine (provided that the chkcryptoboot hook detects the compromise, and is not itself compromised at run-time).

This hook requires grub release >=2.00 to function, and a dedicated, LUKS encrypted /boot partition with its own password in order to be secure.

Install mkinitcpio-chkcryptobootAUR and edit /etc/default/chkcryptoboot.conf. If you want the ability of detecting if your boot partition was bypassed, edit the CMDLINE_NAME and CMDLINE_VALUE variables, with values known only to you. You can follow the advice of using two hashes as is suggested right after the installation. Also, be sure to make the appropriate changes to the kernel command line in /etc/default/grub. Edit the HOOKS= line in /etc/mkinitcpio.conf, and insert the chkcryptoboot hook before encrypt. When finished, regenerate the initramfs.

mkinitcpio-chkcryptobootAUR consists of an install hook and a run-time hook for mkinitcpio. The install hook runs every time the initramfs is rebuilt, and hashes the GRUB EFI stub ($esp/EFI/grub_uefi/grubx64.efi) (in the case of UEFI systems) or the first 446 bytes of the disk on which GRUB is installed (in the case of BIOS systems), and stores that hash inside the initramfs located inside the encrypted /boot partition. When the system is booted, GRUB prompts for the /boot password, then the run-time hook performs the same hashing operation and compares the resulting hashes before prompting for the root partition password. If they do not match, the hook will print an error like this:

In addition to hashing the boot loader, the hook also checks the parameters of the running kernel against those configured in /etc/default/chkcryptoboot.conf. This is checked both at run-time and after the boot process is done. This allows the hook to detect if GRUB's configuration was not bypassed at run-time and afterwards to detect if the entire /boot partition was not bypassed.

For BIOS systems the hook creates a hash of GRUB's first stage boot loader (installed to the first 446 bytes of the bootdevice) to compare at the later boot processes. The main second-stage GRUB boot loader core.img is not checked.

Alternatively to above scripts, a hash check can be set up with AIDE which can be customized via a very flexible configuration file.

While one of these methods should serve the purpose for most users, they do not address all security problems associated with the unencrypted /boot. One approach which endeavours to provide a fully authenticated boot chain was published with POTTS as an academic thesis to implement the STARK authentication framework.

The POTTS proof-of-concept uses Arch Linux as a base distribution and implements a system boot chain with:

As part of the thesis installation instructions based on Arch Linux (ISO as of 2013-01) have been published. If you want to try it, be aware these tools are not in standard repositories and the solution will be time consuming to maintain.

The following forum posts give instructions to use two factor authentication, gpg or openssl encrypted keyfiles, instead of a plaintext keyfile described earlier in this wiki article System Encryption using LUKS with GPG encrypted keys:

Imagine a system with one or more LUKS encrypted partitions (root or others) or volumes and you need to unlock these partitions/volumes during startup. In this case you need to be able to log in remotely and provide a password at the early boot phase. This can be achieved by using one or more mkinitcpio hook(s) that configure a network interface and start some kind of SSH server. Some packages listed below contribute various mkinitcpio build hooks to ease the configuration. The following tutorials add a remote unlocking method in addition to the existing local console password prompt.

For busybox based initramfs the packages mkinitcpio-netconf and/or mkinitcpio-pppAUR provide network connectivity. As SSH server you have the option of using either mkinitcpio-dropbear or mkinitcpio-tinyssh. Those hooks do not install any shell, so you also need to install the mkinitcpio-utils package. The instructions below can be used in any combination of the packages above. When there are different paths, it will be noted.

The netconf hook is normally used with an Ethernet connection. In case you want to setup a computer with wireless only, and unlock it via Wi-Fi, you can use a predefined hook or create a custom hook to connect to a Wi-Fi network before the netconf hook is run.

You can install a predefined hook based on the one in this wiki:

Below example shows a setup using a USB Wi-Fi adapter, connecting to a Wi-Fi network protected with WPA2-PSK. In case you use for example WEP or another initramfs generator, you might need to adjust accordingly.

Remember to setup Wi-Fi, so you are able to login once the system is fully booted. In case you are unable to connect to the Wi-Fi network, try increasing the sleep times a bit.

For systemd based initramfs the AUR package mkinitcpio-systemd-extrasAUR provides a collection of build hooks (aka install hooks) to achieve network connectivity and SSH login during early boot. Depending on the concrete setup this either gives you access to the initramfs environment via busybox' dash or just a password prompt.

When building the initramfs with mkinitcpio this setup copies the already existing configuration of systemd-networkd from the main system and also tries to copy / convert existing SSH server keys from an existing TinySSH or OpenSSH installation. tinyssh needs to be installed (but not necessarily enabled) on the main system. There are additional configuration parameters in case systemd-networkd is not used by the main system. See the documentation of mkinitcpio-systemd-extrasAUR for further details.

With systemd by default you have 90 seconds to unlock the device containing your root filesystem. After that systemd will give up waiting for this device and enter some emergency mode. You are then effectively locked out! To avoid this painful situation define the kernel parameter rootflags=x-systemd.device-timeout=0 in the configuration of your boot loader. With this setting systemd will wait forever.

If you are using dracut instead of mkinitcpio, you might want to check out dracut-sshd as an alternative to the above options.

Another method that can be used to reboot a remote, headless or otherwise inaccessible system whilst not needing to be at the terminal to type the encrypted root drive password, is to use a temporary keyfile. This will need to be placed in a location that is accessible to the kernel at boot, the cryptkey boot parameter will be needed, and that particular keyfile will need to be registered as a valid key by way of the "cryptsetup luksAddKey" command.

This can be done conveniently with the help of passless-bootAUR. The procedure described to setup that tool on the script's readme file might serve as a template for setting up a home-made solution also. Do take a look at the discussion in the Security considerations section.

Solid state drive users should be aware that, by default, TRIM commands are not enabled by the device-mapper, i.e. block-devices are mounted without the discard option unless you override the default.

The device-mapper maintainers have made it clear that TRIM support will never be enabled by default on dm-crypt devices because of the potential security implications.[3][4] Minimal data leakage in the form of freed block information, perhaps sufficient to determine the filesystem in use, may occur on devices with TRIM enabled. An illustration and discussion of the issues arising from activating TRIM is available in the blog of a cryptsetup developer. If you are worried about such factors, keep also in mind that threats may add up: for example, if your device is still encrypted with the previous (cryptsetup <1.6.0) default cipher --cipher aes-cbc-essiv, more information leakage may occur from trimmed sector observation than with the current default. The following cases can be distinguished:

Besides enabling discard support in dm-crypt, it is also required to periodically run fstrim(8) or mount the filesystem (e.g. /dev/mapper/root in this example) with the discard option in /etc/fstab. For details, please refer to the TRIM page.

For a LUKS2 device, TRIM support can be enabled by using the --allow-discards --persistent options when opening it. The allow-discards flag will be written into the LUKS2 header and the option will be automatically used whenever the LUKS2 device is opened.

If the device is already opened, the open action will raise an error, in which case, use the cryptsetup-refresh(8) command instead:

You can confirm the flag is persistently set in the LUKS2 header by looking at the cryptsetup luksDump output:

For LUKS1 and plain dm-crypt, TRIM support needs to be explicitly enabled when opening the device.

To enable TRIM support during boot, set the following kernel parameters.

If using the encrypt hook:

If using the sd-encrypt hook with systemd-based initramfs:

For devices unlocked via /etc/crypttab use option discard, e.g.:

When manually unlocking devices on the console use --allow-discards.

For example, you can open a device with the --allow-discards option to execute a manual fstrim command:

In any case, you can verify whether the device actually was opened with discards by inspecting the dmsetup table output:

Solid state drive users should be aware that, by default, discarding internal read and write workqueue commands are not enabled by the device-mapper, i.e. block-devices are mounted without the no_read_workqueue and no_write_workqueue option unless you override the default.

The no_read_workqueue and no_write_workqueue flags were introduced by internal Cloudflare research Speeding up Linux disk encryption made while investigating overall encryption performance. One of the conclusions is that internal dm-crypt read and write queues decrease performance for SSD drives. While queuing disk operations makes sense for spinning drives, bypassing the queue and writing data synchronously doubled the throughput and cut the SSD drives' IO await operations latency in half. The patches were upstreamed and are available since linux 5.9 and up [5].

To disable workqueue for LUKS devices unlocked via crypttab use one or more of the desired no-read-workqueue or no-write-workqueue options. E.g.:

To disable both read and write workqueue add both flags:

With LUKS2 you can set --perf-no_read_workqueue and --perf-no_write_workqueue as default flags for a device by opening it once with the option --persistent. For example:

When the device is already opened, the open action will raise an error. You can use the refresh option in these cases, e.g.:

You can confirm which flags are persistently set in the LUKS2 header by looking at the cryptsetup luksDump output:

In any case, you can verify whether the device actually was opened with these flags by inspecting the dmsetup table output:

Example for setting both no_read_workqueue and no_write_workqueue with cryptsetup:

You can confirm both flags being set by inspecting the LUKS2 cryptsetup luksDump output:

The encrypt hook only allows for a single cryptdevice= entry (archlinux/mkinitcpio/mkinitcpio#231). In system setups with multiple drives this may be limiting, because dm-crypt has no feature to exceed the physical device. For example, take "LVM on LUKS": The entire LVM exists inside a LUKS mapper. This is perfectly fine for a single-drive system, since there is only one device to decrypt. But what happens when you want to increase the size of the LVM? You cannot, at least not without modifying the encrypt hook.

The following sections briefly show alternatives to overcome the limitation. The first deals with how to expand a LUKS on LVM setup to a new disk. The second with modifying the encrypt hook to unlock multiple disks in LUKS setups without LVM.

The management of multiple disks is a basic LVM feature and a major reason for its partitioning flexibility. It can also be used with dm-crypt, but only if LVM is employed as the first mapper. In such a LUKS on LVM setup the encrypted devices are created inside the logical volumes (with a separate passphrase/key per volume). The following covers the steps to expand that setup to another disk.

First, it may be desired to prepare a new disk according to dm-crypt/Drive preparation. Second, it is partitioned as a LVM, e.g. all space is allocated to /dev/sdY1 with partition type 8E00 (Linux LVM). Third, the new disk/partition is attached to the existing LVM volume group, e.g.:

For the next step, the final allocation of the new diskspace, the logical volume to be extended has to be unmounted. It can be performed for the cryptdevice root partition, but in this case the procedure has to be performed from an Arch Install ISO.

In this example, it is assumed that the logical volume for /home (lv-name homevol) is going to be expanded with the fresh disk space:

Now the logical volume is extended and the LUKS container comes next:

Finally, the filesystem itself is resized:

Done! If it went to plan, /home can be remounted and now includes the span to the new disk:

Note that the cryptsetup resize action does not affect encryption keys, and these have not changed.

Note that sd-encrypt supports multiple partitions out of the box. If several (or all) partitions opened this way share the same passphrase, sd-encrypt will try it for each and not ask for it multiple times. This may be an easier alternative to the following.

It is possible to modify the encrypt hook to allow multiple hard drive decrypt root (/) at boot. One way:

Add cryptdevice2= to your boot options (and cryptkey2= if needed), and add the encrypt2 hook to your mkinitcpio.conf before rebuilding it. See dm-crypt/System configuration.

Maybe you have a requirement for using the encrypt hook on a non-root partition. Arch does not support this out of the box, however, you can easily change the cryptdev and cryptname values in /lib/initcpio/hooks/encrypt (the first one to your /dev/sd* partition, the second to the name you want to attribute). That should be enough.

The big advantage is you can have everything automated, while setting up /etc/crypttab with an external key file (i.e. the keyfile is not on any internal hard drive partition) can be a pain - you need to make sure the USB/FireWire/... device gets mounted before the encrypted partition, which means you have to change the order of /etc/fstab (at least).

Of course, if the cryptsetup package gets upgraded, you will have to change this script again. Unlike /etc/crypttab, only one partition is supported, but with some further hacking one should be able to have multiple partitions unlocked.

The factual accuracy of this article or section is disputed.

If you want to do this on a software RAID partition, there is one more thing you need to do. Just setting the /dev/mdX device in /lib/initcpio/hooks/encrypt is not enough; the encrypt hook will fail to find the key for some reason, and not prompt for a passphrase either. It looks like the RAID devices are not brought up until after the encrypt hook is run. You can solve this by putting the RAID array in /boot/grub/menu.lst, like

If you set up your root partition as a RAID, you will notice the similarities with that setup. GRUB can handle multiple array definitions just fine:

This example follows the same setup as in dm-crypt/Encrypting an entire system#Plain dm-crypt, which should be read first before following this guide.

By using a detached header the encrypted blockdevice itself only carries encrypted data, which gives deniable encryption as long as the existence of a header is unknown to the attackers. It is similar to using plain dm-crypt, but with the LUKS advantages such as multiple passphrases for the masterkey and key derivation. Further, using a detached header offers a form of two factor authentication with an easier setup than using GPG or OpenSSL encrypted keyfiles, while still having a built-in password prompt for multiple retries. See Data-at-rest encryption#Cryptographic metadata for more information.

See dm-crypt/Device encryption#Encryption options for LUKS mode for encryption options before performing the first step to setup the encrypted system partition and creating a header file to use with cryptsetup:

Now follow the LVM on LUKS setup to your requirements. The same applies for preparing the boot partition on the removable device (because if not, there is no point in having a separate header file for unlocking the encrypted disk). Next move the header.img onto it:

Follow the installation procedure up to the mkinitcpio step (you should now be arch-chrooted inside the encrypted system).

There are two options for initramfs to support a detached LUKS header.

Set the following kernel parameters:

Alternatively, instead of using the rd.luks kernel parameters, the options can be specified in a /etc/crypttab.initramfs file:

Next, modify /etc/mkinitcpio.conf to use systemd and to include the file system module for the volume in which the header is located. For example, if it is a FAT volume:

Regenerate the initramfs and you are done.

This method shows how to modify the encrypt hook in order to use a detached LUKS header. Now the encrypt hook has to be modified to let cryptsetup use the separate header (archlinux/mkinitcpio/mkinitcpio#234; base source and idea for these changes published on the BBS). Make a copy so it is not overwritten on a mkinitcpio update:

Now edit the mkinitcpio.conf to add the encrypt2 and lvm2 hooks, the header.img to FILES and the loop to MODULES, apart from other configuration the system requires:

This is required so the LUKS header is available on boot allowing the decryption of the system, exempting us from a more complicated setup to mount another separate USB device in order to access the header. After this set up the initramfs is created.

Next the boot loader is configured to specify the cryptdevice= also passing the new header option for this setup:

To finish, following dm-crypt/Encrypting an entire system#Post-installation is particularly useful with a /boot partition on an USB storage medium.

This article or section is being considered for removal.

Rather than embedding the header.img and keyfile into the initramfs image, this setup will make your system depend entirely on the usb key rather than just the image to boot, and on the encrypted keyfile inside of the encrypted boot partition. Since the header and keyfile are not included in the initramfs image and the custom encrypt hook is specifically for the usb's by-id, you will literally need the usb key to boot.

For the usb drive, since you are encrypting the drive and the keyfile inside, it is preferred to cascade the ciphers as to not use the same one twice. Whether a meet-in-the-middle attack would actually be feasible is debatable. You can do twofish-serpent or serpent-twofish.

sdb will be assumed to be the USB drive, sda will be assumed to be the main hard drive.

Prepare the devices according to dm-crypt/Drive preparation.

Use gdisk to partition the disk according to the layout shown here, with the exception that it should only include the first two partitions. So as follows:

Before running cryptsetup, look at the Encryption options for LUKS mode and Ciphers and modes of operation first to select your desired settings.

Prepare the boot partition but do not mount any partition yet and Format the EFI system partition.

filesize is in bytes but can be followed by a suffix such as M. Having too small of a file will get you a nasty Requested offset is beyond real size of device /dev/loop0 error. As a rough reference, creating a 4M file will encrypt it successfully. You should make the file larger than the space needed since the encrypted loop device will be a little smaller than the file's size.

With a big file, you can use --keyfile-offset=offset and --keyfile-size=size to navigate to the correct position (see Gentoo:Custom Initramfs#Encrypted keyfile).

Now you should have lukskey opened in a loop device (underneath /dev/loop1), mapped as /dev/mapper/lukskey.

Pick an offset and size in bytes (8192 KiB is the maximum keyfile size for cryptsetup).

Follow Preparing the logical volumes to set up LVM on LUKS.

See Partitioning#Discrete partitions for recommendations on the size of your partitions.

Once your root partition is mounted, mount your encrypted boot partition as /mnt/boot and your EFI system partition as /mnt/efi.

Follow the installation guide up to the mkinitcpio step but do not do it yet, and skip the partitioning, formatting, and mounting steps as they have already been done.

In order to get the encrypted setup to work, you need to build your own hook, which is thankfully easy to do and here is the code you need. You will have to follow Persistent block device naming#by-id and by-path to figure out your own by-id values for the usb and main hard drive (they are linked -> to sda or sdb).

You should be using the by-id instead of just sda or sdb because sdX can change and this ensures it is the correct device.

You can name customencrypthook anything you want, and custom build hooks can be placed in the hooks and install folders of /etc/initcpio. Keep a backup of both files (cp them over to the /home partition or your user's /home directory after you make one). /usr/bin/ash is not a typo.

usbdrive is your USB drive by-id, and harddrive is your main hard drive by-id.

Now edit the copied file and remove the help() section as it is not necessary.

The files=() and binaries=() arrays are empty, and you should not have to replace HOOKS=(...) array entirely just edit in customencrypthook lvm2 after block and before filesystems, and make sure systemd and encrypt are removed.

Finish the Installation Guide from the mkinitcpio step. To boot you would need either GRUB or efibootmgr. Note you can use GRUB to support the encrypted disks by Configuring the boot loader but editing the GRUB_CMDLINE_LINUX is not necessary for this set up.

Or use direct UEFI Secure Boot by generating keys with cryptbootAUR then signing the initramfs and kernel and creating a bootable .efi file for your EFI system partition with sbupdate-gitAUR. Before using cryptboot or sbupdate note this excerpt from Secure Boot#Using your own keys:

See efibootmgr(8) for an explanation of the options.

Make sure the boot order puts Arch Linux Signed first. If not change it with efibootmgr --bootorder XXXX,YYYY,ZZZZ --unicode.

This article or section is a candidate for merging with dm-crypt/Device encryption#Keyfiles.

Afterwards, cryptsetup close lukskey and shred or dd the old keyfile with random data before deleting it, then make sure that the new keyfile is renamed to the same name of the old one: key.img or other name.

**Examples:**

Example 1 (unknown):
```unknown
# gdisk /dev/sdx #format if necessary. Alternatively, cgdisk, fdisk, cfdisk, gparted...
# mkfs.ext2 /dev/sdx1 #for BIOS systems
# mkfs.fat -F 32 /dev/sdx1 #for UEFI systems
# mount /dev/sdx1 /mnt
```

Example 2 (unknown):
```unknown
# cp -ai /boot/* /mnt/
```

Example 3 (unknown):
```unknown
# umount /boot
# umount /mnt
# mount /dev/sdx1 /boot
# genfstab -p -U / > /etc/fstab
```

Example 4 (unknown):
```unknown
grub-mkconfig
```

---

## Silent boot

**URL:** https://wiki.archlinux.org/title/Silent_boot

**Contents:**
- Kernel parameters
- Remove console cursor blinking
- sysctl
- agetty
- startx
- fsck
- Make GRUB silent
- Retaining or disabling the vendor logo from UEFI
  - Disabling deferred takeover

This page is for those who prefer to limit the verbosity of their system to a strict minimum, either for aesthetics or other reasons. Following this guide will remove all text from the bootup process. Video demonstration

Change the kernel parameters using the configuration options of your boot loader, to include the following parameters:

If you are still getting messages printed to the console, it may be dmesg sending you what it thinks are important messages. You can change the level at which these messages will be printed by using quiet loglevel=level, where level is any number between 0 and 7, where 0 is the most critical, and 7 is debug levels of printing.

Note that this only seems to work if both quiet and loglevel=level are used, and they must be in that order (quiet first). The loglevel parameter will only change that which is printed to the console, the levels of dmesg itself will not be affected and will still be available through the journal as well as dmesg. For more information, see kernel parameters.

If you also want to stop systemd from printing its version number when booting, you should also append udev.log_level=3 to your kernel parameters. If systemd is used in an initramfs, append rd.udev.log_level=3 instead. See systemd-udevd.service(8) § KERNEL COMMAND LINE for details.

If you are using the systemd hook in the initramfs, you may get systemd messages during initramfs initialization. You can pass systemd.show_status=false to disable them, or systemd.show_status=auto to only suppress successful messages (so in case of errors you can still see them). Actually, auto is already passed to systemd.show_status=auto when quiet is used, however for some motive sometimes systemd inside initramfs does not get it. Below are the parameters that you need to pass to your kernel to get a completely clean boot with systemd in your initramfs:

Also touch ~/.hushlogin to remove the Last login message.

Users of plymouth must use both the quiet and splash kernel parameter, otherwise the details fallback theme is used and shows systemd messages.

The console cursor at boot keeps blinking if you follow these instructions. This can be solved by passing vt.global_cursor_default=0 to the kernel [1].

To recover the cursor in the TTY, run:

To hide any kernel messages from the console, add or modify the kernel.printk line according to [2]:

To hide agetty printed issue and "login:" prompt line from the console[3], create a drop-in snippet for getty@tty1.service.

To hide startx messages, you could redirect its output to /dev/null in your shell profile file (like ~/.bash_profile in Bash or ~/.zprofile in Zsh):

To hide fsck messages during boot, let systemd check the root filesystem. For this, replace udev hook with systemd and remove the fsck hook:

in /etc/mkinitcpio.conf and regenerate the initramfs.

See systemd-fsck@.service(8) for more info on the options you can pass to systemd-fsck - you can change how often the service will check (or not) your filesystems.

To hide GRUB welcome and boot messages, you may install unofficial grub-silentAUR package.

After the installation, it is required to reinstall GRUB to necessary partition first.

Then, take an example as /etc/default/grub.silent, and make necessary changes to /etc/default/grub.

Below three lines are necessary:

Lastly, regenerate the grub.cfg file.

Modern UEFI systems display a vendor logo on boot until handing over control to the boot loader—e.g. Lenovo laptops display a bright red Lenovo logo. This vendor logo is typically blanked by the boot loader—if standard GRUB is used—or by the kernel.

To prevent the kernel from blanking the vendor logo, Linux 4.19 introduced a new configuration option FRAMEBUFFER_CONSOLE_DEFERRED_TAKEOVER that retains the contents of the framebuffer until text needs to be printed on the framebuffer console. Since version 4.19.arch1, the official Arch Linux kernels are compiled with CONFIG_FRAMEBUFFER_CONSOLE_DEFERRED_TAKEOVER=y.

When combined with a low loglevel (to prevent text from being printed), the vendor logo can be retained while the system is initialized. Note that GRUB in the standard configuration blanks the screen; consider booting directly an EFI boot stub and thus leverage deferred takeover.

The kernel command line should use loglevel=3 or rd.udev.log_level=3 as mentioned above. Note that if you often receive Core temperature above threshold, cpu clock throttled messages in the kernel log, you need to use log level 2 to silence these at boot time. Alternatively, if you compile your own kernel, adjust the log level of the message in arch/x86/kernel/cpu/mcheck/therm_throt.c.

If you use Intel graphics, see also Intel graphics#Fastboot.

If the new behavior leads to issues, you can disable deferred takeover by using the fbcon=nodefer kernel parameter.

**Examples:**

Example 1 (unknown):
```unknown
vga=current
```

Example 2 (unknown):
```unknown
quiet loglevel=level
```

Example 3 (unknown):
```unknown
quiet loglevel=3
```

Example 4 (unknown):
```unknown
loglevel=level
```

---

## File systems

**URL:** https://wiki.archlinux.org/title/Reformat

**Contents:**
- Types of file systems
  - Journaling
  - FUSE-based file systems
  - Stackable file systems
  - Read-only file systems
  - Clustered file systems
  - Shared-disk file system
- Identify existing file systems
- Create a file system
- Mount a file system

Individual drive partitions can be set up using one of the many different available file systems. Each has its own advantages, disadvantages, and unique idiosyncrasies. A brief overview of supported file systems follows; the links are to Wikipedia pages that provide much more information.

See filesystems(5) for a general overview and Wikipedia:Comparison of file systems for a detailed feature comparison. File systems already loaded by the kernel or built-in are listed in /proc/filesystems, while all the installed modules can be seen with ls /lib/modules/$(uname -r)/kernel/fs.

xfs.html xfs-delayed-logging-design.html xfs-self-describing-metadata.html

The ext3/4, HFS+, JFS, NTFS, ReiserFS, and XFS file systems use journaling. Journaling provides fault-resilience by logging changes before they are committed to the file system. In the event of a system crash or power failure, such file systems are faster to bring back online and less likely to become corrupted. The logging takes place in a dedicated area of the file system.

ext3/4 offer data-mode journaling, which can optionally log data in addition to the metadata. Data-mode journaling comes with a speed penalty, because it does two write operations: first to the journal and then to the disk. Therefore, data-mode journaling is not enabled by default. The trade-off between system speed and data safety should be considered when choosing the file system type and features.

In the same vein, Reiser4 offers configurable "transaction models": a special model called wandering logs, which eliminates the need to write to the disk twice; write-anywhere, a pure copy-on-write approach; and a combined approach called hybrid which heuristically alternates between the two.

File systems based on copy-on-write (also known as write-anywhere), such as Reiser4, Btrfs, Bcachefs and ZFS, by design operate on full atomicity and also provide checksums for both metadata and inline data (operations entirely occur, or they entirely do not, and in properly functioning hardware data does not corrupt due to operations half-occurring). Therefore, these file systems are by design much less prone to data loss than other file systems and have no need to use traditional journal to protect metadata, because they are never updated in-place. Although Btrfs still has a journal-like log tree, it is only used to speed-up fdatasync/fsync.

FAT, exFAT, ext2, and HFS provide neither journaling nor atomicity, They are for temporary or legacy use and not recommended for use when reliable storage is needed.

To identify existing file systems, you can use lsblk:

An existing file system, if present, will be shown in the FSTYPE column. If mounted, it will appear in the MOUNTPOINT column.

File systems are usually created on a partition, inside logical containers such as LVM, RAID and dm-crypt, or on a regular file (see Wikipedia:Loop device). This section describes the partition case.

Before continuing, identify the device where the file system will be created and whether or not it is mounted. For example:

Mounted file systems must be unmounted before proceeding. In the above example an existing file system is on /dev/sda2 and is mounted at /mnt. It would be unmounted with:

To find just mounted file systems, see #List mounted file systems.

To create a new file system, use mkfs(8). See #Types of file systems for the exact type, as well as userspace utilities you may wish to install for a particular file system.

For example, to create a new file system of type ext4 (common for Linux data partitions) on /dev/sda1, run:

The new file system can now be mounted to a directory of choice.

To manually mount a file system located on a device (e.g., a partition) to a directory, use mount(8). This example mounts /dev/sda1 to /mnt.

This attaches the file system on /dev/sda1 at the directory /mnt, making the contents of the file system visible. Any data that existed at /mnt before this action is made invisible until the device is unmounted.

fstab contains information on how devices should be automatically mounted if present. See the fstab article for more information on how to modify this behavior.

If a device is specified in /etc/fstab and only the device or mount point is given on the command line, that information will be used in mounting. For example, if /etc/fstab contains a line indicating that /dev/sda1 should be mounted to /mnt, then the following will automatically mount the device to that location:

mount contains several options, many of which depend on the file system specified. The options can be changed, either by:

See these related articles and the article of the file system of interest for more information.

To list all mounted file systems, use findmnt(8):

findmnt takes a variety of arguments which can filter the output and show additional information. For example, it can take a device or mount point as an argument to show only information on what is specified:

findmnt gathers information from /etc/fstab, /etc/mtab, and /proc/self/mounts.

To unmount a file system use umount(8). Either the device containing the file system (e.g., /dev/sda1) or the mount point (e.g., /mnt) can be specified:

Unmount the file system and run fsck on the problematic volume.

**Examples:**

Example 1 (unknown):
```unknown
/proc/filesystems
```

Example 2 (unknown):
```unknown
ls /lib/modules/$(uname -r)/kernel/fs
```

Example 3 (unknown):
```unknown
NAME   FSTYPE LABEL     UUID                                 MOUNTPOINT
sdb
└─sdb1 vfat   Transcend 4A3C-A9E9
```

Example 4 (unknown):
```unknown
NAME   FSTYPE   LABEL       UUID                                 MOUNTPOINT
sda
├─sda1                      C4DA-2C4D
├─sda2 ext4                 5b1564b2-2e2c-452c-bcfa-d1f572ae99f2 /mnt
└─sda3                      56adc99b-a61e-46af-aab7-a6d07e504652
```

---

## CUPS

**URL:** https://wiki.archlinux.org/title/CUPS

**Contents:**
- Installation
  - Print steps
- Connection interfaces
  - USB
  - Parallel port
  - Network
    - Adding known location printers
    - Printer discovery
- Printer drivers
  - AirPrint and IPP Everywhere

CUPS is the standards-based, open source printing system developed by OpenPrinting for Linux® and other Unix®-like operating systems.

Arch Linux packages the OpenPrinting CUPS fork, not the Apple CUPS fork.

Install the cups package.

Optionally, install the cups-pdf package if you intend to "print" into a PDF document. By default, PDF files are stored in /var/spool/cups-pdf/username/. The location can be changed in /etc/cups/cups-pdf.conf.

Then enable and start cups.service or alternatively use socket activation to only start CUPS when a program wants to use the service.

This article or section needs expansion.

It is important to know how CUPS works if wanting to solve related issues:

Additional steps for printer detection are listed below for various connection interfaces.

To see if your USB printer is detected, make sure you have the usbutils package installed, then:

To use a parallel port printer, the lp, parport and parport_pc kernel modules are required.

It is not required to rely on dynamic printer discovery on the network (DNS-SD/mDNS) when the address of the printer is known (e.g. obtained via printers display or other network scanning approaches). A CUPS queue can be directly added to use the printer. Documentation for adding the queue with lpadmin can be found in following sections and official documentation at Setting up printers.

To discover, make use of discovered or share printers using DNS-SD/mDNS, setup .local hostname resolution with Avahi and restart cups.service.

To share printers with Samba, e.g. if the system is to be a print server for Windows clients, the samba package will be required.

This article or section needs expansion.

Most recent printers (2010+) support driverless usage by implementing AirPrint and/or IPP_Everywhere (c.f. below).

The drivers for a printer may come from any of the sources shown below. See CUPS/Printer-specific problems for an incomplete list of drivers that others have managed to get working.

To drive a printer, CUPS needs a PPD file and, for most printers, some filters. For details on how CUPS uses PPDs and filters, see [1].

The OpenPrinting Printer List provides driver recommendations for many printers. It also supplies PPD files for each printer, but most are available through foomatic or the recommended driver package.

When a PPD file is provided to CUPS, the CUPS server will regenerate the PPD files and save them in /etc/cups/ppd/.

To test if they are working before creating a PKGBUILD, PPD files can be manually added to /usr/share/cups/model, the driver should be available after the next restart of the cups service.

CUPS includes support for AirPrint and IPP Everywhere printers. These should be discovered automatically if avahi-daemon.service is running without any extra configuration.

Installing an IPP Everywhere printer is described in the CUPS README. It can be done with the -lpadmin command.

The -p option specifies the printer name. The -E option enables the printer and accepts new print jobs immediately. The -v option specifies the device URI for the printer, which tells CUPS how to communicate with the printer. And the -m option specifies the model (driver) to use, in this case the IPP Everywhere ("everywhere") driver that is used for AirPrint and IPP Everywhere printers as well as shared printers and printers supported through Printer Applications.

The USB or Network device URI can be found with the lpinfo -v command.

The Linux Foundation's OpenPrinting workgroup provides cups-filters. Those are backends, filters, and other binaries that were once part of CUPS but have been dropped from the project. They are available in the cups-filters package that is a dependency of cups.

Non-PDF printers require ghostscript to be installed. For PostScript printers, gsfonts may also be required.

The Linux Foundation's OpenPrinting workgroup's foomatic provides PPDs for many printer drivers, both free and non-free. For more information about what foomatic does, see Foomatic from the Developer's View.

To use foomatic, install foomatic-db-engine and at least one of:

The foomatic PPDs may require additional filters, such as min12xxwAUR.

The Gutenprint project provides drivers for Canon, Epson, Lexmark, Sony, Olympus, Brother, HP, Ricoh, PCL printers and some generic printers for use with CUPS and GIMP.

Install gutenprint and foomatic-db-gutenprint-ppds.

Many printer manufacturers supply their own Linux drivers. These are often available in the official Arch repositories or in the AUR.

Some of those drivers are described in more detail in CUPS/Printer-specific problems.

Listed below are additional steps to manually generate the URI if required. Some printers or drivers may need a special URI as described in CUPS/Printer-specific problems.

CUPS should be able to automatically generate a URI for USB printers, for example usb://HP/DESKJET%20940C?serial=CN16E6C364BH.

If it does not, see CUPS/Troubleshooting#USB printers for troubleshooting steps.

The URI should be of the form parallel:device. For instance, if the printer is connected on /dev/lp0, use parallel:/dev/lp0. If you are using a USB to parallel port adapter, use parallel:/dev/usb/lp0 as the printer URI.

If you have set up Avahi as in #Network, CUPS should detect the printer URI. You can also use avahi-discover to find the name of your printer and its address (for instance, BRN30055C6B4C7A.local/10.10.0.155:631).

The URI can also be generated manually, without using Avahi. A list of the available URI schemes for networked printers is available in the CUPS documentation. As exact details of the URIs differ between printers, check either the manual of the printer or CUPS/Printer-specific problems.

The URI for printers on SMB shares is described in the smbspool(8) man page.

For example, smb://BEN-DESKTOP/HP Color LaserJet CP1510 series PCL6 becomes smb://BEN-DESKTOP/HP%20Color%20LaserJet%20CP1510%20series%20PCL6.

This result string can be obtained by running the following command:

Remote CUPS print servers can be accessed through a URI of the form ipp://hostname:631/printers/queue_name. See CUPS/Printer sharing#Printer sharing for details on setting up the remote print server.

See CUPS/Troubleshooting#Networking issues for additional issues and solutions.

CUPS can be fully controlled using the lp* and cups* CLI tools. Alternatively, the #Web interface or one of several #GUI applications can be used.

See CUPS local documentation for more tips on the command-line tools.

Use SNMP to find a URI:

The lpinfo command lists the URI of the printers connected to your system with the -v flag, and lists all of the available drivers (or "models", in CUPS parlance) installed on your system with -m.

The lpadmin utility creates a new queue with -p queue_name. The -E flag added to -p enables and accepts jobs on the printer. The -v flag specifies the device URI. The -m flag specifies the driver (or "model", in CUPS parlance) or PPD file to use.

You can also use the -x flag to remove a printer (read #cups* beforehand).

For a driver-less queue (Apple AirPrint or IPP Everywhere):

For a raw queue; no PPD or filter:

When specifying a PPD instead of a model:

The lpq utility checks the queue. Add the -a flag to check on all queue.

The lprm utility clears the queue. Add a - to remove all entries instead of only the last one by default.

The lpr utility prints. Use -# N to print the file N times, use the -p flag to add a header.

Examples of test prints using lpr:

The lpstat utility, used with the -s flag, checks the status. The -p flag allows to specify which queue to check.

The lpoptions utility uses the same -p queue_name flag as lpadmin shown above. With the -l flag, it lists the options. The -d flag sets the default printer with the argument queue_name. The -o flag sets options to a value:

The cupsaccept, cupsdisable, cupsenable and cupsreject utilities do as they are called. Respectively: setting the printer to accept jobs, disabling a printer, activating a printer, setting the printer to reject all incoming tasks.

As an example of their usage, we will cleanly remove a printer:

Install inkAUR to view the ink levels.

Add your user to the additional lp user group, log out and log in again.

For usage information, run ink without options.

The CUPS server can be fully administered through the web interface, available on http://localhost:631/.

To perform administrative tasks from the web interface, authentication is required; see #Permissions.

Go to the Administration page.

Go to the Printers page, and select a queue to modify.

Go to the Printers page, and select a queue.

If your user does not have sufficient privileges to administer CUPS, the applications will request the root password when they start. To give users administrative privileges without needing root access, see #Configuration.

The CUPS server configuration is located in /etc/cups/cupsd.conf and /etc/cups/cups-files.conf (see cupsd.conf(5) and cups-files.conf(5)). After editing either file, restart cups.service to apply any changes. The default configuration is sufficient for most users.

User groups with printer administration privileges are defined in SystemGroup in the /etc/cups/cups-files.conf. The sys and root and wheel groups are used by default.

CUPS helper programs are run as the cups user and group. This allows the helper programs to access printer devices and read configuration files in /etc/cups/, which are owned by the cups group.

PolicyKit can be configured to allow users to configure printers using a GUI without the admin password.

Here is an example that allows members of the wheel user group to administer printers without a password:

cups is built with libpaper support and libpaper defaults to the Letter paper size (called PageSize in lpoptions). To avoid having to change the paper size for each print queue you add, edit /etc/papersize and set your system default paper size. See paper(1).

To save PDF files in the highly compatible format, normally called Archival PDF, or PDF/A, or PDFA, or ISO 19005.

There is currently no option, so it must be added to the command used by cups to call gs.

The factual accuracy of this article or section is disputed.

By default, all logs are sent to files in /var/log/cups/.

The log level can be changed in /etc/cups/cupsd.conf. See cupsd.conf documentation.

By changing the values of the AccessLog, ErrorLog, and PageLog directives in /etc/cups/cups-files.conf to syslog, CUPS can be made to log to the systemd journal instead. See Fedora:Changes/CupsJournalLogging for information on the original proposed change.

This article or section is out of date.

CUPS can use Avahi browsing to discover unknown shared printers in your network. This can be useful in large setups where the server is unknown. To use this feature, set up .local hostname resolution, and start both avahi-daemon.service and cups-browsed.service. Jobs are sent directly to the printer without any processing so the created queues may not work, however driverless printers such as those supporting IPP Everywhere or AirPrint should work out of the box.

See CUPS/Printer sharing and CUPS/Printer sharing#Remote administration.

CUPS can be configured to directly connect to remote printer servers instead of running a local print server. This requires installation of the libcups package. Some applications will still require the cups package for printing.

To use a remote CUPS server, set the CUPS_SERVER environment variable to printerserver.mydomain:port. For instance, if you want to use a different print server for a single Firefox instance (substitute printserver.mydomain:port with your print server name/port):

To make this configuration permanent create configuration file /etc/cups/client.conf and add a hostname of the remote CUPS server to it:

You can also specify a custom port:

See CUPS/Troubleshooting and CUPS/Printer-specific problems.

**Examples:**

Example 1 (unknown):
```unknown
/var/spool/cups-pdf/username/
```

Example 2 (unknown):
```unknown
/etc/cups/cups-pdf.conf
```

Example 3 (unknown):
```unknown
cups.service
```

Example 4 (unknown):
```unknown
(...)
Bus 001 Device 007: ID 03f0:1004 Hewlett-Packard DeskJet 970c/970cse
```

---

## systemd-nspawn

**URL:** https://wiki.archlinux.org/title/Systemd-nspawn

**Contents:**
- Installation
- Examples
  - Create and boot a minimal Arch Linux container
  - Create a Debian or Ubuntu environment
  - Create a RHEL-derivative environment using Podman or Docker
  - Create a Fedora or AlmaLinux environment
  - Build and test packages
- Management
  - Default systemd-nspawn options
  - machinectl

systemd-nspawn is like the chroot command, but it is a chroot on steroids.

systemd-nspawn may be used to run a command or operating system in a light-weight namespace container. It is more powerful than chroot since it fully virtualizes the file system hierarchy, as well as the process tree, the various IPC subsystems and the host and domain name.

systemd-nspawn limits access to various kernel interfaces in the container to read-only, such as /sys, /proc/sys or /sys/fs/selinux. Network interfaces and the system clock may not be changed from within the container. Device nodes may not be created. The host system cannot be rebooted and kernel modules may not be loaded from within the container.

systemd-nspawn is a simpler tool to configure than LXC or Libvirt.

systemd-nspawn is part of and packaged with systemd.

Create a directory to hold the container, in this example we will use ~/MyContainer.

Use pacstrap from arch-install-scripts package to install a basic Arch system into the container. At minimum we need to install the base package.

Once your installation is finished, enter the container, and set a root password:

Finally, boot into the container:

The -b option will boot the container (i.e. run systemd as PID=1), instead of just running a shell, and -D specifies the directory that becomes the container's root directory.

After the container starts, log in as "root" with your password.

The container can be powered off by running poweroff from within the container. From the host, containers can be controlled by the machinectl tool.

Install debootstrap, and one or both of debian-archive-keyring or ubuntu-keyring depending on which distribution you want.

Then invoke deboostrap with the following structure:

debootstrap cannot resolve dependencies on virtual package[6], and as a consequence it does not install systemd's dbus and libpam-systemd recommended dependencies by default[7][8]. As a consequence, some systemd/dbus-related functionalities (e.g. localectl) as well as managing the container with #machinectl do not work out of the box.

In order to get full functionality in systemd-based systems, add --include=dbus,libpam-systemd to the debootstrap invocation or install those packages once in the container:

Just like Arch, Debian and Ubuntu will not let you log in without a password. To set the root password, run systemd-nspawn without the -b option:

These instructions should be applicable to any distribution which uses dnf. You will need Podman or Docker.

From within that container, you can build the root environment for your machine:

This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.

From there, you should be able to boot the machine:

Install dnf, and edit the /etc/dnf/dnf.conf file to add the required Fedora repositories.

The fedora.gpg file contain the gpg keys for the latest Fedora releases https://getfedora.org/security/. To set up a minimal Fedora 37 container:

If you are using btrfs filesystem create a subvolume instead of creating a directory.

An Enterprise Linux derivative like AlmaLinux has three repositories enabled by default, BaseOS wich contains a core set that provides the basis for all installations, AppStream that includes additional applications, language packages, etc and Extras that contains packages not included in RHEL. So for a minimal container we only need to add the BaseOS repository to /etc/dnf/dnf.conf

To create an AlmaLinux 9 minimal container:

This will install the latest minor version of AlmaLinux 9, you can choose to install a specific point release, but you will need to change the gpgpkey entry to manually point to RPM-GPG-KEY-AlmaLinux-9

Just like Arch, Fedora or AlmaLinux will not let you log in as root without a password. To set up the root password, run systemd-nspawn without the -b option:

See Creating packages for other distributions for example uses.

Containers located in /var/lib/machines/ can be controlled by the machinectl command, which internally controls instances of the systemd-nspawn@.service unit. The subdirectories in /var/lib/machines/ correspond to the container names, i.e. /var/lib/machines/container-name/.

Note that containers started via machinectl or systemd-nspawn@.service use different default options than containers started manually by the systemd-nspawn command. The extra options used by the service are:

This behavior can be overridden in per-container configuration files. See #Configuration for details.

Containers can be managed by the machinectl subcommand container-name command. For example, to start a container:

Similarly, there are subcommands such as poweroff, reboot, status and show. See machinectl(1) § Machine Commands for detailed explanations.

Other common commands are:

machinectl also has subcommands for managing container (or virtual machine) images and image transfers. See machinectl(1) § Image Commands and machinectl(1) § Image Transfer Commands for details. As of 2023Q1, the first 3 examples at machinectl(1) § EXAMPLES demonstrate image transfer commands. machinectl(1) § FILES AND DIRECTORIES discusses where to find suitable images.

Much of the core systemd toolchain has been updated to work with containers. Tools that do usually provide a -M, --machine= option which will take a container name as argument.

See journal logs for a particular machine:

Show control group contents:

See startup time of container:

For an overview of resource usage:

To specify per-container settings and not global overrides, the .nspawn files can be used. See systemd.nspawn(5) for details.

When using a container frequently, you may want to start it at boot.

First make sure that the machines.target is enabled.

Containers discoverable by machinectl can be enabled or disabled:

You can take advantage of control groups to implement limits and resource management of your containers with systemctl set-property, see systemd.resource-control(5). For example, you may want to limit the memory amount or CPU usage. To limit the memory consumption of your container to 2 GiB:

Or to limit the CPU time usage to roughly the equivalent of 2 cores:

This will create permanent files in /etc/systemd/system.control/systemd-nspawn@container-name.service.d/.

According to the documentation, MemoryHigh is the preferred method to keep in check memory consumption, but it will not be hard-limited as is the case with MemoryMax. You can use both options leaving MemoryMax as the last line of defense. Also take in consideration that you will not limit the number of CPUs the container can see, but you will achieve similar results by limiting how much time the container will get at maximum, relative to the total CPU time.

systemd-nspawn containers can use either host networking or private networking:

The host networking mode is suitable for application containers which do not run any networking software that would configure the interface assigned to the container. Host networking is the default mode when you run systemd-nspawn from the shell.

On the other hand, the private networking mode is suitable for system containers that should be isolated from the host system. The creation of virtual Ethernet links is a very flexible tool allowing to create complex virtual networks. This is the default mode for containers started by machinectl or systemd-nspawn@.service.

The following subsections describe common scenarios. See systemd-nspawn(1) § Networking Options for details about the available systemd-nspawn options.

To disable private networking and the creation of a virtual Ethernet link used by containers started with machinectl, add a .nspawn file with the following option:

This will override the -n/--network-veth option used in systemd-nspawn@.service and the newly started containers will use the host networking mode.

If a container is started with the -n/--network-veth option, systemd-nspawn will create a virtual Ethernet link between the host and the container. The host side of the link will be available as a network interface named ve-container-name. The container side of the link will be named host0. Note that this option implies --private-network.

When you start the container, an IP address has to be assigned to both interfaces (on the host and in the container). If you use systemd-networkd on the host as well as in the container, this is done out-of-the-box:

If you do not use systemd-networkd, you can configure static IP addresses or start a DHCP server on the host interface and a DHCP client in the container. See Network configuration for details.

To give the container access to the outside network, you can configure NAT as described in Internet sharing#Enable NAT. If you use systemd-networkd, this is done (partially) automatically via the IPMasquerade=both option in /usr/lib/systemd/network/80-container-ve.network. However, this issues just one iptables (or nftables) rule such as

The filter table has to be configured manually as shown in Internet sharing#Enable NAT. You can use a wildcard to match all interfaces starting with ve-:

Additionally, you need to open the UDP port 67 on the ve-+ interfaces for incoming connections to the DHCP server (operated by systemd-networkd):

If you have configured a network bridge on the host system, you can create a virtual Ethernet link for the container and add its host side to the network bridge. This is done with the --network-bridge=bridge-name option. Note that --network-bridge implies --network-veth, i.e. the virtual Ethernet link is created automatically. However, the host side of the link will use the vb- prefix instead of ve-, so the systemd-networkd options for starting the DHCP server and IP masquerading will not be applied.

The bridge management is left to the administrator. For example, the bridge can connect virtual interfaces with a physical interface, or it can connect only virtual interfaces of several containers. See systemd-networkd#Network bridge with DHCP and systemd-networkd#Network bridge with static IP addresses for example configurations using systemd-networkd.

There is also a --network-zone=zone-name option which is similar to --network-bridge but the network bridge is managed automatically by systemd-nspawn and systemd-networkd. The bridge interface named vz-zone-name is automatically created when the first container configured with --network-zone=zone-name is started, and is automatically removed when the last container configured with --network-zone=zone-name exits. Hence, this option makes it easy to place multiple related containers on a common virtual network. Note that vz-* interfaces are managed by systemd-networkd same way as ve-* interfaces using the options from the /usr/lib/systemd/network/80-container-vz.network file.

Instead of creating a virtual Ethernet link (whose host side may or may not be added to a bridge), you can create a virtual interface on an existing physical interface (i.e. VLAN interface) and add it to the container. The virtual interface will be bridged with the underlying host interface and thus the container will be exposed to the outside network, which allows it to obtain a distinct IP address via DHCP from the same LAN as the host is connected to.

systemd-nspawn offers 2 options:

Both options imply --private-network.

If the host system has multiple physical network interfaces, you can use the --network-interface=interface to assign interface to the container (and make it unavailable to the host while the container is started). Note that --network-interface implies --private-network.

When private networking is enabled, individual ports on the host can be mapped to ports on the container using the -p/--port option or by using the Port setting in an .nspawn file. For example, to map a TCP port 8000 on the host to the TCP port 80 in the container:

This works by issuing iptables (or nftables) rules to the nat table, but the FORWARD chain in the filter table needs to be configured manually as shown in #Use a virtual Ethernet link. Additionally, if you followed Simple stateful firewall, run the following command to allow new connections to the host's wan_interface on a forwarded port to be established:

Domain name resolution in the container can be configured the same way as on the host system. Additionally, systemd-nspawn provides options to manage the /etc/resolv.conf file inside the container:

These corresponding options have many possible values which are described in systemd-nspawn(1) § Integration Options. The default value is auto, which means that:

In the last two cases, the file is copied, if the container root is writeable, and bind-mounted if it is read-only.

For the second case where systemd-resolved runs on the host, systemd-nspawn expects it to also run in the container, so that the container can use the stub symlink file /etc/resolv.conf from the host. If not, the default value auto no longer works, and you should replace the symlink by using one of the replace-* options.

From systemd-nspawn(1) § Execution Options:

systemd-nspawn supports unprivileged containers, though the containers need to be booted as root.

This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.

The easiest way to do this is to let systemd-nspawn automatically choose an unused range of UIDs/GIDs by using the -U option:

If kernel supports user namespaces, the -U option is equivalent to --private-users=pick --private-users-ownership=auto. See systemd-nspawn(1) § User Namespacing Options for details.

If a container has been started with a private UID/GID range using the --private-users-ownership=chown option (or on a filesystem where -U requires --private-users-ownership=chown), you need to keep using it that way to avoid permission errors. Alternatively, it is possible to undo the effect of --private-users-ownership=chown on the container's file system by specifying a range of IDs starting at 0:

The factual accuracy of this article or section is disputed.

See Xhost and Change root#Run graphical applications from chroot.

You will need to set the DISPLAY environment variable inside your container session to connect to the external X server.

X stores some required files in the /tmp directory. In order for your container to display anything, it needs access to those files. To do so, append the --bind-ro=/tmp/.X11-unix option when starting the container.

xhost only provides rather coarse access rights to the X server. More fine-grained access control is possible via the $XAUTHORITY file. Unfortunately, just making the $XAUTHORITY file accessible in the container will not do the job: your $XAUTHORITY file is specific to your host, but the container is a different host. The following trick adapted from stackoverflow can be used to make your X server accept the $XAUTHORITY file from an X application run inside the container:

The second line above sets the connection family to "FamilyWild", value 65535, which causes the entry to match every display. See Xsecurity(7) for more information.

Another simple way to run X applications and avoid the risks of a shared X desktop is using X nesting. The advantages here are avoiding interaction between in-container applications and non-container applications entirely and being able to run a different desktop environment or window manager. The downsides are less performance, and the lack of hardware acceleration when using Xephyr.

Start Xephyr outside of the container using:

Then start the container with the following options:

No other binds are necessary.

You might still need to manually set DISPLAY=:1 in the container under some circumstances (mostly if used with -b).

Alternatively you can boot the container and let e.g. systemd-networkd set up the virtual network interface:

Once your container is booted, run the Xorg binary like so:

This article or section needs expansion.

To enable accelerated 3D graphics, it may be necessary to bind mount /dev/dri to the container by adding the following line to the .nspawn file:

The above trick was adopted from patrickskiba.com. This notably solves the problem of

You can confirm that it has been enabled by running glxinfo or glxgears.

If you cannot install the same NVIDIA driver version on the container as on the host, you may need to also bind the driver library files. You can run pacman -Ql nvidia-utils on the host to see all the files it contains. You do not need to copy everything over. The following systemd override file will bind all the necessary files over when the container is run via machinectl start container-name.

The factual accuracy of this article or section is disputed.

See --bind and --bind-ro in systemd-nspawn(1).

If both the host and the container are Arch Linux, then one could, for example, share the pacman cache:

Or you can specify per-container bind using the file:

See #Per-container settings.

To bind the directory to a different path within the container, add the path be separated by a colon. For example:

In case of #Unprivileged containers, the resulting mount points will be owned by the nobody user. This can be modified with the idmap mount option:

See Init#systemd-nspawn.

To use a Btrfs subvolume as a template for the container's root, use the --template flag. This takes a snapshot of the subvolume and populates the root directory for the container with it.

For example, to use a snapshot located at /.snapshots/403/snapshot:

where my-container is the name of the directory that will be created for the container. After powering off, the newly created subvolume is retained.

One can use the --ephemeral or -x flag to create a temporary btrfs snapshot of the container and use it as the container root. Any changes made while booted in the container will be lost. For example:

where my-container is the directory of an existing container or system. For example, if / is a btrfs subvolume one could create an ephemeral container of the currently running host system by doing:

After powering off the container, the btrfs subvolume that was created is immediately removed.

Since Docker 20.10, it is possible to run Docker containers inside an unprivileged systemd-nspawn container with cgroups v2 enabled (default in Arch Linux) without undermining security measures by disabling cgroups and user namespaces. To do so, edit /etc/systemd/nspawn/myContainer.nspawn (create if absent) and add the following configurations.

Then, Docker should work as-is inside the container.

The factual accuracy of this article or section is disputed.

With recent versions of systemd, you would also need to need the following workaround:

See [19] for more details.

Since overlayfs does not work with user namespaces and is unavailable inside systemd-nspawn, by default, Docker falls back to using the inefficient vfs as its storage driver, which creates a copy of the image each time a container is started. This can be worked around by using fuse-overlayfs as its storage driver. To do so, we need to first expose fuse to the container:

and then allow the container to read and write the device node:

Finally, install the package fuse-overlayfs inside the container. You need to restart the container for all the configuration to take effect.

Create a container in /var/lib/machines/MyContainer/ as explained in #Create and boot a minimal Arch Linux container.

Create a configuration file with BindUser= to map the selected local user name into the container. Note that this requires PrivateUsers=, see systemd-nspawn(1) for details. Files created in the bind-mounted home directory both inside and outside the container will have the same UID and GID.

Having also User= in the configuration specifies the default user used for running commands inside the container, such as the interactive shell:

This is helpful for testing Arch Linux packages from another Linux distribution.

When trying to boot the container via systemd-nspawn -bD /path/to/container (or executing something in the container), and the following error comes up:

even though the permissions of the files in question (i.e. /lib/systemd/systemd) are correct, this can be the result of having mounted the file system on which the container is stored as non-root user. For example, if you mount your disk manually with an entry in fstab that has the options noauto,user,..., systemd-nspawn will not allow executing the files even if they are owned by root.

When logging into the container via machinectl login, the colors and keystrokes in the terminal within the container might be broken. This may be due to an incorrect terminal type in TERM environment variable. The environment variable is not inherited from the shell on the host, but falls back to a default fixed in systemd (vt220), unless explicitly configured. To configure, within the container create a configuration overlay for the container-getty@.service systemd service that launches the login getty for machinectl login, and set TERM to the value that matches the host terminal you are logging in from:

Alternatively use machinectl shell. It properly inherits the TERM environment variable from the terminal.

This article or section needs expansion.

Not possible at this time (June 2019).

**Examples:**

Example 1 (unknown):
```unknown
/sys/fs/selinux
```

Example 2 (unknown):
```unknown
~/MyContainer
```

Example 3 (unknown):
```unknown
# pacstrap -K -c ~/MyContainer base [additional packages/groups]
```

Example 4 (unknown):
```unknown
# systemd-nspawn -D ~/MyContainer
# passwd
# logout
```

---

## Network Time Protocol daemon

**URL:** https://wiki.archlinux.org/title/NTPd

**Contents:**
- Installation
- Configuration
  - Connection to NTP servers
  - Leap seconds file
  - NTP server mode
- Usage
  - Start ntpd at boot
  - Synchronize time once per boot
- Tips and tricks
  - Start ntpd on network connection

Network Time Protocol is the most common method to synchronize the software clock of a GNU/Linux system with internet time servers. It is designed to mitigate the effects of variable network latency and can usually maintain time to within tens of milliseconds over the public Internet. The accuracy on local area networks is even better, up to one millisecond.

The NTP Project provides a reference implementation of the protocol called simply NTP. This article further describes how to set up and run the NTP daemon, both as a client and as a server.

See System time#Time synchronization for other NTP implementations.

Install the ntp package. By default, ntpd works in client mode without further configuration. You can skip to #Usage if you want to use the Arch Linux default configuration file for it. For server configuration, see #NTP server mode.

The main daemon is ntpd, which is configured in /etc/ntp.conf. Refer to ntp.conf(5) for detail.

NTP servers are classified in a hierarchical system with many levels called strata: the devices which are considered independent time sources are classified as stratum 0 sources; the servers directly connected to stratum 0 devices are classified as stratum 1 sources; servers connected to stratum 1 sources are then classified as stratum 2 sources and so on.

It has to be understood that a server's stratum cannot be taken as an indication of its accuracy or reliability. Typically, stratum 2 servers are used for general synchronization purposes: if you do not already know the servers you are going to connect to, you should choose a server pool close to your location from the pool.ntp.org servers (alternative link).

Since ntp version 4.2.7.p465-2, Arch Linux uses its own default vendor pool of NTP servers provided by the NTP Pool Project (see FS#41700). Modify those to suit your needs, e.g. if you want to use your country's servers with an option:

The iburst option is recommended, and sends a burst of packets only if it cannot obtain a connection with the first attempt. The burst option always does this, even on the first attempt, and should never be used without explicit permission and may result in blacklisting.

In order for the system to be able to provide the International Atomic Time to an application that requests it, the list of leap seconds must be loaded. The list is part of the tzdata package and can be loaded by adding the following line to the NTP configuration file:

If setting up an NTP server, check that you have orphan mode enabled, so that, in case it loses internet access, it will continue serving time to the network; enable orphan mode using the tos configuration parameter (you can set up to stratum 15) so that it will never be used unless internet access is lost:

Next, define the rules that will allow clients to connect to your service (localhost is considered a client too) using the restrict command; you should already have a line like this in your file:

This restricts everyone from modifying anything and prevents everyone from querying the status of your time server: nomodify prevents reconfiguring ntpd (with ntpq or ntpdc), and noquery is important to prevent dumping status data from ntpd (also with ntpq or ntpdc).

You can also add other options:

If you want to change any of these, see the full docs for the "restrict" option in ntp.conf(5), the detailed ntp instructions and #Usage.

Following this line, you need to tell ntpd what to allow through into your server; the following line is enough if you are not configuring an NTP server:

If you want to force DNS resolution to the IPv6 namespace, write -6 before the IP address or host name (-4 forces IPv4 instead), for example:

Lastly, specify the drift file (which keeps track of your clock's time deviation) and optionally the log file location:

A very basic configuration file will look like this:

The package has a default client-mode configuration and its own user and group to drop root privileges after starting. If you start it from the console, you should always do so with the -u option:

The -u option is employed by the two included systemd services. These services also use the -g option, which disables a threshold (so-called panic-gate). Hence, they will synchonize time even in case the ntp-server's time exceeds the threshold deviation from the system clock.

Both services are tied to the system's resolver, and will start synchronizing when an active network connection is detected.

Enable the daemon with ntpd.service. See also #Running in a chroot.

Use ntpq to see the list of configured peers and status of synchronization:

The delay, offset and jitter columns should be non-zero. The servers ntpd is synchronizing with are prefixed by an asterisk. It can take several minutes before ntpd selects a server to synchronize with; try checking after 17 minutes (1024 seconds).

Alternatively, enable ntpdate.service to synchronize time once (option -q) and non-forking (option -n) per boot, instead of running the daemon in the background. This method is discouraged on servers, and in general on machines that run without rebooting for more than a few days.

If the synchronized time should be written to the hardware clock as well, configure the provided unit as described in systemd#Editing provided units before starting it:

ntpd can be started by your network manager, so that the daemon only runs when the computer is online.

Append the following lines to your netctl profile:

The ntpd daemon can be brought up/down along with a network connection through the use of NetworkManager's dispatcher scripts. The networkmanager-dispatcher-ntpdAUR package installs one, pre-configured to start and stop the ntpd service with a connection.

KDE can use NTP (ntp must be installed) by right clicking the clock and selecting Adjust date/time. However, this requires the ntp daemon to be disabled before configuring KDE to use NTP. [2]

Most of the articles online about configuring ntpd to receive time from a GPS suggest to use the SHM (shared memory) method. However, at least since ntpd version 4.2.8, a much better method is available. It connects directly to gpsd, so gpsd needs to be installed.

Add these lines to your /etc/ntp.conf:

This will work as long as you have gpsd working. It connects to gpsd via the local socket and queries the "gpsd_json" object that is returned.

To test the setup, first ensure that gpsd is working by running:

Then wait a few minutes and run ntpq -p. This will show if ntpd is talking to gpsd:

Create a new directory /etc/systemd/system/ntpd.service.d/ if it does not exist and a file named customexec.conf inside with the following content:

Then, edit /etc/ntp.conf to change the driftfile path such that it is relative to the chroot directory, rather than to the real system root. Change:

Create a suitable chroot environment so that getaddrinfo() will work by creating pertinent directories and files (as root):

and by bind-mounting the aformentioned files:

Finally, restart ntpd daemon again. Once it restarted you can verify that the daemon process is chrooted by checking where /proc/{PID}/root symlinks to:

should now link to /var/lib/ntp instead of /.

It is relatively difficult to be sure that your driftfile configuration is actually working without waiting a while, as ntpd does not read or write it very often. If you get it wrong, it will log an error; if you get it right, it will update the timestamp. If you do not see any errors about it after a full day of running, and the timestamp is updated, you should be confident of success.

You can limit sockets ntpd is listening to using the interface option:

**Examples:**

Example 1 (unknown):
```unknown
/etc/ntp.conf
```

Example 2 (unknown):
```unknown
/etc/ntp.conf
```

Example 3 (unknown):
```unknown
server 0.fr.pool.ntp.org iburst
server 1.fr.pool.ntp.org iburst
server 2.fr.pool.ntp.org iburst
server 3.fr.pool.ntp.org iburst
```

Example 4 (unknown):
```unknown
leapfile /usr/share/zoneinfo/leap-seconds.list
```

---

## Hard Drive Active Protection System

**URL:** https://wiki.archlinux.org/title/HDAPS

**Contents:**
- Shock detection
  - tp_smapi
  - Invert module parameter
- Shock protection
  - hdapsd
- GUI Utilities
- See also

Hard Drive Active Protection System (HDAPS) protects your hard drive from sudden shocks (such as dropping or banging your laptop on a desk). It does this by parking the disk heads, so that shocks do not cause them to crash into the drive's platters. Hopefully, this will prevent catastrophic failure. Also see Active hard-drive protection.

Your hardware needs to support some kind of shock detection. This is usually in the form of an accelerometer built into your laptop's motherboard. If you have the hardware, you also need a way to communicate what the hardware is detecting to your operating system. This section describes drivers to communicate the accelerometer's state to the OS so it can detect and protect against shocks.

tp_smapi is a set of drivers for many ThinkPad laptops. It is highly recommended if you have a supported ThinkPad, even if you do not plan to use HDAPS. Among a plethora of other useful things, tp_smapi represents the accelerometer output as joystick devices /dev/input/js#.

Install tp_smapi. After a reboot, this will activate most of the drivers, represented through the /sys/devices/platform/smapi filesystem.

The kernel provides its own HDAPS drivers. The tp_smapi package installs hdaps.ko to /lib/modules/$(uname -r)/updates, which will let it supersede the built-in module. Thus, you can simply add hdaps to your MODULES array.

For some ThinkPads, the invert module parameter is needed in order to handle the X and Y rotation axes correctly. In that case, you can add the option in /etc/modprobe.d/modprobe.conf:

invert=1 is an example value used for a ThinkPad T410. The invert option takes the following values:

Note that options can be summed. For instance, invert=5 swaps the axes and inverts them. The maximum value of invert is obviously 7. If you do not know which option is correct for you, just try them out with hdaps-gl or some other GUI (see below). Alternatively, you can determine the exact value for your thinkpad model from this table under the column labelled "HDAPS axis orientation".

As an alternative to reloading the hdaps module, the invert value can also be written directly to /sys/devices/platform/hdaps/invert.

Now that your hardware is reporting its shock detection to the OS, we need to do something with this data. This section describes software utilities to transform the sensor output into shock protection.

hdapsd monitors the output of the HDAPS joystick devices to determine if a shock is about to occur, then tells the kernel to park the disk heads.

You should check your "Load cycle count" in SMART when setting up hdaps, if it is too sensitive the head would park too often and load cycle count would rise too rapidly.

Install hdapsd. You can start hdapsd with hdapsd@device.service, however you do not need to enable it.

The package installs udev rules. Udev will start one hdapsd instance for each rotational, non-removable disk it finds. For more information, see the hdapsd github page.

It may be desirable to tweak the parameters used by hdaspd. Edit hdapsd.service and add e.g --sensitivity=40 -blp to the parameters.

Utilities exist to monitor hdapsd's status so you know what is going on while you are using your laptop. These are entirely optional.

xfce4-hdaps — Xfce4 panel applet that can represents the current status of your hard drive.

HDAPSicon — Formerly thinkhdaps, standalone GTK applet for HDAPS disk protection status.

hdaps-gl — Simple OpenGL application showing the 3D animation of your Thinkpad. Similar to the apllication Lenovo distributes with Windows.

**Examples:**

Example 1 (unknown):
```unknown
/dev/input/js#
```

Example 2 (unknown):
```unknown
/sys/devices/platform/smapi
```

Example 3 (unknown):
```unknown
/etc/modprobe.d/modprobe.conf
```

Example 4 (unknown):
```unknown
options hdaps invert=1
```

---

## Xorg

**URL:** https://wiki.archlinux.org/title/X11

**Contents:**
- Installation
  - Driver installation
    - AMD
- Running
- Configuration
  - Using .conf files
  - Using xorg.conf
- Input devices
  - Input identification
  - Mouse acceleration

X.Org Server — commonly referred to as simply X — is the X.Org Foundation implementation of the X Window System (X11) display server, and it is the most popular display server among Linux users. Its ubiquity has led to making it an ever-present requisite for GUI applications, resulting in massive adoption from most distributions.

For the alternative and successor, see Wayland.

Xorg can be installed with the xorg-server package.

Additionally, some packages from the xorg-apps group are necessary for certain configuration tasks. They are pointed out in the relevant sections.

Finally, an xorg group is also available, which includes Xorg server packages, packages from the xorg-apps group and fonts.

This article or section is a candidate for moving to Graphics processing unit.

The Linux kernel includes open-source video drivers and support for hardware accelerated framebuffers. However, userland support is required for OpenGL, Vulkan and 2D acceleration in X11.

First, identify the graphics card (the Subsystem output shows the specific model):

Then, install an appropriate driver. You can search the package database for a complete list of open-source Device Dependent X (DDX) drivers:

However, hardware-specific DDX is considered legacy nowadays. There is a generic modesetting(4) DDX driver in xorg-server, which uses kernel mode setting and works well on modern hardware. The modesetting DDX driver uses Glamor[1] for 2D acceleration, which requires OpenGL.

If you want to install another DDX driver, note that Xorg searches for installed DDX drivers automatically:

In order for video acceleration to work, and often to expose all the modes that the GPU can set, a proper video driver is required:

Other DDX drivers can be found in the xorg-drivers group.

Xorg should run smoothly without closed source drivers, which are typically needed only for advanced features such as fast 3D-accelerated rendering for games. The exceptions to this rule are recent GPUs (especially NVIDIA GPUs) not supported by open source drivers.

For a translation of model names (e.g. Radeon RX 6800) to GPU architectures (e.g. RDNA 2), see Wikipedia:List of AMD graphics processing units#Features overview.

The Xorg(1) command is usually not run directly. Instead, the X server is started with either a display manager or xinit.

Xorg uses a configuration file called xorg.conf and files ending in the suffix .conf for its initial setup: the complete list of the folders where these files are searched can be found in xorg.conf(5), together with a detailed explanation of all the available options.

The /etc/X11/xorg.conf.d/ directory stores host-specific configuration. You are free to add configuration files there, but they must have a .conf suffix: the files are read in ASCII order, and by convention their names start with XX- (two digits and a hyphen, so that for example 10 is read before 20). These files are parsed by the X server upon startup and are treated like part of the traditional xorg.conf configuration file. Note that on conflicting configuration, the file read last will be processed. For this reason, the most generic configuration files should be ordered first by name. The configuration entries in the xorg.conf file are processed at the end.

For option examples to set, see Fedora:Input device configuration#xorg.conf.d.

Xorg can also be configured via /etc/X11/xorg.conf or /etc/xorg.conf. You can also generate a skeleton for xorg.conf with:

This should create a xorg.conf.new file in /root/ that you can copy over to /etc/X11/xorg.conf.

Alternatively, your proprietary video card drivers may come with a tool to automatically configure Xorg: see the article of your video driver, NVIDIA or AMDGPU PRO, for more details.

For input devices the X server defaults to the libinput driver (xf86-input-libinput), but xf86-input-evdev and related drivers are available as alternative.[2]

Udev, which is provided as a systemd dependency, will detect hardware and both drivers will act as hotplugging input driver for almost all devices, as defined in the default configuration files 10-quirks.conf and 40-libinput.conf in the /usr/share/X11/xorg.conf.d/ directory.

After starting X server, the log file will show which driver hotplugged for the individual devices (note the most recent log file name may vary):

If both do not support a particular device, install the needed driver from the xorg-drivers group. The same applies, if you want to use another driver.

To influence hotplugging, see #Configuration.

For specific instructions, see also the libinput article, the following pages below, or Fedora:Input device configuration for more examples.

See Keyboard input#Identifying keycodes in Xorg.

See Mouse acceleration.

See libinput or Synaptics.

See Keyboard configuration in Xorg.

For a headless configuration, the xf86-video-dummy driver is necessary; install it and create a configuration file, such as the following:

See main article Multihead for general information.

You must define the correct driver to use and put the bus ID of your graphic cards (in decimal notation).

To get your bus IDs (in hexadecimal):

The bus IDs here are 0:2:0 and 1:0:0.

By default, Xorg always sets DPI to 96 since 2009-01-30. A change was made with version 21.1 to provide proper DPI auto-detection, but reverted.

The DPI of the X server can be set with the -dpi command line option.

Having the correct DPI is helpful where fine detail is required (like font rendering). Previously, manufacturers tried to create a standard for 96 DPI (a 10.3" diagonal monitor would be 800x600, a 13.2" monitor 1024x768). These days, screen DPIs vary and may not be equal horizontally and vertically. For example, a 19" widescreen LCD at 1440x900 may have a DPI of 89x87.

To see if your display size and DPI are correct:

Check that the dimensions match your display size.

If you have specifications on the physical size of the screen, they can be entered in the Xorg configuration file so that the proper DPI is calculated (adjust identifier to your xrandr output):

If you only want to enter the specification of your monitor without creating a full xorg.conf, create a new configuration file. For example (/etc/X11/xorg.conf.d/90-monitor.conf):

If you do not have specifications for physical screen width and height (most specifications these days only list by diagonal size), you can use the monitor's native resolution (or aspect ratio) and diagonal length to calculate the horizontal and vertical physical dimensions. Using the Pythagorean theorem on a 13.3" diagonal length screen with a 1280x800 native resolution (or 16:10 aspect ratio):

This will give the pixel diagonal length, and with this value you can discover the physical horizontal and vertical lengths (and convert them to millimeters):

For RandR compliant drivers (for example the open source ATI driver), you can set it by:

To make it permanent, see Autostarting#On Xorg startup.

You can manually set the DPI by adding the option under the Device or Screen section:

GTK very often overrides the server's DPI via the optional X resource Xft.dpi. To find out whether this is happening to you, check with:

With GTK library versions since 3.16, when this variable is not otherwise explicitly set, GTK sets it to 96. To have GTK apps obey the server DPI you may need to explicitly set Xft.dpi to the same value as the server. The Xft.dpi resource is the method by which some desktop environments optionally force DPI to a particular value in personal settings. Among these are KDE and TDE.

DPMS is a technology that allows power saving behaviour of monitors when the computer is not in use. This will allow you to have your monitors automatically go into standby after a predefined period of time.

The Composite extension for X causes an entire sub-tree of the window hierarchy to be rendered to an off-screen buffer. Applications can then take the contents of that buffer and do whatever they like. The off-screen buffer can be automatically merged into the parent window, or merged by external programs called compositing managers. For more information, see Wikipedia:Compositing window manager.

Some window managers (e.g. Compiz, Enlightenment, KWin, marco, metacity, muffin, mutter, Xfwm) do compositing on their own. For other window managers, a standalone composite manager can be used.

This section lists utilities for automating keyboard / mouse input and window operations (like moving, resizing or raising).

See also Clipboard#Tools and an overview of X automation tools.

This article or section is out of date.

To run a nested session of another desktop environment:

This will launch a Window Maker session in a 1024 by 768 window within your current X session.

This needs the package xorg-server-xnest to be installed.

A more modern way of doing a nested X session is with Xephyr.

See xinit#Starting applications without a window manager.

See main article: OpenSSH#X11 forwarding.

With the help of xinput you can temporarily disable or enable input sources. This might be useful, for example, on systems that have more than one mouse, such as the ThinkPads and you would rather use just one to avoid unwanted mouse clicks.

Install the xorg-xinput package.

Find the name or ID of the device you want to disable:

For example in a Lenovo ThinkPad T500, the output looks like this:

Disable the device with xinput --disable device, where device is the device ID or name of the device you want to disable. In this example we will disable the Synaptics Touchpad, with the ID 10:

To re-enable the device, just issue the opposite command:

Alternatively using the device name, the command to disable the touchpad would be:

You can disable a particular input source using a configuration snippet:

device is an arbitrary name, and driver_name is the name of the input driver, e.g. libinput. device_name is what is actually used to match the proper device. For alternate methods of targeting the correct device, such as libinput's MatchIsTouchscreen, consult your input driver's documentation. Though this example uses libinput, this is a driver-agnostic method which simply prevents the device from being propagated to the driver.

Run script on hotkey:

Dependencies: xorg-xprop, xdotool

See also #Killing an application visually.

To block tty access when in an X add the following to xorg.conf:

This can be used to help restrict command line access on a system accessible to non-trusted users.

To prevent a user from killing X when it is running add the following to xorg.conf:

When an application is misbehaving or stuck, instead of using kill or killall from a terminal and having to find the process ID or name, xorg-xkill allows to click on said application to close its connection to the X server. Many existing applications do indeed abort when their connection to the X server is closed, but some can choose to continue.

Xorg may run with standard user privileges instead of root (so-called "rootless" Xorg). This is a significant security improvement over running as root. Note that some popular display managers do not support rootless Xorg (e.g. LightDM or XDM).

You can verify which user Xorg is running as with ps -o user= -C Xorg.

See also Xorg.wrap(1), systemd-logind(8), Systemd/User#Xorg as a systemd user service, Fedora:Changes/XorgWithoutRootRights and FS#41257.

To configure rootless Xorg using xinitrc:

Note that executing startx directly without exec leaves the shell open in the case of a xorg crash. Since some lock screens are executed inside xorg, this can lead to full access to the executing user.

GDM will run Xorg without root privileges by default when kernel mode setting is used.

When Xorg is run in rootless mode, Xorg logs are saved to ~/.local/share/xorg/Xorg.log. However, the stdout and stderr output from the Xorg session is not redirected to this log. To re-enable redirection, start Xorg with the -keeptty flag and redirect the stdout and stderr output to a file:

Alternatively, copy /etc/X11/xinit/xserverrc to ~/.xserverrc, and append -keeptty. See [5].

As explained above, there are circumstances in which rootless Xorg is defaulted to. If this is the case for your configuration, and you have a need to run Xorg as root, you can configure Xorg.wrap(1) to require root:

Wayback is an X11 compatibility layer which allows for running full X11 desktop environments (and window managers) using Wayland components. It's available from the AUR as wayback-x11AUR package.

If a problem occurs, view the log stored in either /var/log/ or, for the rootless X default since v1.16, in ~/.local/share/xorg/. GDM users should check the systemd journal. [6]

The logfiles are of the form Xorg.n.log with n being the display number. For a single user machine with default configuration the applicable log is frequently Xorg.0.log, but otherwise it may vary. To make sure to pick the right file it may help to look at the timestamp of the X server session start and from which console it was started. For example:

X creates configuration and temporary files in current user's home directory. Make sure there is free disk space available on the partition your home directory resides in. Unfortunately, X server does not provide any more obvious information about lack of disk space in this case.

If you use a Matrox card and DRI stopped working after upgrading to Xorg, try adding the line:

to the Device section that references the video card in xorg.conf.

X fails to start with the following log messages:

To correct, uninstall the xf86-video-fbdev package.

Error message: unable to load font `(null)'.

Some programs only work with bitmap fonts. Two major packages with bitmap fonts are available, xorg-fonts-75dpi and xorg-fonts-100dpi. You do not need both; one should be enough. To find out which one would be better in your case, try xdpyinfo from xorg-xdpyinfo, like this:

and use what is closer to the shown value.

If Xorg is set to boot up automatically and for some reason you need to prevent it from starting up before the login/display manager appears (if the system is wrongly configured and Xorg does not recognize your mouse or keyboard input, for instance), you can accomplish this task with two methods.

Depending on setup, you will need to do one or more of these steps:

If you are getting Client is not authorized to connect to server, try adding the line:

to /etc/pam.d/su and /etc/pam.d/su-l. pam_xauth will then properly set environment variables and handle xauth keys.

If the filesystem (specifically /tmp) is full, startx will fail. The log file will contain:

Make some free space on the relevant filesystem and X will start.

Your color depth is set wrong. It may need to be 24 instead of 16, for example.

If X terminates with error message SocketCreateListener() failed, you may need to delete socket files in /tmp/.X11-unix. This may happen if you have previously run Xorg as root (e.g. to generate an xorg.conf).

That error means that only the current user has access to the X server. The solution is to give access to root:

That line can also be used to give access to X to a different user than root.

**Examples:**

Example 1 (unknown):
```unknown
$ lspci -v -nn -d ::03xx
```

Example 2 (unknown):
```unknown
$ pacman -Ss xf86-video
```

Example 3 (unknown):
```unknown
/usr/share/X11/xorg.conf.d/
```

Example 4 (unknown):
```unknown
/etc/X11/xorg.conf.d/
```

---

## NVIDIA/Tips and tricks

**URL:** https://wiki.archlinux.org/title/NVIDIA/Tips_and_tricks

**Contents:**
- Fixing terminal resolution
- Using TV-out
- X with a TV (DFP) as the only display
- Headless (no monitor) resolution
- Check the power source
- Listening to ACPI events
- Displaying GPU temperature in the shell
  - nvidia-settings
  - nvidia-smi
  - nvclock

Since NVIDIA#fbdev is enabled by default, the Linux console should use the native monitor resolution without additional configuration.

If you have disabled fbdev or use an older version of the driver, the resolution may be lower than expected. As a workaround, you can set the resolution in your boot loader configuration.

For GRUB, see GRUB/Tips and tricks#Setting the framebuffer resolution for details. [1] [2]

For systemd-boot, set console-mode in esp/loader/loader.conf. See systemd-boot#Loader configuration for details.

For rEFInd, set use_graphics_for +,linux in esp/EFI/refind/refind.conf.[3] A small caveat is that this will hide the kernel parameters from being shown during boot.

See Wikibooks:NVIDIA/TV-OUT.

The X server falls back to some "default" screen resolution (usually 640x480) if no monitor is automatically detected. This can be a problem when using a DVI/HDMI/DisplayPort connected TV as the main display, and X is started while the TV is turned off or otherwise disconnected.

To force NVIDIA to use the correct resolution, store a copy of the EDID somewhere in the file system so that X can parse the file instead of reading EDID from the display.

To acquire the EDID, start nvidia-settings. It will show some information in tree format, ignore the rest of the settings for now and select the GPU (the corresponding entry should be titled GPU-0 or similar), click the DFP section (again, DFP-0 or similar), click on the Acquire EDID... button and store it somewhere, for example, /etc/X11/dfp0.edid.

If in the front-end mouse and keyboard are not attached, the EDID can be acquired using only the command line. Run an X server with enough verbosity to print out the EDID block:

After the X server has finished initializing, close it and extract the EDID block from the Xorg log file using nvidia-xconfig:

Edit the Xorg configuration by adding to the Device section:

The ConnectedMonitor option forces the driver to recognize the DFP as if it were connected. The CustomEDID provides EDID data for the device, meaning that it will start up just as if the TV/DFP was connected during the X process.

This way, one can automatically start a display manager at boot time and still have a working and properly configured X screen by the time the TV gets powered on.

In headless mode, resolution falls back to 640x480, which is used by VNC or Steam Link. To start in a higher resolution e.g. 1920x1080, specify a Virtual entry under the Screen subsection in xorg.conf:

The NVIDIA X.org driver can also be used to detect the GPU's current source of power. To see the current power source, check the 'GPUPowerSource' read-only parameter (0 - AC, 1 - battery):

NVIDIA drivers automatically try to connect to the acpid daemon and listen to ACPI events such as battery power, docking, some hotkeys, etc. If connection fails, X.org will output the following warning:

While completely harmless, you may get rid of this message by disabling the ConnectToAcpid option in your /etc/X11/xorg.conf.d/20-nvidia.conf:

If you are on laptop, it might be a good idea to install and enable the acpid daemon instead.

There are three methods to query the GPU temperature. nvidia-settings requires that you are using X, nvidia-smi or nvclock do not. Also note that nvclock currently does not work with newer NVIDIA cards such as GeForce 200 series cards as well as embedded GPUs such as the Zotac IONITX's 8800GS.

To display the GPU temp in the shell, use nvidia-settings as follows:

The GPU temps of this board is 49 °C.

In order to get just the temperature for use in utilities such as rrdtool or conky:

Use nvidia-smi which can read temps directly from the GPU without the need to use X at all, e.g. when running Wayland or on a headless server.

To display the GPU temperature in the shell, use nvidia-smi:

Only for temperature:

In order to get just the temperature for use in utilities such as rrdtool or conky:

Install the nvclockAUR package.

There can be significant differences between the temperatures reported by nvclock and nvidia-settings/nv-control. According to this post by the author (thunderbird) of nvclock, the nvclock values should be more accurate.

Depending on the driver version, some overclocking features are enabled by default. Some unsupported overclocking features need to be enabled via the Coolbits option in the Device section:

The Coolbits value is the sum of its component bits in the binary numeral system. The component bits are:

If you use an unsupported version of the driver, you may also need to use these bits:

To enable multiple features, add the Coolbits values together. For example, to enable overclocking and overvoltage of Fermi cores, set Option "Coolbits" "24".

The documentation of Coolbits can be found in /usr/share/doc/nvidia/html/xconfigoptions.html and here.

Use kernel module parameters to enable PowerMizer at its maximum performance level (VSync will not work without this):

With Volta (NV140/GVXXX) GPUs and later, clock boost works in a different way, and maximum clocks are set to the highest supported limit at boot. If that is what you want, then no further configuration is necessary.

The drawback is the lower power efficiency. As the clocks go up, increased voltage is needed for stability, resulting in a nonlinear increase in power consumption, heating, and fan noise. Lowering the boost clock limit will thus increase efficiency.

Boost clock limits can be changed using nvidia-smi, running as root:

To optimize for efficiency, use nvidia-smi to check the GPU utilization while running your favorite game. VSync should be on. Lowering the boost clock limit will increase GPU utilization, because a slower GPU will use more time to render each frame. Best efficiency is achieved with the lowest clocks that do not cause the stutter that results when the utilization hits 100%. Then, each frame can be rendered just quickly enough to keep up with the refresh rate.

As an example, using the above settings instead of default on an RTX 3090 Ti, while playing Hitman 3 at 4K@60, reduces power consumption by 30%, temperature from 75 to 63 degrees, and fan speed from 73% to 57%.

Typically, clock and voltage offsets inserted in the nvidia-settings interface are not saved, being lost after a reboot. Fortunately, there are tools that offer an interface for overclocking under the proprietary driver, able to save the user's overclocking preferences and automatically applying them on boot. Some of them are:

Otherwise, GPUGraphicsClockOffset and GPUMemoryTransferRateOffset attributes can be set in the command-line interface of nvidia-settings on startup. For example:

Where performance_level is the number of the highest performance level. If there are multiple GPUs on the machine, the GPU ID should be specified: [gpu:gpu_id]GPUGraphicsClockOffset[performance_level]=offset.

Modern NVIDIA graphics cards throttle frequency to stay in their TDP and temperature limits. To increase performance it is possible to change the TDP limit, which will result in higher temperatures and higher power consumption.

For example, to set the power limit to 160.30W:

To set the power limit on boot (without driver persistence):

Now enable the nvidia-tdp.timer.

The factual accuracy of this article or section is disputed.

You can adjust the fan speed on your graphics card with nvidia-settings console interface. First ensure that your Xorg configuration has enabled the bit 2 in the Coolbits option.

Place the following line in your xinitrc file to adjust the fan when you launch Xorg. Replace n with the fan speed percentage you want to set.

You can also configure a second GPU by incrementing the GPU and fan number.

If you use a login manager such as GDM or SDDM, you can create a desktop entry file to process this setting. Create ~/.config/autostart/nvidia-fan-speed.desktop and place this text inside it. Again, change n to the speed percentage you want.

To make it possible to adjust the fanspeed of more than one graphics card, run:

The Nvidia Management Library (NVML) provides an API that can manage the GPU's core and memory clock offsets and power limit. To utilise this, you can install python-nvidia-ml-pyAUR and then use the following Python script with your desired settings. This script needs to be run as root after every restart to re-apply the overclock / undervolt.

Some options can be set as kernel module parameters, a full list can be obtained by running modinfo nvidia or looking at nv-reg.h. See Gentoo:NVidia/nvidia-drivers#Kernel module parameters as well.

For example, enabling the following will enable the PAT feature [7], which affects how memory is allocated. PAT was first introduced in Pentium III [8] and is supported by most newer CPUs (see wikipedia:Page attribute table#Processors). If your system can support this feature, it should improve performance.

On some notebooks, to enable any NVIDIA settings tweaking you must include this option, otherwise it responds with "Setting applications clocks is not supported" etc.

By default the NVIDIA Linux drivers save and restore only essential video memory allocations on system suspend and resume. Quoting NVIDIA:

The "still experimental" interface enables saving all video memory (given enough space on disk or RAM).

To save and restore all video memory contents, NVreg_PreserveVideoMemoryAllocations=1 kernel module parameter for the nvidia kernel module needs to be set. While NVIDIA does not set this by default, Arch Linux does so for the supported drivers, making preserve work out of the box.

To verify that NVreg_PreserveVideoMemoryAllocations is enabled, execute the following:

Which should have a line PreserveVideoMemoryAllocations: 1, and also TemporaryFilePath: "/var/tmp", which you can read about below.

Necessary services nvidia-suspend.service, nvidia-hibernate.service, and nvidia-resume.service are enabled by default on supported drivers, as per upstream requirements.

See NVIDIA's documentation for more details.

The factual accuracy of this article or section is disputed.

Dynamic Boost is a system-wide power controller which manages GPU and CPU power, according to the workload on the system. [9]. It can particularly improve performance in GPU-bound applications by raising the power limit accordingly.

The main requirement is laptops with Ampere (or newer) GPUs.

See CPU frequency scaling#nvidia-powerd for detailed instructions.

NVIDIA has a daemon that can be optionally run at boot. In a standard single-GPU X desktop environment the persistence daemon is not needed and can actually create issues [10]. See the Driver Persistence section of the NVIDIA documentation for more details.

To start the persistence daemon at boot, enable the nvidia-persistenced.service. For manual usage see the upstream documentation.

If you are facing limitations of older output standards that can still be mitigated by using YUV 4:2:0, the NVIDIA driver has an undocumented X11 option to enforce that:

This will allow higher resolutions or refresh rates but have detrimental impact on the image quality.

See PRIME#Configure applications to render using GPU.

**Examples:**

Example 1 (unknown):
```unknown
console-mode
```

Example 2 (unknown):
```unknown
esp/loader/loader.conf
```

Example 3 (unknown):
```unknown
use_graphics_for +,linux
```

Example 4 (unknown):
```unknown
esp/EFI/refind/refind.conf
```

---

## systemd/User

**URL:** https://wiki.archlinux.org/title/Systemctl_--user

**Contents:**
- How it works
- Basic setup
  - Environment variables
    - systemd user instance
    - Service example
    - Re-using the shell login environment
    - DISPLAY and XAUTHORITY
    - PATH
    - pam_env
  - Automatic start-up of systemd user instances

systemd offers the ability to manage services under the user's control with a per-user systemd instance, enabling them to start, stop, enable, and disable their own user units. This is convenient for daemons and other services that are commonly run for a single user, such as mpd, or to perform automated tasks like fetching mail.

As per default configuration in /etc/pam.d/system-login, the pam_systemd module automatically launches a systemd --user instance when the user logs in for the first time. This process will survive as long as there is some session for that user, and will be killed as soon as the last session for the user is closed. When #Automatic start-up of systemd user instances is enabled, the instance is started on boot and will not be killed. The systemd user instance is responsible for managing user services, which can be used to run daemons or automated tasks with all the benefits of systemd, such as socket activation, timers, dependency system, and strict process control via cgroups.

Similar to system units, user units are located in the following directories (ordered by ascending precedence):

When a systemd user instance starts, it brings up the per user target default.target. Other units can be controlled manually with systemctl --user. See systemd.special(7) § UNITS MANAGED BY THE USER SERVICE MANAGER.

All the user units will be placed in ~/.config/systemd/user/. If you want to start units on first login, execute systemctl --user enable unit for any unit you want to be autostarted.

Units started by user instance of systemd do not inherit any of the environment variables set in places like .bashrc etc. There are several ways to set environment variables for them:

One variable you may want to set is PATH.

After configuration, the command systemctl --user show-environment can be used to verify that the values are correct. You may need to run systemctl --user daemon-reload for changes to take effect immediately.

The above only addresses default environment variables for user units. However, the systemd user instance itself is also affected by some environment variables. In particular, certain specifiers (see systemd.unit(5) § SPECIFIERS) are affected by XDG variables.

However, the systemd user instance will only use environment variables that are set when it is started. In particular, it will not try parsing files, see upstream bug #29414 (closed WONTFIX). Therefore, if such environment variables are needed, they should be set in a drop-in configuration file, see #Service example.

systemd does not provide introspection tools to check these values, however, something like the following service can be used to help checking that the specifiers expand as expected:

Create the drop-in directory /etc/systemd/system/user@.service.d/ and inside create a file that has the extension .conf (e.g. local.conf):

If you normally set your environment through the shell login mechanisms (i.e. in ~/.profile, ~/.bash_profile, ~/.zprofile, or similar), the shell login environment can be read into a systemd user instance using the systemd.environment-generator(7) logic (as above). Create the following script:

The script invokes your $SHELL as a login shell, and dumps the resulting environment, while removing ephemeral shell variables. This is executed only once, on manager start, and can be reloaded on demand, using systemctl --user daemon-reload.

It provides the same environment block one gets with a non-interactive login shell — the same environment one would see after loging in through Getty or SSH, but not including anything set in ~/.bashrc, ~/.zshrc, and friends — including the system-wide environment from /etc/profile and /etc/profile.d. This is similar to what e.g. gnome-shell does, which is starting a login shell, and updating systemd with the resulting environment.

DISPLAY is used by any X application to know which display to use and XAUTHORITY to provide a path to the user's .Xauthority file and thus the cookie needed to access the X server. If you plan on launching X applications from systemd units, these variables need to be set. systemd provides a script in /etc/X11/xinit/xinitrc.d/50-systemd-user.sh to import those variables into the systemd user session on X launch. [3] So unless you start X in a nonstandard way, user services should be aware of the DISPLAY and XAUTHORITY.

If you customize your PATH and plan on launching applications that make use of it from systemd units, you should make sure the modified PATH is set on the systemd environment. Assuming you set your PATH in .bash_profile, the best way to make systemd aware of your modified PATH is by adding the following to .bash_profile after the PATH variable is set:

Environment variables can be made available through use of the pam_env.so module. See Environment variables#Using pam_env for configuration details.

The systemd user instance is started after the first login of a user and killed after the last session of the user is closed. Sometimes it may be useful to start it right after boot, and keep the systemd user instance running after the last session closes, for instance to have some user process running without any open session. Lingering is used to that effect. Use the following command to enable lingering for your own user, if polkit is installed:

Without polkit or to enable lingering for a different user:

To list all users which have the permit for lingering see column "LINGER" with yes:

or inspect /var/lib/systemd/linger. To revoke lingering:

See systemd#Writing unit files for general information about writing systemd unit files.

The following is an example of a user version of the mpd service:

The factual accuracy of this article or section is disputed.

The following is a user service used by foldingathomeAUR, which takes into account variable home directories where Folding@home can find certain files:

As detailed in systemd.unit(5) § SPECIFIERS, the %h variable is replaced by the home directory of the user running the service. There are other variables that can be taken into account in the systemd manpages.

The journal for the user can be read using the analogous command:

To specify a unit, one can use

systemd-tmpfiles allows users to manage custom volatile and temporary files and directories just like in the system-wide way (see systemd#systemd-tmpfiles - temporary files). User-specific configuration files are read from ~/.config/user-tmpfiles.d/ and ~/.local/share/user-tmpfiles.d/, in that order. For this functionality to be used, it is needed to enable the necessary systemd user units for your user:

The syntax of the configuration files is the same than those used system-wide. See the systemd-tmpfiles(8) and tmpfiles.d(5) man pages for details.

This article or section needs expansion.

There are several ways to run xorg within systemd units. Below there are 3 options, either by starting a new user session with an xorg process, launching xorg from a systemd user service, or launching xinit and application as a service.

Alternatively, xorg can be run from within a systemd user service. This is nice since other X-related units can be made to depend on xorg, etc, but on the other hand, it has some drawbacks explained below.

xorg-server provides integration with systemd in two ways:

Unfortunately, to be able to run xorg in unprivileged mode, it needs to run inside a session. So, right now the handicap of running xorg as user service is that it must be run with root privileges (like before 1.16), and cannot take advantage of the unprivileged mode introduced in 1.16.

This is how to launch xorg from a user service:

1. Make xorg run with root privileges for any user, by editing /etc/X11/Xwrapper.config. This builds on Xorg#Xorg as Root by adding the stipulation that this need not be done from a physical console. That is, allowed_user's default of console is being overwritten with anybody; see Xorg.wrap(1).

2. Add the following units to ~/.config/systemd/user

where ${XDG_VTNR} is the virtual terminal where xorg will be launched, either hard-coded in the service unit, or set in the systemd environment with

3. Make sure to configure the DISPLAY environment variable as explained above.

4. Then, to enable socket activation for xorg on display 0 and tty 2 one would do:

Now running any X application will launch xorg on virtual terminal 2 automatically.

The environment variable XDG_VTNR can be set in the systemd environment from .bash_profile, and then one could start any X application, including a window manager, as a systemd unit that depends on xorg@0.socket.

The factual accuracy of this article or section is disputed.

The service below is an example to run xinit and mate-session with user privilege.

To run a window manager as a systemd service, you first need to run #Xorg as a systemd user service. In the following we will use awesome as an example:

Rather than logging you into a window manager session for your user session by default, you may want to automatically run a terminal multiplexer (such as screen or tmux) in the background.

Create the following:

Separating login from X login is most likely only useful for those who boot to a TTY instead of to a display manager (in which case you can simply bundle everything you start in mystuff.target).

The dependency cruft.target, like the mystuff.target above, allows starting anything which should run before the multiplexer starts (or which you want started at boot regardless of timing), such as a GnuPG daemon session.

You then need to create a service for your multiplexer session. Here is a sample service, using tmux as an example and sourcing a gpg-agent session which wrote its information to /tmp/gpg-agent-info. This sample session, when you start X, will also be able to run X programs, since $DISPLAY is set:

Enable tmux.service, multiplexer.target and any services you created to be run by cruft.target, start user@.service as usual and you should be done.

Arch Linux builds the systemd package with --without-kill-user-processes, setting KillUserProcesses to no by default. This setting causes user processes not to be killed when the user logs out. To change this behavior in order to have all user processes killed on the user's logout, set KillUserProcesses=yes in /etc/systemd/logind.conf.

Note that changing this setting breaks terminal multiplexers such as tmux and GNU Screen. If you change this setting, you can still use a terminal multiplexer by using systemd-run as follows:

For example, to run screen you would do:

Using systemd-run will keep the process running after logout only while the user is logged in at least once somewhere else in the system and user@.service is still running.

After the user logs out of all sessions, user@.service will be terminated too, by default, unless the user has "lingering" enabled [9]. To effectively allow users to run long-term tasks even if they are completely logged out, lingering must be enabled for them. See #Automatic start-up of systemd user instances and loginctl(1) for details.

If you see errors such as this and your login session is broken, it is possible that another system (non-user) service on your system is creating this directory. This can happen for example if you use a docker container that has a bind mount to /run/user/1000. To fix this, you can either fix the container by removing the mount, or disable/delay the docker service.

If you see this message during shutdown, usually with a 2 minute timeout, it means that one of the user services did not stop in a timely manner. This can be caused by a misbehaving application which spawned a transient service earlier. You can simply wait for the timeout to expire, but if this bothers you, you can either create an override for the misbehaving service or reduce the global timeout for all user services.

To troubleshoot this problem, start the systemd debug shell:

Then, reboot or shut down the system. When the problem occurs, switch to the debug shell using Ctrl+Alt+F9. To find out which service is preventing the shutdown, run:

For most open source applications, this problem should be reported to the respective maintainers such that an override isn't necessary. For closed source applications, however, an override can be created like so:

This will shorten the timeout of that particular service to 1 second. The --force parameter is only required for transient services which do not create a .service file on disk. The override will work regardless. Instead of the timeout, KillSignal=SIGKILL can be used. This will cause the service to be killed immediately when the user manager is stopped. Only use this if you know the service can handle it.

If you don't care which service is preventing the shutdown, you can change the global timeout for all user services in a similar manner:

After this timeout, any user services which haven't gracefully stopped will be killed, which is equivalent to a sudden power loss. Adjust this value for your particular use case. Setting the timeout too low may cause data corruption depending on the application.

**Examples:**

Example 1 (unknown):
```unknown
/etc/pam.d/system-login
```

Example 2 (unknown):
```unknown
pam_systemd
```

Example 3 (unknown):
```unknown
systemd --user
```

Example 4 (unknown):
```unknown
/usr/lib/systemd/user/
```

---

## systemd-cryptenroll

**URL:** https://wiki.archlinux.org/title/Systemd-cryptenroll

**Contents:**
- Installation
- List keyslots
- Erasing keyslots
- Enrolling passphrases
  - Regular password
  - Recovery key
- Enrolling hardware devices
  - PKCS#11 tokens or smartcards
  - FIDO2 tokens
  - Trusted Platform Module

From systemd-cryptenroll(1):

systemd-cryptenroll allows enrolling smartcards, FIDO2 tokens and Trusted Platform Module security chips into LUKS devices, as well as regular passphrases. These devices are later unlocked by systemd-cryptsetup@.service(8), using the enrolled tokens.

systemd-cryptenroll is part of and packaged with systemd. However, extra packages are required to use hardware devices as keys:

systemd-cryptenroll can list the keyslots in a LUKS device, similar to cryptsetup luksDump, but in a more user-friendly format.

The --wipe-slot operation can be used in combination with all enrollment options, which is useful to update existing device enrollments:

This is equivalent to cryptsetup luksAddKey.

From systemd-cryptenroll(1):

A recovery key is designed to be used as a fallback if the hardware tokens are unavailable, and can be used in place of regular passphrases whenever they are required.

The --type-device options must point to a valid device path of their respective type. A list of available devices can be obtained by passing the list argument to this option. Alternatively, if you only have a single device of the desired type connected, the auto option can be used to automatically select it.

The token or smartcard must contain a RSA key pair, which will be used to encrypt the generated key that will be used to unlock the volume.

Any FIDO2 token that supports the "hmac-secret" extension can be used with systemd-cryptenroll. The following example would enroll a FIDO2 token to an encrypted LUKS2 block device, requiring only user presence as authentication.

In addition, systemd-cryptenroll supports using the token's built-in user verification methods:

By default, the cryptographic algorithm used when generating a FIDO2 credential is es256 which denotes Elliptic Curve Digital Signature Algorithm (ECDSA) over NIST P-256 with SHA-256. If desired and provided by the FIDO2 token, a different cryptographic algorithm can be specified during enrollment.

Suppose that a previous FIDO2 token has already been enrolled and the user wishes to enroll another, the following generates an eddsa credential which denotes EdDSA over Curve25519 with SHA-512 and authenticates the device with a previous enrolled token instead of a password.

This article or section needs expansion.

systemd-cryptenroll has native support for enrolling LUKS keys in TPMs. It requires the following:

To begin, run the following command to list your installed TPMs and the driver in use:

A key may be enrolled in both the TPM and the LUKS volume using only one command. The following example generates a new random key, adds it to the volume so it can be used to unlock it in addition to the existing keys, and binds this new key to PCR 7 (Secure Boot state):

where /dev/sdX is the full path to the encrypted LUKS volume. Use --unlock-key-file=/path/to/keyfile if the LUKS volume is unlocked by a keyfile instead of a passphrase.

Refer to systemd-cryptenroll(1) and Trusted Platform Module#Accessing PCR registers for common PCR measurements in Linux. Adjust --tpm2-pcrs=7 as necessary (parameters are separated by the + symbol).

The combination of PCRs to bind to depends on the individual case to balance usability and lock-down. For example, you may require UEFI firmware updates without manual intervention to the Secure Boot state, or different boot devices. As another example, Microsoft's Bitlocker prefers PCR 7+11, but may also use other PCR combinations.

To check that the new key was enrolled, dump the LUKS configuration and look for a systemd-tpm2 token entry, as well as an additional entry in the Keyslots section:

To test that the key works, run the following command while the LUKS volume is closed:

where mapping_name is your chosen name for the volume once opened.

See dm-crypt/System configuration#crypttab and dm-crypt/System configuration#Trusted Platform Module and FIDO2 keys in order to unlock the volume at boot time.

See systemd-cryptenroll(1) and crypttab(5) for more information and examples.

**Examples:**

Example 1 (unknown):
```unknown
# systemd-cryptenroll /dev/disk
```

Example 2 (unknown):
```unknown
SLOT TYPE    
   0 password
   1 recovery
   2 tpm2
```

Example 3 (unknown):
```unknown
# systemd-cryptenroll /dev/disk --wipe-slot=SLOT
```

Example 4 (unknown):
```unknown
--wipe-slot
```

---

## System backup

**URL:** https://wiki.archlinux.org/title/System_backup

**Contents:**
- Using Btrfs snapshots
- Using LVM snapshots
- Using rsync
- Using tar
- Using SquashFS
- Bootable backup
  - Update the fstab
  - Update the boot loader's configuration file
  - First boot
- Snapshots and /boot partition

A system backup is the process of backing up the operating system, files and system-specific useful/essential data. [It] primarily ensures that not only the user data in a system is saved, but also the system's state or operational condition. This helps in restoring the system to the last-saved state along with all the selected backup data.[1]

One common and generally effective method is to follow the 3-2-1 strategy:

See Btrfs#Snapshots, #Snapshots and /boot partition, and Snapper.

See LVM#Snapshots, Create root filesystem snapshots with LVM, and #Snapshots and /boot partition.

See rsync#As a backup utility.

See Full system backup with tar.

See Full system backup with SquashFS.

Having a bootable backup can be useful in case the filesystem becomes corrupt or if an update breaks the system. The backup can also be used as a test bed for updates, with the testing repo enabled, etc. If you transferred the system to a different partition or drive and you want to boot it, the process is as simple as updating the backup's /etc/fstab and your boot loader's configuration file.

This section assumes that you backed up the system to another drive or partition, that your current boot loader is working fine, and that you want to boot from the backup as well.

Without rebooting, edit the backup's fstab by commenting out or removing any existing entries. Add one entry for the partition containing the backup like the example here:

Remember to use the proper device name and filesystem type.

For Syslinux, all you need to do is duplicate the current entry, except pointing to a different drive or partition.

For GRUB, it is recommended that you automatically re-generate the main configuration file. If you want to freshly install all GRUB files to somewhere other than /boot, such as /mnt/newroot/boot, use the --boot-directory flag.

Also verify the new menu entry in /boot/grub/grub.cfg. Make sure the UUID is matching the new partition, otherwise it could still boot the old system. Find the UUID of a partition with lsblk:

where /dev/sdXY is the desired partition (e.g. /dev/sdb3). To list the UUIDs of partitions GRUB thinks it can boot, use grep:

Reboot the computer and select the right entry in the boot loader. This will load the system for the first time. All peripherals should be detected and the empty folders in / will be populated.

Now you can re-edit /etc/fstab to add the previously removed partitions and mount points.

If your file system supports snapshots (e.g., LVM or Btrfs), these will most likely exclude the /boot partition or ESP.

You can copy the boot partition automatically on a kernel update to your root partition with a pacman hook (make sure the hook file is owned by root):

Backups that are only manually created are rarely up to date when they are needed. Therefore it is recommended to setup an automated process to ensure backup processes are executed regularly. The most common solutions are provided by systemd/Timers and Cron.

For a local system wide backup that requires read access to all files the following systemd timer and service may be useful as a template for automated backup processes.

To use a timer unit enable and start it like any other unit.

The following example is configured to run with minimal required permissions while preventing modifications from normal users for increased security.

Note that this example will block the shutdown process when it is initiated while the backup is running. This ensures that the backup is not interrupted, but can lead to a delay during shutdown/reboot if many new files need to be saved.

CAP_DAC_READ_SEARCH sets the capability that allows bypassing file read permission checks in the file system; thus all files in the file system will be accessible without requiring root permissions.

For remote backups allow the use of network protocols:

**Examples:**

Example 1 (unknown):
```unknown
/dev/sdaX    /             ext4      defaults                 0   1
```

Example 2 (unknown):
```unknown
syslinux.cfg
```

Example 3 (unknown):
```unknown
/mnt/newroot/boot
```

Example 4 (unknown):
```unknown
--boot-directory
```

---

## Bash

**URL:** https://wiki.archlinux.org/title/Bashrc

**Contents:**
- Invocation
  - Configuration files
  - Shell and environment variables
- Command line
  - Tab completion
    - Single-tab
    - Common programs and options
    - Customize per-command
  - History
    - History completion

Bash (Bourne-Again SHell) is a command-line shell/programming language by the GNU Project. Its name alludes to its predecessor, the long-deprecated Bourne shell. Bash can be run on most Unix-like operating systems, including GNU/Linux.

Bash is the default command-line shell on Arch Linux.

Bash behaviour can be altered depending on how it is invoked. Some descriptions of different modes follow.

If Bash is spawned by login in a TTY, by an SSH daemon, or similar means, it is considered a login shell. This mode can also be engaged using the -l/--login command line option.

Bash is considered an interactive shell when its standard input, output and error are connected to a terminal (for example, when run in a terminal emulator), and it is not started with the -c option or non-option arguments (for example, bash script). All interactive shells source /etc/bash.bashrc and ~/.bashrc, while interactive login shells also source /etc/profile and ~/.bash_profile.

Bash will attempt to execute a set of startup files depending on how it was invoked. See the Bash Startup Files section of the GNU Bash manual for a complete description.

The behavior of Bash and programs run by it can be influenced by a number of environment variables. Environment variables are used to store useful values such as command search directories, or which browser to use. When a new shell or script is launched it inherits its parent's variables, thus starting with an internal set of shell variables[1].

These shell variables in Bash can be exported in order to become environment variables:

Environment variables are conventionally placed in ~/.profile or /etc/profile so that other Bourne-compatible shells can use them.

See Environment variables for more general information.

Bash command line is managed by the separate library called Readline. Readline provides emacs and vi styles of shortcuts for interacting with the command line, i.e. moving back and forth on the word basis, deleting words etc. It is also Readline's responsibility to manage history of input commands. Last, but not least, it allows you to create macros.

Tab completion is the option to auto-complete typed commands by pressing Tab (enabled by default).

It may require up to three tab-presses to show all possible completions for a command. To reduce the needed number of tab-presses, see Readline#Faster completion.

By default, Bash only tab-completes commands, filenames, and variables. The package bash-completion extends this by adding more specialized tab completions for common commands and their options, which can be enabled by sourcing /usr/share/bash-completion/bash_completion (which has been already sourced in Arch's /etc/bash.bashrc). With bash-completion, normal completions (such as ls file.* Tab Tab) will behave differently; however, they can be re-enabled with compopt -o bashdefault program (see [2] and [3] for more detail).

By default, Bash only tab-completes file names following a command. You can change it to complete command names using complete -c:

or complete command names and file names with -cf:

See bash(1) § Programmable Completion for more completion options.

You can bind the up and down arrow keys to search through Bash's history (see: Readline#History and Readline Init File Syntax):

or to affect all readline programs:

The HISTCONTROL variable can prevent certain commands from being logged to the history.

To stop logging of consecutive identical commands:

To remove all but the last identical command:

To avoid saving commands that start with a space:

To avoid saving consecutive identical commands, and commands that start with a space:

To remove all but the last identical command, and commands that start with a space:

See bash(1) § HISTCONTROL for details.

To disable the bash history only temporarily:

The commands entered now are not logged to the $HISTFILE.

For example, now you can hash passwords with printf secret | sha256sum, or hide GPG usage like gpg -eaF secret-pubkey.asc and your secret is not written to disk.

To disable all bash history:

... and just to make sure, destroy your old histfile forever:

Zsh can invoke the manual for the command preceding the cursor by pressing Alt+h. A similar behaviour is obtained in Bash using this Readline bind:

This assumes are you using the (default) Emacs editing mode.

atuin replaces your existing shell history with an SQLite database, and records additional context for your commands. Additionally, it provides optional and fully encrypted synchronization of your history between machines, via an Atuin server.

Enable bash history timestamps (export HISTTIMEFORMAT="%F %T ") before syncing. Atuin works well with tools like blesh-gitAUR and cmd-wrapped to provide an enhanced terminal experience across machines.

alias is a command, which enables a replacement of a word with another string. It is often used for abbreviating a system command, or for adding default arguments to a regularly used command.

Personal aliases can be stored in ~/.bashrc or any separate file sourced from ~/.bashrc. System-wide aliases (which affect all users) belong in /etc/bash.bashrc. See [4] for example aliases.

For functions, see Bash/Functions.

See Bash/Prompt customization.

ble.sh (Bash Line Editor), packed as blesh-gitAUR, is a command line editor written in pure Bash, which is an alternative to GNU Readline. It has many enhanced features like syntax highlighting, autosuggestions, menu-completion, abbreviations, Vim editing mode, and hook functions. Other interesting features include status line, history share, right prompt, transient prompt, and xterm title.

After installing it, source it in an interactive session.

Configurations are explained in depth in the ~/.blerc file and at the wiki. The stable bleshAUR package is also available.

pkgfile includes a "command not found" hook that will automatically search the official repositories, when entering an unrecognized command.

You need to source the hook to enable it, for example:

Then attempting to run an unavailable command will show the following info:

You can disable the Ctrl+z feature (pauses/closes your application) by wrapping your command like this:

Now, when you accidentally press Ctrl+z in adomAUR instead of Shift+z, nothing will happen because Ctrl+z will be ignored.

To clear the screen after logging out on a virtual terminal:

Bash can automatically prepend cd when entering just a path in the shell. For example:

But after adding one line into .bashrc file:

autojump-gitAUR is a python script which allows navigating the file system by searching for strings in a database with the user's most-visited paths.

zoxide is an alternative which has additional features and performance improvements compared to the original autojump and can serve as a drop-in replacement for autojump.

For the current session, to disallow existing regular files to be overwritten by redirection of shell output:

This is identical to set -C.

To make the changes persistent for your user:

To manually overwrite a file while noclobber is set:

pushd and popd can be used to push or pop directories to a stack while switching to them. This can be useful for "replaying" your navigation history.

See bash(1) § DIRSTACK.

When resizing a terminal emulator, Bash may not receive the resize signal. This will cause typed text to not wrap correctly and overlap the prompt. The checkwinsize shell option checks the window size after each command and, if necessary, updates the values of LINES and COLUMNS.

If you have set the ignoreeof option and you find that repeatedly hitting ctrl-d causes the shell to exit, it is because this option only allows 10 consecutive invocations of this keybinding (or 10 consecutive EOF characters, to be precise), before exiting the shell.

To allow higher values, you have to use the IGNOREEOF variable.

The package shellcheck analyzes bash (and other shell) scripts, prints possible errors, and suggests better coding.

There is also the web site shellcheck.net of the same purpose, based on this program.

**Examples:**

Example 1 (unknown):
```unknown
bash script
```

Example 2 (unknown):
```unknown
/etc/bash.bashrc
```

Example 3 (unknown):
```unknown
/etc/profile
```

Example 4 (unknown):
```unknown
~/.bash_profile
```

---

## aria2

**URL:** https://wiki.archlinux.org/title/Aria2

**Contents:**
- Installation
- Configuration
  - aria2.conf
  - Example aria2.conf
    - Option details
      - Example input file #1
      - Example input file #2
    - Additional notes
  - Example aria2.rapidshare
    - Option details

From the project home page:

Install the aria2 package.

To use aria2 as a daemon, you can write a systemd user unit.

aria2 looks at $XDG_CONFIG_HOME/aria2/aria2.conf for a set of global configuration options by default. This behavior can be modified with the --conf-path switch:

The following example downloads aria2.example.rar using the options specified in the configuration file /file/aria2.rapidshare

If $XDG_CONFIG_HOME/aria2/aria2.conf exists and the options specified in /file/aria2.rapidshare are desired, the --no-conf switch must be appended to the command:

The following example does not use the default configuration file and downloads aria2.example.rar using the options specified in the configuration file /file/aria2.rapidshare

If $XDG_CONFIG_HOME/aria2/aria2.conf does not yet exist and you wish to simplify the management of configuration options:

This is essentially the same as if running the following:

Download aria2-1.10.0.tar.bz2 from two separate sources to ~/Desktop before merging as aria2-1.10.0.tar.bz2

Download aria2-1.9.5.tar.bz2 and save to /file/old as aria2.old.tar.bz2

Download aria2-1.10.0.tar.bz2 and save to ~/Desktop as aria2.new.tar.bz2

1 file-allocation=falloc: Recommended for newer file systems such as ext4 (with extents support), btrfs or XFS as it allocates large files (GB) almost instantly. Do not use falloc with legacy file systems such as ext3 as prealloc consumes approximately the same amount of time as standard allocation would while locking the aria2 process from proceeding to download.

2 summary-interval=0: Suppresses download progress summary output and may improve overall performance. Logs will continue to be output according to the value specified in the log-level option.

This configuration can be used to start aria2 as a service. It can be used in conjunction with several of the frontends listed below. Note that rpc-user and rpc-pass are deprecated, but most frontends have not been ported to the new authentication yet. Do not forget to change user, password and Download directory.

Just use the command below:

pacman -Sp lists the URLs of the packages on stdout, instead of downloading them, then | pipes it to the next command. Finally, the -i in aria2c -i - switch to aria2c means that the URLs for files to be downloaded should be read from the file specified, but if - is passed, then read the URLs from stdin.

See pacman/Tips and tricks#aria2.

Some sites may filter the requests based on your User Agent, since aria2 is not a well known downloader, it may be good to use a most known downloader or browser as the Aria's User Agent. Just use the -U option like this:

You can use whatever you want, like -UMozilla/5.0 and so on.

You can use aria2 instead of curl to download source files, just change the DLAGENTS variable as follows:

To use aria2 as a daemon, you should write a systemd user unit. For example:

#Example aria2.daemon shows an example configuration file.

To seed an already downloaded torrent, use the --check-integrity option:

Specifying --seed-ratio=0.0 will seed the file forever.

Aria2 can be used with torsocks for downloading over Tor; keep in mind that this is limited to TCP-based protocols, so torrents will not work.

By default, aria2 will try to use its own implementation of a DNS resolver, which does not work under torsocks. This problem can be fixed by passing the --async-dns=false option:

**Examples:**

Example 1 (unknown):
```unknown
$XDG_CONFIG_HOME/aria2/aria2.conf
```

Example 2 (unknown):
```unknown
--conf-path
```

Example 3 (unknown):
```unknown
aria2.example.rar
```

Example 4 (unknown):
```unknown
/file/aria2.rapidshare
```

---

## Arch boot process

**URL:** https://wiki.archlinux.org/title/Initcpio

**Contents:**
- Firmware types
  - UEFI
  - BIOS
- System initialization
  - UEFI
    - Multibooting
  - BIOS
- Boot loader
  - Feature comparison
- Kernel

In order to boot Arch Linux, a Linux-capable boot loader must be set up. The boot loader is responsible for loading the kernel and initial ramdisk before initiating the boot process. The procedure is quite different for BIOS and UEFI systems.

The firmware is the very first program that is executed once the system is switched on.

The Unified Extensible Firmware Interface has support for reading both the partition table as well as file systems. UEFI does not launch any boot code from the Master Boot Record (MBR) whether it exists or not, instead booting relies on boot entries in the NVRAM.

The UEFI specification mandates support for the FAT12, FAT16, and FAT32 file systems (see UEFI specification version 2.11, section 13.3.1.1), but any conformant vendor can optionally add support for additional file systems; for example, HFS+ or APFS in some Apple's firmwares. UEFI implementations also support ISO 9660 for optical discs.

UEFI launches EFI applications, e.g. boot loaders, boot managers, UEFI shell, etc. These applications are usually stored as files in the EFI system partition. Each vendor can store its files in the EFI system partition under the /EFI/vendor_name directory. The applications can be launched by adding a boot entry to the NVRAM or from the UEFI shell.

The UEFI specification has support for legacy BIOS booting with its Compatibility Support Module (CSM). If CSM is enabled in the UEFI, the UEFI will generate CSM boot entries for all drives. If a CSM boot entry is chosen to be booted from, the UEFI's CSM will attempt to boot from the drive's MBR bootstrap code.

A BIOS or Basic Input-Output System is in most cases stored in a flash memory in the motherboard itself and independent of the system storage. Originally created for the IBM PC to handle hardware initialization and the boot process, it has been replaced progressively since 2010 by UEFI which does not suffer from the same technical limitations.

System switched on, the power-on self-test (POST) is executed. See also Modern CPUs have a backstage cast by Hugo Landau.

If Secure Boot is enabled, the boot process will verify authenticity of the EFI binary by signature.

Since each OS or vendor can maintain its own files within the EFI system partition without affecting the other, multi-booting using UEFI is just a matter of launching a different EFI application corresponding to the particular operating system's boot loader. This removes the need for relying on the chain loading mechanisms of one boot loader to load another OS.

See also Dual boot with Windows.

A boot loader is a piece of software started by the firmware—UEFI or BIOS. It is responsible for loading the kernel with the wanted kernel parameters and any external initramfs images.

A boot manager presents a menu of boot options, or provides some other way to control the boot process—i.e. it just runs other EFI executables.

In the case of UEFI, the kernel itself can be directly launched by the UEFI using the EFI boot stub. A separate boot loader or a boot manager can still be used for the purpose of editing kernel parameters before booting.

Systems with 32-bit IA32 UEFI require a boot loader that supports mixed mode booting.

Since almost no boot loader supports such stacked block devices and since file systems can introduce new features which may not yet be supported by any boot loader (e.g. archlinux/packaging/packages/grub#7, FS#79857, FS#59047, FS#58137, FS#51879, FS#46856, FS#38750, FS#21733 and fscrypt encrypted directories), using a separate /boot partition with a universally supported file system, such as FAT32, is oftentimes more feasible.

See also Wikipedia:Comparison of boot loaders.

The boot loader boots the vmlinux image containing the kernel.

The kernel functions on a low level (kernelspace) interacting between the hardware of the machine and the programs. The kernel initially performs hardware enumeration and initialization before continuing to userspace. See Wikipedia:Kernel (operating system) and Wikipedia:Linux kernel for a detailed explanation.

An initramfs (initial RAM file system) image is a cpio archive providing the necessary files for early userspace (see below) to successfully start the late userspace. This predominantly means all kernel modules, user space tools, associated libraries, supporting files like udev rules, etc. required to locate, access and mount the root file system. With the concept of initramfs it is possible to handle even more complex setups, like e.g. booting from an external drive, stacked devices (logical volumes, software RAIDs, compression, encryption) or running a tiny SSH server in early userspace for remote unlocking or maintenance tasks of the root file system.

The majority of modules will be loaded during later stages of the init process by udev after having switched root to the root file system.

The process is as follows:

Initramfs images are Arch Linux' preferred method for setting up the early userspace and can be generated with mkinitcpio, dracut or booster.

Since 6.13.8 officially supported kernels have Btrfs and Ext4 drivers built-in [4].

This makes it possible for the kernel to use a root partition with these file systems directly and load the rest of external modules needed from there. Although, there are some quirks to keep in mind:

Another thing you really need initramfs for is early microcode loading. But it is not necessary to build full image for that, Arch provides microcode in separate initramfs files, which could be used independently.

If no initramfs image is provided, the kernel always contains still an empty image to start from [8]. So there should be no issues with root partition pinning.

The early userspace stage, a.k.a. the initramfs stage, takes place in rootfs consisting of the files provided by the #initramfs. Early userspace starts by the kernel executing the /init binary as PID 1.

The function of early userspace is configurable, but its main purpose is to bootstrap the system to the point where it can access the root file system. This includes:

Note that the early userspace serves more than just setting up the root file system. There are tasks that can only be performed before the root file system is mounted, such as fsck and resuming from hibernation.

At the final stage of early userspace, the real root is mounted at /sysroot/ (in case of a systemd-based initramfs) or at /new_root/ (in case of a busybox-based one), and then switched to by using systemctl switch-root when using systemd-based initramfs or switch_root(8) when using busybox-based initramfs. The late userspace starts by executing the init program from the real root file system.

The startup of late userspace is executed by the init process. Arch officially uses systemd which is built on the concept of units and services, but the functionality described here largely overlaps with other init systems.

The init process calls getty once for each virtual terminal (typically six of them). getty initializes each terminal and protects it from unauthorized access. When the username and password are provided, getty checks them against /etc/passwd and /etc/shadow, then calls login(1).

The login program begins a session for the user by setting environment variables and starting the user's shell, based on /etc/passwd. The login program displays the contents of /etc/motd (message of the day) after a successful login, just before it executes the login shell. It is a good place to display your Terms of Service to remind users of your local policies or anything you wish to tell them.

Once the user's shell is started, it will typically run a runtime configuration file, such as bashrc, before presenting a prompt to the user. If the account is configured to start X at login, the runtime configuration file will call startx or xinit. Jump to #Graphical session (Xorg) for the end.

This article or section needs expansion.

Additionally, init can be configured to start a display manager instead of getty on a specific virtual terminal. This requires manually enabling its systemd service file. The display manager then starts a graphical session.

xinit runs the user's xinitrc runtime configuration file, which normally starts a window manager or a desktop environment. When the user is finished and exits, xinit, startx, the shell, and login will terminate in that order, returning to getty or the display manager.

**Examples:**

Example 1 (unknown):
```unknown
/EFI/vendor_name
```

Example 2 (unknown):
```unknown
\EFI\BOOT\BOOTx64.EFI
```

Example 3 (unknown):
```unknown
BOOTIA32.EFI
```

Example 4 (unknown):
```unknown
esp/EFI/Linux/
```

---

## netctl

**URL:** https://wiki.archlinux.org/title/Netctl

**Contents:**
- Installation
- Configuration
- Usage
  - Starting a profile
  - Enabling a profile
  - Special systemd units
    - Wired
    - Wireless
- Tips and tricks
  - Example profiles

netctl is a CLI and profile-based network manager and an Arch project.

Install the netctl package.

netctl's #Special systemd units used in automating connections require some additional dependencies; see that section for more information.

Other optional dependencies are shown in the table below.

netctl uses profiles to manage network connections and different modes of operation to start profiles automatically or manually on demand.

The netctl profile files are stored in /etc/netctl/ and example configuration files are available in /etc/netctl/examples/.

To use an example profile, simply copy it from /etc/netctl/examples/ to /etc/netctl/ and configure it to your needs; see basic #Example profiles below. The first parameter you need to create a profile is the network Interface, see Network configuration#Network interfaces for details.

See netctl.profile(5) for a complete list of profile options.

See netctl(1) for a complete list of netctl commands.

Once you have created your profile, attempt to establish a connection, where profile is only the profile name, not the full path:

If the above command results in a failure, then run journalctl -xn as root and netctl status profile to obtain a more in-depth explanation of the failure.

A profile can be enabled to start at boot by using:

This will create and enable a systemd service that will start when the computer boots. Changes to the profile file will not propagate to the service file automatically. After such changes, it is necessary to reenable the profile:

After enabling a profile, it will be started at next boot. Obviously this can only be successful, if the network cable for a wired connection is plugged in, or the wireless access point used in a profile is in range respectively.

If you need to switch multiple profiles frequently (i.e., traveling with a laptop), use #Special systemd units instead of enabling a profile.

netctl provides special systemd services for automatically switching of profiles for wired and wireless connections. See netctl.special(7) for a complete list of special systemd units.

Install the ifplugd package and start/enable the netctl-ifplugd@interface.service systemd unit. DHCP profiles will be started/stopped when the network cable is plugged in/unplugged.

Start/enable netctl-auto@interface.service systemd unit. netctl profiles will be started/stopped automatically as you move from the range of one network into the range of another network (roaming).

Note that interface is not literal, but to be substituted by the name of your device's interface, e.g. netctl-auto@wlp4s0.service. See netctl.profile(5) for details.

It is possible to manually control an interface otherwise managed by netctl-auto without having to stop netctl-auto.service. This is done using the netctl-auto command. For a complete list of available actions see netctl-auto(1).

For a DHCP connection, only the Interface has to be configured after copying the /etc/netctl/examples/ethernet-dhcp example profile to /etc/netctl.

For a static IP configuration copy the /etc/netctl/examples/ethernet-static example profile to /etc/netctl and modify Interface, Address, Gateway and DNS) as needed.

Take care to include the subnet notation of /24. It equates to a netmask of 255.255.255.0) and without it the profile will fail to start. See also CIDR notation. To alias more than one IP address per a NIC set Address=('10.1.10.2/24' '192.168.1.2/24'). To alias more than one DNS server address set Eg. DNS=('1.1.1.1' '1.0.0.1').

The following applies for the standard wireless connections using a pre-shared key (WPA-PSK).

You can also follow the following step to obfuscate the wireless passphrase (wifi-menu does it automatically when using the -o flag):

Users not wishing to have the passphrase to their wireless network stored in plain text have the option of storing the corresponding 256-bit pre-shared key instead, which is calculated from the passphrase and the SSID using standard algorithms.

Calculate your 256-bit PSK using wpa_passphrase:

The pre-shared key (psk) now needs to replace the plain text passphrase of the Key variable in the profile.

If you want a graphical user interface to manage netctl and your connections and you are not afraid of highly experimental unofficial packages, there are some options available. netctl-guiAUR provides a Qt-based graphical interface, DBus daemon and KDE widget. netmenuAUR uses dmenu as its graphical interface.

There is also an application that displays desktop notifications on profile changes and shows a tray icon: netctl-trayAUR.

From kernel documentation:

Copy /etc/netctl/examples/bonding to /etc/netctl/bond0 and edit it, for example:

Now you can disable your old configuration and set bond0 to be started automatically. Switch to the new profile, for example:

Setting the MODE in the netctl configuration is not always successful and it may be necessary to pass options directly to the bonding module on load as noted here. This may be needed to use LACP / mode 4.

This example describes how to use bonding to fallback to wireless when the wired Ethernet goes down. This is most useful when both the wired and wireless interface will be connected to the same network. Your wireless router/access point must be configured in bridge mode.

You will need an additional package: wpa_supplicant.

First, load the module at boot:

Then, configure the options of the bonding driver to use active-backup and configure the primary parameter to the device you want to be the active one (normally the wired interface). Also, be sure to use the same device name as returned when running ip link:

The miimon option is needed, for the link failure detection. The max_bonds option avoids the Interface bond0 already exists error. More information can be obtained on the kernel documentation.

Next, configure a netctl profile to enslave the two hardware interfaces. Use the name of all the devices you want to enslave. If you have more than two wired or wireless interfaces, you can enslave all of them on a bond interface. But, for most cases you will have only two devices, a wired and a wireless one:

Disable any other profiles (specially a wired or wireless) you had enabled before and then enable the failover profile on startup:

Now you need to configure wpa_supplicant to connect to any known network you wish. You should create a file for each interface and enable it on systemd. Create the following file with this content:

And append to the end of this file any network you want to connect to:

To generate the obfuscated PSK you can run wpa_passphrase as on the wpa_supplicant#Connecting with wpa_passphrase page.

Now, enable the wpa_supplicant@ template service on the network interface, for example wpa_supplicant@wlan0.

You can try now to reboot your machine and see if your configuration worked.

Now, you can test your failover setup, by initiating a big download. Unplug your wired interface. Your download should keep going over the wireless interface. Then, plug your wired interface again and it should keep working. You can debug by checking journalctl for the netctl@failover.service and wpa_supplicant@wlan0.service units.

In some cases it may be desirable to allow a profile to use any interface on the system. A common example use case is using a common disk image across many machines with differing hardware (this is especially useful if they are headless). If you use the kernel's naming scheme, and your machine has only one ethernet interface, you can probably guess that eth0 is the right interface. If you use udev's Predictable Network Interface Names, however, names will be assigned based on the specific hardware itself (e.g. enp1s0), rather than simply the order that the hardware was detected (e.g. eth0, eth1). This means that a netctl profile may work on one machine and not another, because they each have different interface names.

A quick and dirty solution is to make use of the /etc/netctl/interfaces/ directory. Choose a name for your interface alias (en-any in this example), and write the following to a file with that name (making sure it is executable).

Then create a profile that uses the interface. Pay special attention to the Interface directive. The rest are only provided as examples.

When the wired profile is started, any machine using the two files above will automatically bring up and configure the first ethernet interface found on the system, regardless of what name udev assigned to it. Note that this is not the most robust way to go about configuring interfaces. If you use multiple interfaces, netctl may try to assign the same interface to them, and will likely cause a disruption in connectivity. If you do not mind a more complicated solution, netctl-auto is likely to be more reliable.

netctl supports hooks in /etc/netctl/hooks/ and per interface hooks in /etc/netctl/interfaces/. You can set any option in a hook/interface that you can in a profile. Most importantly this includes ExecUpPost and ExecDownPre.

When a profile is read, netctl sources all executable scripts in hooks, then it reads the profile file for the connection and finally it sources an executable script with the name of the interface used in the profile from the interfaces directory. Therefore, declarations in an interface script override declarations in the profile, which override declarations in hooks.

The variables $INTERFACE and $ACTION are available in hooks/interfaces only when using netctl-auto

To set or change the DHCP client used for all profiles:

Do not forget to make the file executable.

Alternatively, it may also be specified for a specific network interface by creating an executable file /etc/netctl/interfaces/<interface> with the following line:

This article or section needs expansion.

As stated in netctl.profile(5) § OPTIONS FOR ‘WIRELESS’ CONNECTIONS, the WPAConfigSection variable is an array of configuration lines passed to wpa_supplicant. So, a minimal WPAConfigSection would contain:

If you use DNS* options in your profile, netctl calls resolvconf to overwrite resolv.conf.

Some people have an issue when they connect to a network with netctl, for example:

When looking at the journal from running journalctl -xn as root, either of the following are shown:

1. If your device (wlan0 in this case) is up:

Setting the interface down should resolve the problem:

2. If the error continues, try again after adding the ForceConnect option:

Save it and try to connect with the profile:

On some systems dhcpcd in combination with netctl causes timeout issues on resume, particularly when having switched networks in the meantime. netctl will report that you are successfully connected but you still receive timeout issues. In this case, the old default route still exists and is not being renewed. A workaround to avoid this misbehaviour is to switch to dhclient as the default dhcp client. More information on the issue can be found here.

If you are having timeout issues when requesting leases via DHCP you can set the timeout value higher than netctl's 30 seconds by default. Create a file in /etc/netctl/hooks/ or /etc/netctl/interfaces/, add TimeoutDHCP=40 to it for a timeout of 40 seconds and make the file executable.

If dhcpcd fails to obtain an address, add the -d option to /usr/lib/netctl/dhcp and then run journalctl -xe as root to view the debugging messages which may indicate, for example, that the IP address offered by the server is rejected by the client after detecting that the IP address is already in use.

If you are having timeout issues that are unrelated to DHCP (on a static ethernet connection for example), and are experiencing errors similar to the following when starting your profile:

Then you should increase carrier and up timeouts by adding TimeoutUp= and TimeoutCarrier= to your profile file:

Do not forget to reenable your profile:

Sometimes netctl-auto fails to reconnect when the system resumes from suspend, hibernate or hybrid-sleep. An easy solution is to restart the service for netctl-auto. This can be automated with an additional service like the following:

To enable this service for your wireless card, for example, enable netctl-auto-resume@wlan0.service as root. Change wlan0 to the required network interface.

If the device is not yet running on resume when the unit is started, this will fail. It can be fixed by adding the following dependency in the After line:

Many laptops have a hardware button (or switch) to turn off wireless card, however, the card can also be blocked by the kernel. This can be handled by rfkill.

If you want netctl-auto to automatically unblock your wireless card to connect to a particular network, set RFKill=++auto++ option for the wireless connection of your choice, as specified in the netctl.profile(5).

This is a very misleading response, it really means that you have assigned a default gateway in an earlier netctl control file. When netctl starts up the n-th NIC and goes to set its local route, it fails because there is already a default route from n-1.

Remove it and everything works, except you no longer have a default route and so cannot access things such as the internet. ExecUpPost does not work as it gets executed for each network card.

A possible solution is creating a new service. Replace "FIRST_INTERFACE" and "SECOND_INTERFACE" with your interface names, and replace "192.168.xxx.yyy" with your default gateway.

See wpa_supplicant#Problems with Eduroam.

Profiles still using systemd's old .include directives will produce journal warnings, for example:

See FS#59494 for details.

will update the profile to the new drop-in unit file format.

If you have multiple hooks in /etc/netctl/hooks/, variables like ExecUpPost and ExecDownPre will be executed only from one file. To fix this, define the variables like this:

This will append your commands to be executed with already defined ones.

If you are using network mounts in combination with netctl-ifplugd or netctl-auto, it may be necessary to unmount them before the corresponding services are stopped. One way to achieve this is to create an drop-in file for netctl-ifplugd@.service or netctl-auto@.service with the following content:

This will make sure the network mounts are disconnected before the automatic network service is stopped.

**Examples:**

Example 1 (unknown):
```unknown
systemctl --type=service
```

Example 2 (unknown):
```unknown
/etc/netctl/
```

Example 3 (unknown):
```unknown
/etc/netctl/examples/
```

Example 4 (unknown):
```unknown
/etc/netctl/examples/
```

---

## CPU frequency scaling

**URL:** https://wiki.archlinux.org/title/Power-profiles-daemon

**Contents:**
- Userspace tools
  - i7z
  - turbostat
  - cpupower
  - thermald
  - power-profiles-daemon
  - tuned
  - cpupower-gui
  - gnome-shell-extension-cpupower
  - auto-cpufreq

CPU performance scaling enables the operating system to scale the CPU frequency up or down in order to save power or improve performance. Scaling can be done automatically in response to system load, adjust itself in response to ACPI events, or be manually changed by user space programs.

The Linux kernel offers CPU performance scaling via the CPUFreq subsystem, which defines two layers of abstraction:

A default scaling driver and governor are selected automatically, but userspace tools like cpupower, acpid, Laptop Mode Tools, or GUI tools provided for your desktop environment, may still be used for advanced configuration.

i7zAUR is an i7 (and now i3, i5, i7, i9) CPU reporting tool for Linux. It can be launched from a Terminal with the command i7z or as GUI with i7z-gui.

turbostat can display the frequency, power consumption, idle status and other statistics of the modern Intel and AMD CPUs.

cpupower is a set of userspace utilities designed to assist with CPU frequency scaling. The package is not required to use scaling, but is highly recommended because it provides useful command-line utilities and a systemd service to change the governor at boot.

The configuration file for cpupower is located in /etc/default/cpupower. This configuration file is read by a bash script in /usr/lib/systemd/scripts/cpupower which is activated by systemd with cpupower.service. You may want to enable cpupower.service to start at boot.

thermald is a Linux daemon used to prevent the overheating of Intel CPUs. This daemon proactively controls thermal parameters using P-states, T-states, and the Intel power clamp driver. thermald can also be used for older Intel CPUs. If the latest drivers are not available, then the daemon will revert to x86 model specific registers and the Linux "cpufreq subsystem" to control system cooling.

By default, it monitors CPU temperature using available CPU digital temperature sensors and maintains CPU temperature under control, before hardware takes aggressive correction action. If there is a skin temperature sensor in thermal sysfs, then it tries to keep skin temperature under 45C.

On Tiger Lake laptops (e.g. Dell Latitude 3420), this daemon has been reported as unlocking more performance than what would be otherwise available.

The associated systemd unit is thermald.service, which should be started and enabled. See thermald(8) for more information.

The powerprofilesctl command-line tool from power-profiles-daemon handles power profiles (e.g. balanced, power-saver, performance) through the power-profiles-daemon service. GNOME and KDE also provide graphical interfaces for profile switching; see the following:

See the project's README for more information on usage, use cases, and comparisons with similar projects.

Start/enable the power-profiles-daemon service. Note that when powerprofilesctl is launched, it also attempts to start the service (see the unit status of dbus.service).

tuned is a daemon for monitoring and adaptive tuning of system devices. It can configure GPU power modes, PCIe power management, set sysctl settings, adjust kernel scheduling and more; a daemon that also configures out aspects of power management in the system.

As of release 2.23.0, the project ships with tuned-ppd, a compatibility layer for programs written for power-profiles-daemon, such as the following:

For reasons why tuned should be used instead of power-profiles-daemon, see Fedora's proposal to replace it with tuned. For opposite arguments, see [3].

Start/enable the tuned daemon service. For power-profiles-daemon compatibility, also start/enable the tuned-ppd service. To control tuned from the command line, use tuned-adm to view, set, and recommend profiles.

cpupower-gui-gitAUR is a graphical utility designed to assist with CPU frequency scaling. The GUI is based on GTK and is meant to provide the same options as cpupower. cpupower-gui can enable or disable cores and change the maximum/minimum CPU frequency and governor for each core. The application handles privilege granting through polkit and allows any logged-in user in the wheel user group to change the frequency and governor. See cpupower-gui systemd units for more information on cpupower-gui.service and cpupower-gui-user.service.

gnome-shell-extension-cpupower-gitAUR is a GNOME shell extension that can alter minimum/maximum CPU frequencies and enable/disable frequency boosting.

auto-cpufreqAUR is an automatic CPU speed and power optimizer for Linux based on active monitoring of laptop's battery state, CPU usage, CPU temperature and system load.

The nvidia-powerd daemon provides support for NVIDIA's Dynamic Boost technology on supported laptop platforms. It acts as a system-wide power controller that dynamically redistributes power between the GPU and CPU based on workload demands, while maintaining the system's total thermal budget. [4]

Start/enable the nvidia-powerd service, which is provided by the nvidia-utils package.

Scaling drivers implement the CPU-specific details of setting frequencies specified by the governor. Strictly speaking, the ACPI standard requires power-performance states (P-states) that start at P0, and becoming decreasingly performant. This functionality is called SpeedStep on Intel, and PowerNow! on AMD.

In practice, though, processors provide methods for specifying specific frequencies rather than being restricted to fixed P-states, which the scaling drivers handle.

cpupower requires drivers to know the limits of the native CPU:

The factual accuracy of this article or section is disputed.

To see a full list of available modules, run:

Load the appropriate module (see Kernel modules for details). Once the appropriate cpufreq driver is loaded, detailed information about the CPU(s) can be displayed by running

In some cases, it may be necessary to manually set maximum and minimum frequencies.

To set the maximum clock frequency (clock_freq is a clock frequency with units: GHz, MHz):

To set the minimum clock frequency:

To set the CPU to run at a specified frequency:

Alternatively, you can set the frequency manually:

The available values can be found in /sys/devices/system/cpu/cpu*/cpufreq/scaling_available_frequencies or similar. [5]

Some processors support raising their frequency above the normal maximum for a short burst of time, under appropriate thermal conditions. On Intel processors, this is called Turbo Boost, and on AMD processors this is called Turbo-Core.

intel_pstate has a driver-specific interface for prohibiting the processor from entering turbo P-States:

For scaling drivers other than intel_pstate, if the driver supports boosting, the /sys/devices/system/cpu/cpufreq/boost attribute will be present, and can be used to disable/enable boosting.

To disable boosting, run:

To enable boosting, run:

On Intel processors, x86_energy_perf_policy can also be used to configure Turbo Boost:

amd_pstate has three operation modes: CPPC autonomous (active) mode, CPPC non-autonomous (passive) mode and CPPC guided autonomous (guided) mode. Officially supported kernels are built with CONFIG_X86_AMD_PSTATE_DEFAULT_MODE=3 which means the default for them is the active mode. This can be changed with the kernel parameter amd_pstate=active, amd_pstate=passive or amd_pstate=guided. To revert to the acpi_cpufreq driver, set amd_pstate=disable instead.

Scaling governors are power schemes determining the desired frequency for the CPU. Some request a constant frequency, others implement algorithms to dynamically adjust according to the system load. The governors included in the kernel are:

Depending on the scaling driver, one of these governors will be loaded by default:

To activate a particular governor, run:

Alternatively, you can activate a governor on every available CPU manually:

See the kernel documentation for details.

To set the threshold for stepping up to another frequency:

To set the threshold for stepping down to another frequency:

The sampling rate determines how frequently the governor checks to tune the CPU. sampling_down_factor is a tunable that multiplies the sampling rate when the CPU is at its highest clock frequency, thereby delaying load evaluation and improving performance. Allowed values for sampling_down_factor are 1 to 100000. This tunable has no effect on behavior at lower CPU frequencies/loads.

To read the value (default = 1), run:

To set the value, run:

Since Linux 5.9, it is possible to set the cpufreq.default_governor kernel option.[8] To set the desired scaling parameters at boot, configure the cpupower utility and enable its systemd service. Alternatively, systemd-tmpfiles or udev rules can be used.

Both Intel and AMD define a way to have the CPU decide its own speed based on (1) a performance range from the system and (2) a performance/power hint specifying the preference. The fully-autonomous mode is activated when:

The most important feature of active governing is that only two governors appear available, powersave and performance. They do not work at all like their normal counterpart, however: these levels are translated into an Energy Performance Preference hint for the CPU's internal governor. As a result, they both provide dynamic scaling, similar to the schedutil or ondemand generic governors respectively, differing mostly in latency. The performance algorithm should give better power saving functionality than the old ondemand governor for Intel HWP.

The intel-pstate driver has, confusingly, an "active" mode that works without the CPU's active decision. This mode turns on when kernel cmdline forces an "active" mode but HWP is unavailable or disabled. It will still only provide powersave and performance, but the driver itself does the governing in a way similar to schedutil and performance (i.e. it stays at the maximum P-state). There is no real benefit to this mode compared to passive intel-pstate.

It is possible to select in-between hints with the sysfs interfaces available. The interface is identical between AMD and Intel, where the files /sys/devices/system/cpu/cpu*/cpufreq/energy_performance_preference describe the current preference and /sys/devices/system/cpu/cpu*/cpufreq/energy_performance_available_preferences providing a list of available preferences. One can also pass a number between 0 (favor performance) and 255 (favor power). A fallback implementation is provided for Intel CPUs without EPP, translating strings to EPB levels (described in next section) but failing on numbers.

x86_energy_perf_policy supports configuration of EPP hints via the --hwp-epp switch on Intel CPUs only. It works via direct access of machine-specific registers (MSRs) which differ between Intel and AMD. The program can also restrict the range of HWP frequencies using a range of frequency multipliers.

To enable hardware P-States with x86_energy_perf_policy(8):

The power consumption of modern CPUs is no longer simply dependent on the frequency or voltage setting, as there are modules that can be switched on as needed. Collaborative processor performance control (CPPC) is the P-state replacement provided by ACPI 5.0. Instead of defining a table of static frequency levels, the processor provides many abstract performance levels and the operating system selects from these levels. There are two advantages:

On the other hand, the flexible frequency breaks frequency-invariant utilization tracking, which is important for fast frequency changes by schedutil. A number of vendor-specific methods have been used to make the frequency static under CPPC, with most successes coming from arm64.

cppc_cpufreq is the generic CPPC scaling driver. amd_pstate also uses ACPI CPPC to manage the CPU frequency when the Zen 3 MSR is unavailable – this method, also called "shared memory", has higher latency than MSR.

The Intel performance and energy bias hint (EPB) is an interface provided by Intel CPUs to allow for user space to specify the desired power-performance tradeoff, on a scale of 0 (highest performance) to 15 (highest energy savings). The EPB register is another layer of performance management functioning independently from frequency scaling. It influences how aggressive P-state and C-state selection will be, and informs internal model-specific decision making that affects energy consumption.

Common values and their aliases, as recognized by sysfs and x86_energy_perf_policy(8) are:

The EPB can be set using a sysfs attribute:

With x86_energy_perf_policy:

Users may configure scaling governors to switch automatically based on different ACPI events such as connecting the AC adapter or closing a laptop lid. A quick example is given below; however, it may be worth reading full article on acpid.

Events are defined in /etc/acpi/handler.sh. If the acpid package is installed, the file should already exist and be executable. For example, to change the scaling governor from performance to conservative when the AC adapter is disconnected and change it back if reconnected:

This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.

Some CPU/BIOS configurations may have difficulties to scale to the maximum frequency or scale to higher frequencies at all. This is most likely caused by BIOS events telling the OS to limit the frequency resulting in /sys/devices/system/cpu/cpu0/cpufreq/bios_limit set to a lower value.

Either you just made a specific Setting in the BIOS Setup Utility, (Frequency, Thermal Management, etc.) you can blame a buggy/outdated BIOS or the BIOS might have a serious reason for throttling the CPU on its own.

Reasons like that can be (assuming your machine's a notebook) that the battery is removed (or near death) so you are on AC-power only. In this case, a weak AC-source might not supply enough electricity to fulfill extreme peak demands by the overall system and as there is no battery to assist this could lead to data loss, data corruption or in worst case even hardware damage!

Not all BIOS'es limit the CPU-Frequency in this case, but, for example, most IBM/Lenovo Thinkpads do. Refer to thinkwiki for more thinkpad related info on this topic.

If you checked there is not just an odd BIOS setting and you know what you are doing, you can make the Kernel ignore these BIOS-limitations.

Set the processor.ignore_ppc=1 kernel parameter. For trying this temporarily, change the value in /sys/module/processor/parameters/ignore_ppc from 0 to 1.

Some systems use another mechanism to limit the CPU frequency, e.g., when running without battery or an unofficial power adapter. See Lenovo ThinkPad T480#CPU stuck at minimum frequency for a way to manipulate the BD PROCHOT bit in Intel CPUs and Dell XPS 15 (9560)#General slowness & stuttering for alternative fixes. It does not only apply to the Lenovo ThinkPad T480, but is a common problem in Dell XPS models like the XPS15 9550 and XPS15 9560, too. The bit also is what makes at least some Intel-based MacBooks run with minimum CPU frequency when no battery is connected.

**Examples:**

Example 1 (unknown):
```unknown
/etc/default/cpupower
```

Example 2 (unknown):
```unknown
/usr/lib/systemd/scripts/cpupower
```

Example 3 (unknown):
```unknown
cpupower.service
```

Example 4 (unknown):
```unknown
cpupower.service
```

---

## Intel graphics

**URL:** https://wiki.archlinux.org/title/Intel

**Contents:**
- Installation
- Loading
  - Early KMS
  - Enable GuC / HuC firmware loading
- Xorg configuration
  - With the modesetting driver
  - With the Intel driver
    - AccelMethod
    - Using Intel DDX driver with recent GPUs
    - Disabling TearFree, TripleBuffer, SwapbuffersWait

Since Intel provides and supports open source drivers, Intel graphics are essentially plug-and-play.

For a comprehensive list of Intel GPU models and corresponding chipsets and CPUs, see Wikipedia:Intel Graphics Technology and Gentoo:Intel#Feature support.

Also see Hardware video acceleration.

The Intel kernel module should load fine automatically on system boot.

If it does not happen, then:

Kernel mode setting (KMS) is supported by the i915 and xe drivers, and is enabled early since mkinitcpio v32, as the kms hook is included by default. For other setups, see Kernel mode setting#Early KMS start for instructions on how to enable KMS as soon as possible at the boot process.

Starting with Gen9 (Skylake and onwards), Intel GPUs include a Graphics micro (μ) Controller (GuC) which provides the following functionality:

To use this functionality, first ensure that linux-firmware-intel is installed, as it provides the GuC and HuC firmware files.

Next, the GuC firmware must be loaded. With regards to HuC support, some video features (e.g. CBR rate control on SKL low-power encoding mode) require loading the HuC firmware as well [6].

The new experimental xe driver enables Guc and Huc functionality by default.

For the i915 driver, GuC functionality is controlled by the enable_guc kernel module parameter. Its usage is as follows:

The factual accuracy of this article or section is disputed.

If GuC submission or HuC firmware loading is not enabled by default for your GPU, you can manually enable it.

Set the enable_guc= kernel module parameter. For example, with:

Regenerate the initramfs, on next boot you can verify both GuC and HuC are enabled by using dmesg:

If they are not supported by your graphics adapter you will see:

Alternatively, check using:

Note that the related warning is not fatal, as explained in [10]:

There is generally no need for any configuration to run Xorg.

However, to take advantage of some driver options or if Xorg does not start, you can create an Xorg configuration file.

If you have installed xf86-video-intel but want to load the modesetting driver explicitly instead of letting the DDX driver take priority, for example when trying to compare them:

Create an Xorg configuration file similar to the one below:

Additional options are added by the user on new lines below Driver. For the full list of options, see the intel(4) man page.

You may need to indicate Option "AccelMethod" when creating a configuration file, the classical options are UXA, SNA (default) and BLT.

If you experience issues with default SNA (e.g. pixelated graphics, corrupt text, etc.), try using UXA instead, which can be done by adding the following line to your configuration file:

See the "AccelMethod" option under intel(4) § CONFIGURATION DETAILS.

For Intel GPUs starting from Gen8 (Broadwell), the Iris Mesa driver is needed:

If you use a compositor (the default in modern desktop environment like GNOME, KDE Plasma, Xfce, etc.), then TearFree, TripleBuffer and SwapbuffersWait can usually be disabled to improve performance and decrease power consumption.

The i915 kernel module allows for configuration via module options. Some of the module options impact power saving.

A list of all options along with short descriptions and default values can be generated with the following command:

To check which options are currently enabled, run

You will note that many options default to -1, resulting in per-chip powersaving defaults. It is however possible to configure more aggressive powersaving by using module options.

Framebuffer compression (FBC) is a feature that can reduce power consumption and memory bandwidth during screen refreshes.

The feature will be automatically enabled if supported by the hardware. You can use the command below to verify whether it is is enabled:

If the parm is set to -1, you do not need to do anything. Otherwise, to force-enable FBC, use i915.enable_fbc=1 as kernel parameter or set in /etc/modprobe.d/i915.conf:

Enabling frame buffer compression on pre-Sandy Bridge CPUs results in endless error messages:

The goal of Intel Fastboot is to preserve the frame-buffer as setup by the BIOS or boot loader to avoid any flickering until Xorg has started.[16][17]

To force enable fastboot on platforms where it is not the default already, set i915.fastboot=1 as kernel parameter or set in /etc/modprobe.d/i915.conf:

See Intel GVT-g for details.

Starting with Gen6 (Sandy Bridge and onwards), Intel GPUs provide performance counters used for exposing internal performance data to drivers. The drivers and hardware registers refer to this infrastructure as the Observation Architecture (internally "OA") [18], but Intel's documentation also more generally refers to this functionality as providing Observability Performance Counters [19][dead link 2023-09-16—HTTP 404] [20][dead link 2023-09-16—HTTP 404].

By default, only programs running with the CAP_SYS_ADMIN (equivalent to root) or CAP_PERFMON capabilities can utilize the observation architecture [21] [22]. Most applications will be running without either of these, resulting in the following warning:

To enable performance support without using the capabilities (or root), set the kernel parameter as described in sysctl.

This can be useful for some full screen applications:

If it does not work, try:

where param is one of "Full", "Center" or "Full aspect".

The MESA_GL_VERSION_OVERRIDE environment variable can be used to override the reported OpenGL version to any application. For example, setting MESA_GL_VERSION_OVERRIDE=4.5 will report OpenGL 4.5.

See Hardware video acceleration#Verification.

To try the (experimental) new Xe driver, you need:

Note your PCI ID with:

Then add the following to your Kernel parameters with the appropriate PCI ID:

Make sure you have an alternate solution to boot in order to revert if necessary.

The SNA acceleration method causes tearing on some machines. To fix this, enable the TearFree option in the xf86-video-intel driver by adding the following line to your configuration file:

See the original bug report for more info.

TearFree support was added to the modesetting driver [27][28]. As the last release for the non-XWayland servers was the 21.1 release in 2021, this patch has not reached a stable release, so you will need xorg-server-gitAUR until then.

The intel-driver uses Triple Buffering for vertical synchronization; this allows for full performance and avoids tearing. To turn vertical synchronization off (e.g. for benchmarking) use this .drirc in your home directory:

DRI3 is the default DRI version in xf86-video-intel. On some systems this can cause issues such as this. To switch back to DRI2 add the following line to your configuration file:

For the modesetting driver, this method of disabling DRI3 does not work. Instead, one can set the environment variable LIBGL_DRI3_DISABLE=1.

Should you experience missing font glyphs in GTK applications, the following workaround might help. Edit /etc/environment to add the following line:

See also FreeDesktop bug 88584.

If you experience corrupted and/or frozen graphics in some applications (such as random colors filling the application window, extreme unreasonable blurriness, an application failing to update its graphics at all while performing other tasks without lag, etc), try running the application with OpenGL instead of Vulkan. This has occurred on some configurations with Intel Arc GPUs.

Some issues with X crashing, GPU hanging, or problems with X freezing, can be fixed by disabling the GPU usage with the NoAccel option - add the following lines to your configuration file:

Alternatively, try to disable the 3D acceleration only with the DRI option:

This issue is covered on the Xrandr page.

If after resuming from suspend, the hotkeys for changing the screen brightness do not take effect, check your configuration against the Backlight article.

If the problem persists, try one of the following kernel parameters:

Also make sure you are not using fastboot mode (i915.fastboot kernel parameter), it is known for breaking backlight controls.

If you experience corruption, unresponsiveness, lags or slow performance in Chromium and/or Firefox some possible solutions are:

A few seconds after X/Wayland loads the machine will freeze and journalctl will log a kernel crash referencing the Intel graphics as below:

This can be fixed by disabling execlist support which was changed to default on with kernel 4.0. Add the following kernel parameter:

This is known to be broken to at least kernel 4.0.5.

The video output of a Windows guest in VirtualBox sometimes hangs until the host forces a screen update (e.g. by moving the mouse cursor). Removing the enable_fbc=1 option fixes this issue.

Panel Self Refresh (PSR), a power saving feature used by Intel iGPUs is known to cause flickering in some instances FS#49628 FS#49371 FS#50605. A temporary solution is to disable this feature using the kernel parameter i915.enable_psr=0 or xe.enable_psr=0.

This can solve error messages like [i915] *ERROR* CPU pipe A FIFO underrun.

The classic mesa driver for Gen 3 GPUs included in the mesa-amber package reports OpenGL 2.0 by default, because the hardware is not fully compatible with OpenGL 2.1.[29] OpenGL 2.1 support can be enabled manually by setting /etc/drirc or ~/.drirc options like:

One of the low-resolution video ports may be enabled on boot which is causing the terminal to utilize a small area of the screen. To fix, explicitly disable the port with an i915 module setting with video=SVIDEO-1:d in the kernel command line parameter in the boot loader. See Kernel parameters for more info.

If that does not work, try disabling TV1 or VGA1 instead of SVIDEO-1. Video port names can be listed with xrandr.

According to a Linux kernel issue, sound will not be output through HDMI if intel_iommu=on. To fix this problem, use the following kernel parameter:

Or alternatively, disable IOMMU:

The factual accuracy of this article or section is disputed.

Low-powered Intel processors and/or laptop processors have a tendency to randomly hang or crash due to the problems with the power management features found in low-power Intel chips. If such a crash happens, you will not see any logs reporting this problem. Adding the following Kernel parameters may help to resolve the problem.

ahci.mobile_lpm_policy=1 fixes a hang on several Lenovo laptops and some Acer notebooks due to problematic SATA controller power management. That workaround is strictly not related to Intel graphics but it does solve related issues. Adding this kernel parameter sets the link power management from firmware default to maximum performance and will also solve hangs when you change display brightness on certain Lenovo machines but increases idle power consumption by 1-1.5 W on modern ultrabooks. For further information, especially about the other states, see the Linux kernel mailing list and Red Hat documentation.

i915.enable_dc=0 disables GPU power management. This does solve random hangs on certain Intel systems, notably Goldmount and Kaby Lake Refresh chips. Using this parameter does result in higher power use and shorter battery life on laptops/notebooks. If this helps, you can try finer-grained DC limitations as documented in modinfo i915 | grep enable_dc.

intel_idle.max_cstate=1 limits the processors sleep states, it prevents the processor from going into deep sleep states. That is absolutely not ideal and does result in higher power use and lower battery life. However, it does solve random hangs on many Intel systems. Use this if you have a Intel Baytrail or a Kaby Lake Refresh chip. Intel "Baytrail" chips were known to randomly hang without this kernel parameter due to a hardware flaw, theoretically fixed 2019-04-26. More information about the max_cstate parameter can be found in the kernel documentation and about the cstates in general on a writeup on GitHub.

If you try adding intel_idle.max_cstate=1 i915.enable_dc=0 ahci.mobile_lpm_policy=1 in the hope of fixing frequent hangs and that solves the issue you should later remove one by one to see which of them actually helped you solve the issue. Running with cstates and display power management disabled is not advisable if the actual problem is related to SATA power management and ahci.mobile_lpm_policy=1 is the one that actually solves it.

Check Linux Reviews for more details.

In case you infrequently wake up to a black screen, but the system otherwise properly resumes with CPU pipe A FIFO underrun messages in the journal and limiting intel_idle.max_cstate=1 reliably prevents that, you can use Suspend and hibernate#Sleep hooks and cpupower-idle-set(1) to effectively control the C-state around the suspend cycle with -D0 and -E to not permanently run the CPU in the lowest C-state.

This article or section is a candidate for merging with Kernel mode setting#Forcing modes and EDID.

For some 165Hz monitors, xrandr might not display the 165Hz option, and the fix in #Adding undetected resolutions does not solve this. In this case, see i915-driver-stuck-at-40hz-on-165hz-screen.

Then append edid in HOOKS of /etc/mkinitcpio.conf, Just like this:

Then regenerate the initramfs.

Users with Raptor Lake and Alder Lake-P 12th gen mobile processor laptops from various vendors experienced freeze and black-screen after waking up from suspending. It is because many laptop vendors ship an incorrect VBT (Video BIOS Table), as described in freedesktop issues 5531 6401, that wrongly describe the actual ports connected to the iGPU. In this case, all of the documented cases concern duplicate eDP entries.

Considering most vendors will not publish a BIOS update for a laptop with a properly working Windows OS, Linux users could only address the issue on the kernel side. There are two methods for a user to prevent the duplicate eDP entries from affecting the kernel: patching the kernel or loading a modified VBT.

For patching the kernel, the duplicate eDP entry needs to be identified by analyzing the output of:

This shows that there is in fact a duplicate eDP, and the kernel should ignore the second entry, but the user is still encouraged to check this. This can then be patched with the following kernel patch in which the index of the duplicate screen can be substituted for ignoreEntry = 1 if it needs be.

A second way to solve this is to edit the VBT by directly erasing the duplicate entry from the VBT.

This works by copying the VBT and editing it with a hex editor and changing the device type corresponding with the duplicate device handle to 00 00:

The modified VBT can then be loaded by copying it to /lib/firmware/modified_vbt passing i915.vbt_firmware=modified_vbt as a kernel parameter and, if required, regenerate the initramfs.

By default some monitors might not be recognized properly by the Intel GPU and have washed out colors because it's not in full-range RGB mode.

If you are using GNOME, an alternative is to add <rgbrange>full</rgbrange> to the ~/.config/monitors.xml configuration. For example:

When getting an error like this when running some programs (eg. vainfo, falkon, mpv...):

The probable cause could be disabled ReBar (Resizable BAR) in BIOS/UEFI. Some motherboards may allow to enable ReBar only when UEFI mode is active, without legacy support.

**Examples:**

Example 1 (unknown):
```unknown
Ctrl+Alt+Fn
```

Example 2 (unknown):
```unknown
/etc/modprobe.d/
```

Example 3 (unknown):
```unknown
/usr/lib/modprobe.d/
```

Example 4 (unknown):
```unknown
enable_guc=3
```

---

## Linux console/Keyboard configuration

**URL:** https://wiki.archlinux.org/title/Keyboard_configuration_in_console

**Contents:**
- Viewing keyboard settings
- Keymaps
  - Listing keymaps
  - Loadkeys
  - Persistent configuration
  - Creating a custom keymap
    - Adding directives
    - Other examples
    - Saving changes
- Adjusting typematic delay and rate

Keyboard mappings, console fonts and console maps are provided by the kbd package (a dependency of systemd), which also provides low-level tools for managing the text console. In addition, systemd provides the localectl(1) tool, which can control both the system locale and keyboard layout settings for both the console and Xorg.

Use localectl status to view the current keyboard configuration.

The keymap files are stored in the /usr/share/kbd/keymaps/ directory tree. A keymap file fully describes the keyboard layout, possibly with symbols for different languages and layout switching is simulated via AltGr_Lock keysym usage.

The include statement can be used to share common parts of keymap files. Where to look for an include file is described in the source code only.

For more details see keymaps(5).

The naming conventions of console keymaps are somewhat arbitrary, but usually they are based on:

For a list of all the available keymaps, use the command:

To search for a keymap, use the following command, replacing search_term with the code for your language, country, or layout:

Alternatively, using find:

It is possible to set a keymap just for the current session. This is useful for testing different keymaps, solving problems etc. The loadkeys tool is used for this purpose:

See loadkeys(1) for details. The same tool is used internally by systemd-vconsole-setup(8) when loading the keymap configured in /etc/vconsole.conf.

A persistent keymap can be set in /etc/vconsole.conf, which is read by systemd on start-up. The KEYMAP variable is used for specifying the keymap. If the variable is empty or not set, the us keymap is used as default value. See vconsole.conf(5) for all options. For example:

For convenience, localectl may be used to set the console keymap. It will change the KEYMAP variable in /etc/vconsole.conf and also set the keymap for the current session:

The --no-convert option can be used to prevent localectl from automatically changing the Xorg keymap to the nearest match. See localectl(1) for more information.

If required, the keymap from /etc/vconsole.conf can be loaded during early userspace by the keymap mkinitcpio hook.

When using the console, you can use hotkeys to print a specific character. Moreover we can also print a sequence of characters and some escape sequences. Thus, if we print the sequence of characters constituting a command and afterwards an escape character for a new line, that command will be executed.

One method of doing this is editing the keymap file. However, since it will be rewritten anytime the package it belongs to is updated, editing this file is discouraged. It is better to integrate the existing keymap with a personal keymap. The loadkeys utility can do this.

First, create a keymap file. This keymap file can be anywhere, but one method is to mimic the directory hierarchy in /usr/local: create the /usr/local/share/kbd/keymaps directory, then edit /usr/local/share/kbd/keymaps/personal.map.

As a side note, it is worth noting that such a personal keymap is useful also to redefine the behaviour of keys already treated by the default keymap: when loaded with loadkeys, the directives in the default keymap will be replaced when they conflict with the new directives and conserved otherwise. This way, only changes to the keymap must be specified in the personal keymap.

Two kinds of directives are required in this personal keymap. First of all, the keycode directives, which matches the format seen in the default keymaps. These directives associate a keycode with a keysym. Keysyms represent keyboard actions. The actions available include outputting character codes or character sequences, switching consoles or keymaps, booting the machine, and many other actions. The full currently active keymap can be obtained with

Most keysyms are intuitive. For example, to set key 112 to output an 'e', the directive will be:

To set key 112 to output a euro symbol, the directive will be:

Some keysym are not immediately connected to a keyboard actions. In particular, the keysyms prefixed by a capital F and one to three digits (F1-F246) constituting a number greater than 30 are always free. This is useful directing a hotkey to output a sequence of characters and other actions:

Then, F70 can be bound to output a specific string:

When key 112 is pressed, it will output the contents of F70. In order to execute a printed command in a terminal, a newline escape character must be appended to the end of the command string. For example, to enter a system into hibernation, the following keymap is added:

In order to make use of the personal keymap, it must be loaded with loadkeys:

However this keymap is only active for the current session. In order to load the keymap at boot, specify the full path to the file in the KEYMAP variable in /etc/vconsole.conf. The file does not have to be gzipped as the official keymaps provided by kbd.

The typematic delay indicates the amount of time (typically in milliseconds) a key needs to be pressed and held in order for the repeating process to begin. After the repeating process has been triggered, the character will be repeated with a certain frequency (usually given in Hz) specified by the typematic rate. These values can be changed using the kbdrate command. Note that these settings are configured separately for the console and for Xorg.

For example to set a typematic delay to 200ms and a typematic rate to 30Hz, use the following command:

Issuing the command without specifying the delay and rate will reset the typematic values to their respective defaults; a delay of 250ms and a rate of 11Hz:

A systemd service can be used to set the keyboard rate. For example:

Then start/enable the kbdrate.service systemd service.

Layout switching can only be simulated by establishing a different layout on one of the higher layers (typically the 3rd, AltGr).

For a list of layouts that likely support this, run

You can test this by passing main and augmenting layout to loadkeys, e.g.

If it worked, the 3rd level shift (AltGr) will allow you to access the second layout, the AltGr_Lock (often Alt+Shift, you will have to inspect the keymap file with zless) will provide an effective toggle.

For a permanent configuration, set the KEYMAP_TOGGLE next to the KEYMAP in /etc/vconsole.conf.

**Examples:**

Example 1 (unknown):
```unknown
localectl status
```

Example 2 (unknown):
```unknown
/usr/share/kbd/keymaps/
```

Example 3 (unknown):
```unknown
$ localectl list-keymaps
```

Example 4 (unknown):
```unknown
search_term
```

---

## Syslinux

**URL:** https://wiki.archlinux.org/title/Syslinux

**Contents:**
- Supported file systems
- Installation
- Installing the Syslinux boot loader
  - BIOS systems
    - Automatically
    - Manually
      - MBR partition table
      - GUID partition table
  - UEFI systems
    - Limitations of UEFI Syslinux

Syslinux is a collection of boot loaders capable of booting from drives, CDs, and over the network via PXE.

Some of the supported file systems are FAT, NTFS, ext2, ext3, ext4, XFS, UFS/FFS, and uncompressed single-device Btrfs.

Install the syslinux package.

BIOS booting will also require the gptfdisk package for BIOS/GPT setups and mtools if your /boot partition is FAT-formatted.

UEFI booting requires installing the efibootmgr package.

Installing the package is not the same as installing the boot loader. After installing the relevant package(s), the boot loader code itself needs to be installed (to the adequate area, usually the VBR or ESP) so to be able to boot the system; the following sections provide alternative instructions depending on the characteristics of your particular system.

Syslinux boot process on BIOS happens in stages:

After executing the syslinux-install_update script, do not forget to edit /boot/syslinux/syslinux.cfg by following #Configuration and #Kernel parameters.

The syslinux-install_update script will install the boot loader code (usually to the VBR), copy *.c32 modules to /boot/syslinux/, set the boot flag, install the boot code in the MBR and copy /usr/share/syslinux/syslinux.cfg to /boot/syslinux/syslinux.cfg. It can handle MBR and GPT disks along with software RAID:

If you use a separate boot partition, make sure that it is mounted. Check with lsblk; if you do not see a /boot mountpoint, mount it before you go any further.

Run syslinux-install_update with flags: -i (install the files), -a (mark the partition active with the boot flag), -m (install the MBR boot code):

If this command fails with Syslinux BIOS install failed, the problem is likely that the extlinux binary could not find the partition containing /boot:

This can happen, for example, when upgrading from LILO which, while booting a current custom kernel, turned a kernel command line parameter of say root=/dev/sda1 into its numeric equivalent root=801, as evidenced by /proc/cmdline and the output of the mount command. Remedy the situation by either continuing with the manual install described below while specifying --device=/dev/sda1 to extlinux, or simply by first rebooting into a stock Arch Linux kernel; its use of an initramfs avoids the problem.

Now is the time to edit /boot/syslinux/syslinux.cfg by following #Configuration and #Kernel parameters.

Your boot partition, on which you plan to install Syslinux, must contain a FAT, ext2, ext3, ext4, or Btrfs file system. You do not have to install it on the root directory of a file system, e.g., with device /dev/sda1 mounted on /boot. For example, you can install Syslinux in the syslinux subdirectory:

Copy all .c32 files from /usr/lib/syslinux/bios/ to /boot/syslinux/ if you desire to use any menus or configurations other than a basic boot prompt. Do not symlink them.

Now install the boot loader. For FAT, ext2/3/4, or btrfs boot partition use extlinux, where the device has been mounted:

Alternatively, for a FAT boot partition use syslinux, where the device is unmounted:

After this, proceed to install the Syslinux bootstrap code appropriate for the partition table:

as described in the next sections.

See Master Boot Record for further general information.

For an MBR partition table, ensure your boot partition is marked as "active" in your partition table (the "boot" flag is set). Applications capable of doing this include fdisk and parted. It should look like this:

An alternative MBR which Syslinux provides is: altmbr.bin. This MBR does not scan for bootable partitions; instead, the last byte of the MBR is set to a value indicating which partition to boot from. Here is an example of how altmbr.bin can be copied into position:

In this case, a single byte of value 5 (hexadecimal) is appended to the contents of altmbr.bin and the resulting 440 bytes are written to the MBR on device sda. Syslinux was installed on the first logical partition (/dev/sda5) of the disk.

For a GPT, ensure that attribute bit 2 "Legacy BIOS bootable" is set for the /boot partition. For Parted it can be set using the "legacy_boot" flag. Using sgdisk the command to set the attribute is:

This will set the attribute "legacy BIOS bootable" on partition 1 of /dev/sda. To check:

Setup Syslinux in the EFI system partition as follows.

Copy Syslinux files to the ESP:

Create a UEFI boot entry for Syslinux using efibootmgr:

where /dev/sdXY is the EFI system partition containing the boot loader.

Create or edit esp/EFI/syslinux/syslinux.cfg by following #Configuration.

This article or section is out of date.

The Syslinux configuration file, syslinux.cfg, should be created in the same directory where you installed Syslinux. In our case, /boot/syslinux/ for BIOS systems and esp/EFI/syslinux/ for UEFI systems.

The boot loader will look for either syslinux.cfg (preferred) or extlinux.conf

This is a simple configuration file that will show a boot: prompt and will automatically boot after 5 seconds. If you want to boot directly without seeing a prompt, set PROMPT to 0.

Syslinux also allows you to use a boot menu. To use it, copy the menu and libutil modules to your Syslinux directory:

Since version 5.00, additional lib*.c32 library modules are frequently needed too. See the Syslinux wiki for the module dependency tree.

For more details about the menu system, see the Syslinux wiki.

Syslinux also allows you to use a graphical boot menu. To use it, copy the vesamenu COM32 module to your Syslinux folder:

Since version 5.00, additional lib*.c32 library modules are frequently needed too. See the Syslinux wiki for the module dependency tree.

This configuration uses the same menu design as the Arch Install CD, its configuration can be found at gitlab.archlinux.org. The Arch Linux background image can be downloaded from there, too. Copy the image to /boot/syslinux/splash.png.

Since Syslinux 3.84, vesamenu.c32 supports the MENU RESOLUTION $WIDTH $HEIGHT directive. To use it, insert MENU RESOLUTION 1440 900 into your configuration for a 1440x900 resolution. However, the background picture has to have exactly the right resolution, as Syslinux will otherwise refuse to load the menu.

To center the menu and adjust resolution, use MENU RESOLUTION, MENU HSHIFT $N and MENU VSHIFT $N where $N is a positive number. The default values are both 0 which is the upper-left hand corner of your monitor. Conversely, a negative number starts from the opposite end of the screen (e.g. VHSHIFT -4 would be 4 rows from the bottom of the screen).

To move the menu to the center, add or edit these values:

VESA standards are commonly a maximum of 25 rows and 80 columns, so going higher than those values might move the menu off the screen, potentially requiring editing from a rescue CD.

The kernel parameters are set by using the APPEND directive in syslinux.cfg: for each LABEL entry, a maximum of one APPEND line is accepted (i.e. spanning multiple lines is not valid).

It is recommended to make the following changes for the "fallback" entry as well.

In the simplest case, the value of the root parameter needs to be replaced; see Persistent block device naming for supported methods.

Change root=UUID=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx to point to the correct root volume.

If you use dm-crypt encryption change the APPEND line to use your encrypted volume:

If booting a btrfs subvolume, amend the APPEND line with rootflags=subvol=root_subvolume. For example, where /dev/disk/by-uuid/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx has been mounted as a btrfs subvolume called 'ROOT' (e.g. mount -o noatime,subvol=ROOT /dev/disk/by-uuid/xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx /mnt), then the APPEND line would need to be modified as follows:

A failure to do so will otherwise result in the following error message: ERROR: Root device mounted successfully, but /sbin/init does not exist.

If you do not want to see the Syslinux menu at all, use the #Boot prompt, and set PROMPT to 0 and comment out any UI menu entries. Setting the TIMEOUT variable to 0 might also be a good idea. Make sure there is a DEFAULT set in your syslinux.cfg. Holding either Shift or Alt, or setting either Caps Lock or Scroll Lock, during boot will allow for options other than default to be used. See the upstream wiki for additional alternatives.

Syslinux has two levels of boot loader security: a menu master password, and a per-menu-item password. In syslinux.cfg, use

to set a master boot loader password, and

within a LABEL block to password-protect individual boot items.

The passwd can be either a cleartext password or hashed: see official documentation.

Syslinux BIOS cannot directly chainload files located on other partitions; however, chain.c32 can boot a partition boot sector (VBR) or another disk's MBR.

If you want to chainload other operating systems (such as Windows) or boot loaders, copy the chain.c32 module to the Syslinux directory (additional lib*.c32 library modules might be needed too; for details, see the instructions in the previous section). Then create a section in the configuration file:

hd0 3 is the third partition on the first BIOS drive - drives are counted from zero, but partitions are counted from one.

If you are unsure about which drive your BIOS thinks is "first", you can instead use the MBR identifier, or if you are using GPT, the filesystem labels. To use the MBR identifier, run the command

replacing /dev/sdb with the drive you wish to chainload. Using the returned hexadecimal number, 0xf00f1fd3 in this case, the syntax in syslinux.cfg is:

For more details about chainloading, see the Syslinux wiki.

If you have GRUB installed on the same partition, you can chainload it by using:

Alternatively, it is also possible to load GRUB as a linux kernel by prepending lnxboot.img to core.img. The file lnxboot.img is part of core/grub and can be found in /usr/lib/grub/i386-pc.

This may be required for booting from ISO images.

The factual accuracy of this article or section is disputed.

Chainloading another boot loader such as Windows' is pretty obvious, as there is a definite boot loader to chain to. But with Syslinux, it is only able to load files residing on the same partition as the configuration file. Thus, if you have another version of Linux on a separate partition, without a shared /boot, it becomes necessary to employ EXTLINUX rather than the other OS's default boot loader (eg. GRUB2). Essentially, EXTLINUX can be installed on the partition superblock/VBR and be called as a separate boot loader right from the MBR installed by Syslinux. EXTLINUX is part of The Syslinux Project and is included with the syslinux package.

The following instructions assume you have Syslinux installed already. These instructions will also assume that the typical Arch Linux configuration path of /boot/syslinux is being used and the chainloaded system's / is on /dev/sda3.

From a booted Linux (likely the partition that Syslinux is set up to boot), mount the other system's root partition to your desired mount point. In this example this will be /mnt. Also, if a separate /boot partition is used on the second operating system, that will also need to be mounted. The example assumes this is /dev/sda2.

Install EXTLINUX to the partition VBR, and copy necessary *.c32 files

Create /mnt/boot/syslinux/syslinux.cfg. You can use the other Linux boot loader menu file for reference. Below is an example:

And then add an entry to your main syslinux.cfg

Note that the other Linux entry in <other-OS>/boot/syslinux/syslinux.cfg will need to be edited each time you update this OS's kernel unless it has symlinks to its latest kernel and initramfs in /. Since we are booting the kernel directly and not chainloading the other OS default boot loader.

Use this LABEL section to launch memtest:

HDT (Hardware Detection Tool) displays hardware information. Like before, the .c32 file has to be copied from /boot/syslinux/. Additional lib*.c32 library modules might be needed too. For PCI info, copy /usr/share/hwdata/pci.ids to /boot/syslinux/pci.ids and add the following to your configuration file:

Use the following sections to reboot or power off your machine:

To clear the screen when exiting the menu, add the following line:

If you often have to edit your boot command with diverse parameters in the Syslinux boot prompt, then you might want to remap your keyboard layout. This allows you to enter "=", "/" and other characters easily on a non-US keyboard.

To create a compatible keymap (e.g. a german one) run:

Now edit syslinux.cfg and add:

See the Syslinux wiki for more details.

to hide the menu while displaying only the timeout. Press any key to bring up the menu.

PXELINUX is provided by the syslinux package.

For BIOS clients, copy the lpxelinux.0 and ldlinux.c32 to the boot directory of the client.

We also created the pxelinux.cfg directory, which is where PXELINUX searches for configuration files by default. Because we do not want to discriminate between different host MACs, we then create the default configuration.

Or if you are using NBD, use the following append line:

PXELINUX uses the same configuration syntax as SYSLINUX; refer to the upstream documentation for more information.

The kernel and initramfs will be transferred via TFTP, so the paths to those are going to be relative to the TFTP root. Otherwise, the root filesystem is going to be the NFS mount itself, so those are relative to the root of the NFS server.

To actually load PXELINUX, replace filename "/grub/i386-pc/core.0"; in /etc/dhcpd.conf with filename "/lpxelinux.0".

Syslinux supports booting from ISO images directly using the memdisk module, see Multiboot USB drive#Using Syslinux and memdisk for examples.

See Working with the serial console#Syslinux.

It is possible to temporarily change the default Syslinux action and boot another label only during the next boot. The following command shows how to boot the archfallback label once:

During the next boot, the specified label will be booted without any Syslinux prompt showing up. The default Syslinux boot behaviour will be restored on the next reboot.

An error message such as "Failed to load ldlinux.c32" during the initial boot can be triggered by many diverse reasons. One potential reason could be a change in file system tools or in a file system structure, depending on its own version.

See also [11] (the whole page might be relevant for troubleshooting too).

You can type in the LABEL name of the entry that you want to boot (as per your syslinux.cfg). If you used the example configurations, just type:

If you get an error that the configuration file could not be loaded, you can pass your needed boot parameters, e.g.:

If you do not have access to boot: in ramfs, and therefore temporarily unable to boot the kernel again,

In the case of a badly corrupted root partition (in which the journal is damaged), in the ramfs emergency shell, mount the root file system:

And grab the tune2fs binary from the root partition (it is not included in Syslinux):

Follow the instructions at ext2fs: no external journal to create a new journal for the root partition.

Certain motherboard manufacturers have less compatibility for booting from USB devices than others. While an ext4 formatted USB drive may boot on a more recent computer, some computers may hang if the boot partition containing the kernel and initramfs are not on a FAT16 partition. To prevent an older machine from loading ldlinux and failing to read syslinux.cfg, create a partition (≤ 2 GB) and format to FAT16 using dosfstools:

then install and configure Syslinux.

and then boot up with the Arch install disk. Next, use cfdisk to delete the /boot partition, and recreate it. This time it should begin at the proper sector, 63. Now mount your partitions and chroot into your mounted system, as described in the installation guide. Restore /boot with the command

Check if /etc/fstab is correct, run:

You will also get this error if you are trying to boot from an md RAID 1 array and created the array with a too new version of the metadata that Syslinux does not understand. As of August 2013 by default mdadm will create an array with version 1.2 metadata, but Syslinux does not understand metadata newer than 1.0. If this is the case you will need to recreate your RAID array using the --metadata=1.0 flag to mdadm.

Solution: Make sure the partition that contains /boot has the boot flag enabled. Also, make sure the boot flag is not enabled on the Windows partition. See the installation section above.

The MBR that comes with Syslinux looks for the first active partition that has the boot flag set. The Windows partition was likely found first and had the boot flag set. If you wanted, you could use the MBR that Windows or MS-DOS fdisk provides.

You select a menu entry and it does nothing, it just "refreshes" the menu. This usually means that you have an error in your syslinux.cfg file. Hit Tab to edit your boot parameters. Alternatively, press Esc and type in the LABEL of your boot entry (e.g. arch). Another cause could be that you do not have a kernel installed. Find a way to access your file system (through live CD, etc) and make sure that /mount/vmlinuz-linux exists and does not have a size of 0. If this is the case, reinstall your kernel.

The ldlinux.sys file has the immutable attribute set, which prevents it from being deleted or overwritten. This is because the sector location of the file must not change or else Syslinux has to be reinstalled. To remove it, run:

Problem: As of linux-3.0, the modesetting driver tries to keep the current contents of the screen after changing the resolution (at least it does so with my Intel, when having Syslinux in text mode). It seems that this goes wrong when combined with the vesamenu module in Syslinux (the white block is actually an attempt to keep the Syslinux menu, but the driver fails to capture the picture from vesa graphics mode).

If you have a custom resolution and a vesamenu with early modesetting, try to append the following in syslinux.cfg to remove the white block and continue in graphics mode:

If Windows is installed on a different drive than Arch and you have trouble chainloading it, try the following configuration:

Replace the mbr code with the one your Windows drive has (details above), and append swap to the options.

In some cases (e.g. boot loader unable to boot kernel) it is highly desirable to get more information from the boot process. Syslinux prints error messages to screen but the boot menu quickly overwrites the text. To avoid losing the log information, disable UI menu in syslinux.cfg and use the default "command-line" prompt. It means:

To get more detailed debug log, recompile the syslinux package with additional CFLAGS:

Booting from btrfs with compression is not supported.[12] This error will show:

Booting from multiple-device btrfs is not supported.[13] (As of 21-Jul-2016 line 1246 in validate_device_btrfs() in main.c) This head-scratching error will show (assuming you are installing on sda1):

**Examples:**

Example 1 (unknown):
```unknown
/usr/lib/syslinux/bios/mbr.bin
```

Example 2 (unknown):
```unknown
/usr/lib/syslinux/bios/gptmbr.bin
```

Example 3 (unknown):
```unknown
/boot/syslinux/ldlinux.sys
```

Example 4 (unknown):
```unknown
extlinux --install
```

---

## PulseAudio

**URL:** https://wiki.archlinux.org/title/PulseAudio

**Contents:**
- Installation
  - Front-ends
    - Console
    - Graphical
- Configuration
  - Configuration files
    - daemon.conf
    - default.pa
    - system.pa
    - client.conf

PulseAudio is a general purpose sound server intended to run as a middleware between your applications and your hardware devices, either using Advanced Linux Sound Architecture (ALSA) or Open Sound System (OSS). It also offers easy network streaming across local devices using Avahi if enabled. While its main purpose is to ease audio configuration, its modular design allows more advanced users to configure the daemon precisely to best suit their needs.

Install the pulseaudio package.

Some PulseAudio modules are not included in the main package and must be installed separately if needed:

There are a number of front-ends available for controlling the PulseAudio daemon:

By default, PulseAudio is configured to automatically detect all sound cards and manage them. It takes control of all detected ALSA devices and redirects all audio streams to itself, making the PulseAudio daemon the central configuration point. The daemon should work mostly out of the box, only requiring a few minor tweaks.

While PulseAudio usually runs fine out of the box and requires only minimal configuration, advanced users can change almost every aspect of the daemon by either altering the default configuration file to disable modules or writing your own from scratch.

PulseAudio runs as a server daemon that can run either system-wide or on per-user basis using a client/server architecture. The daemon by itself does nothing without its modules except to provide an API and host dynamically loaded modules. The audio routing and processing tasks are all handled by various modules, including PulseAudio's native protocol itself (provided by module-native-protocol-unix). Clients reach the server through one of many protocol modules that will accept audio from external sources, route it through PulseAudio and eventually have it go out through a final other module. The output module does not have to be an actual sound output: it can dump the stream into a file, stream it to a broadcasting server such as Icecast, or even just discard it.

You can find a detailed list of all available modules at PulseAudio Loadable Modules. To enable them you can just add a line load-module module-name-from-list to ~/.config/pulse/default.pa.

PulseAudio will first look for configuration files in the home directory ~/.config/pulse/, and if they are not found, the system-wide configuration from /etc/pulse/ will be applied.

This is the main configuration file to configure the daemon itself. It defines base settings like the default sample rates used by modules, resampling methods, realtime scheduling and various other settings related to the server process. These can not be changed at runtime without restarting the PulseAudio daemon. The defaults are sensible for most users, see the pulse-daemon.conf(5) man page for additional information. Boolean options accepts any of these: true, yes, on and 1 as well as false, no, off and 0.

This file is a startup script and is used to configure modules. It is actually parsed and read after the daemon has finished initializing and additional commands can be sent at runtime using pactl(1) or pacmd(1). The startup script can also be provided on the command line by starting PulseAudio in a terminal using pulseaudio -nC. This will make the daemon load the CLI module and will accept the configuration directly from the command line, and output resulting information or error messages on the same terminal. This can be useful when debugging the daemon or just to test various modules before setting them permanently on disk. The manual page is quite self-explanatory, consult pulse-cli-syntax(5) for the details of the syntax.

The default configuration also loads module-gsettings to apply settings specified by paprefs.

This file is a startup script used in place of default.pa when PulseAudio is running in system-wide mode.

This is the configuration file read by every PulseAudio client application. It is used to configure runtime options for individual clients. It can be used to set and configure the default sink and source statically as well as allowing (or disallowing) clients to automatically start the server if not currently running. If autospawn is enabled, clients will automatically start PulseAudio if it is not already running when a client attempts to connect to it. This can be useful if you do not want PulseAudio to always be running to conserve system resources. Otherwise, you really should have it start with your X11 session.

The main command to configure a server during runtime is pacmd. Run pacmd --help for a list options, or just run pacmd to enter the shell interactive mode and Ctrl+d to exit. All modifications will immediately be applied.

Once your new settings have been tested and meet your needs, edit the default.pa accordingly to make the change persistent. See PulseAudio/Examples for some basic settings.

It is important to understand that the "sources" (processes, capture devices) and "sinks" (sound cards, servers, other processes) accessible and selectable through PulseAudio depend upon the current hardware "Profile" selected. These "Profiles" are those ALSA "pcms" listed by the command aplay -L, and more specifically by the command pacmd list-cards, which will include a line "index:", a list beginning "profiles:", and a line "active profile: <...>" in the output, among other things. "Profiles" correspond to different card input/output configurations, notably the number of available input/output channels.

The "active profile" can be set with the command pacmd set-card-profile INDEX PROFILE, with no comma separating INDEX and PROFILE, where INDEX is just the number on the line "index:" and a PROFILE name is everything shown from the beginning of any line under "profiles:" to just before the colon and first space, as shown by the command pacmd list-cards. For instance, pacmd set-card-profile 0 output:analog-stereo+input:analog-stereo.

It may be easier to select a "profile" with a graphical tool like pavucontrol, under the Configuration tab, or KDE System Settings, under the Sound tab. Each audio "card", which are those devices listed by the command aplay -l, or again by the command pacmd list-cards, will have its own selectable "profile". When a "profile" has been selected, the then available "sources" and "sinks" can be seen by using the commands pacmd list-sources and pacmd list-sinks. Note that the "index" of the available sources and sinks will change each time a card profile is changed.

The selected "Profile" can be an issue for some applications, especially the Adobe Flash players, typically /usr/lib/mozilla/plugins/libflashplayer.so and /usr/lib/PepperFlash/libpepflashplayer.so. Often, these Flash players will only work when one of the Stereo profiles is selected, and otherwise, will play video with no sound, or will simply "crash". When all else fails, you might try selecting a different profile.

Of course, when configuring some variation of Surround Sound in PulseAudio, the appropriate Surround profile will have to be selected, before Surround Sound will work, or in order to do things like remap the speaker channels.

If the only profile you seem to have is "HiFi", this means that you are using ALSA Use Case Manager profiles instead of PulseAudio profiles. See PulseAudio/Examples#Disabling UCM/"HiFi" for information on how to get back to using PulseAudio profiles.

Since PulseAudio runs as a daemon as the current user, clients needs to know where to find the daemon socket to connect to it as well as a shared random cookie file clients use to authenticate with it. By default, clients should be able to locate the daemon without problem using environment variables, X11 root window properties, the default-server option in client.conf and finally by trying the default location $XDG_RUNTIME_DIR/pulse/native (typically unix:/run/user/user-id/pulse/native). However, if you have clients that needs to access PulseAudio outside of your X11 session like mpd running as a different user, you will need to tell it how to connect to your PulseAudio instance. See PulseAudio/Examples#Allowing multiple users to share a PulseAudio daemon for a complete example. An authentication cookie containing random bytes is enabled by default to ensure audio does not leak from one user to another on a multi-user system. If you already control who can access the server using user/group permissions, you can disable the cookie by passing auth-cookie-enabled=0 to module-native-protocol-unix.

These two variables are the important ones in order for libpulse clients to locate PulseAudio if you moved its socket to somewhere else. See pulseaudio(1) for more details and other useful environment variables clients will read.

When using SSH X11 forwarding (i.e. when the DISPLAY and SSH_CONNECTION environment variables are present), libpulse clients also use window properties on the root window of the X11 server to help find the daemon.

X11 properties can be queried using xprop -root, or with pax11publish -d to read pulse-specific properties. pax11publish can also be used to update the properties from environment variables (pax11publish -e, or pax11publish -r to remove them entirely). If possible, it is recommended to let PulseAudio do it by itself using the module-x11-publish module or the start-pulseaudio-x11 command.

PulseAudio on Arch has pulseaudio.socket enabled by default for the systemd/User instance. This means that PulseAudio will automatically start when needed.

For more information, see PulseAudio: Running.

Stop the pulseaudio.socket and pulseaudio.service user units.

If you have applications that do not support PulseAudio explicitly but rely on ALSA, these applications will try to access the sound card directly via ALSA and will therefore bypass PulseAudio. PulseAudio will thus not have access to the sound card any more. As a result, all applications relying on PulseAudio will not be working any more, leading to this issue. To prevent this, you will need to install the pulseaudio-alsa package. It contains the necessary /etc/alsa/conf.d/99-pulseaudio-default.conf for configuring ALSA to use PulseAudio. Also make sure that ~/.asoundrc does not exist, as it would override the /etc/asound.conf file.

Also install lib32-libpulse and lib32-alsa-plugins if you run a x86_64 system and want to have sound for 32-bit multilib programs like Wine and Steam.

To prevent applications from using OSS emulation and bypassing PulseAudio (thereby preventing other applications from playing sound), make sure the module snd_pcm_oss is not being loaded at boot. If it is currently loaded (lsmod | grep oss), disable it by executing:

To enable PulseAudio DTS (Digital Theater System) via ALSA install dcaencAUR package and enable it:

Finally restart PulseAudio. If experience volume issues with your DTS device and/or PulseAudio, you may fix it by looking for more setting option at dcaenc's GitLab.

Although pulseaudio-alsa contains the necessary configuration file to allow ALSA applications to use PulseAudio's default device, ALSA's pulse plugin is more versatile than that:

The source code can be read to know all available options.

You may want to use ALSA directly in most of your applications while still being able to use applications which require PulseAudio at the same time. The following steps allow you to make PulseAudio use dmix instead of grabbing ALSA hardware device.

There are multiple ways of making OSS-only programs output to PulseAudio:

Install the ossp package and start osspd.service.

Programs using OSS can work with PulseAudio by starting it with padsp(1) (included with libpulse):

You can also add a custom wrapper script like this:

Make sure /usr/local/bin comes before /usr/bin in your PATH.

gst-plugins-good provides the pulseaudio plugin for applications using GStreamer.

OpenAL Soft should use PulseAudio by default, but can be explicitly configured to do so:

Edit the libao configuration file:

Be sure to remove the dev=default option of the alsa driver or adjust it to specify a specific PulseAudio sink name or number.

PulseEffects is a GTK advanced utility for applying several audio effects (e.g. Noise reduction, Equalizer etc.) to audio input and output.

You may need to also install its optional dependency lsp-plugins in order to get plugins to work. If PulseEffects plugins are greyed out after installing plugins, trying to start the daemon produces an error, or no devices are shown in the Settings > PulseAudio tab, consider clearing the cache as shown in [4].

A collection of PulseEffects presets can be found in community presets.

If you want to use a different equalizer rather that the one integrated in #PulseEffects, there are the following options.

Install pulseaudio-equalizer-ladspa, an equalizer based on LADSPA swh-plugins. Launch pulseaudio-equalizer-gtk GUI and tweak the parameters to match your expectations.

PulseAudio has an integrated 10-band equalizer system. In order to use it, install pulseaudio-equalizer and load module-equalizer-sink:

Also load module-dbus-protocol if your configuration does not load it by default.

To start the GUI, run qpaeq.

To load the equalizer module on every boot, create a .pa file in /etc/pulse/default.pa.d/ or edit ~/.config/pulse/default.pa and add the following lines:

Dynamic range compression can be done with #PulseEffects, however PulseEffects might introduce much overhead and latency to audio stream, so if you only need a compression effect and a minor load on the system, other options are available using a module-ladspa-sink.

Steve Harris LADSPA is a set of plugins containing various compression modules. Install swh-plugins and edit the configuration as the following:

You have to specify your card sink name, get it from pacmd list-sinks. In order to apply the changes, stop and restart PulseAudio. The above configuration has empty control options using the default values.

To tweak the module with custom control parameters, fill them respecting the right order.

Other plugins can be found in Steve Harris' LADSPA Plugin Documentation.

For a more professional compressor, you can use the one developed by Calf Studio Gear[dead link 2025-08-16—SSL error]. Install calf-ladspaAUR and edit the configuration as the following

The plugin has 11 control options. If you want to insert custom values, read the following table and do not forget to specify them in the right order.

Arch does not load the PulseAudio echo-cancellation module by default, therefore, we have to add it in /etc/pulse/default.pa.d/. First you can test if the module is present with pacmd and entering list-modules. If you cannot find a line showing name: <module-echo-cancel> you have to create:

then restart PulseAudio:

and check if the module is activated by starting pavucontrol. Under Recording, the input device should show Echo-Cancel Source Stream from.

Turning on beamforming=1 in the aec_args can also significantly reduce background noise if you have more than one microphone (which is common on many new laptops). However, beamforming requires specifying your mic_geometry (see below).

If you want existing streams to be automatically moved to the new sink and source, you have to load the module-switch-on-connect with ignore_virtual=no before.

This article or section is out of date.

Here is a list of possible 'aec_args' for 'aec_method=webrtc' with their default values [6][7]:

If you are using the module-echo-cancel, you probably do not want other applications to do additional audio post processing. Here is a list for disabling audio post processing in following applications:

Since the module-echo-cancel is not always needed, or must be reloaded if the source_master or sink_master has changed, it is nice to have a easy way to load or reload the module-echo-cancel.

Create the following script and make it executable:

To run the script easily from the graphical environment, you can create a desktop launcher for it.

Installing the package noise-suppression-for-voice will allow real-time noise suppression based on RNNoise: Learning Noise Suppression [12]. Configuration details can be found on the projects Github site [13]. One can install Cadmus (cadmus-debAUR or cadmus-appimageAUR) which is a GUI frontend for @werman's PulseAudio real-time noise suppression plugin.

Another alternative is noisetorchAUR which is also built on top of RNNoise. There is not only input noise cancellation but also an output.

Refer to QEMU#Creating an audio backend for a detailed guide on how to configure PulseAudio within QEMU.

Install pulseaudio-alsa and make alsamixer.appAUR dockapp for the windowmakerAUR use PulseAudio, e.g.:

Here is a two examples where the first one is for ALSA and the other one is for PulseAudio. You can run multiple instances of it. Use the -w option to choose which of the control buttons to bind to the mouse wheel.

See #ALSA for details.

Make it switch to PulseAudio output:

To make XMMS2 use a different output sink, e.g.:

See also the official guide.

PulseAudio will automatically be used by KDE / Qt applications. It is supported by default in the KDE sound mixer. For more information see the KDE page in the PulseAudio wiki.

If the phonon-gstreamer backend is used for Phonon, GStreamer should also be configured as described in #GStreamer.

Audacious natively supports PulseAudio. In order to use it, set File > Settings > Audio > Output plugin > PulseAudio Output.

Configure MPD to use PulseAudio. See also Music Player Daemon/Tips and tricks#PulseAudio.

MPlayer natively supports PulseAudio output with the -ao pulse option. It can also be configured to default to PulseAudio output, in ~/.mplayer/config for per-user, or /etc/mplayer/mplayer.conf for system-wide:

mpv supports PulseAudio same as written for #MPlayer. Configuration in ~/.config/mpv/mpv.conf per-user, or /etc/mpv/mpv.conf system-wide.

guvcview when using the PulseAudio input from a Webcam may have the audio input suspended resulting in no audio being recorded. You can check this by executing:

If the audio source is "suspended" then create the folowing .pa file:

And then either restarting PulseAudio or your computer will only idle the input source instead of suspending it. guvcview will then correctly record audio from the device.

This article or section is a candidate for merging with PulseAudio/Examples#PulseAudio over network.

One of PulseAudio's unique features is its ability to stream audio from clients over TCP to a server running the PulseAudio daemon reliably within a LAN. Ensure that client and server systems agree on the time (i.e., use NTP), or audio streams may be choppy or may not work at all. For a more detailed guide visit the official PulseAudio documentation.

To enable the TCP module on the server (the computer that actually outputs sound), create the following .pa file:

Or you can use the paprefs GUI application (root is not required). Go to Network Server > Enable network access to local sound devices.

To make sure module-native-protocol-tcp is loaded on the server, you can use:

It is a requirement that both the client and server share the same cookie. Ensure that the clients and server share the same cookie file found under ~/.config/pulse/cookie. It does not matter whose cookie file you use (the server or a client's), just that the server and client(s) share the same one.

If it is undesirable to copy the cookie file from clients, anonymous clients can access the server by passing auth-anonymous to module-native-protocol-tcp on the server (again in /etc/pulse/default.pa.d/):

It is also possible to authenticate based on client IP address:

Change the LAN IP subnet to match that of those clients you wish to have access to the server.

The PulseAudio daemon normally starts as a user service when a user logs in and attempts to play some sort of audio. For running a dedicated PulseAudio server accepting client connections over TCP, the daemon must be started on boot as a system service. Note that in most desktop use cases, system mode likely is not the right choice.

To run PulseAudio in a system mode, first we need to set up users and groups needed by system-wide PulseAudio server instance [14]:

Create the service pulseaudio.service in /etc/systemd/system containing the following:

Then enable pulseaudio.service at the system level. You will also need to disable the user-level PulseAudio service across the whole system by masking the pulseaudio.socket with the --global flag.

This is necessary even if you are accessing the system over SSH, to make sure the user-level PulseAudio service will never start.

When PulseAudio starts in the system mode, /etc/pulse/system.pa is used instead of default.pa, so be sure to put any necessary settings in system.pa.

For a single shell or command you can set the PULSE_SERVER environment variable to the host name or IP address of the desired PulseAudio server:

Alternatively, you can create or modify ~/.config/pulse/client.conf or /etc/pulse/client.conf to set a default-server persistently:

It is also possible to specify multiple servers separated by spaces which are subsequently tried by PulseAudio[16]:

This article or section is a candidate for merging with PulseAudio/Examples.

Map the following commands to your volume keys: XF86AudioRaiseVolume, XF86AudioLowerVolume and XF86AudioMute.

First find out which sink corresponds to the audio output you would like to control. To list available sinks:

Suppose sink 0 is to be used, to raise the volume:

To mute/unmute the volume:

To mute/unmute the microphone:

Replace user with the user running PulseAudio:

To get PulseAudio to handle X11 bell events, run the following commands after the X11 session has been started:

Or use configuration files /etc/pulse/default.pa.d/ or ~/.config/pulse/default.pa:

To adjust the volume of the X11 bell, run the following command:

100 is a percentage. This requires the xorg-xset package. See Autostarting for a way to run these commands automatically when the X11 session is started.

The switch-on-connect module switches the output sound to new devices when connected. For example, if you plug in a USB headset, the output will be switched to that. If you unplug it, the output will be set back to the last device.

This module is disabled by default for being too aggressive, but can be enabled by adding the line load-module module-switch-on-connect to your ~/.config/pulse/default.pa.

Some sound cards present the option of multiple analog outputs, being switchable through using PulseAudio ports. But switching manually can become a chore, so you can automate this with the pactl command.

List available ports:

Current port can be obtained through:

Switch the active port:

This process can be automated through a simple script. This script then can be given a shortcut by the user:

This script is intended to swap between two ports. First checking the current port then swapping it. Users might need to change the sink index number and the port names to fit their machine.

When entering a voice call (e.g. in Microsoft Teams, maybe others too) any media applications might be muted. To disable this behaviour you can simply disable this module in PulseAudio configuration:

See PulseAudio/Examples.

See PulseAudio/Troubleshooting.

**Examples:**

Example 1 (unknown):
```unknown
load-module module-name-from-list
```

Example 2 (unknown):
```unknown
~/.config/pulse/default.pa
```

Example 3 (unknown):
```unknown
~/.config/pulse/
```

Example 4 (unknown):
```unknown
/etc/pulse/
```

---

## Kernel module

**URL:** https://wiki.archlinux.org/title/Kernel_modules

**Contents:**
- Obtaining information
- Automatic module loading
  - Early module loading
  - systemd
- Manual module handling
- Setting module options
  - Using modprobe
  - Using modprobe.d
  - Using kernel command line
- Aliasing

Kernel modules are pieces of code that can be loaded and unloaded into the kernel upon demand. They extend the functionality of the kernel without the need to reboot the system.

To create a kernel module, you can read The Linux Kernel Module Programming Guide. A module can be configured as built-in or loadable. To dynamically load or remove a module, it has to be configured as a loadable module in the kernel configuration (the line related to the module will therefore display the letter M).

To rebuild a kernel module automatically when a new kernel is installed, see Dynamic Kernel Module Support (DKMS).

Usually modules depend on the kernel release and are stored in the /usr/lib/modules/kernel_release/ directory.

To show what kernel modules are currently loaded:

To show information about a module:

To list the options that are set for a loaded module use systool(1) from sysfsutils:

To display the comprehensive configuration of all the modules:

To display the configuration of a particular module:

List the dependencies of a module (or alias), including the module itself:

Today, all necessary modules loading is handled automatically by udev, so if you do not need to use any out-of-tree kernel modules, there is no need to put modules that should be loaded at boot in any configuration file. However, there are cases where you might want to load an extra module during the boot process, or blacklist another one for your computer to function properly.

Early module loading depends on the initramfs generator used:

Kernel modules can be explicitly listed in files under /etc/modules-load.d/ for systemd to load them during boot. Each configuration file is named in the style of /etc/modules-load.d/program.conf. Configuration files simply contain a list of kernel modules names to load, separated by newlines. Empty lines and lines whose first non-whitespace character is # or ; are ignored.

See modules-load.d(5) for more details.

Kernel modules are handled by tools provided by the kmod package, which is installed as a dependency of a kernel package. You can use these tools manually. To load a module:

To load a module by a file name—i.e. one that is not installed in the /usr/lib/modules/kernel_release/ directory—use any of:

To unload—remove—a module, use any of:

To pass a parameter to a kernel module, you can pass them manually with modprobe or assure certain parameters are always applied using a modprobe configuration file or by using the kernel command line. If the module is built into the kernel, the kernel command line must be used and other methods will not work.

The basic way to pass parameters to a module is using the modprobe command. Parameters are specified on command line using simple key=value assignments:

Configuration files in the /etc/modprobe.d/ directory can be used to pass module settings to udev, which will use modprobe to manage the loading of the modules during system boot. Files in this directory can have any name, given that they end with the .conf extension. The file name matters, see modprobe.d(5) § CONFIGURATION DIRECTORIES AND PRECEDENCE. To show the effective configuration:

Multiple module parameters are separated by spaces, in turn a parameter can receive a list of values which is separated by commas:

You can also pass options to the module using the kernel command line. This is the only working option for modules built into the kernel. For all common boot loaders, the following syntax is correct:

Simply add this to the appropriate line in your boot loader configuration, as described in Kernel parameters#Boot loader configuration.

Aliases are alternate names for a module. For example: alias my-mod really_long_modulename means you can use modprobe my-mod instead of modprobe really_long_modulename. You can also use shell-style wildcards, so alias my-mod* really_long_modulename means that modprobe my-mod-something has the same effect. Create an alias:

Aliases can be internal—contained in the module itself. Internal aliases are usually used for #Automatic module loading when it is needed by an application, e.g. when the kernel detects a new device. To see the module internal aliases:

To see both configured and internal aliases:

Blacklisting, in the context of kernel modules, is a mechanism to prevent the kernel module from loading. This could be useful if, for example, the associated hardware is not needed, or if loading that module causes problems: for instance there may be two kernel modules that try to control the same piece of hardware, and loading them together would result in a conflict.

Some modules are loaded as part of the initramfs. mkinitcpio -M will print out all automatically detected modules: to prevent the initramfs from loading some of those modules, blacklist them in a .conf file under /etc/modprobe.d and it shall be added in by the modconf hook during image generation. Running mkinitcpio -v will list all modules pulled in by the various hooks (e.g. filesystems hook, block hook, etc.). Remember to add that .conf file to the FILES array in /etc/mkinitcpio.conf if you do not have the modconf hook in your HOOKS array (e.g. you have deviated from the default configuration), and once you have blacklisted the modules regenerate the initramfs, and reboot afterwards.

Disable an alias by overriding. For example, to prevent Bluetooth module autoloading (assuming a module named off does not exist):

To disable all internal aliases for a given module use the blacklist keyword. For example, to prevent the pcspkr module from loading on boot to avoid sounds through the PC speaker:

There is a workaround for the behaviour described in the #alias and #blacklist notes. The install configuration command instructs modprobe to run a custom command instead of inserting the module in the kernel as normal, so you can simulate the successful module loading with:

You can force the module to always fail loading with /bin/false: this will effectively prevent the module—and any other that depends on it—from loading by any means, and a log error message may be produced.

You can also blacklist modules from the boot loader boot entry configuration.

Simply add module_blacklist=module_name_1,module_name_2,module_name_3 to your kernel command line, as described in Kernel parameters#Boot loader configuration.

Another use case for a command line option is to disable hardware-specific components of a module without disabling the module entirely. For example, disabling a microphone while retaining other sound out options. See BBS#303475 for a few examples.

In case a specific module does not load and the boot log (accessible by running journalctl -b as root) says that the module is blacklisted, but the directory /etc/modprobe.d/ does not show a corresponding entry, check another modprobe source directory at /usr/lib/modprobe.d/ for blacklisting entries.

A module will not be loaded if the "vermagic" string contained within the kernel module does not match the value of the currently running kernel. If it is known that the module is compatible with the current running kernel the "vermagic" check can be ignored with modprobe --force-vermagic.

**Examples:**

Example 1 (unknown):
```unknown
/usr/lib/modules/kernel_release/
```

Example 2 (unknown):
```unknown
uname --kernel-release
```

Example 3 (unknown):
```unknown
/etc/modprobe.d/
```

Example 4 (unknown):
```unknown
$ modinfo module_name
```

---

## Rescached

**URL:** https://wiki.archlinux.org/title/Rescached

**Contents:**
- Features
- How cache in rescached works
- Installation
- Post-installation configuration
- Integration with OpenResolv
- Web user interface
- See also

rescached is a daemon that caching internet name and address on local memory for speeding up DNS resolution.

rescached is not a reimplementation of a DNS server like BIND. The primary goal of rescached is only to cache DNS queries and answers to minimize unneeded traffic to the outside network. It is intended for personal systems or serving a small group of users.

List of current features,

Each DNS record in cache have the time last accessed field, which defined how the cache will be ordered in memory. The last queried host-name will be at the bottom of cache list, and the oldest queried host-name will at the top of cache list.

The following table illustrate list of caches in memory,

Every cache.prune_delay (let say every 5 minutes), rescached will try to pruning old records from cache. If the accessed-at value of record in cache is less than current-time + cache.threshold (remember that "cache.threshold" value must be negative) it will remove the record from cache.

Install the rescached-gitAUR package.

The default configuration enables a direct start of the daemon.

Rescached configuration resides in /etc/rescached/rescached.cfg. Select entries to change are:

After editing the configuration file, modify the resolv.conf file and replace the current set of resolver addresses with loopback address

Other programs may overwrite this setting; see Domain name resolution#Overwriting of /etc/resolv.conf for details.

Finally, enable and start rescached.service.

Rescached can detect change on resolv.conf file generated by resolvconf. To use this feature set the file.resolvconf to /etc/rescached/resolv.conf in configuration file. In /etc/resolvconf.conf set either dnsmasq_resolv, pdnsd_resolv, or unbound_conf to point to the same file /etc/rescached/resolv.conf.

The rescached service provide a web user interface that can be accessed at http://127.0.0.1:5380. The following user interfaces are available on the latest release,

The front page allow user to monitor active caches, query the caches, and removing the caches.

The Environment page allow user to modify the rescached configuration on the fly.

The Hosts Blocks page allow user to enable or disable the external sources of hosts blocks list.

The Hosts.d page allow user to manage hosts file, creating new hosts file, create new record, or delete a record.

The Zone.d page allow user manage zone file, creating new zone file, adding or deleting new resource record in the zone file.

**Examples:**

Example 1 (unknown):
```unknown
/etc/rescached/hosts.d/
```

Example 2 (unknown):
```unknown
/etc/rescached/hosts.d/
```

Example 3 (unknown):
```unknown
/etc/rescached/zone.d/
```

Example 4 (unknown):
```unknown
cache.prune_delay
```

---

## systemd/Sandboxing

**URL:** https://wiki.archlinux.org/title/Systemd/Sandboxing

**Contents:**
- General
- Capabilities
- Common directives
  - Without special configuration
  - Configurable directives
- Advanced directives
  - chroot jail
- system.conf
  - Disabling non-native syscalls
  - Enabling more unit statistics

systemd enables users to harden and sandbox system service units. Because of technical limitations, and ironically security reasons, user units can not be hardened or sandboxed properly since this would make privilege escalation issues possible. This does not affect system units which use the User= directive.

Because of the nature of other unit types, only service units can be hardened/sandboxed in the traditional sense. See systemd.exec(5) and systemd.resource-control(5) for more information.

Since hardening/sandboxing effectively restricts an application, it is not possible to use all the sandboxing directives. A web server for example should not use PrivateNetwork=true since it usually needs network access.

systemd-analyze security unit generates a score for the unit showing all the used directives, which can be helpful to determine what settings to try next.

Unfortunately, systemd's error messages on misconfiguration relating to sandboxing are sometimes vague and/or misleading. Setting the log level temporarily to debug may help getting actually relevant information.

capabilities(7) are used to grant a process certain elevated privileges. For example, CAP_NET_BIND_SERVICE can be used so that an otherwise unprivileged process can bind ports below 1024, eliminating the need for it to start with root privileges at all. Another notable example is CAP_DAC_READ_SEARCH to grant a backup program unrestricted read access to all locations.

In service units, you can accomplish this by using AmbientCapabilities= to grant it capabilities and CapabilityBoundingSet= to ensure nothing beyond the intended scope is granted:

Most of these directives can be applied to most applications without causing too many problems.

Simple boolean settings which can either be enabled or not. They can not be configured.

Most of these directives are quite powerful and will affect a lot. It is recommended to consider using at least some of them.

These directives are not useful for most units and are thus used more rarely.

It is possible to severely restrict what a process can see by specifying TemporaryFileSystem=/:ro and mounting required paths into this chroot-like jail. RootDirectory requires a directory to be present, whereas TemporaryFileSystem does not and will override / seamlessly. Both, and especially the latter, appear to be secure chroot-like directives, which can not be broken out easily, as they do not use the chroot syscall.

All required paths must be mounted into this jail via BindReadOnlyPaths and BindPaths:

This is a minimal example and most application will need more paths whitelisted. Some common paths include:

It will be likely that debugging is at some point necessary when trying to sandbox a unit for the first time. If a unit can not be started at all and fails with status=203/EXEC, either the executable itself or required libraries are not accessible. Starting with broad paths at first (e.g allowing the entirety of /usr) and narrowing it down later can help, too.

Changes to /etc/systemd/system.conf are global, so they will affect every unit. See systemd-system.conf(5)

Non-native binaries, in almost all cases 32-bit binaries, may partially compromise the security of the system because they do not have access to more hardening. There have been some relatively minor vulnerabilities, like CVE-2009-0835, which affected non-native syscalls.

This works well on most systems, but it needs to be at least partially disabled if e.g multilib is in use. Especially gaming with Wine may be impacted. Using systemd-run or modifying the session slice to override SystemCallArchitectures can be used to disable restrictions partially.

systemd does not track all resource usage of a unit by default. Enable Default*Accounting to get more statistics in the systemctl status output and the journal. This is not strictly a security setting, but it will certainly make debugging easier and can provide useful insights into resource usage.

**Examples:**

Example 1 (unknown):
```unknown
PrivateNetwork=true
```

Example 2 (unknown):
```unknown
systemd-analyze security unit
```

Example 3 (unknown):
```unknown
Hello world
```

Example 4 (unknown):
```unknown
# systemctl log-level debug
```

---

## cgroups

**URL:** https://wiki.archlinux.org/title/Cgroups

**Contents:**
- Installing
- With systemd
  - Hierarchy
  - Find cgroup of a process
  - cgroup resource usage
  - Custom cgroups
  - As service
    - Service unit file
    - Grouping unit under a slice
  - As root

Control groups (or cgroups as they are commonly known) are a feature provided by the Linux kernel to manage, restrict, and audit groups of processes. Compared to other approaches like the nice(1) command or /etc/security/limits.conf, cgroups are more flexible as they can operate on (sub)sets of processes (possibly with different system users).

Control groups can be accessed with various tools:

For Arch Linux, systemd is the preferred and easiest method of invoking and configuring cgroups as it is a part of the default installation.

Make sure you have one of these packages installed for automated cgroup handling:

Current cgroup hierarchy can be seen with systemctl status or systemd-cgls command.

The cgroup name of a process can be found in /proc/PID/cgroup.

For example, the cgroup of the shell:

The systemd-cgtop command can be used to see the resource usage:

systemd.slice(5) systemd unit files can be used to define a custom cgroup configuration. They must be placed in a systemd directory, such as /etc/systemd/system/. The resource control options that can be assigned are documented in systemd.resource-control(5).

This is an example slice unit that only allows 30% of one CPU to be used:

Remember to do a daemon-reload to pick up any new or changed .slice files.

Resources can be directly specified in service definition or as a drop-in file:

This example limits the service to 1 gigabyte.

Service can be specified what slice to run in:

systemd-run can be used to run a command in a specific slice.

--uid=username option can be used to spawn the command as specific user.

The --shell option can be used to spawn a command shell inside the slice.

Unprivileged users can divide the resources provided to them into new cgroups, if some conditions are met.

Cgroups v2 must be utilized for a non-root user to be allowed managing cgroup resources.

Not all resources can be controlled by user.

For user to control cpu and io resources, the resources need to be delegated. This can be done with a drop-in file.

For example if your user id is 1000:

Reboot and verify that the slice your user session is under has cpu and io controller:

The user slice files can be placed in ~/.config/systemd/user/.

To run the command under certain slice:

You can also run your login shell inside the slice:

cgroups resources can be adjusted at run-time using systemctl set-property command. Option syntax is the same as in systemd.resource-control(5).

For example, cutting off internet access for all user sessions:

One layer lower than management with systemd is the cgroup virtual file system. "libcgroup" provides a library and utilities for making management easier, so we will use them here as well.

The reason for using the lower level is simple: systemd does not provide an interface for every single interface file in cgroups and nor should it be expected for it to provide them at any point in the future. It is completely harmless to read from them for additional insights on a cgroup's resource use.

One cgroup should only have one set of programs writing to it to avoid race conditions, the "single-writer rule". This is not enforced by the kernel, but following this recommendation prevents hard-to-debug issues from happening. To set the boundary at which systemd stops managing child cgroups, see the Delegate= property. Otherwise do not be surprised if system to overwrites what you have set.

One of the powers of cgroups is that you can create "ad-hoc" groups on the fly. You can even grant the privileges to create custom groups to regular users. groupname is the cgroup name:

Now all the tunables in the group groupname are writable by your user:

Cgroups are hierarchical, so you can create as many subgroups as you like. If a normal user wants to make new subgroup called foo:

As previously mentioned, only one thing should write to a cgroup at any point. This does not affect non-write operations including spawning new processes inside a group, moving processes to a group, or reading properties from cgroup files.

libcgroup contains a simple tool for running new processes inside a cgroup. If a normal user wants to run a bash shell under a our previous groupname/foo:

Inside of the shell, we can confirm which cgroup it belongs to with:

This makes use of /proc/$PID/cgroup, a file that exists in every process. Manually writing to the file causes the cgroup to change as well.

To move all 'bash' commands to this group:

Internally (i.e. without cgclassify the kernel provides two ways to move processes between cgroups. These two are equivalent:

A new subdirectory is crated for groupname/foo at its creation, located at /sys/fs/cgroup/groupname/foo. These files can be read and written to change the group's properties. (Again, writing is not recommended unless delegation is done!)

Let us try to see how much memory all the processes in our group is taking up:

To limit the RAM (not swap) usage of all processes, run the following:

To change the CPU priority of this group (the default is 100):

You can find more tunables or statistics by listing the cgroup directory.

If you want your cgroups to be created at boot, you can define them in /etc/cgconfig.conf instead. This causes a service booted at launch to configure your cgroups. See the relevant manual page about the syntax of this file; we will make no instruction on how to use a truly deprecated mechanism.

The following example shows a cgroup that constrains a given command to 2GB of memory.

The following example shows a command restricted to 20% of one CPU core.

Doing large calculations in MATLAB can crash your system, because Matlab does not have any protection against taking all your machine's memory or CPU. The following examples show a cgroup that constrains Matlab to first 6 CPU cores and 5 GB of memory.

Launch Matlab like this (be sure to use the right path):

For commands and configuration files, see relevant man pages, e.g. cgcreate(1) or cgrules.conf(5)

Before our current cgroup v2 there was an earlier version called v1. V1 allowed a lot of additional flexibility including a non-unified hierarchy and thread-granular management. This was, in retrospect, a bad idea (see the rationales for v2):

To avoid further chaos, cgroup v2 has two key design rules on top of the removal of features:

Before systemd v258, the kernel parameters SYSTEMD_CGROUP_ENABLE_LEGACY_FORCE=1 systemd.unified_cgroup_hierarchy=0 could be used to force booting with cgroup-v1 (the first parameter was added in v256 to make it harder to use cgroup-v1). However, this feature has now been removed. It is still worth knowing about because some software like to put systemd.unified_cgroup_hierarchy=0 in your kernel command-line without telling you, causing your entire system to break.

**Examples:**

Example 1 (unknown):
```unknown
/etc/security/limits.conf
```

Example 2 (unknown):
```unknown
/etc/cgrules.conf
```

Example 3 (unknown):
```unknown
cgconfig.service
```

Example 4 (unknown):
```unknown
cgconfig.conf
```

---

## GDM

**URL:** https://wiki.archlinux.org/title/GDM

**Contents:**
- Installation
- Starting
  - Autostarting applications
- Configuration
  - Login screen background image
  - dconf configuration
    - Login screen logo
    - Changing the cursor theme
    - Changing the icon theme
    - Larger font for log-in screen

From GDM - GNOME Display Manager: "The GNOME Display Manager (GDM) is a program that manages graphical display servers and handles graphical user logins."

Display managers provide X Window System and Wayland users with a graphical login prompt.

GDM can be installed with the gdm package, and is installed as part of the gnome group.

To start GDM at boot time, enable gdm.service.

To automatically start applications after logging in, follow the instructions in Autostarting#On desktop environment startup that pertain to your desktop environment.

The factual accuracy of this article or section is disputed.

Firstly, you need to extract the existing GNOME Shell theme to a directory in your home directory. You can do this using the following script:

Navigate to the created directory. You should find that the theme files have been extracted to it (under a theme subfolder). Now copy your preferred background image to this directory.

Next, you need to create a file under the theme directory with the following content (the list below is for Gnome 47, just in case verify that the contents of the theme subfolder match the list here, including the background image):

Replace filename with the filename of your background image or remove the line to use a hex color value instead.

Now, open the gnome-shell-light.css and gnome-shell-dark.css files and change the #lockDialogGroup definition as follows:

Set background-size to the resolution that GDM uses; this might not necessarily be the resolution of the image. For a list of display resolutions, see Display resolution.

Again, set filename to be the name of the background image, filename should be within quotes in the CSS. background-size can also be set to auto if you want the image scaled to fill.

If using multiple monitors, the image ends up spanning across the monitors, so it might be better to use an SVG file for the background.

If you only want to change the background color, adjust the #lockDialogGroup definition as follows:

where color is the new hex-encoded background color.

Next, compile the theme using the following command:

Then, copy the resulting gnome-shell-theme.gresource file to the /usr/share/gnome-shell directory (keep a backup of the original).

Finally, logout and you should find that gdm is using your preferred background image (you might need to restart gdm if the changes aren't applied immediately).

If a subsequent update resets the gnome-shell-theme.gresource file, simply repeat the above steps, verifying that the contents of the XML match the extracted file list.

For more information, please see the following forum thread. A shell script to automate the above steps is available on DimaZirix's github repository.

Some GDM settings are stored in a dconf database. They can be configured either by adding keyfiles to the /etc/dconf/db/gdm.d directory and then recompiling the GDM database by running dconf update as root or by logging into the GDM user on the system and changing the setting directly using the gsettings command line tool. Note that for the former approach, a GDM profile file is required—this must be created manually as it is no longer shipped upstream, see below:

For the latter approach, you can log into the GDM user with the command below:

Create the following keyfile:

Then recompile the GDM database.

Alternatively, execute the following as the GDM user to change the logo:

To disable the logo, you can set the value to an empty string:

GDM disregards GNOME cursor theme settings and it also ignores the cursor theme set according to the XDG specification. To change the cursor theme used in GDM, create the following keyfile:

Then recompile the GDM database. Alternatively, execute the following as the GDM user temporarily and change the cursor theme:

The same methods can be used to change the icon theme. Create the following keyfile:

Then, recompile the GDM database. Alternatively, execute the following as the GDM user temporarily and change the icon theme:

Click on the accessibility icon at the top right of the screen (a white circle with the silhouette of a person in the centre) and check the Large Text option.

To set a specific scaling factor, create the following keyfile:

Then recompile the GDM database. Alternatively, execute the following as the GDM user temporarily and change the font:

This tweak disables the audible feedback heard when the system volume is adjusted (via keyboard) on the login screen.

Create the following keyfile:

Then recompile the GDM database. Alternatively execute the following as the GDM user temporarily and turn off the sound:

Create the following keyfile:

Then recompile the GDM database. Alternatively, execute the following as the GDM user temporarily and configure the behavior:

where action can be one of nothing, suspend or hibernate.

Tap-to-click is disabled in GDM (and GNOME) by default, but you can easily enable it with a dconf setting.

To enable tap-to-click, create the following keyfile:

Then recompile the GDM database. Alternatively, execute the following as the GDM user temporarily and enable the action:

This article or section is out of date.

To disable or enable the Accessibility Menu, create the following keyfile:

Then recompile the GDM database. Alternatively, execute the following as the GDM user temporarily and change the status:

The menu is disabled when the key is false, enabled when it is true.

To enable Night Light on GDM, run

GDM requires the XKBLAYOUT parameter to be set in /etc/vconsole.conf; without it it defaults to a standard us (qwerty) layout, i.e it will not honor the value set in KEYMAP.

One generally applicable way to do so is to use localectl --no-convert set-x11-keymap [keymap]: see Keyboard configuration in Xorg#Setting keyboard layout for details.

The system language will be applied to GDM. If a system has multiple users, it is possible to set a language for GDM different to the system language. In this case, firstly ensure that gnome-control-center is installed. Then, start gnome-control-center and choose Region & Language. In the header bar, check the Login Screen toggle button. Finally, click on Language and choose your language from the list. You will be prompted for your root password. Note that the Login Screen button will not be visible in the header bar unless multiple users are present on the system [5].

To enable automatic login with GDM, add the following to /etc/gdm/custom.conf (replace username with your own):

or for an automatic login with a delay:

You can set the session used for automatic login (replace gnome-xorg with desired session):

If you want to bypass the password prompt in GDM then simply add the following line on the first line of /etc/pam.d/gdm-password:

Then, add the group nopasswdlogin to your system. See User group for group descriptions and group management commands.

Now, add your user to the nopasswdlogin group and you will only have to click on your username to login.

When using a fingerprint to login, it will not unlock the keychain, so you will still be prompted for the keychain password. You might want to disallow login and keep the fingerprint to unlock your session. To do this, just disable fingerprint for the GDM user.

Execute the following as the GDM user temporarily and change this setting:

GDM uses polkit and logind to gain permissions for shutdown. You can shutdown the system when multiple users are logged in by setting:

You can find all available logind options (e.g. reboot-multiple-sessions) in org.freedesktop.login1(5).

It is not advised to login as root, but if necessary you can edit /etc/pam.d/gdm-password and add the following line before the line auth required pam_deny.so:

The file should look something like this:

You should be able to login as root after restarting GDM.

The users for the gdm user list are gathered by AccountsService. It will automatically hide system users (UID < 1000). To hide ordinary users from the login list create or edit a file named after the user to hide in /var/lib/AccountsService/users/ to contain at least:

Remote login can be configured via graphical interface by navigating to System > Remote Desktop > Remote Login in Gnome Settings app.

To display current status and credentials, the following command can be used:

To generate new TLS key and certificate:

If -n rdp-tls part is omitted, hostname will be used as the name instead.

To set TLS key and certificate:

Finally, to enable Remote Login:

Some desktop environments store display settings in ~/.config/monitors.xml, based on which xrandr commands are generated. GDM has a file serving a similar purpose located at /etc/xdg/monitors.xml (This is the global mutter configuration and will impact new GNOME session too).

If you have your monitors setup as you like (resolution, refresh rate, orientation, scaling, primary and so on) in ~/.config/monitors.xml and want GDM to honor those settings:

To automatically re-configure the monitor setup on each boot, use a drop-in file for gdm.service:

The relevant parts of monitors.xml for screen rotation and scaling are:

Changes will take effect on logout. This is necessary because GDM does not respect xorg.conf.

If you have modified Mutter's experimental-features setting (usual done for fractional scaling or Variable), make sure the dconf setup is done globally not just to you user, see fractional scaling. This due to these setting can make the monitors.xml incompatible, see GDM bug 1028. i

You can use the xhost command to configure X server access permissions.

For instance, to grant GDM the right to access the X server, use the following command:

To use Wayland in GDM with the NVIDIA driver, you must fulfill the three following conditions:

If, instead of GDM, a black screen appears, try disabling integrated graphics in your computer's BIOS settings.

In some cases, GNOME fails to start and transfers control back to GDM, which in turn causes the login screen to reappear. You may try setting the following environment variable as suggested in BBS#2126478:

If GDM starts up properly on boot, but fails after repeated attempts on logout, try adding this line to the daemon section of /etc/gdm/custom.conf:

See Xorg#Rootless Xorg.

The Wayland backend is used by default, and the Xorg backend is used only if the Wayland backend cannot be started. You may wish to use the Xorg backend instead if, for example:

To use the Xorg backend by default, uncomment the following line in /etc/gdm/custom.conf:

After removing gdm, systemd may report the following:

To remove this warning, login as root and delete the primary user gdm and then delete the group gdm:

Verify that gdm is successfully removed via pwck and grpck with root privileges. To round it off, you may want to double-check no unowned files for gdm remain.

GDM uses a separate dconf database to control power management. To apply your user's power settings, copy them to GDM's dconf database:

where username is your username.

To only disable auto-suspend on AC, run:

(To also disable auto-suspend on battery, run the command with battery instead of ac.)

Restart GDM to activate your changes.

For newer versions of GNOME (Tested on 49.1):

Supplementary forum links:

Wayland requires Kernel Mode Setting (KMS) running in order to work, and on some machines the GDM process start earlier than KMS, resulting in GDM unable to see Wayland and working only with X.Org. This might result in messages like the following showing up in your log:

Alternatively, the same issue may lead to GDM not appearing or monitor only displaying the TTY output.

You can solve this problem by starting KMS earlier. You may also wish to just verify that Wayland is enabled in the GDM config (see above).

At first, without an NVIDIA device, GDM starts and works normally on Wayland, but stops working once an NVIDIA eGPU is plugged in (or the nvidia module is loaded for other reasons). A typical symptom of the problem is a black screen with a blinking cursor upon logouts and GDM restarts and the following message in GDM's logs (accessed by running journalctl -u gdm -b as root):

The solution is the same as #GDM ignores Wayland and uses X.Org by default: Prevent /usr/lib/gdm-disable-wayland from running upon nvidia module loading.

Notice that GDM on Wayland will no longer work once /usr/lib/gdm-disable-wayland has run. This is because WaylandEnable=false has been written into /run/gdm/custom.conf, which overrides /etc/gdm/custom.conf. To fix the situation without a reboot, remove /run/gdm/custom.conf and then restart GDM.

See systemd/FAQ#Failure to enable unit due to preexisting symlink.

**Examples:**

Example 1 (unknown):
```unknown
gdm.service
```

Example 2 (unknown):
```unknown
extractgst.sh
```

Example 3 (unknown):
```unknown
#!/bin/sh
gst=/usr/share/gnome-shell/gnome-shell-theme.gresource
workdir=${HOME}/shell-theme

for r in `gresource list $gst`; do
	r=${r#\/org\/gnome\/shell/}
	if [ ! -d $workdir/${r%/*} ]; then
	  mkdir -p $workdir/${r%/*}
	fi
done

for r in `gresource list $gst`; do
        gresource extract $gst $r >$workdir/${r#\/org\/gnome\/shell/}
done
```

Example 4 (unknown):
```unknown
gnome-shell-theme.gresource.xml
```

---

## Network Time Protocol daemon

**URL:** https://wiki.archlinux.org/title/Network_Time_Protocol_daemon

**Contents:**
- Installation
- Configuration
  - Connection to NTP servers
  - Leap seconds file
  - NTP server mode
- Usage
  - Start ntpd at boot
  - Synchronize time once per boot
- Tips and tricks
  - Start ntpd on network connection

Network Time Protocol is the most common method to synchronize the software clock of a GNU/Linux system with internet time servers. It is designed to mitigate the effects of variable network latency and can usually maintain time to within tens of milliseconds over the public Internet. The accuracy on local area networks is even better, up to one millisecond.

The NTP Project provides a reference implementation of the protocol called simply NTP. This article further describes how to set up and run the NTP daemon, both as a client and as a server.

See System time#Time synchronization for other NTP implementations.

Install the ntp package. By default, ntpd works in client mode without further configuration. You can skip to #Usage if you want to use the Arch Linux default configuration file for it. For server configuration, see #NTP server mode.

The main daemon is ntpd, which is configured in /etc/ntp.conf. Refer to ntp.conf(5) for detail.

NTP servers are classified in a hierarchical system with many levels called strata: the devices which are considered independent time sources are classified as stratum 0 sources; the servers directly connected to stratum 0 devices are classified as stratum 1 sources; servers connected to stratum 1 sources are then classified as stratum 2 sources and so on.

It has to be understood that a server's stratum cannot be taken as an indication of its accuracy or reliability. Typically, stratum 2 servers are used for general synchronization purposes: if you do not already know the servers you are going to connect to, you should choose a server pool close to your location from the pool.ntp.org servers (alternative link).

Since ntp version 4.2.7.p465-2, Arch Linux uses its own default vendor pool of NTP servers provided by the NTP Pool Project (see FS#41700). Modify those to suit your needs, e.g. if you want to use your country's servers with an option:

The iburst option is recommended, and sends a burst of packets only if it cannot obtain a connection with the first attempt. The burst option always does this, even on the first attempt, and should never be used without explicit permission and may result in blacklisting.

In order for the system to be able to provide the International Atomic Time to an application that requests it, the list of leap seconds must be loaded. The list is part of the tzdata package and can be loaded by adding the following line to the NTP configuration file:

If setting up an NTP server, check that you have orphan mode enabled, so that, in case it loses internet access, it will continue serving time to the network; enable orphan mode using the tos configuration parameter (you can set up to stratum 15) so that it will never be used unless internet access is lost:

Next, define the rules that will allow clients to connect to your service (localhost is considered a client too) using the restrict command; you should already have a line like this in your file:

This restricts everyone from modifying anything and prevents everyone from querying the status of your time server: nomodify prevents reconfiguring ntpd (with ntpq or ntpdc), and noquery is important to prevent dumping status data from ntpd (also with ntpq or ntpdc).

You can also add other options:

If you want to change any of these, see the full docs for the "restrict" option in ntp.conf(5), the detailed ntp instructions and #Usage.

Following this line, you need to tell ntpd what to allow through into your server; the following line is enough if you are not configuring an NTP server:

If you want to force DNS resolution to the IPv6 namespace, write -6 before the IP address or host name (-4 forces IPv4 instead), for example:

Lastly, specify the drift file (which keeps track of your clock's time deviation) and optionally the log file location:

A very basic configuration file will look like this:

The package has a default client-mode configuration and its own user and group to drop root privileges after starting. If you start it from the console, you should always do so with the -u option:

The -u option is employed by the two included systemd services. These services also use the -g option, which disables a threshold (so-called panic-gate). Hence, they will synchonize time even in case the ntp-server's time exceeds the threshold deviation from the system clock.

Both services are tied to the system's resolver, and will start synchronizing when an active network connection is detected.

Enable the daemon with ntpd.service. See also #Running in a chroot.

Use ntpq to see the list of configured peers and status of synchronization:

The delay, offset and jitter columns should be non-zero. The servers ntpd is synchronizing with are prefixed by an asterisk. It can take several minutes before ntpd selects a server to synchronize with; try checking after 17 minutes (1024 seconds).

Alternatively, enable ntpdate.service to synchronize time once (option -q) and non-forking (option -n) per boot, instead of running the daemon in the background. This method is discouraged on servers, and in general on machines that run without rebooting for more than a few days.

If the synchronized time should be written to the hardware clock as well, configure the provided unit as described in systemd#Editing provided units before starting it:

ntpd can be started by your network manager, so that the daemon only runs when the computer is online.

Append the following lines to your netctl profile:

The ntpd daemon can be brought up/down along with a network connection through the use of NetworkManager's dispatcher scripts. The networkmanager-dispatcher-ntpdAUR package installs one, pre-configured to start and stop the ntpd service with a connection.

KDE can use NTP (ntp must be installed) by right clicking the clock and selecting Adjust date/time. However, this requires the ntp daemon to be disabled before configuring KDE to use NTP. [2]

Most of the articles online about configuring ntpd to receive time from a GPS suggest to use the SHM (shared memory) method. However, at least since ntpd version 4.2.8, a much better method is available. It connects directly to gpsd, so gpsd needs to be installed.

Add these lines to your /etc/ntp.conf:

This will work as long as you have gpsd working. It connects to gpsd via the local socket and queries the "gpsd_json" object that is returned.

To test the setup, first ensure that gpsd is working by running:

Then wait a few minutes and run ntpq -p. This will show if ntpd is talking to gpsd:

Create a new directory /etc/systemd/system/ntpd.service.d/ if it does not exist and a file named customexec.conf inside with the following content:

Then, edit /etc/ntp.conf to change the driftfile path such that it is relative to the chroot directory, rather than to the real system root. Change:

Create a suitable chroot environment so that getaddrinfo() will work by creating pertinent directories and files (as root):

and by bind-mounting the aformentioned files:

Finally, restart ntpd daemon again. Once it restarted you can verify that the daemon process is chrooted by checking where /proc/{PID}/root symlinks to:

should now link to /var/lib/ntp instead of /.

It is relatively difficult to be sure that your driftfile configuration is actually working without waiting a while, as ntpd does not read or write it very often. If you get it wrong, it will log an error; if you get it right, it will update the timestamp. If you do not see any errors about it after a full day of running, and the timestamp is updated, you should be confident of success.

You can limit sockets ntpd is listening to using the interface option:

**Examples:**

Example 1 (unknown):
```unknown
/etc/ntp.conf
```

Example 2 (unknown):
```unknown
/etc/ntp.conf
```

Example 3 (unknown):
```unknown
server 0.fr.pool.ntp.org iburst
server 1.fr.pool.ntp.org iburst
server 2.fr.pool.ntp.org iburst
server 3.fr.pool.ntp.org iburst
```

Example 4 (unknown):
```unknown
leapfile /usr/share/zoneinfo/leap-seconds.list
```

---

## Avahi

**URL:** https://wiki.archlinux.org/title/Avahi

**Contents:**
- Installation
- Using Avahi
  - Hostname resolution
    - Configuring mDNS for custom TLD
    - Tools
  - Firewall
  - Link-Local (Bonjour/Zeroconf) chat
  - Obtaining IPv4LL IP address
- Adding services
  - SSH

From Wikipedia:Avahi (software):

Install the avahi package and enable the avahi-daemon.service or use socket activation.

Avahi provides local hostname resolution using a "hostname.local" naming scheme. To enable it, install the nss-mdns package and start/enable avahi-daemon.service.

Then, edit the file /etc/nsswitch.conf and change the hosts line to include mdns_minimal [NOTFOUND=return] before resolve and dns:

The mdns_minimal module handles queries for the .local TLD only. Note the [NOTFOUND=return], which specifies that if mdns_minimal cannot find *.local, it will not continue to search for it in dns, myhostname, etc.

In case you want Avahi to support other TLDs, you should:

Avahi includes several utilities which help you discover the services running on a network. For example, run this to discover services in your network:

If this command yields nothing, it is likely due to a firewall blocking mDNS traffic.

If you just want to do an mDNS query to resolve a .local hostname to an IP address (similar to dig or nslookup), use:

Note that the getent hosts command can do both DNS and mDNS lookups.

The Avahi Zeroconf Browser avahi-discover shows the various services on your network. Note that it needs Avahi's optional dependencies gtk3, python-dbus and python-gobject. You can also browse SSH and VNC Servers using bssh and bvnc respectively.

Be sure to open UDP port 5353 if you are using a firewall.

Avahi can be used for Bonjour protocol support under Linux. Check Wikipedia:Comparison of instant messaging clients or List of applications/Internet#Instant messaging clients for a list of clients supporting the Bonjour protocol.

This article or section is a candidate for merging with dhcpcd.

The dhcpcd client can attempt to obtain an IPv4LL address if it failed to get one via DHCP. By default this option is disabled. To enable it, comment noipv4ll string:

Alternatively, run avahi-autoipd:

Avahi advertises the services whose *.service files are found in /etc/avahi/services. Files in this directory must be readable by the avahi user/group.

If you want to advertise a service for which there is no *.service file, it is very easy to create your own. As an example, let us say you wanted to advertise a quote of the day (QOTD) service operating per RFC:865 on TCP port 17 which you are running on your machine

The first thing to do is to determine the <type>. avahi.service(5) indicates that the type should be "the DNS-SD service type for this service. e.g. '_http._tcp'". Since the DNS-SD register was merged into the IANA register in 2010, we look for the service name on the IANA register or in /etc/services file. The service name shown there is qotd. Since we are running QOTD on tcp, we now know the service is _qotd._tcp and the port (per IANA and RFC 865) is 17.

Our service file is thus:

For more complicated scenarios, such as advertising services running on a different server, DNS sub-types and so on, consult avahi.service(5).

Keep in mind that Avahi does not support arbitrary strings in the <type> field, you can only set values known in service database of Avahi. If you want to register something custom you will likely have to edit the database definition, build an updated version and distribute it to your hosts.

Avahi comes with an example service file to advertise an SSH server. To enable it:

If you have an NFS share set up, you can use Avahi to be able to automount them in Zeroconf-enabled browsers (such as Konqueror on KDE and Finder on macOS) or file managers such as GNOME/Files.

Create a .service file in /etc/avahi/services with the following contents:

The port is correct if you have insecure as an option in your /etc/exports; otherwise, it needs to be changed (note that insecure is needed for macOS clients). The path is the path to your export, or a subdirectory of it. For some reason the automount functionality has been removed from Leopard, however a script is available. This was based upon this post.

With the Avahi daemon running on both the server and client, the file manager on the client should automatically find the server.

You can also auto-discover regular FTP servers, such as vsftpd. Install the vsftpd package and change the settings of vsftpd according to your own personal preferences (see this thread on ubuntuforums.org or vsftpd.conf(5)).

Create a .service file in /etc/avahi/services with the following contents:

The FTP server should now be advertised by Avahi. You should now be able to find the FTP server from a file manager on another computer in your network. You might need to enable #Hostname resolution on the client.

This is a known bug that is caused by a hostname race condition. One possible workaround is disabling IPv6 to attempt to prevent the race condition. If multiple interfaces are present use allow-interfaces to limit Avahi to a single interface. Another possible workaround is to disable the cache to prevent Avahi from checking for host name conflicts altogether, but this prevents Avahi from performing lookups.

nss-mdns only works if the DNS server listed in /etc/resolv.conf returns NXDOMAIN to SOA queries for the "local" domain.[1]

Check if your configured DNS server answers the SOA query for the "local" domain with NXDOMAIN first. For example:

If the DNS server responds with NXDOMAIN, you do not need to follow the steps below. Avahi should be able to find resources in the network normally, even if using systemd-resolved.

In older versions of systemd-resolved the global setting for MulticastDNS=no in resolved.conf(5) lead to Avahi-incompatible response codes for the "local" domain. This resulted in Avahi not finding resources (printers) correctly. See systemd issue 21659 for reference.

However, if the DNS query above fails to return NXDOMAIN for the "local" domain, you can use the full mdns NSS module instead of mdns_minimal and create /etc/mdns.allow to allow only the "local" domain. For example:

If your Avahi instance starts and operates correctly, but nss does not seem to forward requests to mdns, this may be caused by stuck socket /run/avahi-daemon/socket. This can be verified e.g. with strace. In this case you may have to restart both avahi-daemon.service and avahi-daemon.socket to make it work correctly.

If you use kdeconnect, there are mDNS conflicts with avahi as kdeconnect also runs its mdns server. This can cause hostname conflicts, like renaming your host to myhostname-2 after network restarts.

To correct this, either remove kdeconnect, or build a version without mDNS support, like in kdeconnect-no-mdnsAUR.

**Examples:**

Example 1 (unknown):
```unknown
avahi-daemon.service
```

Example 2 (unknown):
```unknown
systemd-resolved.service
```

Example 3 (unknown):
```unknown
avahi-daemon.service
```

Example 4 (unknown):
```unknown
/etc/nsswitch.conf
```

---

## Electronic identification

**URL:** https://wiki.archlinux.org/title/Electronic_identification

**Contents:**
- Installation
  - Hardware specific packages
    - ACS smart cards
    - Cr-75 card reader
- Belgium
  - Installing the eID middleware
  - Installing the card reader's driver
  - Installing the browser integration
    - Chrome/Chromium
    - Firefox/Librewolf

This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.

An electronic identification ("eID") is an electronic identification solution of citizens or organizations, for example in view to access benefits or services provided by government authorities, banks or other companies. Apart from online authentication many eICs also give users the option to sign electronic documents with a digital signature.

Install the ccid package, all electronic identification requires this package. Then see #Hardware specific packages.

For pinentry support, install pinentry.

Install the acsccid package.

For more information about ACS smart cards, see [1].

Install the libcr75-gitAUR package for the device with the 1307:0361 ID.

Import the (continuous build) keys from [2]. See makepkg#Signature checking. Install the eid-mwAUR package, then run:

In the window that opens, the PCSC daemon status should be listed as "running". If it is not the case, start pcscd.service. Even if the service reports that it's socket activated or automatically started instead of "running", the service may need to be started manually.

Look at the brand of the card reader; there is a high chance it is ACS (Advanced Card System Ltd). If it is ACS, go to https://belgeid.be/en/product/acr38 and download the Linux driver. Follow the described install driver process.

Chrome does not need a plugin. Chromium require opensc and p11-kit as well. This article provides some instructions.

A browser extension will need to be installed. Additionally, the eID module will need to manually be added to the Firefox security devices configuration. The steps are:

For Librewolf, also symlink the directories created by the middleware to /usr/lib/librewolf/, to avoid an error notification when starting Librewolf:

A test page is available from the government to check if eID is configured correctly. Troubleshooting hints may be available in the official documentation, although Arch Linux is not officially supported.

Also note that using Flatpak or Snap is/was not supported, as those do not allow PKCS#11 modules such as eID to be loaded. Fixed for snap for all opensc readers: closed launchpad bug.

Signing emails with Thunderbird and documents with LibreOffice is explained in a blog post by Luc Stroobant.

Depending on system configuration, it may be possible to run Adobe Reader DC under wine. The Belgium government has a relevant FAQ item on digital signatures.

If using Adobe Reader is not possible, the Belgian Federal Public Services' Signing Box offers an upload tool to sign PDFs. The website prompts to install two dependencies: an extra eID middleware beidconnectAUR and a browser extension.

Although okular and papers provide native support for digital PDF signing, signatures are not reported as valid by Signing Box. An open bug ticket exists for okular.

Install ca-certificates-icp_brAUR as the Brazilian root CAs are not part of Mozilla's NSS due to a long standing issue.

The above package should be enough. If you have any issue, check [ITI's installation instructions https://www.gov.br/iti/pt-br/assuntos/navegadores] for Chromium, Firefox and other popular web browsers, and for Java.

1. Install safesignidentityclientAUR and opensc.

2. Start/enable pcscd.service

Navigate to Edit -> Preference -> Advanced -> Certificates -> Security Devices and click "Load" to load a module using /usr/lib/libaetpkss.so and name it ICP-Brasil A3 - Safe Sign Identity Client.

Test it by going to Receita Federal's e-CAC.

Ensure Chrome is closed and run:

Navigate to Edit > Preference > Advanced > Certificates > Security Devices and click Load to load a module using /usr/lib/akd/certiliamiddleware/pkcs11/libEidPkcs11.so. You can assign any name to it, i.e. Cro PKCS#11 Module.

See https://www.id.ee/en/.

Once ccid and opensc is installed and pcscd.socket is started, install qdigidoc4AUR. One of the dependency xml-security-cAUR is verified with a signature that you have to import to your GnuPG keyring.

If you have an ACS card reader, acsccid is required.

DigiDoc4 has an optional GNOME/Files right click menu integration that requires nautilus-python to be installed.

Current browser ID-Card stack is based on Web eID. It provides consistent user experience on all supported platforms for both PIN 1 authentication and PIN 2 document signing.

Web eID consists of two components. Both need to be installed.

web-eid-nativeAUR is verified with a signature and you need to import developer PGP keys to your GnuPG keyring.

Not all sites have migrated to the new Web eID PIN 1 JavaScript API and use the older Mutual TLS (also some times called TLS-CCA. You still need to configure opensc PKCS #11 provider in the browsers by running this command:

You will also need to restart the browser afterwards.

Atostek ID is the official client of the Finnish Digital and Population Data Services Agency. Atostek ID was released at the end of 2024. Atostek ID replaces the previous card reader software DigiSign Client. However, DigiSign Client still receives updates until the end of 2025. These updates will focus on bug fixes and security updates. No new features are planned for the old client.

Having both clients installed at the same time is not recommended.

Official information on the card reader software is available at: https://dvv.fi/en/download-card-reader-software.

Atostek ID is the current official card reader software.

Install atostekidAUR. Since Atostek ID is a GUI application that relies heavily on the system tray, pay attention that relevant optional dependencies are installed for your desktop environment.

Make sure to enable and start the pcscd.socket. Otherwise the client might not start, or applications that rely on the client might crash.

New government issued ID cards need to be activated before usage. This needs to be done only once.

Connect the card reader to your computer and insert your smart card. Launch the application. If the card has not been activated previously, the client will automatically display the activation window. Activation is required only once per card. If the activation window does not appear, choose "Activate card" from the application menu. When the activation window opens, enter the activation code (PUK) provided in the code letter that came with your card. Next, set a PIN for the authentication certificate (PIN1) and another PIN for the signing certificate (PIN2). It is highly recommended that PIN1 and PIN2 are different. Once you have entered the required information, click OK.

Atostek ID supports digital authentication and signing. It also utilizes the PKCS #11 interface. The card reader software can be used, for example, to authenticate with a web browser, to sign PDF documents, and logging in to the system. See the official instructions at: https://dvv.fi/en/download-card-reader-software

Navigate to Security Devices page (Search it via Preferences), then click Load and set Module Name to p11-kit-proxy and module filename to p11-kit-proxy.so. Finally restart Firefox. The card can be tested at: https://dvv.fi/en/test-the-use-of-a-certificate.

Signature Creation Service (SCS) is a specification of the Digital and Population Data Services Agency. It is used, for example, in patient information systems. SCS utilizes self-signed certificates, which can be generated by running:

This will create a certificate (scsca.cer) and a private key (scsca.p12) in $HOME/.local/share/Atostek Oy/Atostek ID/. The certificate will need to be installed manually to your web browser.

Atostek ERA Smartcard is a feature intended for social and healthcare users who are registered to use the ERA system. It utilizes self-signed certificates, which can be generated by running:

This will create a certificate (erasmartcard_ehoito_fi_ca.cer) and a private key (erasmartcard_ehoito_fi_cert.p12) in $HOME/.local/share/Atostek Oy/Atostek ID/. The certificate will need to be installed manually to your web browser.

When authenticating and signing with Firefox, the PIN dialogs are not delegated to the Atostek ID client. Instead, the native PIN dialogs of Firefox are shown. It is possible that Firefox will ask PINs multiple times.

DigiSign Client is the old card reader software. It will be supported until the end of 2025. The Finnish Digital and Population Data Services Agency recommends using the new #Atostek ID client instead.

First install the prerequisites as described in #Installation. Then install vrk-mpollux-digisign-clientAUR. Launch the client, connect your reader and put in your card. Click the icon in your status bar once it turns yellow. This should trigger the card activation process if you have not activated it before.

Navigate to Security Devices page (Search it via Preferences), then click Load and set Module Name to DigiSign PKCS#11-moduuli and module filename to /usr/lib/libcryptoki.so. Finally restart Firefox. The card can be tested at: https://dvv.fi/en/test-the-use-of-a-certificate.

For some devices, you need to install pcsc-cyberjackAUR and copy the default configuration file /etc/pcsc-cyberjack/cyberjack.conf.default to the same folder, without the .default suffix. Restart pcsc.service and applications like ausweisapp2AUR should recognize the scanner. The ReinerSCT RFID will blink its LED, which it does not when the driver is not installed correctly.

You can also use a smartphone as the card reader, if both your computer and the smartphone are in the same network. You must install and run AusweisApp on the phone (available for Android / iPhone).

For document signing install the eparakstitajs3AUR package. No additional software is necessary to use it with eParaksts mobile or eID Scan.

To use the eID card, install latvia-eid-middlewareAUR and the prerequisite packages listed in #Installation, and make sure to enable and start pcscd.service. To use it in a browser, additionally install the browser extension eparaksts-token-signingAUR.

To use the eID card with Smart-ID, install the following packages:

Navigate to Edit > Preference > Advanced > Certificates > Security Devices and click Load to load a module using /usr/lib/idplugclassic/libidplug-pkcs11.so. You can assign any name to it, i.e. RO CEI PKCS#11 Module.

Install ca-certificates-dnieAUR. To sign documents using your identity card, install autofirmaAUR.

BankID is the leading electronic identification in Sweden.

**Examples:**

Example 1 (unknown):
```unknown
$ about-eid-mw
```

Example 2 (unknown):
```unknown
pcscd.service
```

Example 3 (unknown):
```unknown
about-eid-mw
```

Example 4 (unknown):
```unknown
/usr/lib/librewolf/
```

---

## Active Directory integration

**URL:** https://wiki.archlinux.org/title/Active_Directory_integration

**Contents:**
- Introduction
- Needed software
- Initial configuration of services
  - DNS configuration
  - NTP configuration
  - Kerberos configuration
  - Samba configuration
    - Base Samba configuration file
      - Adding the idmap configuration for domains with RFC2307 extensions
      - Adding the idmap configuration for domains without RFC2307 extensions

This article or section needs expansion.

This article describes how to integrate an Arch Linux system with an existing Windows domain network using Samba.

Before continuing, you must have an existing Active Directory domain, and have a user with the appropriate rights within the domain to: query users and add computer accounts (Domain Join).

This document is not an intended as a complete guide to Active Directory nor Samba. Refer to the resources section for additional information.

This article explains how to configure an Arch Linux system to participate in an Active Directory domain. This article was written and tested on a fresh installation, and it is assumed that all configuration files are in their unmodified, post-installation state. For the duration of the article, the example Active Directory domain will use the following configuration:

In most small networks, the DCs (domain controllers) also hold the DNS server role. This may not be true in larger networks. Generally, DCs also hold the NTP role, but not always. Consult your network administrator to verify correct values for DNS and NTP servers.

In order to use samba effectively, you will need to install the following packages: samba, smbclient, and ntp. (timedatectl can be used as an alternative to ntp.)

Additionally, while not required, the following packages will be useful for testing and troubleshooting: bind, krb5, and if a printing is desired (whether you want to share printers, or use printers on another Samba/Windows host), cups.

Active Directory depends entirely on DNS for name resolution. It is imperative that the /etc/resolv.conf file is configured with both the correct DNS servers and a domain search suffix. Whether configured via DHCP or static configuration, ensure that these values are correct for your domain. For the example domain configuration, the following contents are appropriate (be sure to replace 192.168.1.1, 192.168.1.2, and internal.domain.tld with appropriate values for your network):

If you elected to install the bind package, you can test DNS configuration with the following commands (be sure to replace server1 and internal.domain.tld with appropriate values for your network):

You should get output similar to the following (adjust appropriately for only one DC, or more than two):

In an Active Directory domain, more specifically for Kerberos ticketing, it is imperative that time is synchronized with all other hosts on the network. A margin of error no more than five minutes is required. For the example domain configuration, an appropriate /etc/ntp.conf file should have the following contents (be sure to replace server1, server2, and internal.domain.tld with appropriate values for your network):

Enable and start the ntpd.service unit.

The Samba documentation recommends a minimal Kerberos configuration, with just enough information in the [libdefaults] section to hand off the work of discovering domain details to DNS. Unfortunately, this does not work well in practice. Continuing with the example domain configuration, modify the /etc/krb5.conf file with the following contents (be sure to replace instances of INTERNAL, internal.domain.tld, SERVER1, and INTERNAL.DOMAIN.TLD with appropriate values for your network):

A default installation of samba does not ship with an example /etc/samba/smb.conf file. For our example domain configuration, use the following base settings (replace instances of INTERNAL and INTERNAL.DOMAIN.TLD with appropriate values for your network):

If you do not wish to share local printers configured in cups, then add the following to the [Global] section of the /etc/samba/smb.conf file:

The remainder of the configuration depends on whether your domain supports RFC2307 Unix/NFS Attributes. Consult with your domain administrator if unsure.

Be certain that the values below do not overlap with system values, and that all users have at least the uidNubmer attribute, and that those users' PrimaryGroup has a gid attribute. Append to the following to the [Global] section of the /etc/samba/smb.conf file (replace INTERNAL with the NetBIOS domain name):

Additionally, if user accounts in AD have a gidNumber attribute, you can use it instead of the RID for the user's Primary Group by appending the following setting (again in the [Global] section):

If your administrator has not extended the AD schema to include the RFC2307 attributes, use the following idmap configuration in the [Global] section of the /etc/samba/smb.conf file (replace INTERNAL with the NetBIOS domain name):

To join the AD domain, simply issue the following command (be sure to replace Administrator with a user that has privileges to join the AD domain).

Enable and start the smb.service, nmb.service, and winbind.service services.

Modify the /etc/nsswitch.conf file to allow Samba to map names to uid and gid:

Verify connectivity by listing the AD domain users and groups that system is aware of:

You should get a list of AD users followed by AD groups.

Rather than configuring options directly in the Linux-PAM configuration files, set defaults for the pam_winbind module in /etc/security/pam_winbind.conf:

For most services, it will be sufficient to modify only the /etc/pam.d/system-auth file. Any configuration for programs that do not include this file will also need to be modified directly. Create a backup of the /etc/pam.d/system-auth file and use the following configuration:

If you have other services that do not include the /etc/pam.d/system-auth file, modify the configuration to mirror all pam_unix.so entries for pam_winbind.so and change all required to sufficient. A good example is the su configuration. Create a backup of the /etc/pam.d/su file and use the following in its place:

The above pam_winbind configuration will not use the default location of the Kerberos ticket (KRB5CCNAME), which is at /tmp/krb5cc_UID. Instead, it stores the automatically refreshed Kerberos ticket to /run/user/UID/krb5cc. Append the following to your krb5.conf to let Kerberos know your new location:

To test your changes, start a new console or ssh session (do not exit your existing session until you have tested thoroughly) and try to login using the AD credentials. The domain name is optional, as this was set in the Winbind configuration as 'default realm'. Please note that in the case of ssh, you will need to modify the /etc/ssh/sshd_config file to allow kerberos authentication (see below).

Run klist to verify that you have received a kerberos ticket. You should see something similar to:

Finally, you should test login as both the root user and a local unprivileged user before logging out of your existing (working) session.

Active Directory serves as a central location for network administration and security. It is responsible for authenticating and authorizing all users and computers within a Windows domain network, assigning and enforcing security policies for all computers in a network and installing or updating software on network computers. For example, when a user logs into a computer that is part of a Windows domain, it is Active Directory that verifies their password and specifies whether they are a system administrator or normal user. Server computers on which Active Directory is running are called domain controllers.

Active Directory uses Lightweight Directory Access Protocol (LDAP) versions 2 and 3, Microsoft's version of Kerberos and DNS.

If you are not familiar with Active Directory, there are a few keywords that are helpful to know.

This section works with the default configuration of Windows Server 2012 R2.

Digital signing is enabled by default in Windows Server, and must be enabled at both the client and server level. For certain versions of Samba, Linux clients may experience issues connecting to the domain and/or shares. It is recommended you add the following parameters to your smb.conf file:

If that is not successful, you can disable Digital Sign Communication (Always) in the AD group policies. In your AD Group Policy editor, locate:

Under Local policies > Security policies > Microsoft Network Server > Digital sign communication (Always) activate define this policy and use the disable radio button.

If you use Windows Server 2008 R2, you need to modify that in GPO for Default Domain Controller Policy > Computer Setting > Policies > Windows Setting > Security Setting > Local Policies > Security Option > Microsoft network client: Digitally sign communications (always).

Please note that disabling this GPO affects the security of all members of the domain.

The next few steps will begin the process of configuring the Host. You will need root or sudo access to complete these steps.

Install the following packages:

Active Directory is heavily dependent upon DNS. You will need to update /etc/resolv.conf to use one or more of the Active Directory domain controllers:

Replacing <IP1> and <IP2> with valid IP addresses for the AD servers. If your AD domains do not permit DNS forwarding or recursion, you may need to add additional resolvers.

Read System time#Time synchronization to configure an NTP service.

On the NTP servers configuration, use the IP addresses for the AD servers, as they typically run NTP as a service. Alternatively, you can use other known NTP servers provided the Active directory servers sync to the same stratum.

Ensure that the service is configured to sync the time automatically very early on startup.

Let us assume that your AD is named example.com. Let us further assume your AD is ruled by two domain controllers, the primary and secondary one, which are named PDC and BDC, pdc.example.com and bdc.example.com respectively. Their IP adresses will be 192.168.1.2 and 192.168.1.3 in this example. Take care to watch your syntax; upper-case is very important here.

Now you can query the AD domain controllers and request a kerberos ticket (uppercase is necessary):

You can use any username that has rights as a Domain Administrator.

Run klist to verify you did receive the token. You should see something similar to:

If you get errors stating that /etc/security/pam_winbind.conf was not found, create the file and add the following:

With this setup, winbind will create user keytabs on the fly (krb5_ccache_type = FILE) at login and maintain them. You can verify this by simply running klist in a shell after logging in as an AD user but without needing to run kinit. You may need to set additional permissions on /etc/krb5.keytab eg 640 instead of 600 to get this to work (see FS#52621 for example)

Samba is a free software re-implementation of the SMB/CIFS networking protocol. It also includes tools for Linux machines to act as Windows networking servers and clients.

In this section, we will focus on getting Authentication to work first by editing the 'Global' section first. Later, we will go back and add shares.

You need an AD Administrator account to do this. Let us assume this is named Administrator. The command is 'net ads join'

Hopefully, you have not rebooted yet! Fine. If you are in an X-session, quit it, so you can test login into another console, while you are still logged in.

Enable and start the individual Samba daemons smbd.service, nmbd.service, and winbindd.service.

Next we will need to modify the NSSwitch configuration, which tells the Linux host how to retrieve information from various sources and in which order to do so. In this case, we are appending Active Directory as additional sources for Users, Groups, and Hosts.

Let us check if winbind is able to query the AD. The following command should return a list of AD users:

We can do the same for AD groups:

To ensure that our host is able to query the domain for users and groups, we test nsswitch settings by issuing the 'getent' command.

The following output shows what a stock Arch Linux install looks like:

Try out some net commands to see if Samba can communicate with AD:

Now we will change various rules in PAM to allow Active Directory users to use the system for things like login and sudo access. When changing the rules, note the order of these items and whether they are marked as required or sufficient is critical to things working as expected. You should not deviate from these rules unless you know how to write PAM rules.

In case of logins, PAM should first ask for AD accounts, and for local accounts if no matching AD account was found. Therefore, we add entries to include pam_winbind.so into the authentication process.

The Arch Linux PAM configuration keeps the central auth process in /etc/pam.d/system-auth. Starting with the stock configuration from pambase, change it like this:

Delete it, and replace with:

Keep it, and add this below:

Delete it, and replace with:

Keep it, and add this line immediately above it:

Below the pam_unix line, add these:

In order for logged-in Active Directory users to be able to change their passwords with the 'passwd' command, the file /etc/pam.d/passwd must include the information from system-auth.

Delete it, and replace with:

Now, start a new console session (or ssh) and try to login using the AD credentials. The domain name is optional, as this was set in the Winbind configuration as 'default realm'. Please note that in the case of ssh, you will need to modify the /etc/ssh/sshd_config file to allow kerberos authentication (KerberosAuthentication yes).

Both should work. You should notice that /home/example/test.user will be automatically created. Log into another session using an linux account. Check that you still be able to log in as root - but keep in mind to be logged in as root in at least one session!

Earlier we skipped configuration of the shares. Now that things are working, go back to /etc/samba/smb.conf, and add the exports for the host that you want available on the windows network.

In the above example, the keyword NETWORK is to be used. Do not mistakenly substitute this with your domain name. For adding groups, prepend the '@' symbol to the group. Note that Domain Admins is encapsulated in quotes so Samba correctly parses it when reading the configuration file.

This explains how to generate a machine keytab file which you will need e.g. to enable password-free kerberized ssh to your machine from other machines in the domain. The scenario in mind is that you have a bunch of systems in your domain and you just added a server/workstation using the above description to your domain onto which a lot of users need to ssh in order to work - e.g. GPU workstation or an OpenMP compute node, etc. In this case you might not want to type your password every time you log in. On the other hand the key authentication used by many users in this case can not give you the necessary credentials to e.g. mount kerberized NFSv4 shares. So this will help you to enable password-free logins from your clients to the machine in question using kerberos ticket forwarding.

run 'net ads keytab create -U administrator' as root to create a machine keytab file in /etc/krb5.keytab. It will prompt you with a warning that we need to enable keytab authentication in our configuration file, so we will do that in the next step. In my case it had problems when a key tab file is already in place - the command just did not come back it hang ... In that case you should rename the existing /etc/krb5.keytab and run the command again - it should work now.

verify the content of your keytab by running:

Now you need to tell winbind to use the file by adding these lines to the /etc/samba/smb.conf:

It should look something like this:

Restart the winbindd.service

Check if everything works by getting a machine ticket for your system by running

This should not give you any feedback but running 'klist' should show you sth like:

Some common mistakes here are a) forgetting the trailing $ or b) ignoring case sensitivity - it needs to look exactly like the entry in the keytab (usually you cannot to much wrong with all capital)

All we need to do is add some options to our sshd_config and restart the sshd.service.

Edit /etc/ssh/sshd_config to look like this in the appropriate places:

Restart the sshd.service.

First we need to make sure that the tickets on our client are forwardable. This is usually standard but we better check anyways. You have to look for the forwardable option and set it to 'true' in the Kerberos configuration file /etc/krb5.conf

Secondly we need to add the options

to our .ssh/config file to tell ssh to use this options - alternatively they can be invoked using the -o options directly in the ssh command (see ssh(1) for help).

make sure you have a valid ticket - if in doubt run 'kinit'

then use ssh to connect to you machine

you should get connected without needing to enter your password.

if you have key authentication additionally activated then you should perform

to see which authentication method it actually uses.

For debugging you can enable DEBUG3 on the server and look into the journal using journalctl.

In case your clients are not using domain accounts on their local machines (for whatever reason) it can be hard to actually teach them to kinit before ssh to the workstation. Therefore I came up with a nice workaround:

On a system let the user run:

Now test the file by invoking:

It should not promt you to give your password nor should it give any other feedback. If it worked you are basically done - just put the line above into your ~./bashrc - you can now get kerberos tickets without typing a password and with that you can connect to your workstation without typing a password while being completely kerberized and able to authenticate against NFSv4 and CIFS via tickets - pretty neat.

The file 'username.keytab' is not machinespecific and can therefore be copied around. E.g. we created the files on a linux machine and copied them to our Mac clients as the commands on Macs are different ...

sssd can be used instead of Samba to integrate with AD. See SSSD documentation.

**Examples:**

Example 1 (unknown):
```unknown
/etc/resolv.conf
```

Example 2 (unknown):
```unknown
/etc/resolv.conf
```

Example 3 (unknown):
```unknown
nameserver 192.168.1.1
nameserver 192.168.1.2
search internal.domain.tld
```

Example 4 (unknown):
```unknown
$ nslookup -type=SRV _kerberos._tcp.internal.domain.tld.
$ nslookup -type=SRV _ldap._tcp.internal.domain.tld.
$ nslookup server1.internal.domain.tld.
```

---
