# Arch-Wiki - Other

**Pages:** 276

---

## Arch Testing Team

**URL:** https://wiki.archlinux.org/title/Arch_Testing_Team

**Contents:**
- Contributing
- Guidelines
- Coordination

The Arch Testing Team is a group within the Arch community in charge of making sure that packages submitted to the testing repositories are functional. This includes, making sure that the package installs correctly, that it does not cause breakage with packages of which it depends on, among others.

Arch Testers sign off packages after vouching for their correctness so that they can be moved from the testing repositories into the core or extra repositories.

You can apply to be an official Arch tester by sending an email to arch-testing-accounts@archlinux.org and requesting a tester account, introducing yourself and specifying at least your desired username.

This article or section needs expansion.

If you are given a tester account, you should be able to log in into archweb and see a signoffs tab on it. The signoffs tab will contain a list of packages that are currently in the testing repositories and need at least two signoffs (i.e., a rubber-stamp vouching for the correctness of a package).

You may then test the listed packages locally and signing them off if they are correct by clicking on the signoff button for each package.

In order to test an arch package, keep the following aspects in mind:

You can coordinate with other testers on the #archlinux-testing IRC channel.

---

## msmtp

**URL:** https://wiki.archlinux.org/title/Msmtp

**Contents:**
- Installation
- Basic setup
  - OAuth2 Setup
    - oama
- Using the mail command
- Test functionality
- Cronie default email client
- Password management
  - GNOME Keyring
  - GnuPG

msmtp is a very simple and easy to use SMTP client with fairly complete sendmail compatibility.

Install the msmtp package. Additionally, install msmtp-mta, which creates a sendmail alias to msmtp.

Since msmtp version 1.8.6 you can place your user configuration either at ~/.msmtprc or $XDG_CONFIG_HOME/msmtp/config. The following is an example of a msmtp configuration (the file is based on the per-user example file located at /usr/share/doc/msmtp/msmtprc-user.example; the system configuration file belongs at /etc/msmtprc and its corresponding example file is located at /usr/share/doc/msmtp/msmtprc-system.example).

The user configuration file must be explicitly readable/writeable by its owner or msmtp will fail:

To avoid saving the password in plain text in the configuration file, use passwordeval to launch an external program, or see the #Password management section below. This example using Gnu PG is commonly used to perform decryption of a password:

OAuth2 can be used to securely authenticate msmtp when basic username/password authentication is unsupported by the site configuration or otherwise undesirable.

msmtp alone lacks the ability to renew or authorize OAuth2 credentials. A comprehensive solution is using the oama utility which provides IMAP/SMTP clients with renewal capabilities and authorization of OAuth2 credentials.

To use oama, install oama-binAUR and configure msmtp to use it:

Access token renewal happens automatically in the background transparent to the user.

To send mails using the mail command you must install the package s-nail, which also provides the mailx command. You will also need to provide a sendmail-compatible MTA, either by installing msmtp-mta (which symlinks sendmail to msmtp) or by editing /etc/mail.rc to set the sendmail path:

A .msmtprc file will need to be in the home of every user who wants to send mail or alternatively the system wide /etc/msmtprc can be used.

msmtp also understands aliases. Add the following line to the defaults section of msmtprc or your local configuration file:

and create an aliases file in /etc

The account option (--account=,-a) tells which account to use as sender:

Or, send both a subject and a body:

Or, with the addresses in a file:

This article or section is out of date.

To make Cronie use msmtp rather than sendmail, make sure msmtp-mta is installed, or edit the cronie.service systemd unit:

Then you must tell cronie or msmtp what your email address is. One way to accomplish this is by adding it to the msmtp configuration:

Alternatively, you can add it directly to the crontab:

Passwords for msmtp can be stored in plaintext, encrypted files, or a keyring.

Storing passwords in GNOME Keyring is supported natively in msmtp. Setup the keyring as described on the linked wiki page and install libsecret. Then, store a password by running:

msmtp should now find the password automatically.

The password directive may be omitted. In that case, if the account in question has auth set to a legitimate value other than off, invoking msmtp from an interactive shell will ask for the password before sending mail. msmtp will not prompt if it has been called by another type of application, such as Mutt. For such cases, the --passwordeval parameter can be used to call an external keyring tool like GnuPG.

To do this, set up GnuPG, including gpg-agent to avoid having to enter the password every time. Then, create an encrypted password file for msmtp, as follows. Create a secure directory with 700 permissions located on a tmpfs to avoid writing the unencrypted password to the disk. In that directory create a plain text file with the mail account password. Then, encrypt the file with your private key:

Remove the plain text file and move the encrypted file to the final location, e.g. ~/.mail/.msmtp-credentials.gpg. In ~/.msmtprc add:

Normally this is sufficient for a GUI password prompt to appear when, for example, sending a message from Mutt. If gpg prompt for the passphrase cannot be issued, then start the gpg-agent before. A simple hack to start the agent is to execute a external command in your muttrc using the backtick `command` syntax. For example, you can put something like the following in your muttrc:

Mutt will execute this when it starts, gpg-agent will cache your password, msmtp will be happy and you can send mail.

An alternative is to place passwords in ~/.netrc, a file that can act as a common pool for msmtp, OfflineIMAP, and associated tools.

You may store your credentials inside of the pass password manager.

If you are using your main password (which is customarily stored in the first line of your pass file) to login into your SMTP server, you can add the following to your .msmptrc:

If you are using Gmail, and have set up an app password, the following configuration will suit you better. Save your app password inside your pass password file, but with a msmtp: prefix:

Then add the following to your .msmptrc:

In either case, trying to send an email with msmtp will trigger pass, which may ask you for your pass master password if you have not entered it recently.

Although msmtp is great, it requires that you be online to use it. This is not ideal for people on laptops with intermittent connections to the Internet or dialup users. Several scripts have been written to remedy this fact, collectively called msmtpqueue.

The scripts are installed under /usr/share/doc/msmtp/msmtpqueue. You might want to copy the scripts to a convenient location on your computer, (/usr/local/bin is a good choice).

Finally, change your MUA to use msmtp-enqueue.sh instead of msmtp when sending e-mail. By default, queued messages will be stored in ~/.msmtpqueue. To change this location, change the QUEUEDIR=$HOME/.msmtpqueue line in the scripts (or delete the line, and export the QUEUEDIR variable in .bash_profile like so: export QUEUEDIR="$XDG_DATA_HOME/msmtpqueue").

When you want to send any mail that you have created and queued up run:

Adding /usr/local/bin to your PATH can save you some keystrokes if you are doing it manually. The README file that comes with the scripts has some handy information, reading it is recommended.

The msmtp source distribution includes an msmtprc syntax-highlighting script for Vim, which is available at /usr/share/vim/vimfiles/syntax/msmtp.vim. The filetype is not detected automatically. The easiest way to enable it is by adding a modeline at the top or bottom of the file(s), i.e.:

Look for sendmail_path option in your php.ini and edit like this:

Note that you can not use a user configuration file (ie: one under ~/) if you plan on using msmtp as a sendmail replacement with php or something similar. In that case just create /etc/msmtprc, and remove your user configuration (or not if you plan on using it for something else). Also make sure it is readable by whatever you are using it with (php, django, etc...).

From the msmtp manual: Accounts defined in the user configuration file override accounts from the system configuration file. The user configuration file must have no more permissions than user read/write

So it is impossible to have a conf file under ~/ and have it still be readable by the php user.

To test it place this file in your php enabled server or using php-cli.

php-fpm will fail to send mails and logs the warning: PHP Warning: mail(mail.log): failed to open stream unless you set the permissions of your /etc/msmtprc to user read/write (600).

If you see one of the following messages:

It probably means your tls_trust_file parameter of the configuration file is not right.

Just follow the fine manual. It explains you how to find out the server certificate issuer of a given smtp server. Then you can explore the /usr/share/ca-certificates/ directory to find out if by any chance, the certificate you need is there. If not, you will have to get the certificate on your own. If you are using your own certificate, you can make msmtp trust it by adding the following to your ~/.msmtprc:

If the user have the certificate used by the server in a PEM file, he can find the fingerprint of the certificate stored using openssl:

If you are trying to send mail through Gmail and are receiving this error, have a look at this thread or just use the second Gmail example above.

If you are completely desperate, but are 100% sure you are communicating with the right server, you can always temporarily disable the cert check:

If you see the following message:

You may be affected by this bug. Recompile with --with-ssl=openssl (msmtp is compiled with GnuTLS by default).

If you get a "server sent empty reply" error, this probably means the mail server does not support STARTTLS over port 587, but requires TLS over port 465.

To let msmtp use TLS over port 465, add the following line to ~/.msmtprc:

It can also happen on Zoho SMTP servers when the mail has no blank line between mail headers and mail body (see Debian bug #917260). The solution to this is to add an extra space in between:

If you get the following error

Try changing your auth setting to plain, instead of gssapi in your .msmtprc file [1]:

Try enabling authentication with

**Examples:**

Example 1 (unknown):
```unknown
$XDG_CONFIG_HOME/msmtp/config
```

Example 2 (unknown):
```unknown
/usr/share/doc/msmtp/msmtprc-user.example
```

Example 3 (unknown):
```unknown
/etc/msmtprc
```

Example 4 (unknown):
```unknown
/usr/share/doc/msmtp/msmtprc-system.example
```

---

## NSD

**URL:** https://wiki.archlinux.org/title/NSD

**Contents:**
- Installation
  - Migration to nsd for bind users
- Initial setup
- Starting and running nsd
- Testing nsd
- Configuring unbound
- WAN facing dns
- See also

This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.

Install the nsd package.

Once the package is installed there are useful migration notes for users who currently run bind as their dns server in the file:

Many users will wish to run nsd as their authoritative dns server concurrently with unbound as the validating, recursive, caching dns server on a single machine. It may be useful to refer to the wiki page for unbound.

Most likely you will run nsd with a DNS caching servers such as Unbound. So, to avoid conflicts, this configuration use port 53530 for nsd, since port 53 is used by the DNS caching server. nsd will listen for requests on localhost. Additionally, the only firewall port that then needs to be open for dns queries coming from external machines (or other machines on the same local network) is port 53.

When installed, a commented sample configuration file is placed at /etc/nsd/nsd.conf.sample. Below is a minimal working example:

See [1] for more examples.

Note that when running version 4.3.5 there will be a benign message in the status referring to "unable to initgroups nsd: Operation not permitted". This can be avoided by adding the following line to /etc/nsd/nsd.conf

and the unit status will no longer have the initgroups message.

Before starting up nsd you can check the zone files using the nsd-checkconf command with the zone file name as a parameter.

Before starting up nsd you can validate both the configuration and zone configs with:

In order to build the zone database that makes nsd run exceptionally quickly the database file must be rebuilt each time a zone or configuration file is changed, and the following command is executed:

In order to start nsd, start/enable the nsd.service systemd service.

nsd can be run concurrently with bind during the testing phase. You can check forward and reverse local lookups on the port at 53530 using:

where w.x.y.z is a local address within the LAN.

Once this is working then if you are running unbound as the caching recursive server then you can switch the unbound configuration to forward queries from local machines on the same network to query nsd by using the following structure in unbound.conf (and see unbound), where it is assumed that nsd is listening to port 53530:

Once the unbound.conf contains the above then restart unbound and check that local queries for the nsd zone entries works. Once it is all tested then you can switch unbound to listen on both 127.0.0.1 as well as on the external interface for the local network by having the lines in unbound.conf including:

where 10.0.0.1 is the ip address of the dns server running both nsd and unbound and providing local dns for other machines on the 10.x.x.x network.

The examples here all assume that only ipv4 is being used. Corresponding configurations for ipv6 should be included where necessary, and further details on the parameters for that can be found in the man files for the two packages as well as examples that can be found with web searches.

It is also possible to change the configuration files and interfaces on which the server is listening so that dns queries from machines outside of the local network can access specific machines within the LAN. This is useful for web and mail servers which are accessible from anywhere, and the same techniques can be employed as has been achieved using bind for many years, in combination with appropriate port forwarding in the network firewall machines, to allow incoming requests to access the correct machine.

**Examples:**

Example 1 (unknown):
```unknown
/usr/share/doc/nsd/NSD-FOR-BIND-USERS
```

Example 2 (unknown):
```unknown
/etc/nsd/nsd.conf.sample
```

Example 3 (unknown):
```unknown
/etc/nsd/nsd.conf
```

Example 4 (unknown):
```unknown
server:
    server-count: 1
    ip-address: 127.0.0.1
    port: 53530
    do-ip4: yes
    hide-version: yes
    identity: "Home network authoritative DNS"
    zonesdir: "//etc/nsd"
key:
    name: "keyname"
    algorithm: hmac-md5
    secret: "secretkey"
zone:
    name: "example.com"
    zonefile: "example.com.zone"
```

---

## GTK

**URL:** https://wiki.archlinux.org/title/GTK

**Contents:**
- Installation
- Themes
  - GTK 3 and GTK 4
  - GTK 2
  - Themes supporting GTK 2 and GTK 3
  - GTK and Qt
- Configuration tools
- Configuration
  - Basic theme configuration
  - Dark theme variant

From the GTK website:

GTK was initially made by the GNU Project for GIMP, but it is now a very popular toolkit with bindings for many languages. This article will explore the tools used to configure the GTK theme, style, icon, font and font size, and also detail manual configuration.

Multiple versions of GTK are currently available. They can be installed with the following packages:

In GTK 3 and GTK 4, the default theme is Adwaita, but HighContrast and HighContrastInverse themes are also included.

To apply a specific theme, set the gtk-theme property in the org.gnome.desktop.interface namespace via a dconf editor:

If you are not using a dconf property, you can use GTK_THEME to apply a GTK 3 and GTK 4 themes. For example to launch GNOME Calculator with the dark variant of Adwaita:

In GTK 2, the default theme is Raleigh, but Arch Linux has a custom configuration file at /usr/share/gtk-2.0/gtkrc, which sets the default theme to Adwaita.

To change the GTK 2 theme, use GTK2_RC_FILES. For example to launch GIMP with the theme Raleigh:

More themes can be installed from the official repositories or the AUR. Manually extracted themes go in ~/.themes/ or ~/.local/share/themes/ directory.

There are a number of additional GTK themes in the AUR, example: search for gtk-theme.

If you have GTK and Qt (KDE) applications on your desktop then you know that their looks do not blend well. If you wish to make your GTK styles match your Qt styles please read Uniform look for Qt and GTK applications.

Most major desktop environments provide tools to configure the GTK theme, icons, font and font size, and manage these settings via XSettings:

Other GUI tools generally overwrite the configuration files.

Both GTK 2 and GTK 3 are supported:

Only GTK 2 is supported:

GTK settings can be specified manually in configuration files, but desktop environments and applications can override these settings. Depending on GTK version, these files are located at:

To manually change the GTK theme, icons, font and font size, add the following to the configuration files, for example:

If the theme is not applied for GTK 3, use gsettings in addition:

Similarly, if the icon theme is not applied for GTK 3 , use gsettings:

For downloading and installing icons manually, see Icons.

Some GTK 3 themes contain a dark theme variant, but it is only used by default when the application requests it explicitly. To use dark theme variant with all GTK 3 applications, set:

Keyboard shortcuts (otherwise known as accelerators in GTK) may be changed by hovering the mouse over the respective menu item, and pressing the desired key combination. To enable this feature, set:

To have Emacs-like key bindings in GTK applications add the following:

XFCE has a similar setting:

The configuration files e.g. in /usr/share/themes/Emacs/ determine what the Emacs bindings are, and can be changed.

Copying sections to the users ~/.gtkrc-2.0 and ~/.themes/your-new-key-theme/gtk-3.0/gtk-keys.css for GTK 2 and 3 respectively, allows for changes on a per user basis.

This setting controls the delay between pointing the mouse at a menu and that menu opening. This delay is measured in milliseconds.

If you have a small screen or you just do not like big icons and widgets, you can resize things easily.

To have icons without text in toolbars (valid values), use

To use smaller icons, use a line like this:

Or to remove icons from buttons completely:

You can also remove icons from menus:

See also [1] and [2].

To remove the client-side decorations (CSD)[3] minimize and maximize buttons from gtk3 windows:

To turn off pasting from the clipboard (PRIMARY selection) when the middle mouse button is clicked:

Open the file-chooser within the current working directory and not the recent location. Normally the current working directory is the Home directory.

Change setting with the following command:

Add the following to ~/.config/gtk-2.0/gtkfilechooser.ini:

Prior to GTK 3.6, clicking on either side of the slider in the scrollbar would move the scrollbar in the direction of the click by approximately one page. Since GTK 3.6, the slider will move directly to the position of the click. This behaviour can be reverted in some applications by creating the file with the content below:

Since GTK 3.15, overlay scrollbars are enabled by default, meaning that scrollbars will be shown only on mouseover in GTK 3 applications. This behavior can be reverted by setting the following environment variable: GTK_OVERLAY_SCROLLING=0. See Environment variables#Graphical environment.

Alternatively, overlay scrollbars can be disabled in the GTK 3 settings since GTK 3.24.9. To do so, the value of gtk-overlay-scrolling has to be set to false in the [Settings] section of the settings file:

GTK 4 will no longer support GTK_OVERLAY_SCROLLING. It has already been dropped from master. As of GTK 4, the overlay nature of the scrollbars is part of the toolkit. The blanket toggle has been removed to prevent developers from breaking applications that have not been tested with both combinations. To allow application developers to decide what their applications should look like, the toolkit instead provides a mechanism to opt-out or add a setting for users. The function gtk_scrolled_window_set_overlay_scrolling() can be used to enable/disable overlay scrolling on a per-application basis. Application developers can optionally use GSettings to have a user setting bound to the property.

The positions of the overlay scrollbars are indicated by thin dashed lines in the application window. These dashed lines will be present even when overlay scrolling is disabled using the environment variable discussed in the section above. To remove the indicator lines, create the following file:

GTK example configurations:

GDK (the underlying abstraction layer of GTK) supports multiple backends to display GTK applications.

The GDK Wayland backend is supported only by gtk3 or newer and is the default backend when using Wayland display server.

Applications that use versions of GTK prior to gtk3 do not have Wayland support, and need to use Xwayland in order to run on a Wayland session using the X11 backend.

When using the Wayland backend, some variables are not sourced from settings.ini. Any key that is present in the GSettings schema org.gnome.desktop.interface are read from there instead of settings.ini.

An example of such variables are gtk-color-scheme and icon-theme, which must have their keys set with GSettings in order to theme GTK Applications under Wayland. Alternatively, if only the theme needs to be customized, the environment variable GTK_THEME can be set.

See the relevant article on the sway wiki for more details on this.

If Xorg display server is in use, the backend defaults to x11 automatically.

It is possible to force GTK3 applications running on a wayland session to use the X11 backend through Xwayland by setting the environment variable GDK_BACKEND=x11.

The GDK Broadway backend provides support for displaying GTK applications in a web browser, using HTML5 and web sockets. [4]

When using broadwayd, specify the display number to use, prefixed with a colon, similar to X. The default display number is 0 (zero).

Point your browser to http://127.0.0.1:port

To Start applications

Alternatively can set address and port

In general, if a selected theme has support for both GTK 2 and GTK 3, the theme will be applied to all GTK 2 and GTK 3 applications. If a selected theme has support for only GTK 2, it will be used for GTK 2 applications and the default GTK theme will be used for GTK 3 applications. If the selected theme has support for only GTK 3, it will be used for GTK 3 applications and the default GTK theme will be used for GTK 2 applications. Thus for application theme consistency, it is best to use a theme which has support for both GTK 2 and GTK 3.

You could find what themes installed on your system have both an GTK 2 and GTK 3 version by using this command (does not work with names containing spaces):

As user theme files ($XDG_CONFIG_HOME/gtk-3.0/settings.ini, ~/.gtkrc-2.0) are not read by other accounts, the selected theme will not apply to X applications run as root. Possible solutions include:

GTK 3.12 introduced client-side decorations, which move the title-bar away from the window manager. This may present issues such as double title-bars, no title-bar at all, double shadows with compositing enabled, or being unable to move a frozen application.

To remove the shadow and gap around windows (for example in combination with a tiling window manager), create the following file:

Note that if visual problems persist, you may want to use the GTK Inspector to find the offending elements as explained here [5].

To adjust the buttons in the header bar, use the gtk-decoration-layout setting. [6] The below examples removes all buttons:

To remove client-side decorations altogether, you can use gtk3-classicAUR which contains a patch that disables them by default. To enable client-side decorations, set the GTK_CSD environment variable with any value.

Alternatively, you can use gtk3-nocsd-gitAUR, see README for GTK3 or gtk4-nocsd-gitAUR, see README for GTK4.

See [7], and [8] for a workaround using Xcompose (US international layout).

If you do not use any Gnome Accessibility features, you may receive warnings like:

To suppress these warnings, execute programs with NO_AT_BRIDGE=1 or set that as a global environment variable.

If you are using a window manager which uses a window decoration theme that mimics the GTK theme background color, you may find that the titlebar color no longer completely matches the application color in some GTK 3 applications. As a workaround, create the following file:

Define GDK_CORE_DEVICE_EVENTS=1 to use GTK2 style input, instead of xinput2. [10]

Install gtk2-patched-filechooser-icon-viewAUR and gtk3-patched-filechooser-icon-viewAUR to have the option to view files as thumbnails instead of list in the GTK file chooser.

The factual accuracy of this article or section is disputed.

For some applications in GNOME's Wayland session, your ~/.config/gtk-3.0/settings.ini file is misconfigured. This can happen if you try other GTK based desktop environments. These are the offending values:

Simply set them to 0 or remove the whole file to use GNOME defaults.

GTK3 depends on polkit through colord, which is required for printing. However printing works fine without polkit installed; at least with a monochrome printer and package versions gtk3-print-backends=3.22.19-2 and colord=1.4.1-1.

Depending on the theme of choice's support for GTK 2, UI controls may still have the default Raleigh appearance, possibly with a different color palette. This is due to these themes requiring the GTK 2 Murrine engine, which is missing (GTK 2 programs should complain about it on their standard error output). Install the gtk-engine-murrineAUR package.

GTK file chooser uses the same type-ahead-find feature as GNOME/Files. This can be very jarring and does not fit in very well with other desktop environments.

Some applications support XDG Desktop Portal which allows application to use the native file chooser. If that does not work you can restore type-ahead functionality by using a patched GTK, for example gtk3-classicAUR.

GTK 4 switched to grayscale antialiasing without hinting when rendering fonts. A setting is available that will restore some of the GTK 3 behavior [11]. It is on by default for non-HiDPI screens (as of August, 2023), which should produce good results for most users. Subpixel antialiasing is not available.

Because GTK switched to a new GPU renderer ngl (and vulkan in later versions), whose performance are worse than the old gl renderer(#6438 and possibly more issues), GTK 4 applications may feel sluggish and consume more resources than before.

This can be reverted by setting the environment variables below:

Recent GTK4 versions have switched to the vulkan renderer; this is problematic for users with an NVIDIA dGPU, as these will be used by default now due to GTK selecting the first usable GPU when enumerating devices and NVIDIA usually presenting the dGPU as the first one. The most reliable solution for the time being is to revert to one of the OpenGL-based renderers; this can be done by setting the GSK_RENDERER=ngl environment variable (or GSK_RENDERER=gl for the old GL backend, which may perform better).

Alternatively, set the GDK_VULKAN_DEVICE=device_index environment variable; run a GTK application with GDK_VULKAN_DEVICE=list set first to find the correct device index. This has the downside of still waking up the dGPU on application start/shutdown and is more likely to break in some way if using a MUX switch to switch to the dGPU-only mode.

Finally, NVIDIA driver can be told to put the dGPU last in Vulkan device enumeration with __NV_PRIME_RENDER_OFFLOAD=1 __VK_LAYER_NV_optimus=non_NVIDIA_only environment variables. While it works, it is a very bad idea to set this globally, as it will make all OpenGL applications use the dGPU (__VK_LAYER_NV_optimus=non_NVIDIA_only is only respected if __NV_PRIME_RENDER_OFFLOAD=1 is set, and the latter causes OpenGL applications to use the dGPU).

Users of pre-RDNA AMD graphics cards may experience some images and previews being generated glitched (#7559).

As a workaround, set the GDK_DISABLE=dmabuf environment variable [12].

**Examples:**

Example 1 (unknown):
```unknown
org.gnome.desktop.interface
```

Example 2 (unknown):
```unknown
$ gsettings set org.gnome.desktop.interface gtk-theme Adwaita
```

Example 3 (unknown):
```unknown
$ GTK_THEME=Adwaita:dark gnome-calculator
```

Example 4 (unknown):
```unknown
ADW_DISABLE_PORTAL=1
```

---

## List of applications/Internet

**URL:** https://wiki.archlinux.org/title/Pastebin

**Contents:**
- Network connection
  - Network managers
  - VPN clients
  - Proxy servers
  - Anonymizing networks
  - Network tunnels
  - Deep packet inspection circumvention
  - Speedtest tools
- Web browsers
  - Console

See Network configuration#Network managers.

Tools to avoid censorship, bandwidth throttle without anonymization. See Wikipedia:Deep packet inspection, Wikipedia:Internet censorship circumvention for an introduction to the topic.

See also Wikipedia:Comparison of web browsers.

See also Wikipedia:Gecko (software).

See also Wikipedia:Blink (web engine).

See also Wikipedia:WebKit.

Most of these support ad-blocking via wyebadblock.

See also Wikipedia:Goanna (software).

See also Wikipedia:Gemini (protocol)#Software.

A web server serves HTML web pages and other files via HTTP to clients like web browsers. The major web servers can be interfaced with programs to serve dynamic content (web applications).

See also Category:Web server and Wikipedia:Comparison of web server software.

The Python standard library module http.server can also be used from the command-line.

Apache also supports WSGI with mod_wsgi.

See also Wikipedia:Comparison of download managers.

See also #LAN messengers.

See also Wikipedia:Comparison of FTP client software.

Some file managers like Dolphin, GNOME Files and Thunar also provide FTP functionality.

See also Wikipedia:List of FTP server software.

Some download managers are also able to connect to the BitTorrent network: Aria2, LFTP, FatRat, KGet, MLDonkey, uGet.

See also Wikipedia:Comparison of BitTorrent clients.

See also Wikipedia:Comparison of file-sharing applications.

See also Wikipedia:Pastebin.

Pastebin services are often used to quote text or images while collaborating and troubleshooting. Pastebin clients provide a convenient way to post from the command line.

Some services can be used with more general command line tool, such as cURL. For extensions, such as line numbers, one can use more command line tools. Such as cat -n.

See also Wikipedia:Comparison of email clients

See also Wikipedia:Mail retrieval agent.

See also Wikipedia:Comparison of instant messaging clients and Wikipedia:Comparison of VoIP software.

This section lists all client software with instant messaging support.

The number of networks supported by these clients is very large but they (like any multi-protocol clients) usually have very limited or no support for network-specific features.

See also Wikipedia:Comparison of Internet Relay Chat clients.

See also Wikipedia:XMPP.

See also Wikipedia:List of SIP software#Clients.

See also Matrix and Matrix Clients.

See also Tox and comparison clients

See also Avahi#Link-Local (Bonjour/Zeroconf) chat and Wikipedia:Comparison of LAN messengers.

See also Ring and Tox.

See also Wikipedia:Comparison of instant messaging protocols.

See also Wikipedia:List of SIP software#Servers.

See also Wikipedia:Collaborative software.

Web feeds aggregators. Some email clients are also able to act as news aggregator: Claws Mail RSSyl plugin, Evolution, SeaMonkey Mail & Newsgroups, Thunderbird.

See also Wikipedia:Comparison of feed aggregators.

Some media players are also able to act as podcast client: Amarok, Cantata, Clementine, Goggles Music Manager, Rhythmbox, VLC media player. git-annex can also function as podcatcher.

See also Wikipedia:List of podcatchers.

Some email clients are also able to act as Usenet newsreader: Claws Mail, Evolution, NeoMutt, SeaMonkey Mail & Newsgroups, Sylpheed, Thunderbird.

See also: Wikipedia:List of Usenet newsreaders, Wikipedia:Comparison of Usenet newsreaders.

See also Wikipedia:Blog software and Wikipedia:List of content management systems.

See also Wikipedia:Remote desktop software and Wikipedia:Comparison of remote desktop software.

See also Chrome Remote Desktop for a web browser based solution.

**Examples:**

Example 1 (unknown):
```unknown
eric6_browser
```

Example 2 (unknown):
```unknown
$ command | curl -F 'file=@-' 0x0.st
```

Example 3 (unknown):
```unknown
$ curl -F 'file=@-' 0x0.st < file
```

Example 4 (unknown):
```unknown
pastebinit -l
```

---

## International communities

**URL:** https://wiki.archlinux.org/title/International_communities

**Contents:**
- International (English)
- Arabic
- Czech republic
- Catalan
- Chinese
  - Mainland China
  - Taiwan
- Croatian
- Danish
- French

https://archlinux.org is the home of the official Arch Linux website, forums, and wiki. However, several unofficial community-run sites are available for international users who would rather communicate in their own language. You can find a list of these communities below.

Please note that our official wiki also supports internationalized content. In addition, we have an international forum where you can post in your native language. These tools can be used in addition to, in parallel with, or as a part of the international communities listed below.

This list is by no means complete. If you know of a community that is not listed, please feel free to add it. If you are looking for a community that does not have an existing community, start one or visit the Other Languages forum.

English communication platforms can be found in Getting involved.

---

## Ratpoison

**URL:** https://wiki.archlinux.org/title/Ratpoison

**Contents:**
- Installation
- Ratpoison and display managers
- Using Ratpoison
- Useful key bindings
- Tips and tricks
  - Screen reader accessibility
  - Java applications
  - Switch to another window manager temporarily
  - Multiple workspaces
  - URxvt and xterm

This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.

Ratpoison is a manually tiling window manager written in C that allows the user to manage graphical windows without a mouse. The user interface is inspired by GNU Screen.

By default, Ratpoison keybindings work quite similarly to Emacs and GNU Screen. Commands begin by pressing the prefix key (by default Ctrl+t), and are then followed by another key combination such as Ctrl+Space to move to the next window.

Install the ratpoisonAUR package from AUR.

Many display managers (e.g., LightDM) source the available sessions from /usr/share/xsessions/ and most window managers and desktop environments install their desktop entries there. However, Ratpoison instead creates a desktop entry file in /etc/X11/sessions/ratpoison.desktop. To allow display managers to find Ratpoison one may need to copy the Ratpoison to desktop entry /usr/share/xsessions/ratpoison.desktop. You can alternatively use a symbolic link. If the /usr/share/xsessions directory does not exist, create it as root.

To start Ratpoison, you need to first logout and select "Ratpoison" from your Display manager. Alternatively, you can configure xinit and use startx.

After X11 starts up you will see a black screen and a little textbox on the upper right of it that says "Welcome to Ratpoison". Now type Ctrl+t and then ? to get a list of keybindings. If you are used to GNU Screen or Tmux, you will feel at home very soon.

You are able to define custom keystrokes and even override existing ones in ~/.ratpoisonrc

Here is an example config file:

By default, Ratpoison is not accessible with Orca screen-reader. There is a project at gitlab.com/stormdragon2976/strychnine that replaces the default Ratpoison widgets with GTK counterparts.

Follow the on screen prompts, and you will get a ~/.ratpoisonrc that will automatically launch Orca when it starts. See the included README.md for more options.

Java GUI applications assume stacking window managers, and do not go to fullscreen properly with the default Ratpoison configuration. See Java#Impersonate another window manager and Java#Gray window, applications not resizing with WM, menus immediately closing for solutions.

If a program misbehaves under Ratpoison, you can temporarily switch to another window manager. You need to be sure that your temporary window manager doesn't kill all programs when it exits. Notably at least IceWM is known to unfortunately not work because of that.

To switch to another window manager temporarily, you can use tmpwm Ratpoison command with ctrl+t : tmpwm <RET>

By default, Ratpoison only has one workspace. But Ratpoison comes with rpws script that can be used to add more workspaces.

Edit your ~/.ratpoisonrc, and add:

That creates 2 workspaces. By default, you can access to them by using Alt+F1 to access the first, Alt+F2 to access the second, etc.

You can also add binds to them, like this:

That allows to access the first workspace with Ctrl+t Ctrl+1 (assuming Ctrl+t as your escape/prefix key).

URxvt and xterm, as they are installed by default, send resize hints to the window manager. This works in most tiling window managers, but not in Ratpoison. The end result is that URxvt/xterm resizes itself in multiples of the font size, rather than resizing to the whole screen, and chances that there are unfilled gaps are high. There are two solutions to this problem, documented below.

If you use URxvt, the rxvt-unicode-fontspacing-noinc-vteclear-secondarywheelAUR package, among other improvements, sends no resize hints to the window manager. If you install this version of URxvt rather than the default, URxvt will resize properly within Ratpoison.

We can use the URxvt/xterm option internalBorder and set the border of Ratpoison to 0.

A trial and error process must be done to find the exact number of internalBorder for each combination of resolution and font size. (the border of Ratpoison must be set to 0 before doing the tests) The term command line option -b can be used to test for the correct number and then can be saved on the following files.

If a combination cannot be found, you could try changing the font size and the font family also (that changes the required border number).

Examples for launching programs when Ratpoison starts. File ~/.ratpoisonrc is executed by Ratpoison on startup.

To launch URxvt with a Tmux session:

To launch Chromium with the cache in a tmpfs:

Example for setting transparency using xcompmgr and Nitrogen. First start nitrogen and set the desired wallpaper. Then use this in your ~/.ratpoisonrc

sloppy.c is a companion program for Ratpoison and can be found in /usr/share/ratpoison/. To enable focus-follows-mouse (also called "sloppy focus"), run the following commands:

To autostart focus-follows-mouse feature, add the following to your ~/.ratpoisonrc:

Sloppy.c inhibits continued use of the keyboard to make focus changes. An improved version of focus follows mouse which does not interfere with keyboard-driven focus changes is available ratpoison-sloppymoveAUR

To autostart ratpoison-sloppymove, add the following to your ~/.ratpoisonrc:

**Examples:**

Example 1 (unknown):
```unknown
/usr/share/xsessions/
```

Example 2 (unknown):
```unknown
/etc/X11/sessions/ratpoison.desktop
```

Example 3 (unknown):
```unknown
/usr/share/xsessions/ratpoison.desktop
```

Example 4 (unknown):
```unknown
/usr/share/xsessions
```

---

## Help:Reading

**URL:** https://wiki.archlinux.org/title/Restart

**Contents:**
- Organization
- Formatting
- Root, regular user or another user
- Append, add, create, edit
  - Make executable
- Source
- Installation of packages
  - Official packages
  - Arch User Repository
- Control of systemd units

Because the vast majority of the ArchWiki contains indications that may need clarification for users new to Arch Linux (or GNU/Linux in general), this rundown of basic procedures was written both to avoid confusion in the assimilation of the articles and to deter repetition in the content itself.

Most articles on the ArchWiki do not attempt to provide a holistic introduction to a single topic; they are instead written in adherence to the "Don't Repeat Yourself" principle, under the assumption that the user will seek out and read any supporting material that they do not yet understand. Where possible, such supporting material is indicated in the article via special formatting, see #Formatting.

Because of this organization, it may be necessary to examine several related sources in order to fully understand an ArchWiki article. In particular, users who are new to Arch (or GNU/Linux in general) should expect to end up reading a great number of articles even when solving simple problems. It is especially important to study the supporting material before seeking additional help from other users.

Some lines are written like so:

Others have a different prefix:

The numeral or hash sign (#) indicates that the command needs to be run as root, whereas the dollar sign ($) shows that the command should be run as a regular user.

When the commands need to run as a specific user, they will be prefixed by the username in square brackets, for example:

This means you should use a privilege elevation tool, e.g. with sudo:

A notable exception to watch out for:

In this example, the context surrounding the numeral sign communicates that this is not to be run as a command; it should be edited into a file instead. So in this case, the numeral sign denotes a comment. A comment can be explanatory text that will not be interpreted by the associated program. Bash scripts denotation for comments happens to coincide with the root PS1.

After further examination, "give away" signs include the uppercase character following the # sign. Usually, Unix commands are not written this way and most of the time they are short abbreviations instead of full-blown English words (e.g., Copy becomes cp).

Regardless, most articles make this easy to discern by notifying the reader:

Append to ~/path/to/file:

When prompted to append to, add to, create, or edit one or more files, it is implied that you should use one of the following methods.

To create or modify multiline files, it is suggested to use a text editor. For example, using the nano command to edit the file /etc/bash.bashrc is:

To create or overwrite a file from a string, it may be simpler to use output redirection. The following example creates or overwrites the contents of the file /etc/hostname with the text myhostname.

Output redirection can also be used to append a string to a file. The following example appends the text [custom-repo] to the file /etc/pacman.conf.

When prompted to create directories, use the mkdir command:

After creating a file, if it is meant to be run as a script (whether manually or called by another program), it needs to be set as executable, for example with:

See chmod. Some applications such as file managers may provide graphical interfaces to do the same.

Some applications, notably command-line shells, use scripts for their configuration: after modifying them, they must be sourced in order for the changes to be applied. In the case of bash, for example, this is done by running (you can also replace source with .):

When the wiki suggests modifying such a configuration script, it will not explicitly remind you to source the file, and only in some cases will it point to this section with a reminder link.

When an article invites you to install some packages in the conventional way, it will not indicate the detailed instructions to do so; instead, it will simply mention the names of the packages to be installed.

The subsections below give an overview of the generic installation procedures depending on the package type.

For packages from the official repositories, you will read something like:

This means that you have to run:

The pacman article contains detailed explanations to deal with package management in Arch Linux proficiently.

For packages from the Arch User Repository (AUR), you will read something like:

This means that in general you have to follow the foobarAUR link, download the PKGBUILD archive, extract it, verify the content and finally run, in the same folder:

The Arch User Repository article contains all the detailed explanations and best practices to deal with AUR packages.

When an article invites to start, enable, etc., some systemd unit (e.g. a service), it will not indicate the detailed instructions to do so, but instead you will read something like:

This means that you have to run:

A notable command that does not follow this exact pattern is systemctl daemon-reload which will be called without arguments.

The systemd#Using units section contains structured list of available actions (like start, enable, enable and start, etc.) with their corresponding systemctl commands.

It is important to remember that there are two different kinds of configurations on a GNU/Linux system. System-wide configuration affects all users. Since system-wide settings are generally located in the /etc directory, root privileges are required in order to alter them. For example, to apply a Bash setting that affects all users, /etc/bash.bashrc should be modified.

User-specific configuration affects only a single user. Dotfiles are used for user-specific configuration. For example, the file ~/.bashrc is the user-specific configuration file. The idea is that each user can define their own settings, such as aliases, functions and other interactive features like the prompt, without affecting other users' preferences.

Bash and other Bourne-compatible shells, such as Zsh, also source files depending on whether the shell is a login shell or an interactive shell. See Bash#Configuration files and Zsh#Startup/Shutdown files for details.

Some code blocks may contain so-called pseudo-variables, which, as the name says, are not actual variables used in the code. Instead they are generic placeholders and have to be manually replaced with system-specific configuration items before the code may be run or parsed. Common shells such as bash and zsh provide tab-completion to auto-complete parameters for common commands such as systemctl.

In the articles that comply with Help:Style/Formatting and punctuation, pseudo-variables are formatted in italics. For example:

In this case interface_name is used as a pseudo-variable placeholder in a systemd template unit. All systemd template units, identifiable by the @ sign, require a system-specific configuration item as argument. See systemd#Using units.

In this case the pseudo-variables are used to describe the parameters that must be substituted for them. Details on how to gather them are elaborated on in the section Securely wipe disk#Calculate blocks to wipe manually, which features the command.

This article or section needs expansion.

In case of file examples, pasting pseudo-variables in real configuration files might break the programs that use them.

In most cases, ellipses (...) are not part of the actual file content or code output, and instead represent omitted or optional text that is not relevant for the discussed subject.

For example HOOKS=(... encrypt ... filesystems ...) or:

Be aware though that, in a few instances, ellipses may be a meaningful part of the code syntax: attentive users should be able to recognize these cases by the context.

**Examples:**

Example 1 (unknown):
```unknown
# mkinitcpio -p linux
```

Example 2 (unknown):
```unknown
$ makepkg -s
```

Example 3 (unknown):
```unknown
sudo command
```

Example 4 (unknown):
```unknown
[postgres]$ initdb -D /var/lib/postgres/data
```

---

## Arch Linux press coverage

**URL:** https://wiki.archlinux.org/title/Arch_Linux_press_coverage

**Contents:**
- 2025
- 2024
- 2023
- 2022
- 2021
- 2020
- 2019
- 2018
- 2017
- 2016

See also Wikipedia:Arch Linux and Arch Linux on DistroWatch.

---

## Help:Reading

**URL:** https://wiki.archlinux.org/title/Textedit

**Contents:**
- Organization
- Formatting
- Root, regular user or another user
- Append, add, create, edit
  - Make executable
- Source
- Installation of packages
  - Official packages
  - Arch User Repository
- Control of systemd units

Because the vast majority of the ArchWiki contains indications that may need clarification for users new to Arch Linux (or GNU/Linux in general), this rundown of basic procedures was written both to avoid confusion in the assimilation of the articles and to deter repetition in the content itself.

Most articles on the ArchWiki do not attempt to provide a holistic introduction to a single topic; they are instead written in adherence to the "Don't Repeat Yourself" principle, under the assumption that the user will seek out and read any supporting material that they do not yet understand. Where possible, such supporting material is indicated in the article via special formatting, see #Formatting.

Because of this organization, it may be necessary to examine several related sources in order to fully understand an ArchWiki article. In particular, users who are new to Arch (or GNU/Linux in general) should expect to end up reading a great number of articles even when solving simple problems. It is especially important to study the supporting material before seeking additional help from other users.

Some lines are written like so:

Others have a different prefix:

The numeral or hash sign (#) indicates that the command needs to be run as root, whereas the dollar sign ($) shows that the command should be run as a regular user.

When the commands need to run as a specific user, they will be prefixed by the username in square brackets, for example:

This means you should use a privilege elevation tool, e.g. with sudo:

A notable exception to watch out for:

In this example, the context surrounding the numeral sign communicates that this is not to be run as a command; it should be edited into a file instead. So in this case, the numeral sign denotes a comment. A comment can be explanatory text that will not be interpreted by the associated program. Bash scripts denotation for comments happens to coincide with the root PS1.

After further examination, "give away" signs include the uppercase character following the # sign. Usually, Unix commands are not written this way and most of the time they are short abbreviations instead of full-blown English words (e.g., Copy becomes cp).

Regardless, most articles make this easy to discern by notifying the reader:

Append to ~/path/to/file:

When prompted to append to, add to, create, or edit one or more files, it is implied that you should use one of the following methods.

To create or modify multiline files, it is suggested to use a text editor. For example, using the nano command to edit the file /etc/bash.bashrc is:

To create or overwrite a file from a string, it may be simpler to use output redirection. The following example creates or overwrites the contents of the file /etc/hostname with the text myhostname.

Output redirection can also be used to append a string to a file. The following example appends the text [custom-repo] to the file /etc/pacman.conf.

When prompted to create directories, use the mkdir command:

After creating a file, if it is meant to be run as a script (whether manually or called by another program), it needs to be set as executable, for example with:

See chmod. Some applications such as file managers may provide graphical interfaces to do the same.

Some applications, notably command-line shells, use scripts for their configuration: after modifying them, they must be sourced in order for the changes to be applied. In the case of bash, for example, this is done by running (you can also replace source with .):

When the wiki suggests modifying such a configuration script, it will not explicitly remind you to source the file, and only in some cases will it point to this section with a reminder link.

When an article invites you to install some packages in the conventional way, it will not indicate the detailed instructions to do so; instead, it will simply mention the names of the packages to be installed.

The subsections below give an overview of the generic installation procedures depending on the package type.

For packages from the official repositories, you will read something like:

This means that you have to run:

The pacman article contains detailed explanations to deal with package management in Arch Linux proficiently.

For packages from the Arch User Repository (AUR), you will read something like:

This means that in general you have to follow the foobarAUR link, download the PKGBUILD archive, extract it, verify the content and finally run, in the same folder:

The Arch User Repository article contains all the detailed explanations and best practices to deal with AUR packages.

When an article invites to start, enable, etc., some systemd unit (e.g. a service), it will not indicate the detailed instructions to do so, but instead you will read something like:

This means that you have to run:

A notable command that does not follow this exact pattern is systemctl daemon-reload which will be called without arguments.

The systemd#Using units section contains structured list of available actions (like start, enable, enable and start, etc.) with their corresponding systemctl commands.

It is important to remember that there are two different kinds of configurations on a GNU/Linux system. System-wide configuration affects all users. Since system-wide settings are generally located in the /etc directory, root privileges are required in order to alter them. For example, to apply a Bash setting that affects all users, /etc/bash.bashrc should be modified.

User-specific configuration affects only a single user. Dotfiles are used for user-specific configuration. For example, the file ~/.bashrc is the user-specific configuration file. The idea is that each user can define their own settings, such as aliases, functions and other interactive features like the prompt, without affecting other users' preferences.

Bash and other Bourne-compatible shells, such as Zsh, also source files depending on whether the shell is a login shell or an interactive shell. See Bash#Configuration files and Zsh#Startup/Shutdown files for details.

Some code blocks may contain so-called pseudo-variables, which, as the name says, are not actual variables used in the code. Instead they are generic placeholders and have to be manually replaced with system-specific configuration items before the code may be run or parsed. Common shells such as bash and zsh provide tab-completion to auto-complete parameters for common commands such as systemctl.

In the articles that comply with Help:Style/Formatting and punctuation, pseudo-variables are formatted in italics. For example:

In this case interface_name is used as a pseudo-variable placeholder in a systemd template unit. All systemd template units, identifiable by the @ sign, require a system-specific configuration item as argument. See systemd#Using units.

In this case the pseudo-variables are used to describe the parameters that must be substituted for them. Details on how to gather them are elaborated on in the section Securely wipe disk#Calculate blocks to wipe manually, which features the command.

This article or section needs expansion.

In case of file examples, pasting pseudo-variables in real configuration files might break the programs that use them.

In most cases, ellipses (...) are not part of the actual file content or code output, and instead represent omitted or optional text that is not relevant for the discussed subject.

For example HOOKS=(... encrypt ... filesystems ...) or:

Be aware though that, in a few instances, ellipses may be a meaningful part of the code syntax: attentive users should be able to recognize these cases by the context.

**Examples:**

Example 1 (unknown):
```unknown
# mkinitcpio -p linux
```

Example 2 (unknown):
```unknown
$ makepkg -s
```

Example 3 (unknown):
```unknown
sudo command
```

Example 4 (unknown):
```unknown
[postgres]$ initdb -D /var/lib/postgres/data
```

---

## Help:Reading

**URL:** https://wiki.archlinux.org/title/Mask

**Contents:**
- Organization
- Formatting
- Root, regular user or another user
- Append, add, create, edit
  - Make executable
- Source
- Installation of packages
  - Official packages
  - Arch User Repository
- Control of systemd units

Because the vast majority of the ArchWiki contains indications that may need clarification for users new to Arch Linux (or GNU/Linux in general), this rundown of basic procedures was written both to avoid confusion in the assimilation of the articles and to deter repetition in the content itself.

Most articles on the ArchWiki do not attempt to provide a holistic introduction to a single topic; they are instead written in adherence to the "Don't Repeat Yourself" principle, under the assumption that the user will seek out and read any supporting material that they do not yet understand. Where possible, such supporting material is indicated in the article via special formatting, see #Formatting.

Because of this organization, it may be necessary to examine several related sources in order to fully understand an ArchWiki article. In particular, users who are new to Arch (or GNU/Linux in general) should expect to end up reading a great number of articles even when solving simple problems. It is especially important to study the supporting material before seeking additional help from other users.

Some lines are written like so:

Others have a different prefix:

The numeral or hash sign (#) indicates that the command needs to be run as root, whereas the dollar sign ($) shows that the command should be run as a regular user.

When the commands need to run as a specific user, they will be prefixed by the username in square brackets, for example:

This means you should use a privilege elevation tool, e.g. with sudo:

A notable exception to watch out for:

In this example, the context surrounding the numeral sign communicates that this is not to be run as a command; it should be edited into a file instead. So in this case, the numeral sign denotes a comment. A comment can be explanatory text that will not be interpreted by the associated program. Bash scripts denotation for comments happens to coincide with the root PS1.

After further examination, "give away" signs include the uppercase character following the # sign. Usually, Unix commands are not written this way and most of the time they are short abbreviations instead of full-blown English words (e.g., Copy becomes cp).

Regardless, most articles make this easy to discern by notifying the reader:

Append to ~/path/to/file:

When prompted to append to, add to, create, or edit one or more files, it is implied that you should use one of the following methods.

To create or modify multiline files, it is suggested to use a text editor. For example, using the nano command to edit the file /etc/bash.bashrc is:

To create or overwrite a file from a string, it may be simpler to use output redirection. The following example creates or overwrites the contents of the file /etc/hostname with the text myhostname.

Output redirection can also be used to append a string to a file. The following example appends the text [custom-repo] to the file /etc/pacman.conf.

When prompted to create directories, use the mkdir command:

After creating a file, if it is meant to be run as a script (whether manually or called by another program), it needs to be set as executable, for example with:

See chmod. Some applications such as file managers may provide graphical interfaces to do the same.

Some applications, notably command-line shells, use scripts for their configuration: after modifying them, they must be sourced in order for the changes to be applied. In the case of bash, for example, this is done by running (you can also replace source with .):

When the wiki suggests modifying such a configuration script, it will not explicitly remind you to source the file, and only in some cases will it point to this section with a reminder link.

When an article invites you to install some packages in the conventional way, it will not indicate the detailed instructions to do so; instead, it will simply mention the names of the packages to be installed.

The subsections below give an overview of the generic installation procedures depending on the package type.

For packages from the official repositories, you will read something like:

This means that you have to run:

The pacman article contains detailed explanations to deal with package management in Arch Linux proficiently.

For packages from the Arch User Repository (AUR), you will read something like:

This means that in general you have to follow the foobarAUR link, download the PKGBUILD archive, extract it, verify the content and finally run, in the same folder:

The Arch User Repository article contains all the detailed explanations and best practices to deal with AUR packages.

When an article invites to start, enable, etc., some systemd unit (e.g. a service), it will not indicate the detailed instructions to do so, but instead you will read something like:

This means that you have to run:

A notable command that does not follow this exact pattern is systemctl daemon-reload which will be called without arguments.

The systemd#Using units section contains structured list of available actions (like start, enable, enable and start, etc.) with their corresponding systemctl commands.

It is important to remember that there are two different kinds of configurations on a GNU/Linux system. System-wide configuration affects all users. Since system-wide settings are generally located in the /etc directory, root privileges are required in order to alter them. For example, to apply a Bash setting that affects all users, /etc/bash.bashrc should be modified.

User-specific configuration affects only a single user. Dotfiles are used for user-specific configuration. For example, the file ~/.bashrc is the user-specific configuration file. The idea is that each user can define their own settings, such as aliases, functions and other interactive features like the prompt, without affecting other users' preferences.

Bash and other Bourne-compatible shells, such as Zsh, also source files depending on whether the shell is a login shell or an interactive shell. See Bash#Configuration files and Zsh#Startup/Shutdown files for details.

Some code blocks may contain so-called pseudo-variables, which, as the name says, are not actual variables used in the code. Instead they are generic placeholders and have to be manually replaced with system-specific configuration items before the code may be run or parsed. Common shells such as bash and zsh provide tab-completion to auto-complete parameters for common commands such as systemctl.

In the articles that comply with Help:Style/Formatting and punctuation, pseudo-variables are formatted in italics. For example:

In this case interface_name is used as a pseudo-variable placeholder in a systemd template unit. All systemd template units, identifiable by the @ sign, require a system-specific configuration item as argument. See systemd#Using units.

In this case the pseudo-variables are used to describe the parameters that must be substituted for them. Details on how to gather them are elaborated on in the section Securely wipe disk#Calculate blocks to wipe manually, which features the command.

This article or section needs expansion.

In case of file examples, pasting pseudo-variables in real configuration files might break the programs that use them.

In most cases, ellipses (...) are not part of the actual file content or code output, and instead represent omitted or optional text that is not relevant for the discussed subject.

For example HOOKS=(... encrypt ... filesystems ...) or:

Be aware though that, in a few instances, ellipses may be a meaningful part of the code syntax: attentive users should be able to recognize these cases by the context.

**Examples:**

Example 1 (unknown):
```unknown
# mkinitcpio -p linux
```

Example 2 (unknown):
```unknown
$ makepkg -s
```

Example 3 (unknown):
```unknown
sudo command
```

Example 4 (unknown):
```unknown
[postgres]$ initdb -D /var/lib/postgres/data
```

---

## PuTTY

**URL:** https://wiki.archlinux.org/title/PuTTY

**Contents:**
- Installation
- Configuration
- Importing keys
- 256 color support

PuTTY is a port of the popular GUI SSH, Telnet, Rlogin and serial port connection client for Windows. It has support for advanced logging and termcap options, as well as a very configurable appearance and the ability to forward ports or create a SOCKS tunnel through an SSH destination. To start, simply run PuTTY, type the hostname of the host you would like to connect to and hit Open.

The settings can be modified through the tabs on the left; however, they will be reset if not saved. To save your settings, type a name into the box under Saved Sessions and click save. To load it again later, click the name of your save and click load. This allows you to persist your visual, termcap and connection settings between connections and also lets you keep one save per regularly used host. The save "Default Settings" is automatically loaded every time you start PuTTY, so save under that name with care.

PuTTY uses its own format to store the private keys on the client side. To import a key that you had previously generated, you need to use the puttygen command.

where keyfile is an existent private keyfile, and output-keyfile.ppk is the file that will receive the key.

If the keyfile is protected with a passphrase, you will be prompted to input it, but the key will still be protected afterwards in the output-keyfile.ppk.

After that, you can import it to make an SSH connection: Connection > SSH > Auth > Private key file for authentication and click on Browse... to add it.

Settings > Connection > Data : Terminal-type string = putty-256color

To test color support, execute the following command[1]:

See 256colours for details.

**Examples:**

Example 1 (unknown):
```unknown
$ puttygen keyfile -o output-keyfile.ppk
```

Example 2 (unknown):
```unknown
output-keyfile.ppk
```

Example 3 (unknown):
```unknown
output-keyfile.ppk
```

Example 4 (unknown):
```unknown
for code in {0..255}; do echo -e "\e[38;05;${code}m $code: Test"; done
```

---

## ArchWiki:About

**URL:** https://wiki.archlinux.org/title/ArchWiki

**Contents:**
- Goals
  - Comprehensive
  - Accessible
- How it works
- How you can help
- Early history
- Contacts
- Thanks

This article is a general overview of the ArchWiki, answering the questions "Who?", "What?", "Why?", and "How?".

The primary objective of ArchWiki is to provide the Arch Linux user community with the most comprehensive and the most accessible documentation on the web. This is a world-wide project with many participants from all corners of the globe. The wiki embraces all Arch Linux principles, in particular with respect to simplicity. ArchWiki is constantly evolving and thrives on collaboration; there is always room for improvement.

One of the primary objectives of ArchWiki is to cover all aspects of computing using Arch Linux. From the media center to managing corporate networks, anything Arch Linux can achieve is within the scope of ArchWiki.

The primary goal of ArchWiki may be grandiose, and thousands of articles are required; ArchWiki must offer a simple and intuitive way of accessing all the information. This is not a simple task, but typical Arch ingenuity and imagination will prevail.

The official arch-wiki-docs package offers users the ability to download a copy of ArchWiki content. See this forum discussion for details.

ArchWiki is maintained by the official Maintenance Team and thousands of contributors. Users edit and organize articles to offer readers the best in Arch Linux documentation. The contents are created by the community, for the community: most articles are not written by a single person, but represent a cumulative effort of many people that correct erroneous information, update outdated articles, and otherwise keep improving the ArchWiki contents.

ArchWiki is maintained by volunteers. Contributors are never too numerous, and help is always needed. If you feel capable of editing wiki pages, consider donating some of your time and energy to the community.

Please see ArchWiki:Contributing to discover ways to contribute.

See ArchWiki:Maintenance Team for a list of ArchWiki coordinators.

---

## File permissions and attributes

**URL:** https://wiki.archlinux.org/title/Chmod

**Contents:**
- Viewing permissions
  - Examples
- Changing permissions
  - Text method
    - Text method shortcuts
    - Copying permissions
  - Numeric method
  - Bulk chmod
- Changing ownership
- Access Control Lists

File systems use permissions and attributes to regulate the level of interaction that system processes can have with files and directories.

Use the ls command's -l option to view the permissions (or file mode) set for the contents of a directory, for example:

The first column is what we must focus on. Taking an example value of drwxrwxrwx+, the meaning of each character is explained in the following tables:

Each of the three permission triads (rwx in the example above) can be made up of the following characters:

See info Coreutils -n "Mode Structure" and chmod(1) for more details.

Let us see some examples to clarify:

Archie has full access to the Documents directory. They can list, create files and rename, delete any file in Documents, regardless of file permissions. Their ability to access a file depends on the file's permissions.

Archie has full access except they can not create, rename, delete any file. They can list the files and (if the file's permissions allow it) may access an existing file in Documents.

Archie can not do ls in the Documents directory but if they know the name of an existing file then they may list, rename, delete or (if the file's permissions allow it) access it. Also, they are able to create new files.

Archie is only capable of (if the file's permissions allow it) accessing those files the Documents directory which they know of. They can not list already existing files or create, rename, delete any of them.

You should keep in mind that we elaborate on directory permissions and it has nothing to do with the individual file permissions. When you create a new file it is the directory that changes. That is why you need write permission to the directory.

Let us look at another example, this time of a file, not a directory:

Here we can see the first letter is not d but -. So we know it is a file, not a directory. Next the owner's permissions are rw- so the owner has the ability to read and write but not execute. This may seem odd that the owner does not have all three permissions, but the x permission is not needed as it is a text/data file, to be read by a text editor such as Gedit, EMACS, or software like R, and not an executable in its own right (if it contained something like python programming code then it very well could be). The group's permissions are set to r--, so the group has the ability to read the file but not write/edit it in any way  it is essentially like setting something to read-only. We can see that the same permissions apply to everyone else as well.

chmod is a command in Linux and other Unix-like operating systems that allows to change the permissions (or access mode) of a file or directory.

To change the permissions  or access mode  of a file, use the chmod command in a terminal. Below is the command's general structure:

Where who is any from a range of letters, each signifying who is being given the permission. They are as follows:

The permissions are the same as discussed in #Viewing permissions (r, w and x).

Now have a look at some examples using this command. Suppose you became very protective of the Documents directory and wanted to deny everybody but yourself, permissions to read, write, and execute (or in this case search/look) in it:

Before: drwxr-xr-x 6 archie web 4096 Jul 5 17:37 Documents

After: drwx------ 6 archie web 4096 Jul 6 17:32 Documents

Here, because you want to deny permissions, you do not put any letters after the = where permissions would be entered. Now you can see that only the owner's permissions are rwx and all other permissions are -.

This can be reverted with:

Before: drwx------ 6 archie web 4096 Jul 6 17:32 Documents

After: drwxr-xr-x 6 archie web 4096 Jul 6 17:32 Documents

In the next example, you want to grant read and execute permissions to the group, and other users, so you put the letters for the permissions (r and x) after the =, with no spaces.

You can simplify this to put more than one who letter in the same command, e.g:

Now let us consider a second example, suppose you want to change a foobar file so that you have read and write permissions, and fellow users in the group web who may be colleagues working on foobar, can also read and write to it, but other users can only read it:

Before: -rw-r--r-- 1 archie web 5120 Jun 27 08:28 foobar

After: -rw-rw-r-- 1 archie web 5120 Jun 27 08:28 foobar

This is exactly like the first example, but with a file, not a directory, and you grant write permission (just so as to give an example of granting every permission).

The chmod command lets add and subtract permissions from an existing set using + or - instead of =. This is different from the above commands, which essentially re-write the permissions (e.g. to change a permission from r-- to rw-, you still need to include r as well as w after the = in the chmod command invocation. If you missed out r, it would take away the r permission as they are being re-written with the =. Using + and - avoids this by adding or taking away from the current set of permissions).

Let us try this + and - method with the previous example of adding write permissions to the group:

Before: -rw-r--r-- 1 archie web 5120 Jun 27 08:28 foobar

After: -rw-rw-r-- 1 archie web 5120 Jun 27 08:28 foobar

Another example, denying write permissions to all (a):

Before: -rw-rw-r-- 1 archie web 5120 Jun 27 08:28 foobar

After: -r--r--r-- 1 archie web 5120 Jun 27 08:28 foobar

A different shortcut is the special X mode: this is not an actual file mode, but it is often used in conjunction with the -R option to set the executable bit only for directories, and leave it unchanged for regular files, for example:

It is possible to tell chmod to copy the permissions from one class, say the owner, and give those same permissions to group or even all. To do this, instead of putting r, w, or x after the =, put another who letter. e.g:

Before: -rw-r--r-- 1 archie web 5120 Jun 27 08:28 foobar

After: -rw-rw-r-- 1 archie web 5120 Jun 27 08:28 foobar

This command essentially translates to "change the permissions of group (g=), to be the same as the owning user (=u). Note that you cannot copy a set of permissions as well as grant new ones e.g.:

In that case chmod throw an error.

chmod can also set permissions using numbers.

Using numbers is another method which allows you to edit the permissions for all three owner, group, and others at the same time, as well as the setuid, setgid, and sticky bits. This basic structure of the code is this:

Where xxx is a 3-digit number where each digit can be anything from 0 to 7. The first digit applies to permissions for owner, the second digit applies to permissions for group, and the third digit applies to permissions for all others.

In this number notation, the values r, w, and x have their own number value:

To come up with a 3-digit number you need to consider what permissions you want owner, group, and all others to have, and then total their values up. For example, if you want to grant the owner of a directory read write and execution permissions, and you want group and everyone else to have just read and execute permissions, you would come up with the numerical values like so:

This is the equivalent of using the following:

To view the existing permissions of a file or directory in numeric form, use the stat(1) command:

Where the %a option specifies output in numeric form.

Most directories are set to 755 to allow reading, writing and execution to the owner, but deny writing to everyone else, and files are normally 644 to allow reading and writing for the owner but just reading for everyone else; refer to the last note on the lack of x permissions with non executable files: it is the same thing here.

To see this in action with examples consider the previous example that has been used but with this numerical method applied instead:

Before: -rw-r--r-- 1 archie web 5120 Jun 27 08:28 foobar

After: -rw-rw-r-- 1 archie web 5120 Jun 27 08:28 foobar

If this were an executable the number would be 774 if you wanted to grant executable permission to the owner and group. Alternatively if you wanted everyone to only have read permission the number would be 444. Treating r as 4, w as 2, and x as 1 is probably the easiest way to work out the numerical values for using chmod xxx filename, but there is also a binary method, where each permission has a binary number, and then that is in turn converted to a number. It is a bit more convoluted, but here included for completeness.

Consider this permission set:

If you put a 1 under each permission granted, and a 0 for every one not granted, the result would be something like this:

You can then convert these binary numbers:

The value of the above would therefore be 775.

Consider we wanted to remove the writable permission from group:

The value would therefore be 755 and you would use chmod 755 filename to remove the writable permission. You will notice you get the same three digit number no matter which method you use. Whether you use text or numbers will depend on personal preference and typing speed. When you want to restore a directory or file to default permissions e.g. read and write (and execute) permission to the owner but deny write permission to everyone else, it may be faster to use chmod 755/644 filename. However if you are changing the permissions to something out of the norm, it may be simpler and quicker to use the text method as opposed to trying to convert it to numbers, which may lead to a mistake. It could be argued that there is not any real significant difference in the speed of either method for a user that only needs to use chmod on occasion.

You can also use the numeric method to set the setuid, setgid, and sticky bits by using four digits.

For example, chmod 2777 filename will set read/write/executable bits for everyone and also enable the setgid bit.

Generally directories and files should not have the same permissions. If it is necessary to bulk modify a directory tree, use find to selectively modify one or the other.

To chmod only directories to 755:

To chmod only files to 644:

chown changes the owner of a file or directory, which is quicker and easier than altering the permissions in some cases.

Consider the following example, making a new partition with GParted for backup data. Gparted does this all as root so everything belongs to root by default. This is all well and good but when it comes to writing data to the mounted partition, permission is denied for regular users.

As you can see the device in /dev is owned by root, as is the mount location (/media/Backup). To change the owner of the mount location one can do the following:

Before: drwxr-xr-x 5 root root 4096 Jul 6 16:01 Backup

After: drwxr-xr-x 5 archie root 4096 Jul 6 16:01 Backup

Now the partition can have data written to it by the new owner, archie, without altering the permissions (as the owner triad already had rwx permissions).

Access Control Lists provides an additional, more flexible permission mechanism for file systems by allowing to set permissions for any user or group to any file.

The umask utility is used to control the file-creation mode mask, which determines the initial value of file permission bits for newly created files.

Apart from the file mode bits that control user and group read, write and execute permissions, several file systems support file attributes that enable further customization of allowable file operations.

The e2fsprogs package contains the programs lsattr(1) and chattr(1) that list and change a file's attributes, respectively.

These are a few useful attributes. Not all filesystems support every attribute.

See chattr(1) for a complete list of attributes and for more info on what each attribute does.

For example, if you want to set the immutable bit on some file, use the following command:

To remove an attribute on a file just change + to -.

See Extended attributes.

Use the --preserve-root flag to prevent chmod from acting recursively on /. This can, for example, prevent one from removing the executable bit systemwide and thus breaking the system. To use this flag every time, set it within an alias. See also [1].

**Examples:**

Example 1 (unknown):
```unknown
$ ls -l /path/to/directory
```

Example 2 (unknown):
```unknown
total 128
drwxr-xr-x 2 archie archie  4096 Jul  5 21:03 Desktop
drwxr-xr-x 6 archie archie  4096 Jul  5 17:37 Documents
drwxr-xr-x 2 archie archie  4096 Jul  5 13:45 Downloads
-rw-rw-r-- 1 archie archie  5120 Jun 27 08:28 customers.ods
-rw-r--r-- 1 archie archie  3339 Jun 27 08:28 todo
-rwxr-xr-x 1 archie archie  2048 Jul  6 12:56 myscript.sh
```

Example 3 (unknown):
```unknown
drwxrwxrwx+
```

Example 4 (unknown):
```unknown
info ls -n "What information is listed"
```

---

## Help:Reading

**URL:** https://wiki.archlinux.org/title/Create

**Contents:**
- Organization
- Formatting
- Root, regular user or another user
- Append, add, create, edit
  - Make executable
- Source
- Installation of packages
  - Official packages
  - Arch User Repository
- Control of systemd units

Because the vast majority of the ArchWiki contains indications that may need clarification for users new to Arch Linux (or GNU/Linux in general), this rundown of basic procedures was written both to avoid confusion in the assimilation of the articles and to deter repetition in the content itself.

Most articles on the ArchWiki do not attempt to provide a holistic introduction to a single topic; they are instead written in adherence to the "Don't Repeat Yourself" principle, under the assumption that the user will seek out and read any supporting material that they do not yet understand. Where possible, such supporting material is indicated in the article via special formatting, see #Formatting.

Because of this organization, it may be necessary to examine several related sources in order to fully understand an ArchWiki article. In particular, users who are new to Arch (or GNU/Linux in general) should expect to end up reading a great number of articles even when solving simple problems. It is especially important to study the supporting material before seeking additional help from other users.

Some lines are written like so:

Others have a different prefix:

The numeral or hash sign (#) indicates that the command needs to be run as root, whereas the dollar sign ($) shows that the command should be run as a regular user.

When the commands need to run as a specific user, they will be prefixed by the username in square brackets, for example:

This means you should use a privilege elevation tool, e.g. with sudo:

A notable exception to watch out for:

In this example, the context surrounding the numeral sign communicates that this is not to be run as a command; it should be edited into a file instead. So in this case, the numeral sign denotes a comment. A comment can be explanatory text that will not be interpreted by the associated program. Bash scripts denotation for comments happens to coincide with the root PS1.

After further examination, "give away" signs include the uppercase character following the # sign. Usually, Unix commands are not written this way and most of the time they are short abbreviations instead of full-blown English words (e.g., Copy becomes cp).

Regardless, most articles make this easy to discern by notifying the reader:

Append to ~/path/to/file:

When prompted to append to, add to, create, or edit one or more files, it is implied that you should use one of the following methods.

To create or modify multiline files, it is suggested to use a text editor. For example, using the nano command to edit the file /etc/bash.bashrc is:

To create or overwrite a file from a string, it may be simpler to use output redirection. The following example creates or overwrites the contents of the file /etc/hostname with the text myhostname.

Output redirection can also be used to append a string to a file. The following example appends the text [custom-repo] to the file /etc/pacman.conf.

When prompted to create directories, use the mkdir command:

After creating a file, if it is meant to be run as a script (whether manually or called by another program), it needs to be set as executable, for example with:

See chmod. Some applications such as file managers may provide graphical interfaces to do the same.

Some applications, notably command-line shells, use scripts for their configuration: after modifying them, they must be sourced in order for the changes to be applied. In the case of bash, for example, this is done by running (you can also replace source with .):

When the wiki suggests modifying such a configuration script, it will not explicitly remind you to source the file, and only in some cases will it point to this section with a reminder link.

When an article invites you to install some packages in the conventional way, it will not indicate the detailed instructions to do so; instead, it will simply mention the names of the packages to be installed.

The subsections below give an overview of the generic installation procedures depending on the package type.

For packages from the official repositories, you will read something like:

This means that you have to run:

The pacman article contains detailed explanations to deal with package management in Arch Linux proficiently.

For packages from the Arch User Repository (AUR), you will read something like:

This means that in general you have to follow the foobarAUR link, download the PKGBUILD archive, extract it, verify the content and finally run, in the same folder:

The Arch User Repository article contains all the detailed explanations and best practices to deal with AUR packages.

When an article invites to start, enable, etc., some systemd unit (e.g. a service), it will not indicate the detailed instructions to do so, but instead you will read something like:

This means that you have to run:

A notable command that does not follow this exact pattern is systemctl daemon-reload which will be called without arguments.

The systemd#Using units section contains structured list of available actions (like start, enable, enable and start, etc.) with their corresponding systemctl commands.

It is important to remember that there are two different kinds of configurations on a GNU/Linux system. System-wide configuration affects all users. Since system-wide settings are generally located in the /etc directory, root privileges are required in order to alter them. For example, to apply a Bash setting that affects all users, /etc/bash.bashrc should be modified.

User-specific configuration affects only a single user. Dotfiles are used for user-specific configuration. For example, the file ~/.bashrc is the user-specific configuration file. The idea is that each user can define their own settings, such as aliases, functions and other interactive features like the prompt, without affecting other users' preferences.

Bash and other Bourne-compatible shells, such as Zsh, also source files depending on whether the shell is a login shell or an interactive shell. See Bash#Configuration files and Zsh#Startup/Shutdown files for details.

Some code blocks may contain so-called pseudo-variables, which, as the name says, are not actual variables used in the code. Instead they are generic placeholders and have to be manually replaced with system-specific configuration items before the code may be run or parsed. Common shells such as bash and zsh provide tab-completion to auto-complete parameters for common commands such as systemctl.

In the articles that comply with Help:Style/Formatting and punctuation, pseudo-variables are formatted in italics. For example:

In this case interface_name is used as a pseudo-variable placeholder in a systemd template unit. All systemd template units, identifiable by the @ sign, require a system-specific configuration item as argument. See systemd#Using units.

In this case the pseudo-variables are used to describe the parameters that must be substituted for them. Details on how to gather them are elaborated on in the section Securely wipe disk#Calculate blocks to wipe manually, which features the command.

This article or section needs expansion.

In case of file examples, pasting pseudo-variables in real configuration files might break the programs that use them.

In most cases, ellipses (...) are not part of the actual file content or code output, and instead represent omitted or optional text that is not relevant for the discussed subject.

For example HOOKS=(... encrypt ... filesystems ...) or:

Be aware though that, in a few instances, ellipses may be a meaningful part of the code syntax: attentive users should be able to recognize these cases by the context.

**Examples:**

Example 1 (unknown):
```unknown
# mkinitcpio -p linux
```

Example 2 (unknown):
```unknown
$ makepkg -s
```

Example 3 (unknown):
```unknown
sudo command
```

Example 4 (unknown):
```unknown
[postgres]$ initdb -D /var/lib/postgres/data
```

---

## AWK

**URL:** https://wiki.archlinux.org/title/AWK

**Contents:**
- Installation
  - Alternative implementations
- Troubleshooting
  - Assignment to ARGC variable via -v option does not preserve in runtime
- See also

AWK is a small programming language designed for text processing. Its name is derived from the surnames of its authors: Alfred Aho, Peter Weinberger, and Brian Kernighan. The language is standardized and widely available on Unix-like systems.

On Arch, the awk(1p) command is provided by gawk, which is installed by default, with native Unicode support and a load of extrafeatures.

Like many other core utilities, there are several more-or-less compliant implementations available:

Although undocumented, it appears that many implementations will reset the ARGC variable internally after processing the variable assignment of -v options specified on command line. Therefore, to get desired value of ARGC variable in runtime (e.g. BEGIN code blocks), it's required to set the variable in code block directly:

**Examples:**

Example 1 (unknown):
```unknown
BEGIN {
  ARGC=1;
  ...
}
```

---

## Persistent block device naming

**URL:** https://wiki.archlinux.org/title/Persistent_block_device_naming

**Contents:**
- Persistent naming methods
  - by-label
  - by-uuid
  - by-id and by-path
    - World Wide Name
  - by-partlabel
  - by-partuuid
  - by-designator and gpt-auto
- Using persistent naming
  - fstab

This article describes how to use persistent names for your block devices. This has been made possible by the introduction of udev and has some advantages over bus-based naming. If your machine has more than one drive sharing a naming scheme, the order in which their corresponding device nodes are added is arbitrary. This may result in block device names (e.g. /dev/sda and /dev/sdb, /dev/nvme0n1 and /dev/nvme1n1, /dev/mmcblk0 and /dev/mmcblk1) switching around on each boot, culminating in an unbootable system, kernel panic, or a block device disappearing. Persistent naming solves these issues.

There are different schemes providing persistent naming managed by udev.

The directories in /dev/disk/ are created and destroyed dynamically, depending on whether there are devices in them.

The following sections describe what the different persistent naming methods are and how they are used.

The lsblk command can be used for viewing graphically the first persistent schemes:

For those using GPT, use the blkid command instead. The latter is more convenient for scripts, but more difficult to read.

Almost every file system type can have a label. All your volumes that have one are listed in the /dev/disk/by-label directory.

Most file systems support setting the label upon file system creation, see the man page of the relevant mkfs.* utility. For some file systems it is also possible to change the labels. Following are some methods for changing labels on common file systems:

The label of a device can be obtained with lsblk:

UUID is a mechanism to give each filesystem a unique identifier. These identifiers are generated by filesystem utilities (e.g. mkfs.*) when the device gets formatted and are designed so that collisions are unlikely. All GNU/Linux filesystems (including swap and LUKS headers of raw encrypted devices) support UUID. FAT, exFAT and NTFS filesystems do not support UUID, but are still listed in /dev/disk/by-uuid/ with a shorter UID (unique identifier):

The UUID of a device can be obtained with lsblk:

The advantage of using the UUID method is that it is much less likely that name collisions occur than with labels. Further, it is generated automatically on creation of the filesystem. It will, for example, stay unique even if the device is plugged into another system (which may perhaps have a device with the same label).

The disadvantage is that UUIDs make long code lines hard to read and break formatting in many configuration files (e.g. fstab or crypttab). Also every time a volume is reformatted a new UUID is generated and configuration files have to get manually adjusted.

by-id creates a unique name depending on the hardware serial number, by-path depending on the shortest physical path (according to sysfs). Both contain strings to indicate which subsystem they belong to (i.e. pci- for by-path, and ata- for by-id), so they are linked to the hardware controlling the device. This implies different levels of persistence: the by-path will already change when the device is plugged into a different port of the controller, the by-id will change when the device is plugged into a port of a hardware controller subject to another subsystem. [1] Thus, both are not suitable to achieve persistent naming tolerant to hardware changes.

However, both provide important information to find a particular device in a large hardware infrastructure. For example, if you do not manually assign persistent labels (by-label or by-partlabel) and keep a directory with hardware port usage, by-id and by-path can be used to find a particular device.[2] [3]

by-id also creates World Wide Name (WWN) links of storage devices that support it. Unlike other by-id links, WWNs are fully persistent and will not change depending on the used subsystem.

SATA and SAS devices have a wwn- prefix while NVMe devices use a different WWN format and are prefixed with nvme-eui..[4]

GPT partition labels can be defined in the header of the partition entry on GPT disks.

This method is very similar to the filesystem labels, except the partition labels do not get affected if the file system on the partition is changed.

All partitions that have partition labels are listed in the /dev/disk/by-partlabel directory.

The partition label of a device can be obtained with lsblk:

Like GPT partition labels, GPT partition UUIDs are defined in the partition entry on GPT disks.

MBR does not support partition UUIDs, but Linux[5] and software using libblkid[6] (e.g. udev[7]) are capable of generating pseudo PARTUUIDs for MBR partitions. The format is SSSSSSSS-PP, where SSSSSSSS is a zero-filled 32-bit MBR disk signature, and PP is a zero-filled partition number in hexadecimal form. Unlike a regular PARTUUID of a GPT partition, MBR's pseudo PARTUUID can change if the partition number changes.

The dynamic directory is similar to other methods and, like filesystem UUIDs, using UUIDs is preferred over labels.

The partition UUID of a device can be obtained with lsblk:

If all prerequisites of systemd-gpt-auto-generator are met, udev creates symlinks in /dev/disk/by-designator/ based on the partition type. The links are named: root, home, srv, esp, xbootldr, var, etc. See systemd.image-filter(7) for a full list.

For LUKS encrypted partitions, the aforementioned links will point to the unlocked/mapped volumes and additional links with a -luks suffix will point to the encrypted partitions. As long as the LUKS device mapper name matches a designator, links to unlocked volumes will be created even for encrypted partitions on other disks, but -luks suffixed links will not be created in this case.

The /dev/gpt-auto-root symlink points to the root volume block device. If the root partition is encrypted with LUKS, /dev/gpt-auto-root will point to the unlocked/mapped volume and /dev/gpt-auto-root-luks will point to the encrypted partition.

There are various applications that can be configured using persistent naming. Following are some examples of how to configure them.

See the main article: fstab#Identifying file systems.

To use persistent names in kernel parameters, the following prerequisites must be met. On a standard installation following the installation guide both prerequisites are met:

The location of the root filesystem is given by the parameter root on the kernel command line. The kernel command line is configured from the boot loader, see Kernel parameters#Boot loader configuration. To change to persistent device naming, only change the parameters which specify block devices, e.g. root and resume, while leaving other parameters as is. Various naming schemes are supported:

Persistent device naming using label and the LABEL= format, in this example Arch Linux is the LABEL of the root file system.

Persistent device naming using UUID and the UUID= format, in this example 0a3407de-014b-458b-b5c1-848e92a327a3 is the UUID of the root file system.

Persistent device naming using disk id and the /dev path format, in this example wwn-0x60015ee0000b237f-part2 is the id of the root partition.

Persistent device naming using GPT partition UUID and the PARTUUID= format, in this example 98a81274-10f7-40db-872a-03df048df366 is the PARTUUID of the root partition.

Persistent device naming using GPT partition label and the PARTLABEL= format, in this example GNU/Linux is the PARTLABEL of the root partition.

When using GPT partition automounting, the root= parameter can be omitted entirely. If an explicit configuration is desired, use:

**Examples:**

Example 1 (unknown):
```unknown
/dev/nvme0n1
```

Example 2 (unknown):
```unknown
/dev/nvme1n1
```

Example 3 (unknown):
```unknown
/dev/mmcblk0
```

Example 4 (unknown):
```unknown
/dev/mmcblk1
```

---

## Emacs

**URL:** https://wiki.archlinux.org/title/Emacs

**Contents:**
- Installation
- Usage
  - No colors
  - As a daemon
  - As a systemd unit
- Getting help
  - The manuals
- Configuration
- Tips and tricks
  - TRAMP

Emacs is an extensible, customizable, self-documenting real-time display editor. At the core of Emacs lies an Emacs Lisp interpreter, the language in which the majority of Emacs' built-in functionality and extensions are implemented. GNU Emacs uses GTK as its default X toolkit, though it functions equally well within a CLI environment.

Install one of the following packages:

If you want Emacs spell checking to work, also install aspell and an aspell language, such as aspell-en

Before launching emacs, you should know how to close it (especially if you run it in a terminal): use the Ctrl+xCtrl+c key sequence. If you are new to Emacs, it's recommended to work through the official tutorial. To do so, first start Emacs, then select "Tutorial" from the splash screen or press Ctrl+h and then press t.

or, to use it from the console:

or, for fast loading (no .emacs) and editing within CLI:

If you installed the nox version, emacs and emacs -nw will be the same.

A file name can also be provided to open that file immediately:

By default, Emacs starts with a color theme showing hyperlinks in dark blue. To start Emacs on a text terminal without any color theme or scheme:

This will cause all text to appear in the foreground color of the terminal  normally white text on a black background, or black text on a white background.

In order to avoid reloading the Emacs configuration file every time Emacs starts, you can run Emacs as a daemon:

You may then connect to the daemon by running:

Which creates a new frame -c (use -t if you prefer to use it in the terminal) and does not hog the terminal -n (--no-wait). Note that some programs such as Mutt or Git (for commit messages) wait for the editor to finish, so you cannot use the -n parameter. If your default editor is set to use it, you will have to specify an alternate editor (e.g. emacsclient -a "" -t) for those programs.

A systemd unit is included in Emacs 26.1. The unit is installed with Emacs, but it must be enabled as a user unit (not a system-wide) after installing Emacs:

After the service is started, Emacs is ready.

If you want to be able to start graphical emacs frames through emacsclient on Wayland, a specific drop-in snippet is needed (unless you are running emacs-wayland), as shown in EmacsWiki.

Note that systemd user units do not inherit environment variables from a login shell (like ~/.bash_profile). See systemd/User#Environment variables for more information.

If you start emacs as a daemon, you may want to set the VISUAL and EDITOR environment variables to emacsclient so that programs that start an editor use emacsclient instead of starting a new full instance of the editor. Programs that use an external editor include email programs (for editing the message), Git (for editing the commit message), and less (the v command for editing the displayed file). Do not use the -n (--nowait) option to emacsclient, since programs typically expect editing to be finished when the editor exits.

It is also recommended to change any GUI start menu entries (or equivalent) for Emacs to point to emacsclient instead of emacs, so that the emacs daemon is used instead of starting a new emacs process.

Emacs has a built-in tutorial which can be accessed by clicking the first link on the splash screen and selecting Help>Emacs Tutorial from the menu or by pressing C-h t.

To read the tutorial in a language other than English, use the command Alt x, and enter help-with-tutorial-spec-language

Emacs is self-documenting by design. As such, a great deal of information is available to determine the name of a specific command or its keybinding, for example. See all contextual help bindings with C-h C-h. You can access the quick help for the Emacs help system with C-h ?.

Emacs also includes a set of reference cards, useful for beginners and experts alike, see /usr/share/emacs/version/etc/refcards/.

If you really want to master Emacs, the most recommended source of documentation remains the official manuals:

You can access them as HTML documents or PDFs from GNU.org or directly from Emacs itself thanks to the embedded 'info' reader: C-h i. Press m to choose a book.

Some users prefer to read books using 'info' because of its convenient shortcuts, its paragraphs adapting to window width and the font adapted to current screen resolution. Some find it less irritating to the eyes. Finally you can easily copy content from the book to any Emacs buffer, and you can even execute Lisp code snippets directly from the examples.

You may want to read the Info book to know more about it: C-h i m info <RET>. Press ? while in info mode for a quick list of shortcuts.

You can read man pages with Emacs with M-x man <RET>. Note that man pages for most GNU programs aren't as complete as their Info manuals.

One of Emacs's main features is its extensibility and the ease of configuration. Emacs has a built-in customization engine. You can do M-x customize which displays a list of customization options. For how to use this interface, see the Easy Customization info node: (info "(emacs) Easy Customization"). You can set customization opens just for one Emacs session or save them into a configuration file so that they are saved across Emacs sessions. Note that this is what the customization interface does if you select Apply and Save.

When Emacs is started, it normally tries to load a Lisp program from an "initialization file", or "init file" for short. This file, if it exists, specifies how to initialize Emacs for you. Emacs looks for your init file at ~/.emacs, ~/.emacs.el, ~/.emacs.d/init.el, or ~/.config/emacs/init.el. See the info node "Init File" for more: (info "(emacs) Init File")

TRAMP (Transparent Remote Access, Multiple Protocols) is an extension which, as its name suggests, provides transparent access to remote files across a number of protocols. When prompted for a filename, entering a specific form will invoke TRAMP. Some examples:

To prompt for the root password before opening /etc/hosts with root permissions:

To connect to 'remotehost' as 'you' via SSH and open the file ~/example.txt:

The path for TRAMP is typically of the form '/[protocol]:[[user@]host]:<file>'.

To connect to 'myhost' as 'you' and edit /etc/hosts with sudo:

TRAMP supports much more than the examples above might indicate. For more information refer to the TRAMP info manual, which is distributed with Emacs.

By default, Git provides support for using Emacs' Emerge mode as a merge tool. However you may prefer the Ediff mode. Unfortunately this mode is not supported by git for technical reasons. There is still a way to use it by evaluating some elisp code upon emacs call.

Note that the command has to be on a single line. In the above example, we launch a new instance of Emacs. You might want to use emacsclient for quicker startup; it is not recommended though since the Ediff call is not really clean: it could mess with your current Emacs session.

If you want an instant startup you can use the -q parameter. If you want to launch Emacs quickly while preserving at least a part of your configuration, you can call Emacs with

where the light configuration file loads only what you need for Ediff.

See kerneltrap.org and stackoverflow for more details on this trick and the Ediff issue.

Some users like this behavior to avoid the so-called "emacs pinky". A nice way to achieve this in GUI desktops (Xorg or Wayland), terminals, and even the console is to use keyd. Install the package and create this config file:

Then enable and start the keyd service.

Opening a new file in the same emacs-session requires the use of emacsclient. emacs command can be itself wrapped to do the smarter job to open the file if the session exists.

To start session you need to start-server. This snippet will create server in first session of emacs. Add this to your emacs configuration file.

Shell alias method is not adequate for this since you also need to pass variables or start the independent session of your own. Add this to the .bashrc or any rc file of your shell. This will make your emacs command behave like emacsclient if the argument is passed.

If you want to run it in a new session just do emacs file -.

You can use several configurations and tell Emacs to load one or the other.

For example, let us define two configuration files.

This is the full configuration we load for the daemon. But the plugins file is huge and slow to load. If we want to spawn a new Emacs instance that does not need the plugins features, it can be cumbersome to load it every time in the long run.

And now we launch Emacs with

You can create an alias to ease the call.

You can define variables in your configuration file that can be later one modified locally for a file.

Now in any file you can define local variables in two ways, see the manual for complete details

Note that for the values to take effect, you will need to call M-x revert-buffer.

Custom variables are considered unsafe by default. If you try to open a file that contains local variable redefining custom variables, Emacs will ask you for confirmation.

You can declare the variable as secure, thus removing the Emacs prompt for confirmation. You need to specify a predicate that any new value has to verify so that it can be considered safe.

In the previous example, if you attempt to set anything else than a string, Emacs will consider it insecure.

Colors can be easily customized using the face facility.

You can have let Emacs tell you the name of the face where the point is. Use the customize-face function for that. The facility will show you how to set colors, bold, underline, etc.

Emacs in console can handle 256 colors, but you will have to use an appropriate terminal for that. For instance URxvt has support for 256 colors. You can use the list-colors-display for a comprehensive list of supported colors. This is highly terminal-dependent.

Emacs is a powerful LaTeX editor. This is mostly due to the fact you can adapt or create a LaTeX mode to fit your needs best.

Still, there might be some challenges, like SyncTeX support. First you need to make sure your TeX distribution has it. If you installed TeX Live manually, you may need to install the synctex package.

SyncTeX support is viewer-dependent. Here we will use Zathura as an example, so the code needs to be adapted if you want to use another PDF viewer.

Here we define our custom variable. If you are using AucTeX or Emacs default LaTeX-mode, you will have to set the viewer accordingly.

Now open a LaTeX source file with Emacs, compile the document, and launch the viewer. Zathura will spawn. If you press Ctrl+Left click Emacs should place the point at the corresponding position.

You can use systemd-mode.

Alternatively, you can simply tell emacs to colour systemd files (services, timer, etc.), by adding this to your init file:

To use the Xorg clipboard in emacs-nox, install xclip and add the following function to ~/.emacs [1]

See also mwheel.el[dead link 2024-11-05HTTP 404].

Emacs's functionality can be extended with third-party packages. The built-in package manager package.el is the officially supported way to do this, though there are several other package managers written by members of the Emacs community. package.el relies on the variable package-archives to find packages. By default, this includes the Emacs Lisp Package Archive (ELPA). M-x list-packages will create a buffer listing all the packages your Emacs knows about. The manual ((info "(emacs) Packages")) contains much more information.

Third-party package archives can be added. The most widely used of these is MELPA.

A number of popular extensions are available as packages in the official repositories, and more still, via the AUR. The name of such packages usually have a 'emacs-' prefix (for example, emacs-lua-mode), though not always (for example, auctexAUR).

Some packages may require you to make changes to your configuration file in order to activate them so that their features are available during an Emacs session. For example, if you install auctexAUR, you will need to add

to your configuration file. Other packages should let you know how to activate them in the commentary section of their source code or in their README.

You need to install either the mcpp package or the gcc package. The C preprocessor cpp is used to preprocess X resources by xrdb. If a C preprocessor is not installed on the system, xrdb silently skips running the C preprocessor and the symbol WINDOW_FOREGROUND is not expanded to a hexadecimal color code.

When using Gccemacs (either the branch emacs-native-comp or pgtk-nativecomp) and trying to start a systemd service for it, an error message informing that a .eln file was not found might be logged:

As a workaround, edit the WorkingDirectory line of emacs.service in the [Service] section.

By default, the Emacs shell will show raw escape sequences used to print colors. In other words, it will display strange symbols in place of the desired colored output.

Including the following into ~/.emacs amends the problem:

If when you start emacs in X windows all the characters in the main window are white boxes with black borders (the ones you see if you try to view characters for which you do not have the correct font installed), you need to install xorg-fonts-75dpi and/or xorg-fonts-100dpi and restart X windows.

Slow startup times are often caused by one of two things.

To determine which it might be, run Emacs with:

The most common cause of this error is the 'load-path' variable not including the path to the directory within which the extension is located. To solve this, add the appropriate path to the list to be searched prior to attempting to load the extension:

When attempting to use packages for extensions and Emacs has been configured with a prefix other than /usr, the load-path will need to be updated. Place the following in ~/.emacs prior to the instructions provided by the package:

If compiling Emacs by hand, keep in mind that the default prefix is /usr/local.

Searching about this bug on Google, we find this link: https://lists.gnu.org/archive/html/help-gnu-emacs/2009-05/msg00167.html explaining the problem. The normal way to use accent keys does not work as expected. Trying to accent a word like 'fianc' will produce the message above.

A way to solve it is just put the line below on your startup file, ~/.emacs:

And no, it is not a bug, but a feature of new Emacs versions. Reading the subsequent messages about it on the mail list, we found it [2]:

This is because terminals are more limited than Xorg. Some terminals may handle more bindings than other, though. Two solutions:

Due to its single-threaded nature, many operations block Emacs. This could happen in a few ways. For example, Emacs may be waiting for input from you (e.g. you have opened the minibuffer in one frame but are trying to work in another). Alternatively, Emacs could be running code that simply takes a while to finish. Or perhaps you have run across a bug. There are several ways of trying to unblock Emacs without killing the Emacs process.

When working in a terminal, the color, indentation, or anything related to the output might become crazy. This is (probably?) because Emacs was sent a special character at some point which may conflict with the current terminal. If this happens you can do M-x redraw-display, which will redraw the terminal's display. If this problem happens frequently, you might want to bind the command to a key, e.g. by putting something like:

Graphical Emacs does not suffer from this issue.

Export these values in your .bashrc or .zshrc:

It can be a source of errors since in Linux distributions the correct values use lowercase utf (e.g. en_US.utf-8)

To view all available locales use locale -a.

Enable xterm-keys in your tmux configuration:

Because this will break other key combinations, put the following in your emacs config.

See tmux FAQ for details.

KDE users may observe that the Emacs window does not resize properly, but rather, the resized portion is transparent and mouse clicks are sent to the underlying window. To correct this behavior, change KDE's GTK3 theme to something other than oxygen-gtk. For instance, use the Emacs theme which is included with gtk3.

To force Emacs to maximize completely in KDE, click the Emacs icon in the title bar, and select More Actions > Special Window Settings. Then in the Size & Position tab, select Obey geometry restrictions, choose Force in the dropdown menu, and select No from the radio buttons on the right.

Sometimes the texinfo dir file can get out of sync and info pages that exist on the system are not available in the info browser. The script below recreates the dir info file, putting all of the system's info pages in it.

There are numerous "smaller" text editors that are, at least on the surface, similar to GNU Emacs. Here are some of them:

An Emacs "distro" is a collection of emacs packages and customizations. They are are easier to install and use than customizing emacs on your own is (but less custom to you).

**Examples:**

Example 1 (unknown):
```unknown
$ emacs -nw
```

Example 2 (unknown):
```unknown
$ emacs -Q -nw
```

Example 3 (unknown):
```unknown
$ emacs filename.txt
```

Example 4 (unknown):
```unknown
$ emacs -nw --color=no
```

---

## Color output in console

**URL:** https://wiki.archlinux.org/title/Color_output_in_console

**Contents:**
- Background
  - Escape sequences
  - Termcap and terminfo
- Applications
  - diff
  - grep
  - ip
  - less
    - Environment variables
    - Reading from stdin

This article or section needs expansion.

This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.

This page was created to consolidate colorization of CLI outputs.

This article or section needs expansion.

The ANSI escape sequences define a way to put additional information into terminal output, and color is part of this "additional information". Throughout the years the range of terminal colors has been vastly expanded, from the initial eight colors to a full 24-bit truecolor.

The basic color encoding provides 8 normal-brightness colors and 8 brighter versions of these colors. Modern terminal emulators, including the Linux console itself, allows you to specify the precise RGB values that the colors translate to. This mode is supported by almost all terminal emulators.

With the advent of 256-color displays came the 256-color escape. The 256 colors are the 16 basic colors, the 216 RGB colors (laid out in a 6x6x6 cube), and 24 levels of greyscale. Except for the first 16 colors, the scheme is usually not customizable as it has a well-defined mapping to RGB. This mode is supported by most terminal emulators. (A minority of emulators use a similar but incompatible encoding with only 88 colors. You are very unlikely to use them in practice, but they will appear in the terminfo database.)

Less commonly supported is the truecolor mode, allowing one to use 16.7 million (224) colors in RGB (each value ranging from 0 to 255).

Termcap and terminfo, part of ncurses, are databases that provide information on the escape sequences terminals (usually specified by the TERM env-var) understand. The tput(1) and infocmp(1) commands can be used to access them from command-line.

diffutils from version 3.4 includes the --color option (GNU mailing list).

The --color=auto option enables color highlighting. Color codes are emitted only on standard output; not in pipes or redirection.

Color output in grep is also useful with regexp tasks.

Use an alias to permanently enable this option:

The GREP_COLORS variable is used to define colors, and it configures various parts of highlighting. To change the colors, find the needed ANSI escape sequence and apply it. See grep(1)  GREP COLORS for more information.

The -n option includes file line numbers in the output.

ip(8) command from iproute2 supports colors with -c/-color option. When using the auto parameter, colored output will be enabled only when stdout is a terminal. iproute2 can be built with auto as the default, but such a change has been rejected by the Arch package maintainer.

You can use an alias to enable colored output, e.g.:

As with the #man case, we can tell less to emit colors when it is meaning to make bold text and other formatting effects.

Add the following lines to your shell configuration file:

It will set red for bold and blue for underlined.

For more information about the --use-color and -D options, see less(1)  D or [1].

When you run a command and pipe its standard output (stdout) to less for a paged view (e.g. pacman -Qe | less), you may find that the output is no longer colored. This is usually because the program tries to detect if its stdout is an interactive terminal, in which case it prints colored text, and otherwise prints uncolored text. This is good behaviour when you want to redirect stdout to a file, e.g. pacman -Qe > pkglst-backup.txt, but less suited when you want to view output in less.

Some programs provide an option to disable the interactive tty detection:

In case that the program does not provide any similar option, it is possible to trick the program into thinking its stdout is an interactive terminal with the following utilities:

Alternatively, using zpty module from zsh: [2]

To pipe it to other pager (less in this example):

The --color=auto option enables color highlighting. Color codes are emitted only on standard output; not in pipes or redirection.

Use an alias to permanently enable this option:

The LS_COLORS variable is used to define colors, and it configures various parts of highlighting. Use the dircolors(1) command to set it.

An advanced alternative to dircolors that ships with many themes is the vivid package, see vivid --help for usage.

See ls(1) for more information.

This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.

There is a real color facility in grotty(1), but it is strongly discouraged for man pages. Here we fake a colored man by hacking two main pagers, less and most: we replace the sequences for bold, standout, and underline with spiced ones that contain color.

bat can be used as a colorizing pager for man, by setting the MANPAGER environment variable as documented here.

See #less for a more detailed description.

For Fish you could accomplish this with:

Remember to source your config or restart your shell to make the changes take effect.

The basic function of 'most' is similar to less and more, but it has a smaller feature set. Configuring most to use colors is easier than using less, but additional configuration is necessary to make most behave like less. Install the most package.

Edit /etc/man_db.conf, uncomment the pager definition and change it to:

Test the new setup by typing:

Modifying the color values requires editing ~/.mostrc (creating the file if it is not present) or editing /etc/most.conf for system-wide changes. Example ~/.mostrc:

A quick way to add color to manual pages viewed on xterm/uxterm or rxvt-unicode is to modify ~/.Xresources.

which replaces the decorations with the colors. Also add:

if you want colors and decorations (bold or underline) at the same time. See xterm(1)  veryBoldColors for more information.

Launch a new xterm/uxterm or rxvt-unicode and you should see colorful man pages.

This combination puts colors to bold and underlined words in xterm/uxterm or to bold, underlined, and italicized text in rxvt-unicode. You can play with different combinations of these attributes. See the sources (archived) of this item.

Pacman has a color option. Uncomment the Color line in /etc/pacman.conf.

This article or section is a candidate for moving to [[]].

(most of them outdated, but still functioning)

They go with multiple preconfigured presets that can be changed, and new ones can be created/contributed.

Diff has built-in color output, which is reasonable to use. But the following wrappers can be used:

You can enable code syntax coloring in less. First, install source-highlight, then add these lines to your shell configuration file:

Frequent users of the command line interface might want to install lesspipe.

Users may now list the compressed files inside of an archive using their pager:

lesspipe also grants less the ability of interfacing with files other than archives, serving as an alternative for the specific command associated for that file-type (such as viewing HTML via python-html2text).

Re-login after installing lesspipe in order to activate it, or source /etc/profile.d/lesspipe.sh.

See Bash/Prompt customization#Colors.

See Fish#Web interface.

See Customizing the Prompt.

This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.

The colors in the Linux virtual console running on the framebuffer can be changed. This is done by writing the escape code \\e]PXRRGGBB, where X is the hexadecimal index of the color from 0-F, and RRGGBB is a traditional hexadecimal RGB code.

For example, to reuse existing colors defined in ~/.Xresources, add the following to the shell initialization file (such as ~/.bashrc):

The below is a colored example of the virtual console login screen in /etc/issue. Create a backup of the original file with mv /etc/issue /etc/issue.bak as root, and create a new /etc/issue:

Most Xorg terminals, including xterm and urxvt, support at least 16 basic colors. The colors 0-7 are the 'normal' colors. Colors 8-15 are their 'bright' counterparts, used for highlighting. These colors can be modified through X resources, or through specific terminal settings. For example:

Example to set bold, yellow text, with blue background:

Example to print 256 colors across the screen:

This article or section is a candidate for merging with Bash/Prompt_customization.

Replace tput op with whatever tput you want to trace. op is the default foreground and background color.

The following command will let you discover all the terminals you have terminfo support for, and the number of colors each terminal supports. The possible values are: 8, 15, 16, 52, 64, 88 and 256.

See [3] for scripts which display a chart of your current terminal scheme.

Some terminals support the full range of 16 million colors (RGB, each with 8 bit resolution): xterm, konsole, st, etc. The corresponding TERM values xterm-direct, konsole-direct, st-direct, etc. are supported starting with ncurses version 6.1 [4]. For more info about terminal emulators and applications that support true color, see [5].

Note that the Linux kernel supports the SGR (Select Graphic Rendition) escape sequences for true-color, but it is pointless to use it, because the driver maps the 24-bit color specifications to a 256-colors color map in the kernel (see the functions rgb_foreground, rgb_background). For this reason, there is no terminfo entry linux-direct.

**Examples:**

Example 1 (unknown):
```unknown
$ alias diff='diff --color=auto'
```

Example 2 (unknown):
```unknown
--color=auto
```

Example 3 (unknown):
```unknown
alias grep='grep --color=auto'
```

Example 4 (unknown):
```unknown
GREP_COLORS
```

---

## Arch terminology

**URL:** https://wiki.archlinux.org/title/Arch_terminology

**Contents:**
- ABS
- Arch Linux
- Arch Linux Archive
- ArchWiki
- AUR
- BBS
- core
- Custom repository
- Developer
- extra

This page is intended to be a page to demystify common terms used among the Arch Linux community.

ABS stands for Arch build system.

Arch should be referred to as:

Archlinux, ArchLinux, archLinux, aRcHlInUx, etc. are all weird, and weirder mutations.

Officially, the "Arch" in "Arch Linux" is pronounced /rt/ as in an "archer" (bowman) or "archnemesis", and not as in "ark" or "archangel".

The Arch Linux Archive (a.k.a. ALA), formerly known as Arch Linux Rollback Machine (a.k.a. ARM), stores official repositories snapshots, ISO images and bootstrap tarballs across time.

ArchWiki is a place to find documentation about Arch Linux. Anyone can contribute to the wiki.

The Arch User Repository (AUR) is a community-driven repository for Arch users. It contains package descriptionsPKGBUILDsthat allow you to build a package from source with makepkg and then install it via pacman. The AUR was created to organize and share new packages from the community and to help expedite popular packages' inclusion into the extra repository.

A good number of new packages that enter the official repositories start in the AUR. In the AUR, users are able to contribute their own package buildsPKGBUILDand related files. The AUR community has the ability to vote for packages in the AUR. If a package becomes popular enoughprovided it has a compatible license and good packaging techniqueit may be entered into the extra repository (directly accessible by pacman or the ABS).

You can access the Arch Linux User Community Repository at https://aur.archlinux.org.

Bulletin board system, but in Arch's case, it is just the support forum.

The core repository contains the bare packages needed for an Arch Linux system. core has everything needed to get a working command-line system.

Anyone can create a custom local repository and put it online for other users. To create a repository, you need a set of packages and a pacman-compatible database file for your packages. Host your files online and everyone will be able to use your repository by adding it as a regular repository.

Half-gods working to improve Arch for no financial gain. Developers are outranked only by our gods, Judd Vinet and Aaron Griffin, who in turn are outranked by tacos.

Arch core package set is fairly streamlined, but we supplement this with a larger, more complete extra repository. This repository is constantly growing with the help of packages submitted from our strong community.

This is where GUI toolssuch as desktop environments and window managersand common programs are found.

See Arch boot process#initramfs.

KISS principle (Keep It Simple, Stupid)simplicity is a main principle Arch Linux tries to achieve.

makepkg will build packages for you. makepkg will read the metadata required from a PKGBUILD file. All it needs is a build-capable Linux platform, curl, and some build scripts. The advantage to a script-based build is that you only really do the work once. Once you have the build script for a package, you just need to run makepkg and it will do the rest: download and validate source files, check dependencies, configure the build time settings, build the package, install the package into a temporary root, make customization, generate meta-info, and package the whole thing up for pacman to use.

namcap is a package analysis utility that looks for problems with Arch Linux packages or their PKGBUILD files. It can apply rules to the file list, the files themselves, or individual PKGBUILD files.

See pacman#Installing packages.

The role of a package maintainer is to update packages as new versions become available upstream, and to field support questions relating to bugs in said packages. The term applies to:

The maintainer of a package is the person currently responsible for the package. Previous maintainers should be listed as contributors in the PKGBUILD along with others who have contributed to the package.

PKGBUILDs are small Bash scripts that are used to build Arch Linux packages. See Creating packages for more detail.

A repositoryinformally referred as "repo"has the pre-built packages of PKGBUILDs.

Official repositories are split into different parts for easy maintenance.

Pacman uses repositories to search for packages and install them.

A repository can be local (i.e. on your own computer) or remote (i.e. the packages are downloaded before they are installed).

See also #Custom repository.

RTFM (Read The Friendly Manual)this simple message is replied to some newcomers who ask about the functionality of a program when it is clearly defined in a program's manual.

This acronym is an invitation to self-care, not an insult. It is often used when a user is seen as failing to make any attempt to find a solution to the problem themselves. If someone tells you this, they are not trying to offend youthey are just feeling frustrated with a perceived lack of effort.

The best thing to do if you are told to do this is to read the manual page.

If you do not find the answer to your question in the program manual, there are more ways to find the answer. You can:

They are the repositories where major package updates are kept prior to release into the main repositories, so they can be bug tested and upgrade issues can be found. They are disabled by default but can be enabled in /etc/pacman.conf.

The unofficial term traditionally used to refer to the main Arch Linux principles.

Trusted User (TU) is the old name for the package maintainer role.

The same as Custom repository.

The same as ArchWiki.

**Examples:**

Example 1 (unknown):
```unknown
man program_name
```

Example 2 (unknown):
```unknown
/etc/pacman.conf
```

---

## dotfiles

**URL:** https://wiki.archlinux.org/title/Dotfiles

**Contents:**
- Tracking dotfiles directly with Git
- Host-specific configuration
- Tools
  - Tools wrapping Git
- User repositories
- See also

User-specific application configuration is traditionally stored in so called dotfiles (files whose filename starts with a dot). It is common practice to track dotfiles with a version control system such as Git to keep track of changes and synchronize dotfiles across various hosts. There are various approaches to managing dotfiles (e.g. directly tracking dotfiles in the home directory v.s. storing them in a subdirectory and symlinking/copying/generating files with a shell script or a dedicated tool). Apart from explaining how to manage dotfiles this article also contains a list of dotfile repositories from Arch Linux users.

The benefit of tracking dotfiles directly with Git is that it only requires Git and does not involve symlinks. The disadvantage is that host-specific configuration generally requires merging changes into multiple branches.

The simplest way to achieve this approach is to initialize a Git repository directly in your home directory and ignoring all files by default with a gitignore(5) pattern of *. This method however comes with two drawbacks: it can become confusing when you have other Git repositories in your home directory (e.g. if you forget to initialize a repository you suddenly operate on your dotfile repository) and you can no longer easily see which files in the current directory are untracked (because they are ignored).

An alternative method without these drawbacks is the "bare repository and alias method" popularized on Ask Hacker News: What do you use to manage your dotfiles?, which just takes three commands to set up:

Your dotfiles can be replicated on a new system like:

You can then manage your dotfiles with the created alias. If you are using Bash and would like bash completion for this alias, simply install bash-complete-aliasAUR, then add the alias and the following line to your ~/.bashrc.

Another way to get completion in bash is adding the following to your ~/.bashrc (taken from [1]):

A common problem with synchronizing dotfiles across various machines is host-specific configuration.

With Git this can be solved by maintaining a master branch for all shared configuration, while each individual machine has a machine-specific branch checked out. Host-specific configuration can be committed to the machine-specific branch; when shared configuration is modified in the master branch, the per-machine branches need to be rebased on top of the updated master.

In configuration scripts like shell configuration files conditional logic can be used. For example, Bash scripts (i.e. .bashrc) can apply different configuration depending on the machine name (or type, custom variable, etc.):

Similar can also be achieved with .Xresources.[2]

If you find rebasing Git branches too cumbersome, you may want to use a tool that supports file grouping, or if even greater flexibility is desired, a tool that does processing.

If you are uncomfortable with Git, you may want to use one of these tools, which abstract the version control system away (more or less).

**Examples:**

Example 1 (unknown):
```unknown
$ git init --bare ~/.dotfiles
$ alias dotfiles='/usr/bin/git --git-dir="$HOME/.dotfiles/" --work-tree="$HOME"'
$ dotfiles config status.showUntrackedFiles no
```

Example 2 (unknown):
```unknown
$ git clone --bare <git-repo-url> $HOME/.dotfiles
$ alias dotfiles='/usr/bin/git --git-dir="$HOME/.dotfiles/" --work-tree="$HOME"'
$ dotfiles checkout
$ dotfiles config --local status.showUntrackedFiles no
```

Example 3 (unknown):
```unknown
dotfiles.bundle
```

Example 4 (unknown):
```unknown
$ dotfiles bundle create --progress dotfiles.bundle --all
```

---

## 7-Zip

**URL:** https://wiki.archlinux.org/title/7-Zip

**Contents:**
- Installation
- Examples
- Differences between 7z, 7za and 7zr binaries
- See also

7-Zip is a file archiver with a high compression ratio. It was previously provided by the p7zip package.

Install the 7zip package.

The command to run the program is the following:

Alternatively, bsdtar(1) provides 7z archive support and is included within the libarchive package.

Add file/directory to the archive (or create a new one):

Also it is possible to set password with flag -p and hide structure of the archive with flag -mhe=on:

Update existing files in the archive or add new ones:

List the content of an archive:

Extract all files from an archive to the current directory without using directory names:

Extract with full paths:

Extract into a new directory:

Check integrity of the archive:

The package includes three binaries:

**Examples:**

Example 1 (unknown):
```unknown
$ 7z a archive_name file_name
```

Example 2 (unknown):
```unknown
Enter password:
```

Example 3 (unknown):
```unknown
$ 7z a archive_name file_name -p -mhe=on
```

Example 4 (unknown):
```unknown
$ 7z u archive_name file_name
```

---

## Arch-based distributions

**URL:** https://wiki.archlinux.org/title/Arch-based_distributions

**Contents:**
- Active
- Inactive
  - 2025
  - 2024
  - 2023
  - 2022
  - 2021
  - 2020
  - 2019
  - 2018

This page lists operating system distributions which are derived from Arch Linux either in whole or in part, to help enforce the Arch Linux trademark policy. Although it intends to be as comprehensive as possible, it is not an exhaustive list. If the project maintains them, SourceForge project links are preferred to simplify maintenance.

Asterisk indicates the year of first public release based on Arch Linux.

These distributions are no longer developed, but show some of the history surrounding Arch Linux and the greater FOSS community. Sections show the year of the latest release.

---

## Help:Reading

**URL:** https://wiki.archlinux.org/title/Start/enable

**Contents:**
- Organization
- Formatting
- Root, regular user or another user
- Append, add, create, edit
  - Make executable
- Source
- Installation of packages
  - Official packages
  - Arch User Repository
- Control of systemd units

Because the vast majority of the ArchWiki contains indications that may need clarification for users new to Arch Linux (or GNU/Linux in general), this rundown of basic procedures was written both to avoid confusion in the assimilation of the articles and to deter repetition in the content itself.

Most articles on the ArchWiki do not attempt to provide a holistic introduction to a single topic; they are instead written in adherence to the "Don't Repeat Yourself" principle, under the assumption that the user will seek out and read any supporting material that they do not yet understand. Where possible, such supporting material is indicated in the article via special formatting, see #Formatting.

Because of this organization, it may be necessary to examine several related sources in order to fully understand an ArchWiki article. In particular, users who are new to Arch (or GNU/Linux in general) should expect to end up reading a great number of articles even when solving simple problems. It is especially important to study the supporting material before seeking additional help from other users.

Some lines are written like so:

Others have a different prefix:

The numeral or hash sign (#) indicates that the command needs to be run as root, whereas the dollar sign ($) shows that the command should be run as a regular user.

When the commands need to run as a specific user, they will be prefixed by the username in square brackets, for example:

This means you should use a privilege elevation tool, e.g. with sudo:

A notable exception to watch out for:

In this example, the context surrounding the numeral sign communicates that this is not to be run as a command; it should be edited into a file instead. So in this case, the numeral sign denotes a comment. A comment can be explanatory text that will not be interpreted by the associated program. Bash scripts denotation for comments happens to coincide with the root PS1.

After further examination, "give away" signs include the uppercase character following the # sign. Usually, Unix commands are not written this way and most of the time they are short abbreviations instead of full-blown English words (e.g., Copy becomes cp).

Regardless, most articles make this easy to discern by notifying the reader:

Append to ~/path/to/file:

When prompted to append to, add to, create, or edit one or more files, it is implied that you should use one of the following methods.

To create or modify multiline files, it is suggested to use a text editor. For example, using the nano command to edit the file /etc/bash.bashrc is:

To create or overwrite a file from a string, it may be simpler to use output redirection. The following example creates or overwrites the contents of the file /etc/hostname with the text myhostname.

Output redirection can also be used to append a string to a file. The following example appends the text [custom-repo] to the file /etc/pacman.conf.

When prompted to create directories, use the mkdir command:

After creating a file, if it is meant to be run as a script (whether manually or called by another program), it needs to be set as executable, for example with:

See chmod. Some applications such as file managers may provide graphical interfaces to do the same.

Some applications, notably command-line shells, use scripts for their configuration: after modifying them, they must be sourced in order for the changes to be applied. In the case of bash, for example, this is done by running (you can also replace source with .):

When the wiki suggests modifying such a configuration script, it will not explicitly remind you to source the file, and only in some cases will it point to this section with a reminder link.

When an article invites you to install some packages in the conventional way, it will not indicate the detailed instructions to do so; instead, it will simply mention the names of the packages to be installed.

The subsections below give an overview of the generic installation procedures depending on the package type.

For packages from the official repositories, you will read something like:

This means that you have to run:

The pacman article contains detailed explanations to deal with package management in Arch Linux proficiently.

For packages from the Arch User Repository (AUR), you will read something like:

This means that in general you have to follow the foobarAUR link, download the PKGBUILD archive, extract it, verify the content and finally run, in the same folder:

The Arch User Repository article contains all the detailed explanations and best practices to deal with AUR packages.

When an article invites to start, enable, etc., some systemd unit (e.g. a service), it will not indicate the detailed instructions to do so, but instead you will read something like:

This means that you have to run:

A notable command that does not follow this exact pattern is systemctl daemon-reload which will be called without arguments.

The systemd#Using units section contains structured list of available actions (like start, enable, enable and start, etc.) with their corresponding systemctl commands.

It is important to remember that there are two different kinds of configurations on a GNU/Linux system. System-wide configuration affects all users. Since system-wide settings are generally located in the /etc directory, root privileges are required in order to alter them. For example, to apply a Bash setting that affects all users, /etc/bash.bashrc should be modified.

User-specific configuration affects only a single user. Dotfiles are used for user-specific configuration. For example, the file ~/.bashrc is the user-specific configuration file. The idea is that each user can define their own settings, such as aliases, functions and other interactive features like the prompt, without affecting other users' preferences.

Bash and other Bourne-compatible shells, such as Zsh, also source files depending on whether the shell is a login shell or an interactive shell. See Bash#Configuration files and Zsh#Startup/Shutdown files for details.

Some code blocks may contain so-called pseudo-variables, which, as the name says, are not actual variables used in the code. Instead they are generic placeholders and have to be manually replaced with system-specific configuration items before the code may be run or parsed. Common shells such as bash and zsh provide tab-completion to auto-complete parameters for common commands such as systemctl.

In the articles that comply with Help:Style/Formatting and punctuation, pseudo-variables are formatted in italics. For example:

In this case interface_name is used as a pseudo-variable placeholder in a systemd template unit. All systemd template units, identifiable by the @ sign, require a system-specific configuration item as argument. See systemd#Using units.

In this case the pseudo-variables are used to describe the parameters that must be substituted for them. Details on how to gather them are elaborated on in the section Securely wipe disk#Calculate blocks to wipe manually, which features the command.

This article or section needs expansion.

In case of file examples, pasting pseudo-variables in real configuration files might break the programs that use them.

In most cases, ellipses (...) are not part of the actual file content or code output, and instead represent omitted or optional text that is not relevant for the discussed subject.

For example HOOKS=(... encrypt ... filesystems ...) or:

Be aware though that, in a few instances, ellipses may be a meaningful part of the code syntax: attentive users should be able to recognize these cases by the context.

**Examples:**

Example 1 (unknown):
```unknown
# mkinitcpio -p linux
```

Example 2 (unknown):
```unknown
$ makepkg -s
```

Example 3 (unknown):
```unknown
sudo command
```

Example 4 (unknown):
```unknown
[postgres]$ initdb -D /var/lib/postgres/data
```

---

## Screen capture

**URL:** https://wiki.archlinux.org/title/Taking_a_screenshot

**Contents:**
- Screenshot software
  - Dedicated software
    - Application list
    - Usage
      - maim
      - scrot
  - Desktop environment specific
    - Budgie
    - Cinnamon
    - GNOME

This article lists and describes screenshot and screencast software.

This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.

maim is aimed to be an improved scrot.

Save the full screen to file:

Prompt for selection and save to file:

Save the active window to file, assuming xdotool is installed:

Prompt for selection, save without cursor, and store it to clipboard, assuming xclip is installed:

scrot enables taking screenshots from the CLI and offers features such as a user-definable time delay. Unless instructed otherwise, it saves the file in the current working directory.

The above command saves a dated .png file, along with a thumbnail (20% of original), for Web posting. It provides a 5 second delay before capturing in this instance.

You can also use standard date and time formatting when saving to a file. e.g.,

saves the screenshot in a filename with the current year, month, date, hours, minutes, and seconds to a folder in your home directory called "screenshots"

See scrot(1) for more information.

Budgie ships with its own screenshot utility called BudgieScreenshot.

It allows capturing the whole screen using PrintScreen, the active window or a selected area. See Keyboard Shortcuts for the shortcut to a specific action.

The default installation of Cinnamon does not provide a screenshot utility. Installing gnome-screenshot will enable screenshots through the Menu > Accessories > Screenshot or by pressing PrintScreen.

GNOME users can press PrintScreen or click the camera icon in the system menu. You can also optionally install gnome-screenshot and open it via Apps > Accessories > Take Screenshot.

GNOME features built-in screen recording with the Ctrl+Shift+Alt+r key combination. A red circle is displayed in the bottom right corner of the screen when the recording is in progress. After the recording is finished, a file named Screencast from %d%u-%c.webm is saved in the Videos directory. In order to use the screencast feature the gst-plugin-pipewire and gst-plugins-good packages need to be installed.

If you use KDE, you might want to use Spectacle.

Spectacle is provided by the spectacle package.

If you use Xfce you can install xfce4-screenshooter and then add a keyboard binding:

Xfce Menu > Settings > Keyboard > Application Shortcuts

If you want to skip the Screenshot prompt, type $ xfce4-screenshooter -h in terminal for the options.

This article or section is a candidate for merging with ImageMagick#Screenshot taking.

For other desktop environments such as LXDE or window managers such as Openbox and Compiz, one can add the above commands to the hotkey to take the screenshot. For example:

Note that import is part of the imagemagick package. Adding the above command to the PrintScreen key to Compiz allows to take the screenshot to the Pictures folder according to date and time.

Notice that the rc.xml file in Openbox does not understand commas; so, in order to bind that command to the PrintScreen key in Openbox, you need to add the following to the keyboard section of your rc.xml file:

If the PrintScreen above does not work, see Keyboard input and use different keysym or keycode.

See ImageMagick#Screenshot taking.

You also can take screenshots with GIMP (File > Create > Screenshot...).

imlib2 provides a binary imlib2_grab to take screenshots. To take a screenshot of the full screen, type:

See FFmpeg#Screen capture.

See also FFmpeg#Screen capture and Wikipedia:Comparison of screencasting software.

Screencast utilities allow you to create a video of your desktop or individual windows.

Capturing the screen on Wlroots-based compositor can be done using:

Optionally, slurp can be used to select the part of the screen to capture. If your GPU supports vaapi encoding, wl-screenrecAUR can be a more efficient alternative to wf-recorder.

Take a screenshot of the whole screen:

Take a screenshot of current window in Sway:

Take a screenshot of current window in Hyprland:

Take a screenshot of a part of the screen:

Take a screenshot of a part of the screen and put the output into the clipboard using wl-clipboard:

Take a screenshot of a part of the screen, save to a file, and put the output into the clipboard using wl-clipboard:

Capture a video of the whole screen:

Capture a video of a part of the screen:

green-recorderAUR, obs-gnome-screencastAUR and obs-studio support screen recording on Wayland using GNOME screencast feature.

See PipeWire#WebRTC screen sharing.

Chromium and Firefox should now be able to access the screenshare. You can verify this through Mozilla's getUserMedia / getDisplayMedia Test Page.

The Hyprland window manager allows screen casting and recording with OBS (including selection of individual windows and workspaces) when used with xdg-desktop-portal-hyprland [1].

Configuration for screen sharing with selection on Sway is covered by default in /usr/share/xdg-desktop-portal/sway-portals.conf.

The factual accuracy of this article or section is disputed.

This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.

for those on a different wlroots based compositors, such as dwl: you will need to do the following,

install xdg-desktop-portal-wlr with the gtk version then follow XDG Desktop Portal#Configuration with PipeWire installed then restart

See PipeWire#WebRTC screen sharing.

See v4l2loopback#Casting Wayland using wf-recorder.

Install wf-recorder (or wf-recorder-gitAUR) and v4l2loopback-dkms. Load the v4l2loopback kernel module with the following parameters:

Verify that a new virtual video device VirtualVideoDevice has been created:

Start recording the screen with wf-recorder and feed the output to the new virtual video device VirtualVideoDevice created by v4l2loopback:

The yuv420p colour space is required for the video output to be compatible with Zoom [2].

You can now select the above virtual video device as your "webcam" in video calling/video conferencing applications (the device is called VirtualVideoDevice). You can use ffplay (part of ffmpeg), mpv, or gst-launch (part of gstreamer) to verify that the virtual video device indeed outputs your screenshare:

If Firefox is unable to read the video stream and prints a message like "AbortError: Starting video failed", try preloading v4l2compat.so:

As explained above, wf-recorder is able to record only a portion of the screen by first selecting a region with slurp. To use this functionality for sharing a specific region/application window through a virtual video device, start recording the screen with the following modified command:

After selecting a region of the screen, you will be able to access the video feed through the virtual video device /dev/video2 as above.

You can share native Wayland windows (or the whole screen/workspace) to the X11 application. For this you need to use xwaylandvideobridgeAUR. See Fixing Wayland: Xwayland screen casting for details.

You can use the script(1) command, part of the util-linux package. Just run script and from that moment, all the output is going to be saved to the typescript file, including the ANSI codes.

Once you are done, just run exit and the typescript would ready. The resulting file can be converted to HTML using the ansi2htmlAUR package (not to be confused with ansi2html from python-ansi2html).

To convert the typescript file to typescript.html, do the following:

Actually, some commands can be piped directly to ansi2html:

That does not work on every single case, so in those cases, using script is mandatory.

Install a framebuffer and use fbgrab or fbdumpAUR to take a screenshot.

If you merely want to capture the text in the console and not an actual image, you can use setterm, which is part of the util-linux package. The following command will dump the textual contents of virtual console 1 to a file screen.dump in the current directory:

Root permission is needed because the contents of /dev/vcs1 need to be read.

asciinema allows to record a whole terminal session activity, which is saved in a file in its own (open) format. This file can be played with the same tool or an HTML5 version of the tool, and can be shared on the asciinema.org official web site of the application, or on any own hosted HTML version.

Usage: asciinema(1) or asciinema --help.

Interesting arguments for recording:

Other functions than recording:

See KDE#Spectacle screenshot uses old screen state.

If the nvidia proprietary driver is in use and the screen recording is experiencing background clipping, enable the ForceCompositionPipeline setting. See NVIDIA/Troubleshooting#Avoid screen tearing for details.

**Examples:**

Example 1 (unknown):
```unknown
lximage-qt --screenshot
```

Example 2 (unknown):
```unknown
$ maim filename
```

Example 3 (unknown):
```unknown
$ maim --select filename
```

Example 4 (unknown):
```unknown
$ maim --window $(xdotool getactivewindow) filename
```

---

## File permissions and attributes

**URL:** https://wiki.archlinux.org/title/File_permissions_and_attributes

**Contents:**
- Viewing permissions
  - Examples
- Changing permissions
  - Text method
    - Text method shortcuts
    - Copying permissions
  - Numeric method
  - Bulk chmod
- Changing ownership
- Access Control Lists

File systems use permissions and attributes to regulate the level of interaction that system processes can have with files and directories.

Use the ls command's -l option to view the permissions (or file mode) set for the contents of a directory, for example:

The first column is what we must focus on. Taking an example value of drwxrwxrwx+, the meaning of each character is explained in the following tables:

Each of the three permission triads (rwx in the example above) can be made up of the following characters:

See info Coreutils -n "Mode Structure" and chmod(1) for more details.

Let us see some examples to clarify:

Archie has full access to the Documents directory. They can list, create files and rename, delete any file in Documents, regardless of file permissions. Their ability to access a file depends on the file's permissions.

Archie has full access except they can not create, rename, delete any file. They can list the files and (if the file's permissions allow it) may access an existing file in Documents.

Archie can not do ls in the Documents directory but if they know the name of an existing file then they may list, rename, delete or (if the file's permissions allow it) access it. Also, they are able to create new files.

Archie is only capable of (if the file's permissions allow it) accessing those files the Documents directory which they know of. They can not list already existing files or create, rename, delete any of them.

You should keep in mind that we elaborate on directory permissions and it has nothing to do with the individual file permissions. When you create a new file it is the directory that changes. That is why you need write permission to the directory.

Let us look at another example, this time of a file, not a directory:

Here we can see the first letter is not d but -. So we know it is a file, not a directory. Next the owner's permissions are rw- so the owner has the ability to read and write but not execute. This may seem odd that the owner does not have all three permissions, but the x permission is not needed as it is a text/data file, to be read by a text editor such as Gedit, EMACS, or software like R, and not an executable in its own right (if it contained something like python programming code then it very well could be). The group's permissions are set to r--, so the group has the ability to read the file but not write/edit it in any way  it is essentially like setting something to read-only. We can see that the same permissions apply to everyone else as well.

chmod is a command in Linux and other Unix-like operating systems that allows to change the permissions (or access mode) of a file or directory.

To change the permissions  or access mode  of a file, use the chmod command in a terminal. Below is the command's general structure:

Where who is any from a range of letters, each signifying who is being given the permission. They are as follows:

The permissions are the same as discussed in #Viewing permissions (r, w and x).

Now have a look at some examples using this command. Suppose you became very protective of the Documents directory and wanted to deny everybody but yourself, permissions to read, write, and execute (or in this case search/look) in it:

Before: drwxr-xr-x 6 archie web 4096 Jul 5 17:37 Documents

After: drwx------ 6 archie web 4096 Jul 6 17:32 Documents

Here, because you want to deny permissions, you do not put any letters after the = where permissions would be entered. Now you can see that only the owner's permissions are rwx and all other permissions are -.

This can be reverted with:

Before: drwx------ 6 archie web 4096 Jul 6 17:32 Documents

After: drwxr-xr-x 6 archie web 4096 Jul 6 17:32 Documents

In the next example, you want to grant read and execute permissions to the group, and other users, so you put the letters for the permissions (r and x) after the =, with no spaces.

You can simplify this to put more than one who letter in the same command, e.g:

Now let us consider a second example, suppose you want to change a foobar file so that you have read and write permissions, and fellow users in the group web who may be colleagues working on foobar, can also read and write to it, but other users can only read it:

Before: -rw-r--r-- 1 archie web 5120 Jun 27 08:28 foobar

After: -rw-rw-r-- 1 archie web 5120 Jun 27 08:28 foobar

This is exactly like the first example, but with a file, not a directory, and you grant write permission (just so as to give an example of granting every permission).

The chmod command lets add and subtract permissions from an existing set using + or - instead of =. This is different from the above commands, which essentially re-write the permissions (e.g. to change a permission from r-- to rw-, you still need to include r as well as w after the = in the chmod command invocation. If you missed out r, it would take away the r permission as they are being re-written with the =. Using + and - avoids this by adding or taking away from the current set of permissions).

Let us try this + and - method with the previous example of adding write permissions to the group:

Before: -rw-r--r-- 1 archie web 5120 Jun 27 08:28 foobar

After: -rw-rw-r-- 1 archie web 5120 Jun 27 08:28 foobar

Another example, denying write permissions to all (a):

Before: -rw-rw-r-- 1 archie web 5120 Jun 27 08:28 foobar

After: -r--r--r-- 1 archie web 5120 Jun 27 08:28 foobar

A different shortcut is the special X mode: this is not an actual file mode, but it is often used in conjunction with the -R option to set the executable bit only for directories, and leave it unchanged for regular files, for example:

It is possible to tell chmod to copy the permissions from one class, say the owner, and give those same permissions to group or even all. To do this, instead of putting r, w, or x after the =, put another who letter. e.g:

Before: -rw-r--r-- 1 archie web 5120 Jun 27 08:28 foobar

After: -rw-rw-r-- 1 archie web 5120 Jun 27 08:28 foobar

This command essentially translates to "change the permissions of group (g=), to be the same as the owning user (=u). Note that you cannot copy a set of permissions as well as grant new ones e.g.:

In that case chmod throw an error.

chmod can also set permissions using numbers.

Using numbers is another method which allows you to edit the permissions for all three owner, group, and others at the same time, as well as the setuid, setgid, and sticky bits. This basic structure of the code is this:

Where xxx is a 3-digit number where each digit can be anything from 0 to 7. The first digit applies to permissions for owner, the second digit applies to permissions for group, and the third digit applies to permissions for all others.

In this number notation, the values r, w, and x have their own number value:

To come up with a 3-digit number you need to consider what permissions you want owner, group, and all others to have, and then total their values up. For example, if you want to grant the owner of a directory read write and execution permissions, and you want group and everyone else to have just read and execute permissions, you would come up with the numerical values like so:

This is the equivalent of using the following:

To view the existing permissions of a file or directory in numeric form, use the stat(1) command:

Where the %a option specifies output in numeric form.

Most directories are set to 755 to allow reading, writing and execution to the owner, but deny writing to everyone else, and files are normally 644 to allow reading and writing for the owner but just reading for everyone else; refer to the last note on the lack of x permissions with non executable files: it is the same thing here.

To see this in action with examples consider the previous example that has been used but with this numerical method applied instead:

Before: -rw-r--r-- 1 archie web 5120 Jun 27 08:28 foobar

After: -rw-rw-r-- 1 archie web 5120 Jun 27 08:28 foobar

If this were an executable the number would be 774 if you wanted to grant executable permission to the owner and group. Alternatively if you wanted everyone to only have read permission the number would be 444. Treating r as 4, w as 2, and x as 1 is probably the easiest way to work out the numerical values for using chmod xxx filename, but there is also a binary method, where each permission has a binary number, and then that is in turn converted to a number. It is a bit more convoluted, but here included for completeness.

Consider this permission set:

If you put a 1 under each permission granted, and a 0 for every one not granted, the result would be something like this:

You can then convert these binary numbers:

The value of the above would therefore be 775.

Consider we wanted to remove the writable permission from group:

The value would therefore be 755 and you would use chmod 755 filename to remove the writable permission. You will notice you get the same three digit number no matter which method you use. Whether you use text or numbers will depend on personal preference and typing speed. When you want to restore a directory or file to default permissions e.g. read and write (and execute) permission to the owner but deny write permission to everyone else, it may be faster to use chmod 755/644 filename. However if you are changing the permissions to something out of the norm, it may be simpler and quicker to use the text method as opposed to trying to convert it to numbers, which may lead to a mistake. It could be argued that there is not any real significant difference in the speed of either method for a user that only needs to use chmod on occasion.

You can also use the numeric method to set the setuid, setgid, and sticky bits by using four digits.

For example, chmod 2777 filename will set read/write/executable bits for everyone and also enable the setgid bit.

Generally directories and files should not have the same permissions. If it is necessary to bulk modify a directory tree, use find to selectively modify one or the other.

To chmod only directories to 755:

To chmod only files to 644:

chown changes the owner of a file or directory, which is quicker and easier than altering the permissions in some cases.

Consider the following example, making a new partition with GParted for backup data. Gparted does this all as root so everything belongs to root by default. This is all well and good but when it comes to writing data to the mounted partition, permission is denied for regular users.

As you can see the device in /dev is owned by root, as is the mount location (/media/Backup). To change the owner of the mount location one can do the following:

Before: drwxr-xr-x 5 root root 4096 Jul 6 16:01 Backup

After: drwxr-xr-x 5 archie root 4096 Jul 6 16:01 Backup

Now the partition can have data written to it by the new owner, archie, without altering the permissions (as the owner triad already had rwx permissions).

Access Control Lists provides an additional, more flexible permission mechanism for file systems by allowing to set permissions for any user or group to any file.

The umask utility is used to control the file-creation mode mask, which determines the initial value of file permission bits for newly created files.

Apart from the file mode bits that control user and group read, write and execute permissions, several file systems support file attributes that enable further customization of allowable file operations.

The e2fsprogs package contains the programs lsattr(1) and chattr(1) that list and change a file's attributes, respectively.

These are a few useful attributes. Not all filesystems support every attribute.

See chattr(1) for a complete list of attributes and for more info on what each attribute does.

For example, if you want to set the immutable bit on some file, use the following command:

To remove an attribute on a file just change + to -.

See Extended attributes.

Use the --preserve-root flag to prevent chmod from acting recursively on /. This can, for example, prevent one from removing the executable bit systemwide and thus breaking the system. To use this flag every time, set it within an alias. See also [1].

**Examples:**

Example 1 (unknown):
```unknown
$ ls -l /path/to/directory
```

Example 2 (unknown):
```unknown
total 128
drwxr-xr-x 2 archie archie  4096 Jul  5 21:03 Desktop
drwxr-xr-x 6 archie archie  4096 Jul  5 17:37 Documents
drwxr-xr-x 2 archie archie  4096 Jul  5 13:45 Downloads
-rw-rw-r-- 1 archie archie  5120 Jun 27 08:28 customers.ods
-rw-r--r-- 1 archie archie  3339 Jun 27 08:28 todo
-rwxr-xr-x 1 archie archie  2048 Jul  6 12:56 myscript.sh
```

Example 3 (unknown):
```unknown
drwxrwxrwx+
```

Example 4 (unknown):
```unknown
info ls -n "What information is listed"
```

---

## OpenVAS

**URL:** https://wiki.archlinux.org/title/OpenVAS

**Contents:**
- Pre-install
  - PostgreSQL
  - Redis
- Installation
- Initial setup
- Getting started
- See also

This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.

OpenVAS stands for Open Vulnerability Assessment System and is a network security scanner with associated tools like a graphical user front-end. The core component is a server with a set of network vulnerability tests (NVTs) to detect security problems in remote systems and applications.

Set up PostgreSQL before you continue.

Configure Redis as prescribed by the OpenVAS redis configuration. In summary, amend the following to your /etc/redis/redis.conf:

Finally restart redis.service.

Install the following packages to get a full OpenVAS setup, including manager, web frontend, scanner, and so on: openvas-scannerAUR, ospd-openvasAUR, gsaAUR, gvmdAUR. nmap needs to be installed for the scanner to deliver proper results and texlive is needed for PDF report feature to work.

Setup the PostgreSQL DB for gvm:

Grant this user DBA roles:

Make sure to have the following sysctl configurations:

Before doing this check the values of somaxconn (normally this is 4096 for Arch Linux and does not need to be adjusted:

If this is the case just skip the first echo line.

Grant the gvm user access to the redis socket:

You can enable the following timers to update these data on a frequently basis: greenbone-nvt-sync.timer, greenbone-feed-sync.timer, greenbone-scapdata-sync.timer, greenbone-certdata-sync.timer.

Create certificates for the server and clients, default values were used:

Add an administrator user account, be sure to copy the password:

You can also change the password of the user later on

Start ospd-openvas.service, gvmd.service and gsad.service.

Copy the id of the OpenVAS Default scanner and run:

Set the feed import user:

Copy the id of the admin user and run:

Point your web browser to http://127.0.0.1 and login with your admin credentials.

**Examples:**

Example 1 (unknown):
```unknown
/etc/redis/redis.conf
```

Example 2 (unknown):
```unknown
port 0
unixsocket /run/redis/redis.sock
unixsocketperm 770
timeout 0
databases 128
```

Example 3 (unknown):
```unknown
OpenVAS redis configuration
```

Example 4 (unknown):
```unknown
redis.service
```

---

## Pantheon

**URL:** https://wiki.archlinux.org/title/Pantheon

**Contents:**
- Development
- Installation
  - Package sources
    - Official repository
    - Unofficial repository
    - AUR
  - Desktop environment
  - Services
  - Theme and configuration
  - Applications

Pantheon is the desktop environment of elementary OS. It is written in Vala, using GTK 3 and Granite.

elementary OS releases are derived from Ubuntu's LTS releases, typically trailing Ubuntu's cycle by a few weeks or months. However, its constituent packages are updated continuously. See the official github repository and consult their community slack.

The Pantheon desktop environment and elementary OS's curated applications are provided by the pantheon group.

extra-alucryd contains a few packages not yet available in the pantheon group.

PKGBUILDs for many Pantheon-related packages, such as third-party applications developed for elementary OS and unstable development packages, are available in the AUR.

For the minimal Pantheon shell, install pantheon-session, which pulls several dependencies and core components:

These optional packages provide (background) services for Pantheon and elementary OS applications:

These optional packages contribute to the look and feel of the desktop:

These are some of the original, patched, and selected packages that comprise the optional elementary OS software suite:

pantheon-session provides a gnome-session entry for display managers, such as LightDM.

Use xinitrc to launch the Pantheon shell components by appending them at the end of the file, ie:

Configure Pantheon via switchboard and its plugs, which must be installed separately.

Pantheon components, except for plank, store their configuration in the org.pantheon or io.elementary dconf keys.

Pantheon components and elementary OS software are increasingly delegating certain functions to the gala window manager, in preparation for the transition to Wayland. If you are using another window manager with Pantheon components or elementary OS software, you may see errors like the following:

In this situation, you have a few options:

Several dconf keys pantheon-default-settings expects are missing, as it is written for an older version of gnome-settings-daemon. This is not a problem, but if the messages are an annoyance, comment out or remove the specified keys from /usr/share/glib-2.0/schemas/25_pantheon-default-settings.gschema.override.

One of the RequiredComponents in Pantheon's session file may be failing.

This may be worked around by removing the failed component from /usr/share/gnome-session/sessions/pantheon.session.

See gnome-session crashes on session startup.

lightdm-pantheon-greeter attempts to setup monitors from monitors.xml, instead of Xorg configuration, which does not seem to work reliably.

An alternative is to use another greeter, such as lightdm-gtk-greeter.

Install touchegg. However, this package comes with some gestures that could conflict with pantheon's gestures. You can copy /usr/share/touchegg/touchegg.conf to ~/.local/share/touchegg/ and delete these gesture settings. For more instructions, refer to Touchegg.

Install switchboard-plug-desktop and configure in switchboard.

Since cerbere was retired, pantheon-session expects plank to use this xdg autostart to initiate and request gnome-session's built in management to maintain it.

Either install pantheon-dock-gitAUR, or create /etc/xdg/autostart/plank.desktop.

Set the dconf key io.elementary.terminal.settings.background to your desired background color and opacity with an RGBA value, ie the default: rgba(46, 46, 46, 0.95).

Wingpanel does not come with any indicators; they must be installed separately.

At the minimum, you will probably want to install:

When launched #Via display manager, if third-party indicators' XDG Autostarts contain OnlyShowIn=, append Pantheon; to it.

The gala window manager provides Wingpanel with dynamic transparency.

With gtk-theme-elementary, it becomes opaque when a maximized window occupies the screen and otherwise blends with the wallpaper; using other GTK themes may produce a statically opaque panel.

To achieve the behavior within another theme, add the following code to its css or the override file, ~/.config/gtk-3.0/gtk.css:

**Examples:**

Example 1 (unknown):
```unknown
...
io.elementary.wingpanel &
plank &
exec gala
```

Example 2 (unknown):
```unknown
org.pantheon
```

Example 3 (unknown):
```unknown
io.elementary
```

Example 4 (unknown):
```unknown
** (io.elementary.screenshot:10150): ERROR **: 15:17:28.099: ScreenshotBackend.vala:37: Couldn't get dbus proxy: GDBus.Error:org.freedesktop.DBus.Error.ServiceUnknown: The name org.gnome.Shell.Screenshot was not provided by any .service files
```

---

## Kernel

**URL:** https://wiki.archlinux.org/title/Kernel_panic

**Contents:**
- Officially supported kernels
- Compilation
  - kernel.org kernels
  - Unofficial kernels
- Troubleshooting
  - Kernel panics
    - Examine panic message
      - QR code on a blue screen
      - Console way
      - Example scenario: bad module

According to Wikipedia:

Arch Linux is based on the Linux kernel. There are various alternative Linux kernels available for Arch Linux in addition to the latest stable kernel. This article lists some of the options available in the repositories with a brief description of each. There is also a description of patches that can be applied to the system's kernel. The article ends with an overview of custom kernel compilation with links to various methods.

Kernel packages are installed under the /usr/lib/modules/ path and subsequently used to copy the vmlinuz executable image to /boot/. [1] When installing a different kernel or switching between multiple kernels, you must configure your boot loader to reflect the changes. For downgrading the kernel to an older version, see Downgrading packages#Downgrading the kernel.

Community support on forum and bug reporting is available for officially supported kernels.

Following methods can be used to compile your own kernel:

Some of the listed packages may also be available as binary packages via Unofficial user repositories.

Many of these unofficial kernels contain features that need to be enabled manually. Try reading the documentation in the patches themselves (many already include changes to the Documentation/ directory in the kernel source) or searching up the name of the patchset on the web.

A kernel panic occurs when the Linux kernel enters an unrecoverable failure state. The state typically originates from buggy hardware drivers resulting in the machine being deadlocked, non-responsive, and requiring a reboot. Just prior to deadlock, a diagnostic message is generated, consisting of: the machine state when the failure occurred, a call trace leading to the kernel function that recognized the failure, and a listing of currently loaded modules. Thankfully, kernel panics do not happen very often using mainline versions of the kernel--such as those supplied by the official repositories--but when they do happen, you need to know how to deal with them.

If a kernel panic occurs very early in the boot process, you may see a message on the console containing Kernel panic - not syncing:, but once systemd is running, kernel messages will typically be captured and written to the system log. However, when a panic occurs, the diagnostic message output by the kernel is almost never written to the log file on disk because the machine deadlocks before system-journald gets the chance.

Since linux 6.10 (for drm_panic), the kernel will display a panic as a QR code (by default) in a blue screen. The stack trace is visible at the URL given by the QR code. For Arch Linux, it is a link to https://panic.archlinux.org. The URL contains various information and the stack trace compressed by gzip and encoded in the URL fragment which is not transferred to the server (it is processed on the client side).

An example panic with a link and screenshot can be seen in a forum post.

You can revert to the old behavior by passing the parameter panic_screen=kmsg to the drm kernel module (or drm.panic_screen=kmsg as kernel parameter) to display the stack trace in a console.

The "old" style way of viewing the crash on the console as it happens is still available (without resorting to setting up a kdump crashkernel). Boot with the following kernel parameters and attempting to reproduce the panic on tty1:

It is possible to make a best guess as to what subsystem or module is causing the panic using the information in the diagnostic message. In this scenario, we have a panic on some imaginary machine during boot. Pay attention to the lines highlighted in bold:

We can surmise then, that the panic occurred during the initialization routine of module firewire_core as it was loaded. (We might assume then, that the machine's firewire hardware is incompatible with this version of the firewire driver module due to a programmer error, and will have to wait for a new release.) In the meantime, the easiest way to get the machine running again is to prevent the module from being loaded. We can do this in one of two ways:

This article or section is out of date.

The factual accuracy of this article or section is disputed.

You will need a root shell to make changes to the system so the panic no longer occurs. If the panic occurs on boot, there are several strategies to obtain a root shell before the machine deadlocks:

See General troubleshooting#Debugging regressions.

Try linux-mainlineAUR to check if the issue is already fixed upstream. The pinned comment also mentions a repository which contains already built kernels, so it may not be necessary to build it manually, which can take some time.

It may also be worth considering trying the LTS kernel (linux-lts) to debug issues which did not appear recently. Older versions of the LTS kernel can be found in the Arch Linux Archive.

If the issue still persists, bisect the linux-gitAUR kernel and report the bug in accordance to the kernel process for reporting regressions. Depending on the Bugtracker (B:) entry in the MAINTAINERS file this then entails opening an issue via the subsystems mailing lists, Kernel Bugzilla, or in other issue trackers like the DRM Gitlab. It is important to try the "vanilla" version without any patches to make sure it is not related to them. If a patch causes the issue, report it to the author of the patch.

You can shorten kernel build times by building only the modules required by the local system using modprobed-db, or by make localmodconfig. Of course you can completely drop irrelevant drivers, for example sound drivers to debug a network problem.

**Examples:**

Example 1 (unknown):
```unknown
/usr/lib/modules/
```

Example 2 (unknown):
```unknown
/proc/config.gz
```

Example 3 (unknown):
```unknown
CONFIG_IKCONFIG_PROC
```

Example 4 (unknown):
```unknown
Documentation/
```

---

## Solid state drive/NVMe

**URL:** https://wiki.archlinux.org/title/NVMe

**Contents:**
- Installation
- Management
  - SMART
  - Secure erase
  - Firmware update
    - Generic
    - Intel/Solidigm
    - Kingston
    - Samsung
      - Using Samsung Magician Software

This article or section is a candidate for moving to NVMe.

NVM Express (NVMe) is a specification for accessing SSDs attached through the PCI Express bus. As a logical device interface, NVM Express has been designed from the ground up, capitalizing on the low latency and parallelism of PCI Express SSDs, and mirroring the parallelism of contemporary CPUs, platforms and applications.

NVMe devices should show up as /dev/nvme*. See Device file#NVMe for an explanation on their naming.

Extra userspace NVMe tools can be found in nvme-cli or nvme-cli-gitAUR.

See Solid state drive for supported filesystems, maximizing performance, minimizing disk reads/writes, etc.

List all the NVMe SSDs attached with name, serial number, size, LBA format and serial:

List information about a drive and features it supports in a human-friendly way:

List information about a namespace and features it supports:

Output the NVMe error log page:

Create a new namespace, e.g creating a smaller size namespace to overprovision an SSD for improved endurance, performance, and latency:

See nvme help and nvme(1) for a list of all commands along with a terse description.

Output the NVMe SMART log page for health status, temp, endurance, and more:

NVMe support was added to smartmontools in version 6.5.

Currently implemented features (as taken from the wiki):

See S.M.A.R.T. and the official wiki entry for more information, and see this article for contextual information about the output.

See Solid state drive/Memory cell clearing#NVMe drive.

Firmware can be managed using nvme-cli. To display available slots and check whether Slot 1 is read-only:

Download and commit firmware to specified slot. In the example below, firmware is first committed without activation (-a 0). Next, an existing image is activated (-a 2). Refer to the NVMe specification for details on firmware commit action values.

Finally reset the controller to load the new firmware:

This can also be done manually if needed:

After Intel SSD business was acquired by SK Hynix[2][3], its "Memory and Storage Tool" (Intel MAS) lost support for SSDs and can now only be used to manage Optane devices.[4]

Solidigm, the US subsidiary formed from Intel's SSD business acquisition, provides a new utility to manage former Intel SSDs: "The Solidigm Storage Tool, also called SST, assists with managing Solidigm SSDs. It provides access to drive information and health, SMART Attributes, Firmware Updates, diagnostic scans, and secure erase."[5]

Install solidigm-sst-storage-tool-cliAUR, then check whether your drive has an update available:

If so execute the load command as follows, using the index value given in the previous command:

For more information, refer to the user guide provided on the tool's aforementioned official page.

Kingston does not provide separate firmware downloads on their website, instead referring users to a Windows only utility. Firmware files appear to use a predictable naming scheme based on the firmware revision:

https://media.kingston.com/support/downloads/S5Z42105.zip

Then proceed with the generic flashing instructions.

Next to "Samsung Magician Software" for Windows users, Samsung also provides SSD firmware as bootable ISO images:

https://semiconductor.samsung.com/consumer-storage/support/tools/

They can be written onto a bootable CD or USB drive, or you can unpack the image and do everything live:

You need to reboot for the firmware update to take effect.

Instead of using the manufacturer's program you might prefer to use nvme-cli.

As of January 2025, the firmware files on the ISO are encrypted by an AES key hidden inside the fumagician binary [6][7]. You will find two encrypted files next to it:

The actual firmware file is:

You first need to determine the AES key used. The key can be extracted from the fumagician utility by running [8]

You can then use the samdecrypt.sh utility to decrypt the file. First, insert the key found above into the script. Then, decrypt the ZIP file (the result will be written into root/fumagician/2B2QKXG7.bin):

Decompress the ZIP file:

Finally, decrypt the inner file, yielding ssd_firmware/2B2QKXG7_20241112.bin:

You can now upload the firmware image in ssd_firmware/2B2QKXG7_20241112.bin manually as explained in the previous section. However, you will need to provide an extra --xfer argument to the download operation [9]:

Afterwards, the commit operation should work normally.

Western Digital only supports updating via their Windows based Dashboard software. However, the firmware can be downloaded directly if you know where to look.[10]

First, navigate to the list of all drives and find your drive (model=model_number).

Under your particular drive model there will be one or more <url> entries. If there are multiple URLs then you may need to try each one using the directions below and check the <dependency> tag for your current firmware version.

Now, download the drive-specific XML file:

Inside this drive-specific XML file should be a <fwfile> tag with a xxxx.fluf filename. This is the name of the file you want; you can download it by replacing device_properties.xml from the previous URL with this filename.

A full URL example for a SN850X drive:

Once you have the .fluf file, updating can be performed using the generic flashing instructions. Be aware that this is not officially supported by Western Digital, may not work correctly, and could possibly damage your device. Be extra careful that you are updating with the correct drive and version of firmware.

See Advanced Format#NVMe solid state drives.

NVMe SSDs are known to be affected by high operating temperatures and will throttle performance over certain thresholds.[11]

Raw device performance tests can be run with hdparm:

To check NVMe power states, install nvme-cli or nvme-cli-gitAUR, and run nvme get-feature /dev/nvme[0-9] -f 0x0c -H:

When APST is enabled the output should contain "Autonomous Power State Transition Enable (APSTE): Enabled" and there should be non-zero entries in the table below indicating the idle time before transitioning into each of the available states.

If APST is enabled but no non-zero states appear in the table, the latencies might be too high for any states to be enabled by default. The output of nvme id-ctrl /dev/nvme[0-9] (as the root user) should show the available non-operational power states of the NVME controller. If the total latency of any state (enlat + xlat) is greater than 25000 (25ms) you must pass a value at least that high as parameter default_ps_max_latency_us for the nvme_core kernel module. This should enable APST and make the table in nvme get-feature (as the root user) show the entries.

A drive's low-power states may be non-operational states in which the drive accepts only certain management commands. Version 1.3 of the NVMe specification introduced the Non-Operational Power State Permissive Mode (NOPPM) feature to control background processing in non-operational power states. When enabled and the drive is in a non-operational power state, the drive may exceed the current power state's declared power limit (but not the limit of the lowest operational power state) to perform background processing, such as garbage collection or refreshing old data. When disabled, the drive will respect the current state's power limit and defer such processing until it next enters an operational power state, possibly reducing performance until that processing is done. The default setting is vendor- and drive-specific.

To check if your drive supports NOPPM, use nvme id-ctrl /dev/nvme0 -H | grep 'Non-Operational Power State Permissive'; the output will include Not Supported or Supported. To change the setting, use nvme set-feature /dev/nvme0 -f 0x11 -V <value>, where <value> is 0x0 to disable NOPPM (prefer power saving) or 0x1 to enable NOPPM (prefer performance). By default, the setting will revert to default when the drive is reset (on reboot, and possibly on waking from sleep). If your drive supports the Save and Select Feature Support (SSFS) feature, you can make your change persistent by appending the --save option to the nvme set-feature command.

Some NVMe devices may exhibit issues related to power saving (APST). This is a known issue for Kingston A2000 [12] as of firmware S5Z42105 and has previously been reported on Samsung NVMe drives (Linux v4.10) [13][14]. Also reported for some WesternDigital/Sandisk devices [15].

A failure renders the device unusable until system reset, with kernel logs similar to:

Other symptoms are Btrfs storage becoming read-only and Ext4 reporting I/O errors.

As a workaround, add the kernel parameter nvme_core.default_ps_max_latency_us=0 to completely disable APST, or set a custom threshold to disable specific states.

If setting latency still does not works, try adding pcie_aspm=off and pcie_port_pm=off (as suggested by [16]). You may also need to disable NVMe and PCIe power saving measures within the BIOS.

This article or section is out of date.

Since March 2021 a firmware update 9 from Kingston is available. As Kingston only supports Windows, downloads for Linux can be found via heise.de or github. It is expected that, as long as the kernel workaround is in place, the firmware update will not do much as the deepest powersaving states are not reached anyway.

The value passed is the maximum exit latency (Ex_Lat). For example, to disable PS4 set nvme_core.default_ps_max_latency_us=2000.

Some users (for example, see Laptop/HP) have reported suspend failures with certain NVMe drives. As above, the failure renders the device inoperable until system reset, with kernel messages

As a workaround, add the kernel parameter iommu=soft to use a software replacement for the hardware IOMMU. (For further details, see this documentation[dead link 2025-03-15HTTP 404].) This has the potential to cause some slight processing overhead.

The factual accuracy of this article or section is disputed.

Also you can try kernel parameter amd_iommu=off or better amd_iommu=fullflush on HP laptops with AMD CPU and KIOXIA KBG40ZN* nvme's, after you get I/O error with messages like this:

**Examples:**

Example 1 (unknown):
```unknown
# nvme list
```

Example 2 (unknown):
```unknown
# nvme id-ctrl -H /dev/nvme0
```

Example 3 (unknown):
```unknown
# nvme id-ns /dev/nvme0n1
```

Example 4 (unknown):
```unknown
# nvme error-log /dev/nvme0
```

---

## Help:Reading

**URL:** https://wiki.archlinux.org/title/Enabling

**Contents:**
- Organization
- Formatting
- Root, regular user or another user
- Append, add, create, edit
  - Make executable
- Source
- Installation of packages
  - Official packages
  - Arch User Repository
- Control of systemd units

Because the vast majority of the ArchWiki contains indications that may need clarification for users new to Arch Linux (or GNU/Linux in general), this rundown of basic procedures was written both to avoid confusion in the assimilation of the articles and to deter repetition in the content itself.

Most articles on the ArchWiki do not attempt to provide a holistic introduction to a single topic; they are instead written in adherence to the "Don't Repeat Yourself" principle, under the assumption that the user will seek out and read any supporting material that they do not yet understand. Where possible, such supporting material is indicated in the article via special formatting, see #Formatting.

Because of this organization, it may be necessary to examine several related sources in order to fully understand an ArchWiki article. In particular, users who are new to Arch (or GNU/Linux in general) should expect to end up reading a great number of articles even when solving simple problems. It is especially important to study the supporting material before seeking additional help from other users.

Some lines are written like so:

Others have a different prefix:

The numeral or hash sign (#) indicates that the command needs to be run as root, whereas the dollar sign ($) shows that the command should be run as a regular user.

When the commands need to run as a specific user, they will be prefixed by the username in square brackets, for example:

This means you should use a privilege elevation tool, e.g. with sudo:

A notable exception to watch out for:

In this example, the context surrounding the numeral sign communicates that this is not to be run as a command; it should be edited into a file instead. So in this case, the numeral sign denotes a comment. A comment can be explanatory text that will not be interpreted by the associated program. Bash scripts denotation for comments happens to coincide with the root PS1.

After further examination, "give away" signs include the uppercase character following the # sign. Usually, Unix commands are not written this way and most of the time they are short abbreviations instead of full-blown English words (e.g., Copy becomes cp).

Regardless, most articles make this easy to discern by notifying the reader:

Append to ~/path/to/file:

When prompted to append to, add to, create, or edit one or more files, it is implied that you should use one of the following methods.

To create or modify multiline files, it is suggested to use a text editor. For example, using the nano command to edit the file /etc/bash.bashrc is:

To create or overwrite a file from a string, it may be simpler to use output redirection. The following example creates or overwrites the contents of the file /etc/hostname with the text myhostname.

Output redirection can also be used to append a string to a file. The following example appends the text [custom-repo] to the file /etc/pacman.conf.

When prompted to create directories, use the mkdir command:

After creating a file, if it is meant to be run as a script (whether manually or called by another program), it needs to be set as executable, for example with:

See chmod. Some applications such as file managers may provide graphical interfaces to do the same.

Some applications, notably command-line shells, use scripts for their configuration: after modifying them, they must be sourced in order for the changes to be applied. In the case of bash, for example, this is done by running (you can also replace source with .):

When the wiki suggests modifying such a configuration script, it will not explicitly remind you to source the file, and only in some cases will it point to this section with a reminder link.

When an article invites you to install some packages in the conventional way, it will not indicate the detailed instructions to do so; instead, it will simply mention the names of the packages to be installed.

The subsections below give an overview of the generic installation procedures depending on the package type.

For packages from the official repositories, you will read something like:

This means that you have to run:

The pacman article contains detailed explanations to deal with package management in Arch Linux proficiently.

For packages from the Arch User Repository (AUR), you will read something like:

This means that in general you have to follow the foobarAUR link, download the PKGBUILD archive, extract it, verify the content and finally run, in the same folder:

The Arch User Repository article contains all the detailed explanations and best practices to deal with AUR packages.

When an article invites to start, enable, etc., some systemd unit (e.g. a service), it will not indicate the detailed instructions to do so, but instead you will read something like:

This means that you have to run:

A notable command that does not follow this exact pattern is systemctl daemon-reload which will be called without arguments.

The systemd#Using units section contains structured list of available actions (like start, enable, enable and start, etc.) with their corresponding systemctl commands.

It is important to remember that there are two different kinds of configurations on a GNU/Linux system. System-wide configuration affects all users. Since system-wide settings are generally located in the /etc directory, root privileges are required in order to alter them. For example, to apply a Bash setting that affects all users, /etc/bash.bashrc should be modified.

User-specific configuration affects only a single user. Dotfiles are used for user-specific configuration. For example, the file ~/.bashrc is the user-specific configuration file. The idea is that each user can define their own settings, such as aliases, functions and other interactive features like the prompt, without affecting other users' preferences.

Bash and other Bourne-compatible shells, such as Zsh, also source files depending on whether the shell is a login shell or an interactive shell. See Bash#Configuration files and Zsh#Startup/Shutdown files for details.

Some code blocks may contain so-called pseudo-variables, which, as the name says, are not actual variables used in the code. Instead they are generic placeholders and have to be manually replaced with system-specific configuration items before the code may be run or parsed. Common shells such as bash and zsh provide tab-completion to auto-complete parameters for common commands such as systemctl.

In the articles that comply with Help:Style/Formatting and punctuation, pseudo-variables are formatted in italics. For example:

In this case interface_name is used as a pseudo-variable placeholder in a systemd template unit. All systemd template units, identifiable by the @ sign, require a system-specific configuration item as argument. See systemd#Using units.

In this case the pseudo-variables are used to describe the parameters that must be substituted for them. Details on how to gather them are elaborated on in the section Securely wipe disk#Calculate blocks to wipe manually, which features the command.

This article or section needs expansion.

In case of file examples, pasting pseudo-variables in real configuration files might break the programs that use them.

In most cases, ellipses (...) are not part of the actual file content or code output, and instead represent omitted or optional text that is not relevant for the discussed subject.

For example HOOKS=(... encrypt ... filesystems ...) or:

Be aware though that, in a few instances, ellipses may be a meaningful part of the code syntax: attentive users should be able to recognize these cases by the context.

**Examples:**

Example 1 (unknown):
```unknown
# mkinitcpio -p linux
```

Example 2 (unknown):
```unknown
$ makepkg -s
```

Example 3 (unknown):
```unknown
sudo command
```

Example 4 (unknown):
```unknown
[postgres]$ initdb -D /var/lib/postgres/data
```

---

## ArchWiki talk:Requests

**URL:** https://wiki.archlinux.org/title/ArchWiki_talk:Requests

**Contents:**
- Creation requests
  - Left-Handed Adjustments for Desktop Environments
  - iPXE
  - Missing redirects
  - Mirror troubleshooting
  - Random page - only with specific language ?
  - Observability
  - Installation Recipes
  - Article about how to optimize for Privacy
  - GPU article

Is the wiki missing documentation for a popular software package or coverage of an important topic? Or, is existing content in need of correction, updating, or expansion? Write your requests below and share your ideas...

Here, list requests for topics that you think should be covered on ArchWiki. If not obvious, explain why ArchWiki coverage is justified (rather than existing Wikipedia articles or other documentation). Furthermore, please consider researching and creating the initial article yourself (see Help:Editing for content creation help).

I was thinking it would be helpful for lefties if there were a list of configuration options for each desktop environment that facilitate left-handed use of mice and touchpads. I'm not sure if this is related enough to Arch to include in this wiki, but I haven't had a lot of luck finding information for my own DE (KDE) let alone for others. I will start writing down information, and if no one else thinks there should be a separate page for this, I'll just add the information I find to each individual DE's page. ajrl 2013-08-11T15:0906:00

iPXE is a powerful network boot program with many features. Currently, there is no iPXE specific page to describe iPXE in details. There are some pages mentioning iPXE in the wiki, mostly related to network booting, without any further instruction on how to get iPXE to work. So I think it's worth to add a page with detailed iPXE explanation in the wiki. Alive4ever (talk) 10:08, 21 July 2016 (UTC)Reply

--Larivact (talk) 15:35, 21 October 2018 (UTC)Reply

Unfortunately, Mirrors#Troubleshooting is a bit lacking and can definitely benefit from more information such as how to detect a bad mirror. A mirror becoming faulty is not extremely uncommon so this topic should definitely be covered, including who to tell about the bad mirror so it can e.g be removed from the mirrorlist.

This topic is impacting enough people that it is worth putting it into here.

-- NetSysFire (talk) 00:07, 6 August 2021 (UTC)Reply

I like the "Random page", https://wiki.archlinux.org/title/Special:Random , but often it provides me the articles not in English, e.g. today fstab in Portugus which is above my pay-grade :-) Is it possible to call the random page url with an extra option and get e.g. only wiki articles in English ? Thanks. Ua4000 (talk) 09:34, 19 May 2023 (UTC)Reply

This article could start by quoting and linking to the Wikipedia article on the same topic. I also have an idea of adding a brief blurb to the General Recommendations page, or even to the list of applications (with a link to the main Observability article prominently at the top of the section).

As a budding cloud engineer, I've become more interested in observability in web applications/services, and in my own hobby projects. I've done some research into it, and I'm slightly familiar with a few different options. Most of what I found are open source, and I'd like to link to some options from within this Observability page, with possibly full-blown wiki articles describing various options. There are more turnkey, commercial offerings as well, and we could list some of those with links for more information. I actually went with one of those to start off with, mainly so I could learn which features were important to me, before I cobble together some of the more manual options.

When I was searching for ways to monitor my personal web projects, I stumbled upon an ad for New Relic, one of the many commercial services. What struck me as interesting, was that most of New Relic's tools are open source (under an Apache 2.0 license, written in Go). They offer 100GB of data upload per month for free, which has been plenty for observing my hobby projects on my VPS. I've already drafted (but not published) PKGBUILDs for some of the tools I've used, mostly from their pre-compiled debs. There are PKGBUILDs in the AUR (like newrelic-cliAUR and newrelic-infraAUR), but I didn't see a Wiki article describing how to use them. Unfortunately Arch Linux isn't currently supported by New Relic, and their method of bootstrapping New Relic using only the newrelic command (part of newrelic-cliAUR) didn't work for me so I've had to follow their dispersed manual instructions. I've run across a few issues so I've gotten onto their forums to seek help. They have community members who provide the first line of support, and for at least one of my issues they've actually created a New Relic support case for it.

I even learned about Pressure Stall Information, and wrote my own scripts to monitor and alert when CPU, I/O, or memory pressure gets too high. It's in a public Git repository, but I wanted shore up some of the documentation before I share a link to it anywhere.

Would an Arch Wiki article like this be worthwhile? I think it would be, but I didn't want to simply start writing it without having a discussion first. Ectospasm (talk) 18:00, 17 December 2023 (UTC)Reply

Thanks to the rolling release of Arch Linux, I've only actually installed Arch a handful of times. And the Wiki is wonderful in documenting the process. The first time I ever installed Arch, I didn't have another computer to access the wiki, and didn't think to use my phone. I just used separate virtual consoles on the desktop system I was using, and had the wiki loaded in a text mode browser (elinks or lynx, I don't remember which). I may have even enabled GPM so I could copy and paste with my mouse in the Linux console (this part I don't rightly remember).

However, each time I've installed Arch I've always wanted something specific, which generally meant searching, following, and backtracking through several Wiki articles to achieve my end. Now that I have a new laptop, and I'm planning on installing Arch on it very soon, I've drafted a "recipe" for myself to follow. I've been told that publishing such a "recipe" is against the spirit of the Wiki, but I'm finding it's a slog to go through all the articles to piece together how I want to do it, especially without planning ahead. I think many Archers might find such "recipes" useful, if only to get ideas for their own systems.

I think the format of these "recipes" would need to be debated, as I'm not sure what would be best. We definitely don't want to replicate or repeat sections within the wiki. Here's my proposed format, but I'm not settled that this is the best way to do it. First, start with hardware prerequisites for the recipe. A recipe for a file server or Network Attached Server (NAS) would be different from a laptop, or an x86_64 handheld or mobile device, as would a recipe for a virtual private server (VPS) or other virtual machine. The recipe could go on to describe the partition layout, filesystems, bootloader, network topology and corresponding network manager, etc. The reasoning behind each decision can be explained in whatever detail the author wants. It could then provide a link to a (pseudo-)script that executes it, possibly pausing where the user is intended to make a decision. I'm not familiar with the Archinstall, but if these work well perhaps they could be an optional component of the Archinstall script (this might be getting ahead of myself, though).

What would be the proper way to address this kind of documentation, and keep to the spirit of the wiki? Or is this really a bad idea? If it is a bad idea, why would it be so? I mean, such a section in the Wiki could have a disclaimer stating that it is a requirement for readers to understand the installation process, and not blindly follow what's listed. Of course, links back to the Wiki articles describing how to perform each task should be all over these "recipes," I don't think these are a substitute for understanding the installation guide. Maybe the wiki isn't the best place for this; the wiki could collect links to recipes by various users, and the recipes themselves could be hosted on the Arch Linux GitLab instance. Ectospasm (talk) 22:14, 17 December 2023 (UTC)Reply

I know that Arch doesn't want to follow an Ideology like some other distros do, but wants to give everyone any options. But as we do already have an article about how to improve you System towards other goals like Security or Improving performance i think an article, at with at least links to the various ways to improve your system for Privacy would be a good idea.

Especially because on many pages like NetworkManager/Privacy or Firefox/Privacy we already have pages about specific applications Privacy related config and if you search beyond arch, especially when it comes to linux system config, you have to look at a bunch of sites and sometimes change things to make them work on arch.

SolarpunkJulian (talk) 18:00, 17 September 2024 (UTC)Reply

I think it would be useful to create a generic "Graphics processing unit" article which would serve as the main place to discover all graphics hardware related articles. For example, the DRM, OpenGL and Vulkan information in Xorg#Driver installation is not limited to just Xorg. Having the table there is not very discoverable for Wayland users.

--nl6720 (talk) 10:07, 25 July 2025 (UTC)Reply

Here, list requests for correction or other modification of existing articles. Only systemic modifications that affect multiple articles should be included here. If a specific page needs modification, use that page's discussion or talk page instead and one of the article status templates.

As a rolling release, Arch is constantly receiving updates and improvements. Because of this the Arch wiki must be updated quickly to reflect these changes.

Trying to install drives with/out Luks, LVM on internal, external drives is quite complicated currently. Following the ralated articles suggest different ways of reaching the goal. Many different drive name conventions are suggested, eg.:

Some of them don't work with portable external drives. This overcomplicates setting up encrypted drives in different situations. My suggestion is, to change all drive related articles to one specific solution of addressing drives universal. Currently I think of UUDI drive naming as a way to go. This would ease the process of drive naming in all kinds of situations:

' LMV or other virtual file systems are easier to describe and setup

Ok, I know it is a big suggestion. I wanted to bring it up here, bacause I have the impression that following one primary path would help a lot - everyone involved. It doesn't need to be done in one day. While I think to have one suggested guideline would be a good start. Then with thain mind, we all have it easier to change those sections while Writing/editing Wiki entries.

T.ask (talk) 11:22, 30 March 2015 (UTC)Reply

In preparation to making all (php) webapps use a dedicated user, I extended information on PostfixAdmin and realized, that the information on php-fpm is scattered all over the articles of nginx and apache (and probably all over some other web server pages as well). I think the citation/ linking in all of the web server pages and the php web application pages would greatly improve, if this information was moved to a dedicated page, or to a sub-page for PHP, as it is quite PHP specific (unlike e.g. uwsgi). Davezerave (talk) 00:55, 26 May 2019 (UTC)Reply

PHP 8.2 update and introduction of legacy branch.

--Fengchao (talk) 01:23, 15 January 2023 (UTC)Reply

Is there guidance on how long information about kernel and program versions should be kept ? e.g "btrfs supports this as of linux 5.0" or "fdisk supports GPT since...." It's mildly interesting when those are recent changes but a lot of occurrences on the wiki relate to versions which have been removed from repos for years... I think in most cases it adds unnecessary clutter, ideas ?

-- Cvlc (talk) 12:39, 23 January 2023 (UTC)Reply

I don't know if I should post it under here, but here it is.

Since many of the languages' ArchWiki just directly sit on English ArchWiki, having sidebar being half-translated would be a bit weird.

so, on MediaWiki:Sidebar, each second level item represents a link (texts on the left of | is the destination and on the right of it is the "name". I'll call these "name"s as "message"s below, you'll know the reason), and some of the first level item represents a title. When displaying the sidebar, MediaWiki will try to retrieve the name from the MediaWiki: namespace (every page in that namespace is called a system message). For example in current MediaWiki:Sidebar:

Take Interaction as example. MediaWiki will first read the message MediaWiki:Interaction/user's display language (let's say Simplified Chinese, then it's MediaWiki:Interaction/zh-hans and assuming its content is ). If the page is there, the page's content  will substitute "Interaction". If there isn't, then the root page, if still isn't, then leave it as-is. The whole mechanism may differ since the actual mechanism goes through a long list of language fallback (still take zh-hans, it may go through /zh-hans, /zh-hant, /en, root).

So, to localize the sidebar, we will going to create some MediaWiki: pages, or system messages, and their subpages with sub-title the corresponding interlanguage link prefix. Some of the messages are already translated by MediaWiki core, and I'll list those that aren't present now. I'd suggest renaming these pages as well to have kebab-case though, and then change them correspondingly in MediaWiki:Sidebar.

(MediaWiki:Statistics is present due to Special:Statistics.)

Thanks! --Lakejason0 (talk) 13:52, 4 February 2023 (UTC)Reply

The Qt4 major version of the Qt GUI toolkit has not been supported by the Qt Company since 2015. It also has not been a part of the Arch repos (only in the AUR) since 2019-05. I believe that documenting software in the AUR (!= providing official support for AUR packages) is within the scope of ArchWiki; I can only think of dwm right now, but I am sure that there are more. However, maintaining old Qt content means keeping it in the Qt page and related pages (!). As adoption of Qt6 ramps up, I think we should consider removing Qt4 content from the main wiki. I have not searched for the occurrences yet, but I at least volunteer to aggregate them somewhere for User:CodingKoopa/Removed content. -- CodingKoopa (talk) 16:12, 25 June 2023 (UTC)Reply

Intel has changed their domains and all the following links lead to some general landing page:

Actually all Intel links should be checked by a human, because the script used by the bot gets status 404 for most Intel links, even those that work fine...

 Lahwaacz (talk) 09:18, 30 July 2023 (UTC)Reply

As announced on the Mailing list all bugs have been migrated to GitLab, and GitLab is the new bug tracker. The pages refereeing to bugs.archlinux.org (e.g. Bug reporting guidelines) must be updated to point to our GitLab.

Klausenbusk (talk) 20:42, 25 November 2023 (UTC)Reply

Currently implementation detail about specific vendors and in some cases even specific models is spread out across multiple pages. This has two problems:

For example, if I have an ASUS laptop, I will find many things I need to know on the Laptop/ASUS page. But for some reason information on fan control is in Fan_speed_control#ASUS_laptops, there are notes about extra keys on Extra_keyboard_keys, etc.

I would like to propose that all these hardware specific sections are moved to their associated articles. This includes troubleshooting sections for specific software where the information cannot be rewritten to be useful generally.

This would NOT include separate articles about specific topics such as ASUS_Linux, Asusctl, etc that already have their own page. This would also not include example pages such as GRUB/EFI_examples#ASUS which are just using the hardware as an example.

Comic-paralyze-image (talk) 18:51, 1 March 2025 (UTC)Reply

The 2.4 update to Dovecot changed the configuration syntax significantly, and previous configurations are not valid anymore. Their non-comprehensive list of changes can be found here.

As a consequence, a significant number of Wiki pages are now obsolete. This affects at least the following Wiki pages: Dovecot, Virtual_user_mail_system_with_Postfix,_Dovecot_and_Roundcube, SOGo, Exim.

...as this produces better formatted output (date), and can be used without root ? --Cvlc (talk) 10:49, 25 July 2025 (UTC)Reply

Having see the patchset for removing initrd support from the kernel, we may want to update the pages which mention it with the actual thing we're using which is an initramfs.

We may want to keep a mention of what it was in the section where initrd redirects to or archive it.

--Erus Iluvatar (talk) 18:03, 14 September 2025 (UTC)Reply

in picom#slock, first, slock doesn't set a class or window id, one can edit the source code so that the slock window sets one and then catch it using picom's window rules. One can also use the fullscreen match at the end of the section but I think it's preferable to apply these changes only to slock so as to not affect any other window.

second, running `xwininfo & slock` doesn't capture the window information of slock since both of these programs require capturing the cursor which leads to a conflict and slock not starting.

I suggest instructing the reader to write a simple function that sets these in slock as I didn't find a patch for that in the suckless website. here's a simple one I wrote:

--RelativeAlbatros (talk) 18:45, 19 September 2025 (UTC)Reply

Here, list requests for repetitive, systemic modifications to a series of existing articles to be performed by a wiki bot.

Some languages are hosted on external wikis, and I want the bot to automatically add / update language links for languages hosted on external wikis so that users do not need to add them manually. -- Blackteahamburger (talk) 15:27, 20 May 2020 (UTC)Reply

Can the hints for Template:Broken package link marked on the localization page use the localized version? This allows users who do not understand English to understand the status of the software package. -- Blackteahamburger (talk) 11:34, 26 May 2020 (UTC)Reply

We already flag external links, broken packages in the repositories and AUR: could we do the same for interwiki links? I've seen a few of those manually fixed recently, in particular on translations. --Erus Iluvatar (talk) 19:41, 2 March 2022 (UTC)Reply

As you can see in Special:Diff/835558, dead translations are not automatically detected and removed. It should not be too hard to get the bot to scan for this. Bonus points if #Dead interwiki links above could also detect dead translations that are hosted on other wikis, e.g the german archwiki. -- NetSysFire (talk) 09:19, 12 June 2025 (UTC)Reply

Not all translated pages have a Template:TranslationStatus. It would be ideal if the bot can flag such pages automatically. -- NetSysFire (talk) 09:19, 12 June 2025 (UTC)Reply

Here, list redirects of ambiguous value, that do not have a history outside of changing the redirect target. Such redirects may have been created when moving a page which had a bad title initially, or before MediaWiki search function provided suggestions.

We do not delete pages with a history (i.e. with a content)use Template:Archive in a page instead.

Remove DeveloperWiki:HOWTO Be A Packagerit is too cool to exist. I mean, we have DeveloperWiki:How to be a packager, original name is useless.  Andrei Korshikov (talk) 14:02, 3 September 2025 (UTC)Reply

I agree with @Alad that "screen" is about "GNU screen" and their friends (especially tmux). I want to delete screen disambiguation page and rename GNU Screen to screen. How to do it right?

 Andrei Korshikov (talk) 20:13, 6 September 2025 (UTC)Reply

**Examples:**

Example 1 (unknown):
```unknown
[community]
```

Example 2 (unknown):
```unknown
MediaWiki:Sidebar
```

Example 3 (unknown):
```unknown
...
* Interaction
** :Category:Help|help
** {{ns:Project}}:Contributing|Contributing
...
```

Example 4 (unknown):
```unknown
MediaWiki:Table of contents
MediaWiki:Interaction
MediaWiki:Contributing
MediaWiki:Recent talks
MediaWiki:Requests
```

---

## FVWM

**URL:** https://wiki.archlinux.org/title/FVWM

**Contents:**
- Installing
- Starting
  - Autostart
- Configuration
  - The virtual desktop
  - Keyboard and mouse bindings
  - Window decoration
    - Titlebar buttons
    - Button styles
    - Title and border styles

This article or section is a candidate for moving to Fvwm.

This article or section is out of date.

Fvwm3 was created as a fork of Fvwm2 for any future development with the intent to break compatibility with Fvwm2 to allow major changes. Fvwm3 version 1.0.0 was published on 3 September 2020.

Fvwm is an ICCCM-compliant multiple virtual desktop window manager for the X Window system. It is configured by editing text-based configuration files. Although using FVWM does not require any knowledge of programming languages, it is possible to extend FVWM with M4, C, and Perl preprocessing. FVWM also has a Perl library which allows one to create modules. FVWM stands for F Virtual Window Manager with the preferred interpretation being that the F does not stand for anything in particular [1].

This article or section is out of date.

Install the fvwmAUR package.

The following packages provide themes and icons for FVWM: fvwm-crystalAUR, fvwm-iconsAUR, fvwm-themesAUR, fvwm-themes-extraAUR. FVWM Crystal provides a separate session for a desktop environment like experience.

Select FVWM from the session menu in a display manager of choice. Otherwise, add exec fvwm to your user's .xinitrc.

For FVWM Crystal, select FVWM-Crystal from the session menu or add exec fvwm-crystal to your user's .xinitrc.

See xinitrc for details, such as preserving the logind session.

FVWM provides a number of functions to start modules or applications when initialising, restarting or exiting the window manager.

You can add your own actions to any of these functions using the AddToFunc command. For example, if one wanted to start network-manager-applet on startup (but not for any subsequent restarts of the window manager) one could add the nm-applet command to the InitFunction:

You can also use just StartFunction and prepend your commands with Test commands which check whether the window manager has started or restarted and run the action only if the test is true. Using this method, nm-applet could be started in the following manner:

The following configuration file locations are supported:

The following configuration locations are supported as of version 2.6.8, but may not be supported in the future:

As of version 2.6.7, FVWM ships with a new default configuration, located in /usr/share/fvwm/default-config. As such, the older sample configuration files are no longer provided. However, they can still be viewed on GitHub. The fvwm-themes project also provides ready-made configurations though it should be noted that these have not been updated since 2003 and may require modifications to work correctly with more recent FVWM versions.

For its virtual desktop, FVWM implements both workspaces (used by window managers such as Metacity and Openbox) and viewports (used by window managers such as Compiz). See [2] for a description of the differences between workspaces and viewports. FVWM refers to workspaces as desks and viewports as pages.

Pages in FVWM are arranged in a grid. The number of pages used can be defined with the DesktopSize command. For instance, adding DesktopSize 3x3 to your configuration file will give you 9 pages, arranged in a 3x3 grid. Pages can be navigated using the pager module or with the GoToPage command which could be mapped to a keyboard shortcut or menu entry. For instance, the command GoToPage -1p +0p will move the viewport 1 page to the left of the current page.

The number of desks available in FVWM is very large, the minimum desk number is -2147483648 and the maximum is 2147483647. By default, FVWM starts on desk 0. Desks can be navigated using the pager module (if it is configured to show a number of desks) or with the GoToDesk command - GoToDesk +1 will move to the next desk, relative to the currently used desk.

Keyboard or bindings can be defined in the configuration file with the Key or Mouse command. The syntax of the command takes the following form: Key/Mouse (window) button/key name context modifiers action. For instance, the following example will launch an XTerm on an Alt+F2 keypress: Key F2 A 1 Exec xterm. Note that the (window) argument is optional.

The context value defines where the binding will be applied. The following contexts are valid: R (root window), W (application window), D (desktop manager window - PCManFM desktop manager for instance), T (title bar), S (window side, top, bottom), [ ] - _ (left side, right side, top, bottom respectively), F (window frame corners), < > ^ v (left, right, top, bottom corners respectively), I (icon window), 0-9 (titlebar buttons) and A (all contexts). Any combination of these letters is also acceptable.

The modifier value can be one of the following: A (any), C (control), S (shift), M (meta), N (none), or 1-5, representing the X Modifiers - run xmodmap to see which X modifier is which. Multiple modifiers should not be spaced. For instance, to use the modifier Control+Alt, you would supply as the modifier argument C1.

The action must be an FVWM function to run, such as the Quit function or Menu function. Execution of external commands, such as xterm, can be achieved with the Exec function, as shown above.

FVWM can provide up to 10 window buttons. These are numbered 0-9. Even numbers indicate buttons located on the right hand side of the titlebar whilst odd numbers indicate buttons located on the left. The layout is as follows:

Window buttons will remain hidden unless a Mouse command is used which specifies one of the titlebar buttons as the context. For instance, to activate the rightmost titlebar button and make it close the window on a left click, one would use the following command:

Mouse is the name of the command, 1 is the mouse button, 2 is the number of the rightmost titlebar button, A is the modifier (any) and Close is the action to be taken. See #Keyboard and mouse bindings for more information.

The style of your titlebar buttons can be configured with the ButtonStyle command. This takes the following syntax:

The state can be one of ActiveUp, ActiveDown, InactiveUp, InactiveDown. ActiveUp and ActiveDown refer to the un-pressed and pressed button states of the active window. Likewise for the Inactive states. One can also use just Active or Inactive which are shortcuts for both the pressed and un-pressed button states.

The style argument can be one of the following:

A number of vector styles are documented here. You can also create your own vector buttons using this vector buttons viewer. Finally, see this page for some example decoration configurations that use pixmaps, including imitations of Crux (a Sawfish theme), Mac OS and Windows 98.

The flag affects the state for a button. Some examples of flags include Raised, Sunk and Flat. For more information, see fvwm(1) and look for the ButtonStyle section.

The window titles and borders can be configured with the TitleStyle and BorderStyle commands respectively.

TitleStyle can take the following arguments:

The justify argument can be LeftJustified, RightJustified or Centered. TitleStyle and BorderStyle can take the following arguments:

See #Button styles for the state, style and flag arguments.

Menus can be created with the AddToMenu command. This takes the following syntax:

Remove the menu-title and Title arguments to create a menu with no title. Subsequent entries in the menu take the following syntax:

See the following example:

Use the Popup command to show other menus. For instance, to include the menu defined above in another menu one could use the following syntax:

FVWM menus also support icons. To give a menu entry an icon, provide the path to the icon enclosed in % signs after the entry name - see below:

Use the xdg_menu tool, provided by archlinux-xdg-menu, to automatically generate menus - see Xdg-menu#Fvwm2.

Menus in FVWM can be dynamic, meaning that their content is refreshed every time the menu is opened. This can be useful when using a menu generator, such as one that constructs a menu of applications from XDG desktop entries, and you want the content to always be up to date.

Dynamic menus can be created in FVWM by using the DynamicPopUpAction and DynamicPopDownAction keywords. The former can be used to create/recreate the menu when it is opened whilst the latter can be used to clear the menu when it is closed. It is important to note that for a submenu to be dynamic, the parent menu that it is included in must also be created dynamically even if it does not contain any dynamic content.

Consider a hypothetical menu generator called my-menu which generates output of the sort below.

Now suppose that we wished to incorporate my-menu into our root menu dynamically. This means that the root menu must also be created dynamically. To accomplish this, we need to define a function that constructs the root menu and a function that calls the dynamic menu generator using the PipeRead command. Then in the menu definitions themselves, DynamicPopUpAction can be used to call those functions. See the example below.

The Style command allows one to configure various aspects of the window manager itself and also to set behaviors for certain windows. The syntax is Style window-name stylename. The window-name argument can be a window name, class, title name or resource string. Use * to match all windows. See fvwm(1) for all available styles - some examples are provided below:

FVWM provides many built in functions, examples being Close to close a window or Exec which allows the execution of an external command. Users can also define their own functions or add to existing functions using the AddToFunc command. This uses the following syntax: AddToFunc func-name I|M|C|H|D action. The letter codes stand for the following: I - execute immediately, M - execute when the user moves the mouse, C - execute on mouse click, H - execute when the user holds the mouse button, D - execute when the user double clicks the mouse button. Below is a trivial example of a function:

One can also use conditional commands (see fvwm(1) and look for the Conditional Commands section). For instance, suppose one wanted a function that would close all windows in the current page other than the one which has focus. That function can be defined as below:

The All conditional matches all windows that meet the conditions defined in parenthesis. Functions can execute more than one action, just add one line for each action beginning with a plus sign:

Modules are separate programs, spawned by FVWM that can add extra functionality. Modules can be spawned using the following syntax: Module ModuleName (identifier) ModuleArgs.

Sometimes, one might want to spawn multiple instances of the same type of module, each with their own separate configuration. In this case, one should spawn the module with an identifier, for instance:

where Panel1 and Panel2 are the identifiers.

Most modules will have a number of module commands which can be used to configure the module's appearance or behavior. Use the following syntax:

For instance, if one spawned an FvwmPager with the identifier MyPager then one could configure it in the following fashion:

The FvwmPager is a module which provides a visual representation of the desks and pages provided by the window manager. Like all modules, it must be spawned by FVWM. To start an FvwmPager, add something similar to the following to your StartFunction:

With no arguments, the FvwmPager will only display the viewports for desk 0. With the arguments 0 9, FvwmPager will show the 10 desks from 0-9.

With the argument * the FvwmPager will show only one desk but it will always be the desk that is currently being used.

See FvwmPager(1) for a list of module commands.

FvwmButtons is a module which can create a box of buttons which can perform actions when pressed. FvwmButtons can also "swallow" application windows. This might be useful for swallowing a system tray window or a clock window. The size of the panel is automatically determined (the panel will resize to accommodate all elements) however it is also possible to manually set a size using the Geometry command. This takes a standard X geometry. The basic commands needed to create an FvwmButtons panel are outlined below.

Set the number of rows and columns:

Note that items are added by filling each row from left to right, from the top row to the bottom one.

Create a button (this example creates a button with a title and an icon which launches an XTerm when left clicked):

You can omit any of these arguments if you so choose.

Swallow a window (this example swallows a stalonetray):

The Swallow command takes a number of "hangon" arguments. Here, the UseOld and NoClose arguments have been used. UseOld means that an existing window will be swallowed if it exists. NoClose means that the window will not be closed if the FvwmButtons process is terminated. The stalonetray argument is the class of the window that we want to swallow. Replace as appropriate. The last argument is a command to start the application that is to be swallowed. In this example, it is assumed that the application has already been started so the argument provided is Nop which is a function that does nothing. One could replace this with "Exec stalonetray" to start the application from the FvwmButtons module instead of assuming that the application has been started elsewhere.

Containers are spaces defined which can span multiple rows and columns or subdivide a row or column into more rows or columns. This can be useful for instance if one wants to allocate a certain percentage of space to an element. Say one wants to swallow an XClock and allocate 100% of the width and 80% of the height to the XClock. This can be defined as such:

Note that a container is created by defining a certain number of columns and rows and then using the keyword container. Elements inside the container are defined underneath this line and the container is then closed with the End command.

For a full list of options, see FvwmButtons(1).

FvwmEvent is a module which can be used to bind a function or an audio file to a window manager event (such as the closing of a window). In the case of an audio file, the audio will be played when the event that it is bound to occurs. FvwmEvent can be spawned from within Fvwm by adding the following to your StartFunction:

FvwmEvent can also be spawned with an identifier - see #Modules. Once spawned, FvwmEvent will run in the background, waiting for events that it has been configured to recognize. Events can be configured in the following form:

where windowshade is the event and Lower is the command to be executed when that event occurs. In the case where you wish to execute a function with arguments, that function and its arguments need to be quoted - see below:

For a full list of events, see FvwmEvent(1).

FvwmIdent is a module which can display many items of information about a particular window that it is called on, such as the window's class name, resource name, layer, geometry and more. This information is displayed in a separate window which is created by the module. FvwmIdent can be started using the Module FvwmIdent command. This could be bound to a menu entry or a hotkey. When an FvwmIdent is started in the context of a window, that window's information will be displayed. Otherwise, the user will be prompted to select a window manually.

FVWM color styles can take a number of color code types such as hexcolors (#ffffff for instance), rbg colors (rgb:ff/ff/ff for instance) as well as the pre-defined X11 colors.

The following styles might be useful:

Colorsets in FVWM are a set of four colors (a foreground color, a background color, a shadow color and a highlight color) as well as an optional background pixmap. Any part of FVWM that uses a particular colorset will be affected if that colorset is changed.

All colorsets are identified by a number. Any numbering convention can be used; the fvwm-themes project documents one such convention that uses the first 40 colorsets (0-39). See [4].

Colorsets can be created with the ColorSet command - syntax: ColorSet number options. See fvwm(1) - the Colorsets section - for more information.

For styles such as Font, use xorg-xfontsel to determine the correct font names for X11 fonts - see X Logical Font Description and Font configuration for more information. You can also specify xft fonts, for example: "xft:DejaVu Sans:size=10".

Windows in FVWM can be iconified (minimized). This means that the window will disappear from view and be replaced by an icon on the desktop, similar to the behavior in Microsoft Windows 3.1.

The program itself will supply the icon. One can also set a default icon to be used, in case a program does not have an icon to supply: Style "*" Icon /path/to/default/icon.png.

To disable icons altogether, use the following style: Style "*" NoIcon. This means that the window will simply disappear when iconified.

The positioning of icons can be controlled using the IconBox style - note that this is not the same thing as the FvwmIconBox module - along with the IconFill style. Use Style "*" IconBox none to disable the IconBox. This means that icons will be placed at the position of the top left corner of the window. Else, use Style "*" IconBox l t r b where l t r b stand for left, top, right and bottom respectively. These arguments should be the number of pixels away from the screen edge that the IconBox edge should be. Hence, arguments of +0 +0 -0 -0 means that the IconBox will fill the entire screen.

Multiple IconBox styles can be defined but they must be defined on the same line, for instance:

When the first IconBox overflows, icons will then be placed in the second IconBox. Note that if the last IconBox overflows then the default IconBox will be used which covers the entire screen and fills from top to bottom - left to right.

Use the IconFill style to control how icons will be filled. A style of Style "*" IconFill left bottom means that icons will be filled from left to right - bottom to top (Motif Window Manager behavior). A style of Style "*" IconFill top left means that icons will be filled from top to bottom - left to right. Note that the IconFill style should be defined on the same line as the IconBox style, for instance:

FVWM provides a number of options that allow it to mimic the appearance and behaviour of MWM (Motif Window Manager).

The sizing of icons is not regular as different programs provide icons of differing sizes. Use the IconSize style to force a regular size: Style "*" IconSize 48 48 for instance. Note that icons larger than the given size will be clipped.

By default, icons also have no background. You can use the IconBackgroundColorset style to force icons to have a background. See #Colorsets.

Use the OpaqueMoveSize unlimited command to view the window itself when moving.

Use Style "*" ResizeOpaque to view the window itself when resizing.

To disable scrolling to the next viewport when moving the mouse pointer to the screen edge, use the following command: EdgeScroll 0 0.

Ensure that perl-tk and perl-x11-protocol are installed. The use the following style command: Style "*" ClickToFocus. See fvwm(1) for other focus behaviors.

The following functions can tile a window to the left half, right half, top half or bottom half of the screen, or to each corner of the screen, when called and return the window to its original position and size when called again.

If you are using ClickToFocus, you might wish to automatically transfer keyboard focus when switching pages or desks to the previously focused window in that page or desk. Otherwise, you will have to click on the window you wish to interact with on every page or desk switch. This can be accomplished by using the FvwmEvent module to bind a function which focuses the currently or previous focused window to the new_page and new_desk events.

Below is an example of such a function:

The $0 argument should be either CurrentPage or CurrentDesk. The $1 argument is for supplying the NoWarp argument to FlipFocus - by default, FlipFocus will initiate a page switch to the page containing the focused window. This behaviour is useful when switching desks but must be disabled when switching pages because windows can span more than one page.

With the function appropriately configured, ensure that FvwmEvent is started and then bind it to the required events as show below:

Window decorations (borders and titlebars) can be toggled on or off for a selected window using the function defined below.

Some applications, such as XTerm, supply a maximum size to the window manager that might be smaller than the screen size. This means that if such an application is maximized, it will not cover the whole screen. To force FVWM to ignore these hints, use the following: Style "*" ResizeHintOverride.

NumLock, CapsLock and ScrollLock can intefere with ClickToFocus as well as mouse and key bindings. To disable this behavior, use the following command: IgnoreModifiers L25.

You may find that with some programs the starting position of its window changes every time the program is launched. Similarly some windows automatically resize themselves which can also affect their position.

This is typically due to PPosition (program position) or USPosition (user specified position) hints which applications can set and which FVWM respects by default. For troublesome windows, you can configure FVWM to ignore the hints that are causing the problem. The first step is to get the class name or resource name of the window in question - use the #FvwmIdent module for this. Then try disabling either the PPosition or USPosition hints for the window. For example:

For windows whose position is affected by resizing this can typically be fixed by setting the FixedPPosition style on the window which causes FVWM to ignore attempts by the window to change its position. This can be set in conjunction with ignoring USPosition hints if necessary, see below:

**Examples:**

Example 1 (unknown):
```unknown
exec fvwm-crystal
```

Example 2 (unknown):
```unknown
AddToFunc InitFunction
+ I Exec nm-applet
```

Example 3 (unknown):
```unknown
AddToFunc StartFunction
+ I Test (Init) Exec nm-applet
```

Example 4 (unknown):
```unknown
$HOME/.fvwm/config
```

---

## Identity management

**URL:** https://wiki.archlinux.org/title/Identity_management

**Contents:**
- Software

Identity management (IDM), sometimes also identity and access management (IAM), deals with how users gain a digital identity, the roles, and sometimes the permission granted to this identity.

There is a number of software which helps doing identity management. Amongst, in alphabetical order, tools and related:

---

## Java

**URL:** https://wiki.archlinux.org/title/Java

**Contents:**
- Installation
  - OpenJDK
  - OpenJFX
  - Other implementations
  - Development tools
    - Decompilers
    - GUI Frontends
- Switching between JVM
  - List compatible Java environments installed
  - Change default Java environment

From the Wikipedia article:

Arch Linux officially supports the open source OpenJDK versions 8, 11, 17, 21, and 25  Long-Term Support (LTS) versions. All these JVMs can be installed without conflict and switched between using helper script archlinux-java (installed with java-runtime-common package). Several other Java environments are available in Arch User Repository but are not officially supported.

Two common packages are respectively pulled as dependency, named java-runtime-common (containing common files for Java Runtime Environments) and java-environment-common (containing common files for Java Development Kits).

The provided /etc/profile.d/jre.sh and /etc/profile.d/jre.csh files point to a linked location /usr/lib/jvm/default/bin, set by the archlinux-java helper script.

Most executables of the Java installation are provided by direct links in /usr/bin/, while others are available in $PATH.

OpenJDK is an open-source implementation of the Java Platform, Standard Edition (Java SE), designated as the official reference implementation. There are several distributors of OpenJDK builds such as Adoptium (formerly known as AdoptOpenJDK) and Amazon Corretto. The Arch Linux OpenJDK packages are built from the upstream OpenJDK source code.

JDK, full JRE and headless JRE conflict with each other, as the smaller packages are subsets:

IcedTea-Web  Java Web Start and the deprecated Java browser plugin.

OpenJDK EA  OpenJDK Early-Access build for development version from Oracle.

OpenJDK GA  OpenJDK General-Availability Release build from Oracle.

OpenJDK Wakefield  Support implementation in JDK for the Wayland display server.

OpenJFX is the open-source implementation of JavaFX. You do not need to install this package if you are using Oracle JDK. This package only concerns users of the open source implementation of Java (OpenJDK project), and its derivatives.

For integrated development environments, see List of applications/Utilities#Integrated development environments and the Java IDEs subsection specifically.

To discourage reverse engineering an obfuscator like proguardAUR can be used.

The helper script archlinux-java (package : java-runtime-common) provides such functionalities:

Note the (default) denoting that java-11-openjdk is currently set as default. Invocation of java and other binaries will rely on this Java install. Also note on the previous output that only the JRE part of OpenJDK 8 is installed here.

Note that archlinux-java will not let you set an invalid Java environment. In the previous example, jre8-openjdk is installed but jdk8-openjdk is not so trying to set java-8-openjdk will fail:

There should be no need to unset a Java environment as packages providing them should take care of this. Still should you want to do so, just use command unset:

If an invalid Java environment link is set, calling the archlinux-java fix command tries to fix it. Also note that if no default Java environment is set, this will look for valid ones and try to set it for you. Officially supported package "OpenJDK 8" will be considered first in this order, then other installed environments.

If you want to launch an application with another version of Java than the default one (for example if you have both versions 18the defaultand 11 installed on your system, and you want to use Java 11), you can wrap your application in a small shell script to locally change the default path of Java:

For a systemd service you can append JAVA_HOME to environment variables in the drop-in file:

This section is targeted at packagers willing to provide packages in the AUR for an alternate JVM and be able to integrate with the Arch Linux JVM scheme (i.e. to be compatible with archlinux-java); to do so, packages should:

Also please note that:

Behavior of most Java applications can be controlled by supplying predefined variables to Java runtime. From this forum post, a way to do it consists of adding the following line in your ~/.bash_profile (or /etc/profile.d/jre.sh to affect programs that are not run by sourcing ~/.bash_profile):

For example, to use system anti-aliased fonts and make swing use the GTK look and feel:

Three such variables exist, the options which are explained later in the table below take priority.

Both closed source and open source implementations of Java are known to have improperly implemented anti-aliasing of fonts. This can be fixed with the following options: -Dawt.useSystemAAFontSettings=on, -Dswing.aatext=true

See Java Runtime Environment fonts for more detailed information.

Setting the JDK_JAVA_OPTIONS environment variables makes java (openjdk) write to stderr messages of the form: 'Picked up JDK_JAVA_OPTIONS=...'. To suppress those messages in your terminal you can unset the environment variable in your ~/.bashrc and alias java to pass those same options as command line arguments:

Non interactive shells, like the launcher scripts for Java programs, (usually) do not read the ~/.bashrc, but still inherited exported variables from their parent process (which in turn inherited it at some point from the login shell which read the ~/.bash_profile). As for the cases when they do, one puts generally a statement at the top of the ~/.bashrc to avoid the file being read. That way, the variables are passed to programs launched via the desktop menu and in the case of an interactive shell where the message would disturb aliases are used instead (which in turn cannot be used in scripts).

If your Java programs look ugly, you may want to set up the default look and feel for the swing components:

Some Java programs insist on using the cross platform Metal look and feel. In some of these cases you can force these applications to use the GTK look and feel by setting the following property:

In Java releases prior to version 9, the GTK LookAndFeel is linked against GTK2, whilst many newer desktop applications use GTK3. This incompatibility between GTK versions may break applications utilizing Java plugins with GUI, as the mixing of GTK2 and GTK3 in the same process is not supported (for example, LibreOffice 5.0).

The GTK LookAndFeel can be run against GTK versions 2, 2.2 and 3, defaulting to GTK3. This can be overridden by setting the following property:

Depending on the GUI framework, HiDPI#Java applications can be enabled using different methods.

Switching to OpenGL-based hardware acceleration pipeline will improve 2D performance

Since JDBC drivers often use the port in the URL to establish a connection to the database, it is considered "remote" (i.e., by default, MySQL does not listen on the port) even though they may be running on the same host. Thus, to use JDBC and MySQL, you should enable remote access to MySQL, following the instructions in MariaDB#Grant remote access.

If IntelliJ IDEA outputs The selected directory is not a valid home for JDK with the system Java SDK path, you may have to install a different JDK package and select it as IDEA's JDK.

You may use the wmname from suckless.org to make the JVM believe you are running a different window manager. This may solve a rendering issue of Java GUIs occurring in window managers like Awesome or Dwm or Ratpoison. This works because the JVM contains a hard-coded list of known, non-re-parenting window managers. For maximum irony, some users prefer to impersonate LG3D, the non-re-parenting window manager written by Sun, in Java. Try setting compiz, Metacity or LG3D.

You must restart the application in question after issuing the wmname command.

Alternatively, the javaagent JavaMatePatch, created to set the WM name in MATE and resolve the bug with java swing apps working incorrectly when launched in full screen, can be used. Add -javaagent:JavaMatePatch-1.0.0-SNAPSHOT.jar=window_manager_name to the java options to use it.

In addition to the suggestions mentioned below in #Better font rendering, some fonts may still not be legible afterwards. If this is the case, there is a good chance Microsoft fonts are being used. Install ttf-ms-fontsAUR.

If some applications are completely missing texts it may help to use the options under #Tips and tricks as suggested in FS#40871.

The standard Java GUI toolkit has a hard-coded list of "non-reparenting" window managers. If using one that is not on that list, there can be some problems with running some Java applications. One of the most common problems is "gray blobs", when the Java application renders as a plain gray box instead of rendering the GUI. Another one might be menus responding to your click, but closing immediately.

There are several things that may help:

For more information, see Problems with Java applications, Applet java console.

If your system freezes while debugging a JavaFX Application, you can try to supply the JVM option -Dsun.awt.disablegrab=true.

See https://bugs.java.com/bugdatabase/view_bug?bug_id=6714678

Creating instance of MediaPlayer class from JavaFX's sound modules might throw following exception (both Oracle JDK and OpenJDK)

which is a result of some incompatibilities of JavaFX with modern ffmpeg build delivered within Arch Linux repository.

Working solution is to install ffmpeg-compat-55AUR. Alternatively, installing ffmpeg3.4AUR may work if the previous version fails to build.

See https://www.reddit.com/r/archlinux/comments/70o8o6/using_a_javafx_mediaplayer_in_arch/

If a Java application is not able to open a link to, for example, your web browser, install gvfs. This is required by the Desktop.Action.BROWSE method. See [1] An application printing the error message java.lang.UnsupportedOperationException: The BROWSE action is not supported on the current platform! is a solid indicator for this problem.

Possible issues / solutions:

**Examples:**

Example 1 (unknown):
```unknown
/etc/profile
```

Example 2 (unknown):
```unknown
/etc/profile.d/jre.sh
```

Example 3 (unknown):
```unknown
/etc/profile.d/jre.csh
```

Example 4 (unknown):
```unknown
/usr/lib/jvm/default/bin
```

---

## locate

**URL:** https://wiki.archlinux.org/title/Locate

**Contents:**
- Installation
- Usage
- Troubleshooting
  - Btrfs
- See also

locate is a common Unix tool for quickly finding files by name. It offers speed improvements over the find tool by searching a pre-constructed database file, rather than the filesystem directly. The downside of this approach is that changes made since the construction of the database file cannot be detected by locate. This problem can be minimised by scheduled database updates.

Over time, alternative implementations have replaced each others, from slocate (secure locate) that only showed files accessible to the user, to mlocate (merging locate) that merges databases at each update, which offers a performance speedup since it can skip previously examined files to today's plocate (posting locate) is a locate based on posting lists, consuming the database ahead-of-time and making a much faster (and smaller) index out of it.

Install the plocate package.

While the GNU findutils also include a locate implementation, Arch's findutils package does not.

Before plocate(1) can be used, the database will need to be created, this is done with the updatedb(8) command, which (as the name suggests) updates the database.

plocate contains a plocate-updatedb.timer unit, which invokes a database update each day and is enabled upon installation. Start it manually if you want to use it before reboot. You can also manually run updatedb as root at any time.

To save time, updatedb can be (and by default is) configured to ignore certain filesystems and paths by editing /etc/updatedb.conf. updatedb.conf(5) describes the semantics of this file. It is worth noting that among the paths ignored in the default configuration (PRUNEPATHS) are /media and /mnt, so locate may not discover files on external devices.

The default configuration prevents Btrfs filesystems from being included in the results. To allow including btrfs mountpoints, add:

Note of course that this also means other bind mountpoints also will be included. If you need to exclude these mountpoints, the PRUNEPATHS setting in the same configuration file can be used.

**Examples:**

Example 1 (unknown):
```unknown
plocate-updatedb.timer
```

Example 2 (unknown):
```unknown
/etc/updatedb.conf
```

Example 3 (unknown):
```unknown
/etc/updatedb.conf
```

Example 4 (unknown):
```unknown
PRUNE_BIND_MOUNTS = "no"
```

---

## OpenPGP-card-tools

**URL:** https://wiki.archlinux.org/title/OpenPGP-card-tools

**Contents:**
- Installation
- Configuration
- Interact with OpenPGP cards
- Tips and tricks
  - Machine readable output
  - Import an OpenPGP private key
  - Export SSH public key
  - Sign data
  - Decrypt encrypted data
  - Switch identities of a Nitrokey Start

Openpgp-card-tools is a software package offering the commandline tool oct(1) for interacting with OpenPGP smartcards (using ccid).

Install the openpgp-card-tools package.

The oct(1) tool relies on pcsclite and ccid. It requires to enable and start the pcscd.socket.

The oct(1) tool provides several subcommands, which provide functionality related to OpenPGP cards:

The oct(1) tool offers machine readable output format for all subcommands by using the --output-format option.

To list attached cards in JSON output format:

With oct-admin-import(1) it is possible to directly import an OpenPGP private key.

To import a private key with the fingerprint 0123456789012345678901234567890123456789 to the card with the identifier 0123:01234567:

With oct-ssh(1) it is possible to export the SSH public key (among other data) for the authentication slot of the card.

The below provides an example with dummy data:

To strip all data unnecessary for an ~/.ssh/authorized_keys file (see ssh(1)  FILES), use the --key-only option:

With oct-sign(1) it is possible to sign data using a signing key on a card.

The following commands use an example card to sign the file hello.txt:

With oct-decrypt(1) it is possible to decrypt data using the encryption slot of a card.

In the below example a message is encrypted using sq-encrypt(1), using the OpenPGP public key archie.pub.

The Nitrokey Start offers using three separate identities on a single hardware token, each with their separate signing, encryption and authentication slot. Effectively, this is equal to having three separate OpenPGP smartcards with separate card identifiers.

With oct-system-set-identity(1) it is possible to switch between these identities.

To switch to the second identity, use:

To switch back to the first identity, use:

Use oct-list(1) to list all connected cards that are available to pcscd(8). If the connected card is not showing up, it is likely that it is blocked by another process, such as scdaemon. The scdaemon(1) can be terminated using

**Examples:**

Example 1 (unknown):
```unknown
pcscd.socket
```

Example 2 (unknown):
```unknown
--output-format
```

Example 3 (unknown):
```unknown
$ oct --output-format=json list
```

Example 4 (unknown):
```unknown
0123456789012345678901234567890123456789
```

---

## Locale

**URL:** https://wiki.archlinux.org/title/Locale

**Contents:**
- Generating locales
- Setting the locale
  - Setting the system locale
  - Overriding system locale per user session
  - Make locale changes immediate
  - Other uses
- Variables
  - LANG: default locale
  - LANGUAGE: fallback locales
  - LC_TIME: date and time format

Locales are used by glibc and other locale-aware programs or libraries for rendering text, correctly displaying regional monetary values, time and date formats, alphabetic idiosyncrasies, and other locale-specific standards.

Locale names are typically of the form language[_territory][.codeset][@modifier], where language is an ISO 639 language code, territory is an ISO 3166 country code, and codeset is a character set or encoding identifier like ISO-8859-1 or UTF-8. See setlocale(3).

For a list of enabled locales, run:

Before a locale can be enabled on the system, it must be generated. This can be achieved by uncommenting applicable entries in /etc/locale.gen, and running locale-gen. Equivalently, commenting entries disables their respective locales. While making changes, consider any localisations required by other users on the system, as well as specific #Variables.

For example for German, uncomment de_DE.UTF-8 UTF-8 (in addition to en_US.UTF-8 UTF-8 which is commonly used as a fallback for various tools):

Save the file, and generate the locale:

To display the currently set locale and its related environmental settings, type:

The locale to be used, chosen among the previously generated ones, is set in locale.conf files. Each of these files must contain a new-line separated list of environment variable assignments, having the same format as output by locale.

To list available locales which have been previously generated, run:

Alternatively, using localectl(1):

To set the system locale, write the LANG variable to /etc/locale.conf, where en_US.UTF-8 belongs to the first column of an uncommented entry in /etc/locale.gen:

See #Variables and locale.conf(5) for details.

The system-wide locale can be overridden in each user session by creating or editing $XDG_CONFIG_HOME/locale.conf (usually ~/.config/locale.conf).

The precedence of these locale.conf files is defined in /etc/profile.d/locale.sh.

Once system and user locale.conf files have been created or edited, their new values will take effect for new sessions at login. To have the current environment use the new settings unset LANG and source /etc/profile.d/locale.sh:

Locale variables can also be defined with the standard methods as explained in Environment variables.

For example, in order to test or debug a particular application during development, it could be launched with something like:

Similarly, to set the locale for all processes run from the current shell (for example, during system installation):

locale.conf files support the following environment variables.

Full meaning of the above LC_* variables can be found on manpage locale(7), whereas details of their definition are described on locale(5).

The locale set for this variable will be used for all the LC_* variables that are not explicitly set.

Programs which use gettext for translations respect the LANGUAGE option in addition to the usual variables. This allows users to specify a list of locales that will be used in that order. If a translation for the preferred locale is unavailable, another from a similar locale will be used instead of the default. For example, an Australian user might want to fall back to British rather than US spelling:

If LC_TIME is set to en_US.UTF-8, for example, the date format will be "MM/DD/YYYY". If wanting to use the ISO 8601 date format of "YYYY-MM-DD" use:

You can print the current timestamp using your locale date and time format with date +"%c".

glibc 2.29 fixed a bug, en_US.UTF-8 started showing in 12-hour format, as was intended. If wanting to use 24-hour format, use LC_TIME=C.UTF-8.

This variable governs the collation rules used for sorting and regular expressions.

Setting the value to C.UTF-8 can for example make the ls command sort dotfiles first, followed by uppercase and lowercase filenames:

The locale set for this variable will always override LANG and all the other LC_* variables, whether they are set or not. If LC_ALL is set to C or C.UTF-8, it will also override LANGUAGE.

LC_ALL is the only LC_* variable which cannot be set in locale.conf files: it is meant to be used only for testing or troubleshooting purposes, for example in /etc/profile.

For encoding problems, check Character encoding#Troubleshooting.

It is possible that the environment variables are redefined in other files than locale.conf. See Environment variables#Defining variables for details.

If you are using a desktop environment, such as GNOME, its language settings may be overriding the settings in locale.conf.

KDE Plasma also allows to change the UI's language through the system settings. If the desktop environment is still using the default language after the modification, deleting the file at ~/.config/plasma-localerc (previously: ~/.config/plasma-locale-settings.sh) should resolve the issue.

If you are using a display manager in combination with accountsservice, follow the instructions in Display manager#Set language for user session.

LightDM will automatically use accountsservice to set a user's locale if it is installed. Otherwise, LightDM stores the user session configuration in ~/.dmrc. It is possible that an unwanted locale setting is retrieved from there as well.

When installing a locale that is not officially supported (e.g., locale-en_xxAUR), some problems can occur, like dead/compose keys not working in some applications or applications reporting missing locales. After installing a custom locale, manual intervention is required to resolve these problems. There are two approaches (replace en_XX.UTF-8 with the identifier of your custom locale):

Set LC_CTYPE to an officially supported locale (like en_US.UTF-8), e.g.:

Modify the Xlib database by adding the following:

In some tools, like nvme-cli, the unit type is selected based on the locale settings; hence, temperatures are shown in Fahrenheit if US locales are used. If you wish to use Metric measurements with a US locale, to get temperatures in Celsius for example, adding LC_MEASUREMENT=metric to /etc/locale.conf should work if the tool searches for LC_MEASUREMENT rather than simply the country. [5]

**Examples:**

Example 1 (unknown):
```unknown
language[_territory][.codeset][@modifier]
```

Example 2 (unknown):
```unknown
$ locale --all-locales
```

Example 3 (unknown):
```unknown
/etc/locale.gen
```

Example 4 (unknown):
```unknown
de_DE.UTF-8 UTF-8
```

---

## Optical disc drive

**URL:** https://wiki.archlinux.org/title/DVD_Burning

**Contents:**
- Burning
  - Install burning utilities
  - Making an ISO image from existing files
    - Basic options
    - graft-points
  - Mounting an ISO image
  - Converting img/ccd to an ISO image
  - Learning the name of your optical drive
  - Reading the volume label of a CD or DVD
  - Creating an ISO image from a CD, DVD, or BD

This article or section needs expansion.

The burning process of optical disc drives consists of creating or obtaining an image and writing it to an optical medium. The image may in principle be any data file. If you want to mount the resulting medium, then it is usually an ISO 9660 file system image file. Audio and multi-media CDs are often burned from a .bin file, under control of a .toc file or a .cue file which tell the desired track layout.

This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.

If you want to use programs with graphical user interface, then follow #Burning CD/DVD/BD with a GUI.

The programs listed here are command line oriented. They are the back ends which are used by most free GUI programs for CD, DVD, and BD. GUI users might get to them when it comes to troubleshooting or to scripting of burn activities.

You need at least one program for creation of file system images and one program that is able to burn data onto your desired media type.

Available programs for ISO 9660 image creation are:

The traditional choice is mkisofs, because it is the older one.

Available programs for burning to media are:

The traditional choices are cdrecord for CD and growisofs for DVD and Blu-ray Disc, because cdrecord was first to offer CD writing without description file and growisofs was first to offer writing to DVD and BD without artificial restrictions by the burn program. For writing TOC/CUE/BIN files to CD, install cdrdao.

The free GUI programs for CD, DVD, and BD burning depend on at least one of the above packages.

xorrisofs supports the mkisofs options which are shown in this document.

cdrskin supports the shown cdrecord options; xorrecord also supports those which do not deal with audio CD.

The simplest way to create an ISO image is to first copy the needed files to one directory, for example: ./for_iso.

Then generate the image file with mkisofs:

Each of those options are explained in the following sections.

It is also possible to let mkisofs to collect files and directories from various paths

-graft-points enables the recognition of pathspecs which consist of a target address in the ISO file system (e.g. /photos) and a source address (e.g. /home/user/photos). Both are separated by a "=" character.

So this example puts the directories /home/user/photos, /home/user/mail and /home/user/holidays/photos, respectively in the ISO image as /photos, /mail and /photos/holidays.

Programs mkisofs and xorrisofs accept the same options. For secure backups, consider using xorrisofs with option --for_backup, which records eventual ACLs and stores an MD5 checksum for each data file.

See the mkisofs(8) and xorrisofs(1) man pages for more info about their options.

You can mount an ISO image if you want to browse its files. To mount the ISO image, we can use:

Do not forget to unmount the image when your inspection of the image is done:

See also Mounting images as user for mounting without root privileges.

To convert an img/ccd image, you can use ccd2iso:

For the remainder of this section the name of your recording device is assumed to be /dev/sr0.

which should report Vendor_info and Identification fields of the drive.

If no drive is found, check whether any /dev/sr* exist and whether they offer read/write permission (wr-) to you or your group. If no /dev/sr* exists then try loading module sr_mod manually.

If you want to get the name/label of the media, use dd:

In order to only copy actual data from the disc and not the empty blocks filling it up, first retrieve its block/sector count and size (2048 most of the time):

Then use dd to copy the data using the obtained values:

If the original medium was bootable, then the copy will be a bootable image. You may use it as a pseudo CD for a virtual machine or burn it onto an optical medium which should then become bootable. [1]

This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.

dvdisasterAUR or dvdisaster-unofficialAUR is a tool, that adds error correction data to optical media. This data can help recover content from scratched or damaged discs.

1. Insert the CD, DVD, or Blu-ray Disc into your optical drive. 2. Make sure the disc is not mounted. You can unmount it using:

3. Run dvdisaster from the command line or find it in your application menu. 4. In the dvdisaster interface, choose Create error correction data. 5. Select the disc type (CD/DVD/BD) from the drop-down menu. 6. Click the Load Disc button to scan the contents of your optical media. 7. dvdisaster will analyze the disc and display its structure. 8. Choose a location where the error recovery (ECC) file will be saved. 9. Set the error correction level. Higher levels provide better recovery at the cost of larger ECC file sizes. 2. Click Generate to begin creating the error correction data. 10. The process may take several minutes depending on the size of your disc. 11. Once the ECC file is created, dvdisaster will prompt you to verify the file. 12. Save both the original disc image (ISO) and the ECC file for future use. 13. It is recommended to store your ISO and ECC files on multiple devices or cloud storage for maximum safety.

Tips for Best Results

This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.

If your optical disc becomes scratched or otherwise damaged, dvdisaster can help recover lost data using an error correction (ECC) file. 1. Insert the damaged CD, DVD, or Blu-ray Disc into your optical drive. 2. Unmount the disc to avoid conflicts:

3. Start dvdisaster 4. In the dvdisaster interface, choose Scan and Repair mode. 5. Load the damaged disc by selecting the drive from the dropdown menu or using the Load Disc button. 5. Click Load ECC to select the corresponding error correction file (usually `.ecc`) created when you first burned the disc. 6. Ensure the ECC file matches the exact disc structure. If you do not have a matching ECC file, this method will not work. 7. Click Scan to begin reading the disc and identifying damaged sectors. 8. dvdisaster will display a visual representation of the discs status, showing good, unreadable, and corrected sectors. 9. Once scanning is complete, click Repair to rebuild missing or corrupt data using the ECC file. 10. The recovered data will be written to a new ISO image file. 11. Choose a destination to save the rebuilt ISO image. Example: `/home/user/recovered_disc.iso` 12. Click **"Save"** to complete the process. 13. Mount the ISO to verify that files have been successfully recovered:

14. Check the contents at `/mnt` to ensure data integrity.

Tips for Best Results

Video: https://www.youtube.com/watch?v=zolvCcxcN4g

Used CD-RW media need to be erased before you can write over the previously recorded data. This is done by

There are two options for blanking: blank=fast and blank=all. Full blanking lasts as long as a full write run. It overwrites the payload data on the CD. Nevertheless this should not be considered as securely making those data unreadable. For that purpose, several full write runs with random data are advised.

Alternative commands are:

To erase the DVD-RW use the dvd+rw-format utility from dvd+rw-tools:

Alternative commands are:

Such fastly blanked DVD-RW are not suitable for multi-session and cannot take input streams of unpredicted length. For that purpose one has to use one of:

The other media types are either write-once (CD-R, DVD-R, DVD+R, BD-R) or are overwritable without the need for erasing (DVD-RAM, DVD+RW, BD-RE).

Formatted DVD-RW media can be overwritten without previous erasure. So consider to apply once in their life time

Unlike DVD-RAM, DVD+RW, and BD-RE, formatted DVD-RW cannot be used as (slow) hard disk directly, but rather need the mediation of driver pktcdvd. See pktsetup(8).

BD-RE need formatting before first use. This is done automatically by the burn programs when they detect the unformatted state. Nevertheless the size of the payload area can be influenced by expert versions of the format commands shown above for DVD-RW.

BD-R can be used unformatted or formatted. Unformatted they are written with full nominal speed and offer maximum storage capacity. Formatted they get checkread during write operations and bad blocks get replaced by blocks from the Spare Area. This reduces write speed to a half or less of nominal speed. The default sized Spare Area reduces the storage capacity by 768 MiB.

growisofs formats BD-R by default. The others do not. growisofs can be kept from formatting. cdrskin and xorriso can write with full nominal speed on formatted BD-RE or BD-R:

To burn a readily prepared ISO image file isoimage.iso onto an optical medium, run for CD:

and for CD, DVD or BD:

You can verify the integrity of the burnt medium to make sure it contains no errors. Always eject the medium and reinsert it before verifying; it will guarantee that any kernel cache will not be used to read the data.

First calculate the MD5 checksum of the original ISO image:

Next calculate the MD5 checksum of the ISO file system on the medium. Although some media types deliver exactly the same amount of data as have been submitted to the burn program, many others append trailing garbage when being read. So you should restrict reading to the size of the ISO image file.

Both runs should yield the same MD5 checksum (here: e5643e18e05f5646046bb2e4236986d8). If they do not, you will probably also get an I/O error message from the dd run. dmesg might then tell about SCSI errors and block numbers, if you are interested.

It is not necessary to store an emerging ISO file system on hard disk before writing it to optical media. Only very old CD drives in very old computers could suffer failed burns due to an empty drive buffer.

If you omit option -o from mkisofs then it writes the ISO image to standard output. This can be piped into the standard input of burn programs.

Option -waiti is not really needed here. It prevents cdrecord from writing to the medium before mkisofs starts its output. This would allow mkisofs to read the medium without disturbing an already started burn run. See next section about multi-session.

On DVD and BD, you may let growisofs operate mkisofs for you and burn its output on-the-fly:

ISO 9660 multi-session means that a medium with readable file system is still writable at its first unused block address, and that a new ISO directory tree gets written to this unused part. The new tree is accompanied by the content blocks of newly added or overwritten data files. The blocks of data files, which shall stay as in the old ISO tree, will not be written again.

Linux and many other operating systems will mount the directory tree in the last session on the medium. This youngest tree will normally show the files of the older sessions, too.

CD-R and CD-RW stay writable (aka "appendable") if cdrecord option -multi was used

Then the medium can be inquired for the parameters of the next session

By help of these parameters and of the readable medium in the drive you can produce the add-on ISO session

Finally append the session to the medium and keep it appendable again

Programs cdrskin and xorrecord do this too with DVD-R, DVD+R, BD-R and unformatted DVD-RW. Program cdrecord does multi-session with at least DVD-R and DVD-RW. They all do with CD-R and CD-RW, of course.

Most re-usable media types do not record a session history that would be recognizable for a mounting kernel. But with ISO 9660 it is possible to achieve the multi-session effect even on those media.

growisofs and xorriso can do this and hide most of the complexity.

By default, growisofs uses mkisofs as a backend for creating ISO images forwards most of its program arguments to . See above examples of mkisofs. It bans option -o and deprecates option -C. By default it uses the mkisofs. You may specify to use one of the others compatible backend program by setting environment variable MKISOFS:

The wish to begin with a new ISO file system on the optical medium is expressed by option -Z

The wish to append more files as new session to an existing ISO file system is expressed by option -M

For details see the growisofs(1) manual and the manuals of mkisofs and xorrisofs.

xorriso learns the wish to begin with a new ISO file system from the blank state of the medium. So it is appropriate to blank it if it contains data. The command -blank as_needed applies to all kinds of re-usable media and even to ISO images in data files on hard disk. It does not cause error if applied to a blank write-once medium.

On non-blank writable media xorriso appends the newly given disc files if command -dev is used rather than -outdev. Of course, no command -blank should be given here

For details see the xorriso(1) man page and especially its examples.

BD-RE and formatted BD-R media are normally written with enabled Defect Management. This feature reads the written blocks while they are still stored in the drive buffer. In case of poor read quality the blocks get written again or redirected to the Spare Area where the data get stored in replacement blocks.

This checkreading reduces write speed to at most half of the nominal speed of drive and BD medium. Sometimes it is even worse. Heavy use of the Spare Area causes long delays during read operations. So Defect Management is not always desirable.

cdrecord does not format BD-R. It has no means to prevent Defect Management on BD-RE media, though.

growisofs formats BD-R by default. The Defect Management can be prevented by option -use-the-force-luke=spare:none. It has no means to prevent Defect Management on BD-RE media, though.

cdrskin, xorriso and xorrecord do not format BD-R by default. They do with cdrskin blank=format_if_needed, resp. xorriso -format as_needed, resp. xorrecord blank=format_overwrite. These three programs can disable Defect Management with BD-RE and already formatted BD-R by cdrskin stream_recording=on, resp. xorriso -stream_recording on, resp. xorrecord stream_recording=on.

Create your audio tracks and store them as uncompressed, 16-bit, 44100-Hz, stereo WAV files.

If the files are 24 bit encoded (for example, an online music service only provides 24 bit WAV files), they need to be converted to 16 bit in order to play in RedBook-compliant CD players. To check, ensure sox is installed, cd to the directory with your WAV files, and run:

If the file have the wrong encoding, convert the files using sox:

To convert MP3 to WAV, ensure lame is installed, cd to the directory with your MP3 files, and run:

In case you get an error when trying to burn WAV files converted with LAME, try decoding with mpg123:

To convert AAC to WAV ensure faad2 is installed and run:

To fix the bitrate of an already existing WAV file (or many other formats), try using sox:

Name the audio files in a manner that will cause them to be listed in the desired track order when listed alphabetically, such as 01.wav, 02.wav, 03.wav, etc.

With cdrtools, use the following command to simulate burning the WAV files as an audio CD:

If everything worked, you can remove the -dummy flag to actually burn the CD.

Alternatively, with cdrdao, create a "Table of content" file with the following command:

This will make it so that no gaps exits between tracks. Optionally, if you would like to insert a X-second gap between certain tracks, you can edit the toc file and insert the following line between the TRACK AUDIO and FILE lines for that track:

Then, we burn the CD:

The speed can be adjusted, lower speed producing a higher quality result. This is because the Audio-CD format has less advanced error correction than the data storage format.

To test the new audio CD, use MPlayer:

To burn a BIN/CUE image run:

ISO images only store a single data track. If you want to create an image of a mixed-mode disc (data track with multiple audio tracks) then you need to make a TOC/BIN pair:

Some software only likes CUE/BIN pair, you can make a CUE sheet with toc2cue (part of cdrdao):

If you are experiencing problems, you may ask for advice at mailing list cdwrite@other.debian.org, or try to write to the one of support mail addresses if some are listed near the end of the program's man page.

Tell the command lines you tried, the medium type (e.g. CD-R, DVD+RW, ...), and the symptoms of failure (program messages, disappointed user expectation, ...). You will possibly get asked to obtain the newest release or development version of the affected program and to make test runs. But the answer might as well be, that your drive dislikes the particular medium.

There are several applications available to burn CDs in a graphical environment.

See also Wikipedia:Comparison of disc authoring software.

Playback of audio CDs requires the libcdio package. To enable KDE Applications like Dolphin to read audio CDs install audiocd-kio.

If you wish to play encrypted DVDs, you must install the libdvd* packages:

Additionally, you must install player software. Popular DVD players are MPlayer, xine and VLC. See the video players list, the specific instructions for MPlayer, and the specific instructions for VLC.

See Blu-ray#Playback.

Ripping is the process of copying audio or video content to a hard disk, typically from removable media or media streams.

See also Wikipedia:Comparison of DVD ripper software.

Often, the process of ripping a DVD can be broken down into two subtasks:

Some utilities perform both tasks, whilst others focus on one aspect or the other.

If you try to burn it may stop at the first step called Normalization.

As a workaround you can disable the normalization plugin using the Edit > Plugins menu

If you get an error like

it may be because there is no device node /dev/dvd on your system. Udev no longer creates /dev/dvd and instead uses /dev/sr0. To fix this, edit the VLC configuration file (~/.config/vlc/vlcrc):

If playing DVD videos causes the system to be very loud, it may be because the disc is spinning faster than it needs to. To temporarily change the speed of the drive, run:

Any speed that is supported by the drive can be used, or 0 for the maximum speed.

Setting CD-ROM and DVD-ROM drive speed

If optical drive is constantly checking for a new disk causing it to make unnecessary noise, consider turning SATA "Hot Plug" on for your optical drive in BIOS.

If playback does not work and you have a new computer (new DVD-Drive) the reason might be that the region code is not set. You can read and set the region code with the regionsetAUR package.

Make sure the region of your DVD reader is set correctly; otherwise, you will get loads of inexplicable CSS-related errors. Use the regionsetAUR package to do so.

If ripping still does not work with the correct region set, refer to the libdvdcss developer documentation for enabling log messages and setting other relevant options.

If you use a GUI program and experience problems which the program's log blames on some backend program, then try to reproduce the problem by the logged backend program arguments. Whether you succeed with reproducing or not, you may report the logged lines and your own findings to the places mentioned in #Burn backend problems section.

Here are some typical messages about the drive disliking the medium. This can only be solved by using a different drive or a different medium. A different program will hardly help.

Brasero with backend growisofs:

Brasero with backend libburn:

Using growisofs from dvd+rw-tools for burning 50GB BD-R DL discs might result in a fatal error and damaged media, such as:

This happened at the 25GB boundary when starting to write the second layer. Using cdrecord from cdrtools works with no problems. Tested with a 'HL-DT-ST BD-RE WH16NS40' LG burner, and Verbatim BD-R DL 6x discs (#96911). FS#47797

If after ejecting a cd, either by using the eject command, or pushing the drive button, the drive disc tray autocloses before being able to remove the disc, try the following command:

If that solves the problem, make the change permanent:

If the above does not work and as a last resort measure, you can unload the disc module from the kernel via:

the disc drive should now behave as expected but will not mount disc anymore. After putting a disc into the drive, reactivate the module via:

the disc should now mount.

See General troubleshooting#Cannot use some peripherals after kernel upgrade.

**Examples:**

Example 1 (unknown):
```unknown
$ mkisofs -V "ARCHIVE_2013_07_27" -J -r -o isoimage.iso ./for_iso
```

Example 2 (unknown):
```unknown
-joliet-long
```

Example 3 (unknown):
```unknown
$ mkisofs -V "BACKUP_2013_07_27" -J -r -o backup_2013_07_27.iso \
  -graft-points \
  /photos=/home/user/photos \
  /mail=/home/user/mail \
  /photos/holidays=/home/user/holidays/photos
```

Example 4 (unknown):
```unknown
-graft-points
```

---

## SQLite

**URL:** https://wiki.archlinux.org/title/SQLite

**Contents:**
- Installation
- Using sqlite3 command line shell
  - Create a database
  - Create table
  - Insert data
  - Search database
- Software
- Using sqlite in shell script
- See also

From the project home page:

Install the sqlite package.

Related packages are:

The SQLite library includes a simple command-line utility named sqlite3 that allows the user to manually enter and execute SQL commands against an SQLite database.

For tools supporting multiple DBMSs, see List of applications/Documents#Database tools.

**Examples:**

Example 1 (unknown):
```unknown
sqlite3_analyzer
```

Example 2 (unknown):
```unknown
/etc/php/php.ini
```

Example 3 (unknown):
```unknown
$ sqlite3 databasename
```

Example 4 (unknown):
```unknown
sqlite> create table tblone(one varchar(10), two smallint);
```

---

## XDG user directories

**URL:** https://wiki.archlinux.org/title/XDG_user_directories

**Contents:**
- Installation
- Creating default directories
- Creating custom directories
- Querying configured directories

From freedesktop.org:

Most file managers indicate XDG user directories with special icons.

Install xdg-user-dirs.

Creating a full suite of localized default user directories within the $HOME directory can be done automatically by running:

When executed, it will also automatically:

The user service xdg-user-dirs-update.service will also be installed and enabled by default, in order to keep your directories up to date by running this command at the beginning of each login session.

Both the local ~/.config/user-dirs.dirs and global /etc/xdg/user-dirs.defaults configuration files use the following environmental variable format to point to user directories: XDG_DIRNAME_DIR="$HOME/directory_name" An example configuration file may likely look like this (these are all the template directories):

As xdg-user-dirs will source the local configuration file to point to the appropriate user directories, it is therefore possible to specify custom folders. For example, if a custom folder for the XDG_DOWNLOAD_DIR variable has named $HOME/Internet in ~/.config/user-dirs.dirs any application that uses this variable will use this directory.

Alternatively, it is also possible to specify custom folders using the command line. For example, the following command will produce the same results as the above configuration file edit:

Once set, any user directory can be viewed with xdg-user-dirs. For example, the following command will show the location of the Templates directory, which of course corresponds to the XDG_TEMPLATES_DIR variable in the local configuration file:

**Examples:**

Example 1 (unknown):
```unknown
$XDG_CONFIG_HOME/user-dirs.dirs
```

Example 2 (unknown):
```unknown
XDG_CONFIG_HOME
```

Example 3 (unknown):
```unknown
$ xdg-user-dirs-update
```

Example 4 (unknown):
```unknown
LC_ALL=C.UTF-8 xdg-user-dirs-update --force
```

---

## Arch is the best

**URL:** https://wiki.archlinux.org/title/Arch_is_the_best

**Contents:**
- History
- The code

Arch is the best is a very sophisticated and exquisite, ego-boosting and mind-blowing (albeit perhaps a bit over-engineered) project which gives proof of Arch's superiority.

The visionary project was originally devised in April 2008 by long time Arch community member lucke as a simple shell script which provided irrefutable proof that "Arch is the best". It was announced to the world with a forum post, thus illuminating other people's minds, who immediately started porting it to multiple different languages, both programming and verbal, so that every being on the planet could fully appreciate and benefit from this revolutionary discovery.

The "Arch is the best" project is ported to many programming languages.

Also a much larger ASCII-transcoding version.

(Non-portable C-INTERCAL I/O had to be used, since INTERCAL-72 does not allow arbitrary characters.)

Or the fancy title version to jumpscare your friends (only works in Java Edition):

Link to scratchblocks where you can see what it would look like as actual scratch code

This language does not work very well in the wiki, so here's the code.

**Examples:**

Example 1 (unknown):
```unknown
("Arch is the best!");
```

Example 2 (unknown):
```unknown
REPORT zwhat_is_the_best.
WRITE 'Arch is the best'.
```

Example 3 (unknown):
```unknown
with Ada.Text_IO;
use Ada.Text_IO;
procedure ArchIsTheBest is
begin
   Put_Line("Arch is the best!");
end ArchIsTheBest;
```

Example 4 (unknown):
```unknown
open import IO
main = run (putStrLn "Arch is the best!")
```

---

## Reset lost root password

**URL:** https://wiki.archlinux.org/title/Password_recovery

**Contents:**
- Using sudo
- Using the debug shell
- Using bash as init
- Using a LiveCD
  - Change root
- See also

This guide will show you how to reset a forgotten root password. Several methods are listed to help you accomplish this.

If you have installed sudo and have configured permissions for either the wheel group or a user whose password you recall, you can change the root password by running sudo passwd root.

With a LiveCD a couple methods are available: change root and use the passwd command, or erase the password field entry directly editing the password file. Any Linux capable LiveCD can be used, albeit to change root it must match your installed architecture type. Here we only describe how to reset your password with chroot, since manual editing the password file is significantly more risky.

**Examples:**

Example 1 (unknown):
```unknown
sudo passwd root
```

Example 2 (unknown):
```unknown
systemd.debug_shell
```

Example 3 (unknown):
```unknown
debug-shell.service
```

Example 4 (unknown):
```unknown
Ctrl+Alt+F9
```

---

## Fonts

**URL:** https://wiki.archlinux.org/title/Fonts

**Contents:**
- Font formats
  - Bitmap formats
  - Outline formats
  - Other formats
- Installation
  - Pacman
  - Creating a package
  - Manual installation
  - Older applications
  - Pango warnings

From Wikipedia:Computer font:

Note that certain font licenses may impose some legal limitations.

Most computer fonts used today are in either bitmap or outline data formats.

These formats can also be gzipped. See #Bitmap for the available bitmap fonts.

For most purposes, the technical differences between TrueType and OpenType can be ignored.

The font editing application FontForge (fontforge) can store fonts in its native text-based formatSpline Font Database (.sfd).

The typesetting application TeX Live and its companion font software Metafont traditionally renders characters using its own methods. Some file extensions used for fonts from these two programs are *pk, *gf, mf and vf. Modern versions can also use TrueType and OpenType fonts.

The SVG format also has its own font description method.

There are various methods for installing fonts.

Fonts and font collections in the enabled repositories can be installed using pacman.

Available fonts may be found by querying packages (e.g. for font or ttf).

You should give pacman the ability to manage your fonts, which is done by creating an Arch package. These can also be shared with the community in the AUR. The packages to install fonts are particularly similar; see Font packaging guidelines.

The family name of a font file can be acquired with the use of fc-query for example: fc-query -f '%{family[0]}\n' /path/to/file. The formatting is described in FcPatternFormat(3).

The recommended way of adding fonts that are not in the repositories of your system is described in #Creating a package. This gives pacman the ability to remove or update them at a later time.

Alternatively, fonts can be installed manually:

The creation of a subdirectory structure is up to the user, and varies among Linux distributions. For clarity, it is good to keep each font in its own directory. Fontconfig will search its default paths recursively, ensuring nested files get picked up.

An example structure might be:

The font files need to have sufficient read permissions for all users, i.e. at least chmod 444 for files, and 555 for directories.

For the Xserver to load fonts directly (as opposed to the use of a font server), the directory for your newly added font must be added with a FontPath entry. This entry is located in the Files section of your Xorg configuration file (e.g. /etc/X11/xorg.conf or /etc/xorg.conf). See #Older applications for more detail.

Finally, update the Fontconfig cache (usually unnecessary as software using the Fontconfig library does this):

With older applications that do not support Fontconfig (e.g. GTK 1.x applications, and xfontsel) the index will need to be created in the font directory:

Or to include more than one folder with one command:

Or if fonts were installed in a different sub-folders under the e.g. /usr/share/fonts:

At times the X server may fail to load the fonts directory and you will need to rescan all the fonts.dir files:

To check that the font(s) is included:

This can also be set globally in /etc/X11/xorg.conf or /etc/X11/xorg.conf.d.

Here is an example of the section that must be added to /etc/X11/xorg.conf. Add or remove paths based on your particular font requirements.

When Pango is in use on your system it will read from Fontconfig to sort out where to source fonts.

If you are seeing errors similar to this and/or seeing blocks instead of characters in your application then you need to add fonts and update the font cache. This example uses the ttf-liberation fonts to illustrate the solution (after successful installation of the package) and runs as root to enable them system-wide.

You can test for a default font being set like so:

This is a selective list that includes many font packages from the AUR along with those in the official repositories.

Works with Pango 1.44 and later:

Packages providing a base font set:

Packages not providing a base font set:

Legacy Microsoft font packages:

Fonts supporting "programming ligatures" (e.g., the display of the "->" sequence as a double-width "" glyph) are identified below with a  sign. For more monospaced fonts, also see #Bitmap and #Families.

This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.

See Localization/Arabic#Fonts.

Read Localization/Bengali#Fonts for details.

Adobe Source Han fonts and Noto CJK fonts have identical glyphs and metrics, but with different branding since the project was commissioned by both Adobe and Google.

Both collections comprehensively support Simplified Chinese, Traditional Chinese, Japanese, and Korean, with a consistent design and look. Noto Sans CJK fonts lack localized menu names, that are not required, but may make fonts more user-friendly for customers whose native language is that of the target language of the font.

See Localization/Chinese#Fonts.

See Localization/Japanese#Fonts.

See Localization/Korean#Fonts.

See also #Latin script.

Almost all Unicode fonts contain the Greek character set (polytonic included). Some additional font packages, which might not contain the complete Unicode set but utilize high quality Greek (and Latin, of course) typefaces are:

See Localization/Indic#Fonts.

Arabic fonts like ttf-scheherazade-new also cover Persian letters. A list of Arabic fonts can be checked in Localization/Arabic#Fonts.

A section of the Unicode standard is designated for pictographic characters called "emoji".

Emoji fonts come in different formats: CBDT/CBLC (Google), SBIX (Apple), COLR/CPAL (Microsoft), SVG (Mozilla/Adobe).

Emojis should work out of the box once you have at least one emoji font installed of a supported format. However, some of the emoji fonts encode their glyphs as large fixed-size bitmaps and thus, for the purpose of displaying at the intended size, rely on bitmap font downscaling, which is enabled by default.

Emoji font fallback according to the standard requires extra code to handle emoji.

For the discovery and input of Emoji see List of applications/Utilities#Text input.

Kaomoji are sometimes referred to as "Japanese emoticons" and are composed of characters from various character sets, including CJK and Indic fonts. The following set of packages covers most of existing kaomoji:

Teranoptia (ttf-teranoptia-furiaeAUR)  is a typeface without letters (an illustrative font), a peculiar contraption that allows you to imagine chimeric creatures just by typing letters with your keyboard.

Additionally, texlive-basic and texlive-fontsextra contain many math fonts such as Latin Modern Math and STIX fonts. See TeX Live#Making fonts available to Fontconfig for configuration.

This article or section is a candidate for merging with Fontconfig.

There are several font aliases which represent other fonts in order that applications may use similar fonts. The most common aliases are: serif for a font of the serif type (e.g. DejaVu Serif); sans-serif for a font of the sans-serif type (e.g. DejaVu Sans); and monospace for a monospaced font (e.g. DejaVu Sans Mono). However, the fonts which these aliases represent may vary and the relationship is often not shown in font management tools, such as those found in KDE Plasma and other desktop environments.

To reverse an alias and find which font it is representing, run:

In this case, DejaVuSansMono.ttf is the font represented by the monospace alias.

This article or section is a candidate for merging with Fontconfig.

Fontconfig automatically chooses a font that matches the current requirement. That is to say, if one is looking at a window containing English and Chinese for example, it will switch to another font for the Chinese text if the default one does not support it.

Fontconfig lets every user configure the order they want via $XDG_CONFIG_HOME/fontconfig/fonts.conf. If you want a particular Chinese font to be selected after your favorite Serif font, your file would look like this:

You can add a section for sans-serif and monospace as well.

For more information, have a look at the Fontconfig manual. See also Font configuration#Set default or fallback fonts.

This article or section is a candidate for merging with Fontconfig.

Applications select and display fonts depending upon Fontconfig preferences and available font glyphs for Unicode text. To list installed fonts for a particular language, issue a command fc-list :lang="two letter language code". For instance, to list installed Arabic fonts or fonts supporting Arabic glyphs:

This article or section is a candidate for merging with Fontconfig.

To list all fonts supporting a particular Unicode codepointblack upwards equilateral arrowhead () in this case:

fc-list does not normalize requests, it is going to list fonts which exactly matches. It is not suitable for generic family names (like monospace) and other ambiguous patterns. To search for monospaced fonts supporting a particular Unicode codepoint you could try the following:

but you have to interpret the result on your own. fc-match by its nature does not guarantee that fonts in the output list are monospaced, nor that they contain the codepoint in question at all.

Matplotlib (python-matplotlib) uses its own font cache, so after updating fonts, be sure to remove ~/.matplotlib/fontList.cache, ~/.cache/matplotlib/fontList.cache, ~/.sage/matplotlib-1.2.1/fontList.cache, etc. so it will regenerate its cache and find the new fonts [5].

See Bidirectional text for troubleshooting problems related to RTL languages.

If braille characters in terminals exhibit rendering issues, try installing a braille font and uninstalling gnu-free-fonts.

Emacs calculates sizes differently than standard Linux desktop applications, and Emacs packages do not all use the same config format, so if points or raw pixel size doesn't work, try using the other value.

Change the setting Editor: Experimental Whitespace Rendering from "svg" to "font" if your monospace fonts have problems scaling certain characters correctly. This is known to help with "Terminus (TTF)" and "IBM 3270" fonts.

**Examples:**

Example 1 (unknown):
```unknown
fc-query -f '%{family[0]}\n' /path/to/file
```

Example 2 (unknown):
```unknown
~/.local/share/fonts/
```

Example 3 (unknown):
```unknown
/usr/local/share/fonts/
```

Example 4 (unknown):
```unknown
mkdir -p /usr/local/share/fonts
```

---

## Wireshark

**URL:** https://wiki.archlinux.org/title/Wireshark

**Contents:**
- Installation
- Capturing privileges
- A few capturing techniques
  - Filtering TCP packets
  - Filtering UDP packets
  - Filter packets to a specific IP address
  - Exclude packets from a specific IP address
  - Filter packets to LAN
  - Filter packets by port
  - Headless capturing with dumpcap

Wireshark is a free and open-source packet analyzer. It is used for network troubleshooting, analysis, software and communications protocol development, and education.

Install the wireshark-qt package for the Wireshark GUI or wireshark-cli for just the tshark(1) CLI.

termshark is an alternative terminal UI.

Do not run Wireshark as root; it is insecure. Wireshark has implemented privilege separation, which means that the Wireshark GUI (or the tshark CLI) can run as a normal user while the dumpcap capture utility runs as root[1].

The wireshark-cli install script sets packet capturing capabilities on the /usr/bin/dumpcap executable.

/usr/bin/dumpcap can only be executed by root and members of the wireshark group, so to use Wireshark as a normal user, you have to add your user to the wireshark user group.

There are a number of different ways to capture exactly what you are looking for in Wireshark, by applying capture filters or display filters.

If you want to see all the current TCP packets, type tcp into the Filter bar or in the CLI, enter:

If you want to see all the current UDP packets, type udp into the Filter bar or in the CLI, enter:

To only see LAN traffic and no internet traffic, run

This will filter traffic within any of the private network spaces.

See all traffic on two ports or more:

dumpcap is part of Wireshark and can be used for capturing packets without the GUI. Used in combination with tmux will allow the capture of packets in a detached session.

To see all dumpcap options, use the -h flag.

The following example will provide a ringbuffer capture. It captures twenty .pcap files of 100MB each, replacing the oldest file with the twenty-first file and so on This allows a continuous capture without exhausting disk space.

**Examples:**

Example 1 (unknown):
```unknown
/usr/bin/dumpcap
```

Example 2 (unknown):
```unknown
/usr/bin/dumpcap
```

Example 3 (unknown):
```unknown
$ tshark -f "tcp"
```

Example 4 (unknown):
```unknown
$ tshark -f "udp"
```

---

## openresolv

**URL:** https://wiki.archlinux.org/title/Resolvconf

**Contents:**
- Installation
- Usage
- Users
- Subscribers
- Tips and tricks
  - Defining multiple values for options

openresolv is a resolvconf implementation, i.e. a resolv.conf management framework.

Although openresolv is most known for allowing multiple applications to modify /etc/resolv.conf, it is currently the only standard way to implement:

Install the openresolv package.

openresolv provides resolvconf(8) and is configured in /etc/resolvconf.conf. See resolvconf.conf(5) for supported options.

Running resolvconf -u will generate /etc/resolv.conf.

This article or section needs expansion.

openresolv can be configured to pass name servers and search domains to DNS resolvers. The supported resolvers are:

See the official documentation for instructions.

The man page does not mention it, but to define multiple values, for options that support it (e.g. name_servers, resolv_conf_options etc.) in /etc/resolvconf.conf, you need to write them space separated inside quotes. E.g.:

**Examples:**

Example 1 (unknown):
```unknown
/etc/resolv.conf
```

Example 2 (unknown):
```unknown
/etc/resolvconf.conf
```

Example 3 (unknown):
```unknown
resolvconf -u
```

Example 4 (unknown):
```unknown
/etc/resolv.conf
```

---

## Firefox

**URL:** https://wiki.archlinux.org/title/Firefox

**Contents:**
- Installation
- Add-ons
  - Adding search engines
    - firefox-extension-arch-search
- Configuration
  - Settings storage
  - Multimedia playback
    - HTML5 DRM/Widevine
    - "Open With" extension
    - Hardware video acceleration

Firefox is a popular open source graphical web browser from Mozilla.

Install the firefox package.

Other alternatives include:

A number of language packs are available for Firefox, other than the standard English. Language packs are usually named as firefox-i18n-languagecode (where languagecode can be any language code, such as de, ja, fr, etc.). For a list of available language packs, see firefox-i18n for firefox, firefox-developer-edition-i18n for firefox-developer-edition and firefox-nightly- for firefox-nightlyAUR.

Firefox is well known for its large library of add-ons which can be used to add new features or modify the behavior of existing features. Firefox's "Add-ons Manager" is used to manage installed add-ons or find new ones.

For instructions on how to install add-ons and a list of add-ons, see Browser extensions.

Search engines may be added to Firefox by creating bookmarks:

Searches are performed by pre-pending the search term with the keyword of the specified search engine: d archwiki will query DuckDuckGo using the search term archwiki

Search engines may also be added to Firefox through add-on extensions; see this page for a list of available search tools and engines.

A very extensive list of search engines can be found at the Mycroft Project.

Install the firefox-extension-arch-searchAUR package to add Arch-specific searches (AUR, wiki, forum, packages, etc) to the Firefox search toolbar.

Firefox exposes a number of configuration options. To examine them, enter in the Firefox address bar:

Once set, these affect the user's current profile, and may be synchronized across all devices via Firefox Sync. Please note that only a subset of the about:config entries are synchronized by this method, and the exact subset may be found by searching for services.sync.prefs in about:config. Additional preferences and third party preferences may be synchronized by creating new boolean entries prepending the value with services.sync.prefs.sync. To synchronize the whitelist for the extension NoScript:

The boolean noscript.sync.enabled must be set to true to synchronize the remainder of NoScript's preferences via Firefox Sync.

Firefox stores the configuration for a profile via a prefs.js in the profile folder, usually ~/.mozilla/firefox/xxxxxxxx.default/.

Firefox also allows configuration for a profile via a user.js file: user.js kept also in the profile folder. A user.js configuration supersedes a prefs.js. The user.js configuration is only parsed at start-up of a profile. Hence, you can test changes via about:config and modify user.js at runtime accordingly. For a useful starting point, see e.g custom user.js which is targeted at privacy/security conscious users.

One drawback of the above approach is that it is not applied system-wide. Furthermore, this is not useful as a "pre-configuration", since the profile directory is created after first launch of the browser. You can, however, let firefox create a new profile and, after closing it again, copy the contents of an already created profile folder into it.

Sometimes, it may be desired to lock certain settings, a feature useful in widespread deployments of customized Firefox. In order to create a system-wide configuration, follow the steps outlined in Customizing Firefox Using AutoConfig:

1. Create /usr/lib/firefox/defaults/pref/autoconfig.js:

2. Create /usr/lib/firefox/firefox.cfg (this stores the actual configuration):

Please note that the first line must contain exactly //. The syntax of the file is similar to that of user.js.

Firefox uses FFmpeg for playing multimedia inside HTML5 <audio> and <video> elements. Use https://cconcolato.github.io/media-mime-support/ to test video or https://hpr.dogphilosophy.net/test/ to test audio, to determine which formats are actually supported.

Firefox uses PulseAudio for audio playback and capture. If PulseAudio is not installed, Firefox uses ALSA instead. Note that by default, Firefox blocks all media with sound from playing automatically [2].

Widevine is a digital rights management tool that Netflix, Amazon Prime Video, and others use to protect their video content. It can be enabled in Settings > General > Digital Rights Management (DRM) Content. If you visit a Widevine-enabled page when this setting is disabled, Firefox will display a prompt below the address bar asking for permission to install DRM. Approve this and then wait for the "Downloading" bar to disappear; now, you are able to watch videos from Widevine protected sites.

Firefox can only play 720p video (or lower) with Widevine, due to not using hardware DRM playback. It is also required that the private mode browsing is disabled, for the window and in the Settings.

The same procedure can be used to associate video downloaders such as youtube-dl.

Hardware video acceleration via VA-API is available under Wayland [3] and Xorg [4] [5].

To enable VA-API in Firefox:

VA-API usage can be verified by checking Firefox's VA-API logs. Run Firefox with the MOZ_LOG="FFmpegVideo:5" environment variable and check in the log output that VA-API is enabled and used (search for the "VA-API" string) when playing a video for example. Pay attention to these logs as they might indicate that only one of the two possible compositors described before (WebRender or OpenGL) works with VA-API on your particular setup.

Firefox can use system-wide installed Hunspell dictionaries as well as dictionaries installed through its own extension system.

To enable spell checking for a specific language, right click on any text field and check the Check Spelling box. To select a language for spell checking, you have to right click again and select your language from the Languages sub-menu.

If your default language choice does not stick, see #Firefox does not remember default spell check language.

Install Hunspell and its dictionaries for the languages you require.

To get more languages, right click on any text field, click Add Dictionaries... and select the dictionary you want to install from the Dictionaries and Language Packs list.

Starting with version 64, Firefox can optionally use XDG Desktop Portals to handle various desktop features, such as opening a file picker, or handling MIME types. Using Desktop Portals allows you to, for example, customize which program is invoked to display a dialog when you select files to upload on a webpage or when picking a download location using Save as.... See XDG Desktop Portal#List of backends and interfaces for a list of available backend options.

Firefox has a number of independent settings for specifying whether each feature should be handled with a Desktop Portal request or whether to use the default GTK feature.

Each setting can have the following values:

In order to use the GNOME file picker, you will need to install xdg-desktop-portal-gnome and change widget.use-xdg-desktop-portal.file-picker from 2 to 1 in about:config.

Firefox can perform Text to Speech synthesis for web pages.

TTS must be setup for the Listen icon to appear in the Reader view. Firefox uses Speech dispatcher which requires a speech synthesis engine. The currently recommended speech synthesis engine is Festival.

See the illustrated steps on Mozilla's website.

The Listen icon (a headphones icon) will only appear if you have performed all the configuration above, Speech Dispatcher is working, and you have started Firefox after you started the Festival server (you cannot start Firefox then Festival).

Furthermore, sometimes a Festival server process may linger after you have tried to kill it, but will terminate after you shut down Firefox.

For common issues, see #Web Speech API has no voices and #Narrate/Listen icon missing in Reader Mode.

The voices in the package festival-us provide better quality audio than those in festival-english but they do not work in Firefox. They do not appear in the list of available voices in Firefox and when you open Reader view you will see error messages like this in the terminal output from the Festival server:

To fix this you need to edit the following files:

For each of these files you need to add some code to the second last line of code of each file, eg for cmu_us_awb_cg.scm add code before this line:

The code you need to add to cmu_us_awb_cg.scm is below. You will need to change the voice name, gender, dialect and description as appropriate for the other two files.

For general enhancements, see Firefox/Tweaks, and for privacy related enhancements, see Firefox/Privacy.

Firefox should respect your GTK theme settings and your OS-wide dark appearance settings (as in the Appearance section of GNOME's settings or KDE system settings). If the latter does not work, make sure to have a suitable xdg-desktop-portal package installed.

Starting with Firefox 68, you can make all the Firefox interfaces and even other websites respect dark themes, irrespective of the system GTK theme and Firefox theme. To do this, set ui.systemUsesDarkTheme to 1 in about:config [13].

As of Firefox 100, further control of the dark theme of web pages that opt-in (using the CSS media query prefers-color-scheme) and Firefox's own in-content pages is possible with layout.css.prefers-color-scheme.content-override. Setting this to 3 will follow the browser theme, setting this to 2 will follow the system wide dark-mode preference (ui.systemUsesDarkTheme as above, which defaults to 0 if the user has not changed the dark-mode preference or if a system does not support a system-wide dark-mode preference), while 1 and 0 will always force light-mode and dark-mode respectively. This setting can also be accessed through the user settings of Firefox under General > Language and Appearance > Website appearance.

If Firefox is unable to automatically detect the right value, it will default to 60 fps. To manually correct this, set layout.frame_rate to the refresh rate of your monitor (e.g. 144 for 144 Hz).

To prevent pages from abusing memory (and possible OOM), we can use Firejail with the rlimit-as option.

To control where new tabs appears (relative or absolute), use browser.tabs.insertAfterCurrent and browser.tabs.insertRelatedAfterCurrent. See [14] for more information.

You can Take a Screenshot by either using the screenshots button that can be added to the toolbar from the customize screen in the Hamburger menu at More tools > Customize toolbar, by pressing Ctrl+Shift+s or by right-clicking on the webpage. See Firefox screenshots for more information, including on the telemetry data collection.

You can also use the screenshot button in the developer tools, which can be added through the developer tools Settings menu, under the Available Toolbox Buttons section. The settings for the developer tools are accessible through the three horizontal dots located at the top right of the developer tools pane.

Starting with version 121, Firefox defaults to Wayland instead of XWayland and does not require any configuration.

You can force Xwayland mode via an environment variable.

To make this permanent, see Environment variables#Graphical environment and start Firefox via the desktop launcher like you normally would.

To verify that it worked, look for Window Protocol in about:support. The presence of x11 means you are running Firefox under Xorg display server, while xwayland means your system is running Wayland but executing Firefox as legacy X11 application.

This article or section is out of date.

To be able to apply different configurations to Firefox windows, change the WM_CLASS string by using Firefox's --class option. Under Wayland, Firefox uses the --name option instead. You can then reference separate Firefox windows in your window manager by using the strings you set.

To start new Firefox instances, multiple profiles are required. To create a new profile:

This article or section is out of date.

Class can be specified when launching Firefox with a not-in-use profile:

Firefox Profilemaker can be used to create a Firefox profile with the defaults you like.

See Firefox/Tweaks#Pixel-perfect trackpad scrolling, Firefox/Tweaks#Enable touchscreen gestures and Firefox/Tweaks#Smooth scrolling.

To have multiple tabs opened when starting Firefox, open a new window and then open the sites you want to have as "home tabs".

Now go to Settings > Home and under Homepage and new windows click the Use Current Pages button.

Alternatively, go directly to Settings > Home and now under Homepage and new windows set the first field to Custom URLs.. and enter the pages you want as new home pages in the following format:

To display two pages at once with the integrated PDF viewer, set pdfjs.spreadModeOnLoad to 1 in about:config.

Firefox supports kiosk mode that shows pages in full screen without browser chrome, context menus, and other features useful for typical desktop browsing. These can be seen on ATMs or information panels where users are not expected to interact with the rest of the system.

To use kiosk mode, start Firefox with:

The startup page can be configured in the settings or supplied as a command-line parameter.

If you need printing, you can prevent Firefox from showing paper size configuration dialogs with:

Starting with Firefox version 89, the compact mode density option was removed from the Customize panel [15], but you can still use compact density. To do this, set browser.uidensity to 1 in about:config.

The UI can be scaled down even further, see Firefox/Tweaks#Configure the DPI value but use values between 0 and 1 instead.

Firefox includes a search provider for GNOME Shell which exposes Firefox bookmarks and history to GNOME Shell search while Firefox is running. However, this provider is disabled by default; to enable it go to about:config and set browser.gnome-search-provider.enabled to true.

The date and time format used in the Library window (the window showing bookmarks, history and downloads, accessible via Ctrl+Shift+o and Ctrl+Shift+h) can be customized by setting intl.date_time.pattern_override.date_short, intl.date_time.pattern_override.time_short, and intl.date_time.pattern_override.connector_short in user.js or about:config. For example, to get a format similar to RFC:3339 ("2022-12-31 22:49"), set the three preferences to yyyy-MM-dd, HH:mm, and {1} {0} , respectively.

Setting the LC_TIME environment variable to en_DK.UTF-8 only worked in old Firefox versions (perhaps 57 and earlier). Mozilla's bug report 1426907 contains further information.

Create and set the option browser.quitShortcut.disabled to true in about:config.

Firefox supports X25519Kyber768, a hybrid post-quantum key exchange for TLS 1.3. Since Firefox 132.0, it is enabled by default. To test that it is working you can visit this Cloudflare Research test page, which will tell you whether you are using a PQ-safe key exchange.

By default, Firefox enables cursor blinking. To stop the cursor from blinking, set ui.caretBlinkTime to 0 in about:config [16].

By default, Chromium reads the NSS Shared DB at ~/.pki/nssdb. Firefox puts the NSS DB in the profile directory, cert9.db and key4.db.

To make Firefox use the NSS Shared DB, add libnsssysinit.so, moduleDBOnly,, and change configdir= [17]. For example, bold below indicates the changes. You should leave the surrounding options intact if they differ from the below example.

You may wish to copy the database files from your Firefox profile directory to ~/.pki/nssdb, if you have custom certificates in them.

The command line switch -safe-mode starts Firefox in Troubleshoot Mode, which disables extensions, themes, hardware acceleration, the JIT and some other features for this session.

This mode can also be enabled by pressing on the hamburger menu while Firefox is open, clicking Help, selecting Troubleshoot Mode and confirming this on the modal dialog that appears. Please note this will require a browser restart.

This mode was previously named Safe Mode until Firefox 88.

Some issues experienced by users in Firefox may be caused by profile issues, such as corruption.

If you have ruled out other causes, it may be worth trying a new Firefox profile for testing purposes to see if this will resolve your issue. More information on how to create a new profile and switch between profiles can be found on the Firefox support page.

If this resolves your issue, you should switch back to your original profile and consider refreshing your Firefox.

Refreshing your profile will retain all browsing and download history, bookmarks, web form auto-fill data, cookies, personal dictionary and passwords, and will transfer them to a brand new profile without extensions, themes, extension data and preferences, among other data. A backup of your old profile will also be retained.

To refresh your profile, navigate to about:support, press Refresh Firefox and confirm this on the modal dialog that appears. about:support can also be accessed by pressing the Hamburger menu, selecting Help and then clicking More troubleshooting information.

More information on refreshing your Firefox, including further details about what is transferred to the new profile, can be found on the Firefox support page.

If you are having issues with hardware video acceleration in Firefox, e.g. in case of freezes or graphical corruption, start Firefox in Troubleshoot Mode for testing purposes to confirm that this is the issue. If this step resolves the issue, merely set media.ffmpeg.vaapi.enabled to false in about:config to disable hardware video acceleration, and restart Firefox.

By default, extensions will not affect pages designated by extensions.webextensions.restrictedDomains. If this is not desired, this field can be cleared (special pages such as about:* will not be affected). Then create and set privacy.resistFingerprinting.block_mozAddonManager to true.

If Firefox takes much longer to start up than other browsers, it may be due to lacking configuration of the localhost in /etc/hosts. See Network configuration#Local network hostname resolution on how to set it up.

Misbehaving Firefox extensions, or too many extensions, may be another source of slow startup. This can be confirmed through the use of Troubleshoot Mode, which will disable extensions on restart.

A further cause of slow start-up may be a profile issue, such as corruption. For more troubleshooting steps around your Firefox profile, see #Firefox refresh.

See Font configuration.

Firefox has a setting which determines how many replacements it will allow from Fontconfig. To allow it to use all your replacement rules, change gfx.font_rendering.fontconfig.max_generic_substitutions to 127 (the highest possible value).

Firefox ships with the Twemoji Mozilla font. To use the system emoji font, set font.name-list.emoji to emoji in about:config. Additionally, to prevent the Mozilla font interfering with your system emoji font, change gfx.font_rendering.opentype_svg.enabled to false or remove /usr/lib/firefox/fonts/TwemojiMozilla.ttf (see also pacman#Skip files from being installed to system).

Inside the browser, mailto links by default are opened by a web application such as Gmail or Yahoo Mail. To set an external email program, go to Settings > General > Applications and modify the action corresponding to the mailto content type; the file path will need to be designated (e.g. /usr/bin/kmail for Kmail).

Outside the browser, mailto links are handled by the x-scheme-handler/mailto mime type, which can be easily configured with xdg-mime. See Default applications for details and alternatives.

See Default applications.

Firefox uses ~/Desktop as the default place for download and upload files. To change it to another folder, set the XDG_DESKTOP_DIR option as explained in XDG user directories.

In Firefox version 98, the behavior of opening files in external programs was silently changed. Instead of downloading them into /tmp and giving that file location to the child process, Firefox now downloads the file as if you had chosen to save it, and then gives the child process the location of the file in your downloads directory. As a result, your downloads will be littered with files you only ever intended to open for viewing. This happens both when you select a program to use to open the file in a dialog and for file types you have configured to automatically open in a specific program. Notably this also happens for some file types that are opened internally in Firefox (such as PDF documents if the in-browser PDF.js viewer is enabled).

Due to an oversight, the dialog prompting you for what to do with the file still describes the old choices (either open or save) while it is in reality always going to save the file. Since this behavior could realistically pose a security and privacy risk to certain users who expect the files to not be saved to disk, you might want to disable the new behavior.

To do this, create and set browser.download.start_downloads_in_tmp_dir to true in about:config.

Alternatively, to prevent Firefox from automatically saving PDFs into the downloads directory while opening them in the in-browser viewer, set browser.download.open_pdf_attachments_inline to true in about:config.

Create browser.cache.disk.parent_directory in about:config and set it's string value to the desired location, for example to /tmp/ or /dev/shm/

Set toolkit.legacyUserProfileCustomizations.stylesheets to true in about:config

To autoscroll on middle-click (default for Windows browsers), you have two ways to enable this feature:

To disable pasting from the clipboard (PRIMARY selection) when the middle mouse button is clicked, set middlemouse.paste to false in about:config.

To load the contents of the clipboard as a URL when the middle mouse button is clicked, middlemouse.contentLoadURL to true in about:config. This was the default behaviour prior to Firefox 57.

According to MozillaZine, the Backspace key was mapped based on which platform the browser was running on. As a compromise, this preference was created to allow the Backspace key to either go back/forward, scroll up/down a page, or do nothing.

To make Backspace go back one page in the tab's history and Shift+Backspace go forward, set browser.backspace_action to 0 in about:config.

To have the Backspace key scroll up one page and Shift+Backspace scroll down one page, set browser.backspace_action to 1. Setting this property to any other value will leave the key unmapped (Arch Linux defaults to 2; in other words, it is unmapped by default).

It may be due to a corrupted cookies.sqlite file in Firefox's profile folder. In order to fix this, just rename or remove cookie.sqlite while Firefox is not running.

Open a terminal of choice and type the following:

The profile id is a random 8 character string.

Restart Firefox and see if it solved the problem.

If it did not work, check if there exists a cookies.sqlite.bak file that you could use to manually restore the cookies.

If Firefox detects an EWMH/ICCCM compliant window manager, it will try to send a WM_STATE message to the root window to request Firefox be made to enter (or leave) full-screen mode (as defined by the window manager). Window managers are allowed to ignore it, but if they do, Firefox will assume the request got denied and propagate it to the end user which results in nothing happening. This may result in not being able to full screen a video. A general workaround is to set the full-screen-api.ignore-widgets to true in about:config.

Related bug reports: Bugzilla 1189622.

The factual accuracy of this article or section is disputed.

This can be fixed using a uBlock Origin filter. To add a filter, click the uBlock Origin extension icon > Three cogwheels (Open the dashboard) > My Filters. Then, add the following to the text field:

After applying the changes and reloading the YouTube window, the filter will take effect. Note that you have to have cosmetic filtering enabled for this to work (the middle icon with the eye).

You can try setting dom.w3c_touch_events.enabled to 0 in about:config.

The default spell checking language can be set as follows:

When you only have system wide dictionaries installed with hunspell, Firefox might not remember your default dictionary language settings. This can be fixed by having at least one dictionary installed as a Firefox plugin. Notice that now you will also have a tab Dictionaries in Add-ons. You may have to change the order of preferred language for displaying pages in about:preferences#general to make the spell check default to the language of the addon dictionary.

Related questions on the StackExchange platform: [18], [19], [20]

Related bug reports: Bugzilla 776028, Ubuntu bug 1026869

Ensure that the setting spellchecker.dictionary_path exists and is set to the path of the system's Hunspell dictionaries: /usr/share/hunspell.

You need some Math fonts, namely Latin Modern Math and STIX (see this MDN page: [21]), to display MathML correctly.

In Arch Linux, these fonts are provided by texlive-fontsextra, but they are not available to fontconfig by default. See TeX Live#Making fonts available to Fontconfig for details. You can also try other Math fonts. In case you encounter this bug [22], installing otf-latinmodern-math can help.

This may be a PulseAudio issue. See the suggested fix in PulseAudio/Troubleshooting#Browsers (firefox) load videos but do no play.

Try disabling smooth scrolling in Settings > General > Browsing. Note that the pages will scroll jerkily. Furthermore, in about:config setting layout.frame_rate to different values (i.e. 0 or 1) can also improve scrolling smoothness.

WebRTC applications for instance Firefox WebRTC getUserMedia test page say that microphone cannot be found. Issue is reproducible for both ALSA or PulseAudio setup. Firefox debug logs show the following error:

You can try setting media.navigator.audio.full_duplex property to false at about:config Firefox page and restart Firefox.

This can also help if you are using the PulseAudio module-echo-cancel and Firefox does not recognise the virtual echo canceling source.

After agreeing to share a microphone or web camera, you may then see a window with a tan background and a red border in the top left corner on your primary window, displaying the following error message:

If this is the case for you, performing the following steps should resolve the issue:

See Mozilla's bug report for more information.

When using screen sharing in Google Meet, you might notice choppy visuals and overall laggy video. A possible fix is to change your browsers user agent to Google Chrome. You can do this with the User agent switcher extension.

Firefox provides a local service for Chinese users, with a local account totally different from the international one. Firefox installed with the firefox package uses the international account system by default, to change into the Chinese local service, you should install the add-on manager on this page, then you can login with your Chinese account now.

If you are using JACK in combination with PulseAudio and cannot hear any sound on some videos, it could be because those videos have mono audio. This happens if your JACK setup uses more than just stereo, but you use normal headphones. To fix this, you simply have to connect the front-center port from the PulseAudio JACK Sink to both playback_1 and playback_2 ports of the system output.

You can also do this automatically using a script:

Keep in mind that the names for the sink and the ports might be different for you. You can check what your JACK setup looks like with a Patchbay like Catia from cadenceAUR.

Recently, Google limited the use of its location service with Arch Linux, which causes the following error when geolocation is enabled on a website: Geolocation error: Unknown error acquiring position. Region-locked services such as Hulu may display a similar error indicating that your location could not be determined even though you have allowed location services for the site.

See FS#65241 for more details.

This article or section is out of date.

To avoid these problems, you can switch to use the Mozilla Location Service. In about:config change the geo.provider.network.url setting to:

This problem has been observed in i3, bspwm and xmonad.

To fix it, navigate to about:config and change ui.context_menus.after_mouseup to true.

Unset the environment variable MOZ_X11_EGL.

Related bug report: Bugzilla 1711039.

There are a couple things you can try: if you are using a desktop environment, check if Firefox is set as the default browser in your system settings. If it is not, then set it, otherwise you can run the following xdg-settings(1) command, provided by the xdg-utils package, to query which browser is set as default on your system:

If no value is returned or it is not Firefox, then run this command to set it:

If Firefox still asks to be set as the default browser, then it may be quieted if it is set to handle http and https URL schemes. To do so, run these xdg-mime(1) commands:

If those do not work either, check if you have set the environment variable GTK_USE_PORTAL (all values trigger the bug), in which case, unset it. Related bug report: Bugzilla 1516290. If that does not work or you did not set it, navigate Firefox to about:config, check if the variable widget.use-xdg-desktop-portal is set to true and, if so, set it to false.

If you wish to disable default browser check entirely, navigate Firefox to about:config and set browser.shell.checkDefaultBrowser to false.

If you experience video stuttering and you notice that Firefox is only hitting one core at 100% when watching videos (especially higher resolution videos), this might help you.

Go into about:config and search for dom.ipc.processCount and change dom.ipc.processCount.file from 1 to a higher number. An ad hoc method to find a good number is to increase it one at a time until you get good results, but 4 seems to be a good value.

In most cases, installing the noto-fonts and making Noto Sans Bengali as defaults in Fonts and Colors settings solves it. However, in some social media sites, Bengali fonts may still be broken. In those cases, Mozilla provides a detailed guide on how to see all the fonts gets loaded in a page. By using Page Inspector, find out all the fonts that are being loaded on that particular page. Removing fonts other than Noto Sans from the system will resolve the issue permanently.

There will be some fonts that have been installed as dependency of other package. For example, chromium installs ttf-liberation as dependency, which loads itself in some Firefox pages automatically and breaks Bengali fonts on those pages. To solve this issue, use the following rule in your font configuration:

Firefox uses for text to speech (tts) speechd. You can use the command spd-say "some test sentence" to test if it reads the text or spd-say -L to get a list of the voices. If there are no voices, too, you can install some with the package espeak-ng. If they do not work out of the box, you maybe have to configure them. You can use the spd-conf command or edit the config file .config/speech-dispatcher/speechd.conf. There should be the following lines active (without # in front of it):

Per https://developer.mozilla.org/en-US/docs/Web/API/Web_Speech_API/Using_the_Web_Speech_API, speech synthesis must be enabled (it is enabled by default). To enable, set media.webspeech.synth.enabled to true in about:config.

Per https://support.mozilla.org/en-US/kb/firefox-protection-against-fingerprinting, Fingerprinting Protection disables the WebSpeech API. If you enabled this option, you will need to disable it for the narrator to work. To disable fingerprinting protection, set privacy.resistFingerprinting to false in about:config.

If you do not see the narrator icon, try setting narrate.filter-voices to false in about:config.

This can be used to check whether speech-dispatcher works at all. If it helps, you may miss voices for the language of the article opened in reader mode (check spd-say -L). If you have voices for the reader article language installed, there may be some incorrect settings or defaults related to speech-dispatcher configuration.

If no file chooser is shown when downloading files, even with the option "Always ask where to save files" enabled in Firefox's settings, then you might not have both xdg-desktop-portal and a suitable implementation. Desktop environments usually provide an implementation, but if you are using a standalone window manager such as i3, then you may need to manually install one. Install xdg-desktop-portal and for example xdg-desktop-portal-gtk.

If you're using a tiling window manager or Wayland compositor, and the HTML notifications appear as normal Firefox windows instead of floating pop-ups, you need to install libnotify and make sure you have a working Desktop notifications server, such as mako.

After renewing the certificate in the card (Spanish DNIe) Firefox continues to use the previous certificate, allows to login but won't authenticate the users on any service. You need to clear the card cache

**Examples:**

Example 1 (unknown):
```unknown
firefox-i18n-languagecode
```

Example 2 (unknown):
```unknown
languagecode
```

Example 3 (unknown):
```unknown
intl.locale.requested
```

Example 4 (unknown):
```unknown
about:config
```

---

## Bubblejail

**URL:** https://wiki.archlinux.org/title/Bubblejail

**Contents:**
- Installation
- Usage
  - Creating Instances
    - Using graphical interface
    - Using command line
  - Running instance
    - Using desktop entry
    - Using command line
- Configuration
  - Using graphical interface

Bubblejail is a Bubblewrap based sandboxing utility.

Bubblejail has a resource oriented permission model. For example, x11 is a resource that can be added to sandbox. This will give sandbox access to X11 display server.

Bubblejail also uses seccomp and D-Bus filtering to enhance security and permission model.

Bubblejail also has a graphical interface that allows creating and configuring sandboxes.

Install bubblejailAUR or bubblejail-gitAUR.

Bubblejail sandboxes are separated in to instances. Each instance is a separate home directory and a permissions configuration.

Each instance usually sandboxes one application.

New instances are usually created from an available profiles. The profile is initial set of permissions and the desktop entry used. If a specific application is missing the profile a generic profile or empty profile can be used and tweaked after creation.

Run the Bubblejail Configuration application. On the first screen at the bottom there will be Create Instance button.

For example, creating a Firefox instance:

When creating an instance a desktop entry will be generated unless --no-desktop-entry option was used.

Desktop entry would have the instance_name bubble name and can be launched from the desktop environment.

Once created the instance can be run with run subcommand:

If no arguments are passed when the default arguments will be used based on the profile used.

If args are passed when those arguments will be executed inside the sandbox. First argument should be the binary name and following arguments will be passed to this binary.

Once the instance has been created its permissions and resources can be modified. For modified permissions to take effect the sandbox needs to be restarted.

The profile used when creating sandbox only affect the initial set of permissions. However, removing certain permissions might prevent targeted application from working correctly.

Run the Bubblejail Configuration application. On the first screen click on the name of the sandbox you want to modify. This will bring up the list of all available permissions and a Save button.

Bubblejail provides edit command that will open the configuration file in the editor defined EDITOR Environment variables.

Bubblejail uses TOML for its configuration. The defined dictionaries gives permission for a specific resource and key value pairs inside dictionaries will set a specific resource settings.

This config defines two resources available to sandbox: the x11 windowing system and shares the Downloads directory in the home folder.

The available services and options are documented in bubblejail.services man page.

Using run command a terminal can be launched inside already running sandbox. The terminal can be used to debug the sandbox.

It is recommended to use the terminal application that requires minimal amount of permissions such as Alacritty which only requires access to windowing system. (either x11 or wayland)

If an application is missing a profile but there is a related software with existing profile that profile can be used.

For example, chromium profile can be used on any chromium derived browser such as qutebrowser.

First, generate the instance using chromium profile but disable the desktop entry creation.

Now a desktop entry can be created from the qutebrowser's desktop entry.

Now the sandboxed qutebrowser can be launched with the qutebrowser bubble desktop entry.

It can happen sometimes, that the process started in the sandbox instance has ended, but Bubblejail does not recognize this. This makes it impossible to run the sandbox instance again. Bubblejail quits with an "Instance already running" error.

To fix this you can delete the contents of the run directory of the failed sandbox instance. The directories for all jails are located at /run/user/UID/bubblejail/instance_name.

**Examples:**

Example 1 (unknown):
```unknown
$ bubblejail create --profile firefox instance_name
```

Example 2 (unknown):
```unknown
--no-desktop-entry
```

Example 3 (unknown):
```unknown
$ bubblejail run instance_name args
```

Example 4 (unknown):
```unknown
$ bubblejail edit instance_name
```

---

## Squid

**URL:** https://wiki.archlinux.org/title/Squid

**Contents:**
- Installation
- Configuration
- Accessing services on local hostnames
- Starting
- Content Filtering
- Frontend
  - Squid 4.x not supported in Webmin
- Ad blocking with adzapper
  - Installation
  - Configuration

This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.

Squid is a caching proxy for HTTP, HTTPS and FTP, providing extensive access controls.

Install the squid package.

By default, the cache directories will be created in /var/cache/squid, and the appropriate permissions set up for those directories. However, for greater control, we need to delve into /etc/squid/squid.conf.

The following options might be of some use to you. If you do not have the option present in your configuration file, add it!

So, in the case of a school's internet proxy:

If you change the cache directory from defaults, you must set the correct permissions on the cache directory before starting Squid, else it will not be able to create its cache directories and will fail to start.

If you plan to access web servers on the LAN using hostnames that are not fully-defined (e.g. http://mywebapp), you may need to enable the dns_defnames option. Without this option, Squid will make a DNS request for the hostname verbatim (mywebapp), which may fail, depending on your LAN's DNS setup. With the option enabled, Squid will append any domain configured in /etc/resolv.conf when making the request (e.g. mywebapp.company.local).

Once you have finished your configuration, you should check that your configuration file is correct:

Then create your cache directories:

Then you can start/enable squid.service.

If you are looking for a content filtering solution, see Privoxy.

If you would like a web-based frontend for managing Squid, Webmin is your best bet.

If you receive an error indicating your version of webmin is unsupported:

you will need to modify the file /opt/webmin/squid/index.cgi (see issue #952)

Adzapper is a plugin for Squid. It catches ads of all sorts (even Flash animations) and replaces them with an image of your choice, so the layout of the page is not altered very much.

AdZapper is not presently in the official repositories or the AUR. The script itself, and detailed information on configuration and usage, can be found at https://adzapper.sourceforge.net.

(squid 2.6.STABLE13-1)

If you want, you can edit /etc/adzapper/adzapper.conf to configure adzapper to your liking. The configuration out of the box works wonderfully well though.

Transparency happens by redirecting all www requests eth0 picks up, to Squid. You will need to add a port with an intercept (for squid 3.2) parameter. Note that at least one port must be available without the intercept parameter:

From a terminal with root privileges, run:

Then start the iptables.service systemd unit.

Replace SQUIDIP with the public IP(s) which squid may use for its listening port and outbound connections.

Edit /etc/shorewall/rules and add

Restart the shorewall systemd unit.

Squid can be configured to require a user and password in order to use it. We will use digest http auth

First create a users file with htdigest -c /etc/squid/users MyRealm username. Enter a password when prompted.

Then add these lines to your squid.conf:

And restart squid. Now you will be prompted to enter a username and password when accessing the proxy.

You can add more users with htdigest /etc/squid/users MyRealm newuser. You probably would like to install Apache package, which contains htdigest tool.

Set up samba and winbindd and test it with

Grant r-x access to /var/cache/samba/winbindd_privileged/ directory for squid user/group

Then add something like this to squid.conf:

Reference: Squid Proxy Hide Systems Real IP Address

Reference: Intercept HTTPS CONNECT messages with SSL-Bump

openssl x509 -in myCA.pem -outform DER -out myCA.der

The result file (myCA.der) should be imported into the 'Authorities' section of users' browsers. For example, in FireFox:

/usr/lib/squid/security_file_certgen -c -s /var/cache/squid/ssl_db -M 4MB

Restart squid.service.

If you are using both squid and NetworkManager, the following error means that squid is launched before the Wi-Fi connection is enabled by NetworkManager (/etc/resolv.conf is empty).

Disable the squid.service systemd unit with the following script:

Make sure it is executable

**Examples:**

Example 1 (unknown):
```unknown
/var/cache/squid
```

Example 2 (unknown):
```unknown
/etc/squid/squid.conf
```

Example 3 (unknown):
```unknown
http_port 3128
http_port 3129
```

Example 4 (unknown):
```unknown
http_access
```

---

## Vulkan

**URL:** https://wiki.archlinux.org/title/Vulkan

**Contents:**
- Installation
- Verification
- Switching
  - Switching between devices
- Software rendering
- Vulkan hardware database
- Troubleshooting
  - NVIDIA - vulkan is not working and can not initialize
    - Environment variables
    - GPU switching

Learn more on Vulkan's website.

To run a Vulkan application, you will need to install the vulkan-icd-loader package (and lib32-vulkan-icd-loader from the multilib repository if you also want to run 32-bit applications), as well as Vulkan drivers for your graphics card(s). There are several packages providing a vulkan-driver and lib32-vulkan-driver:

The following are software rasterizers, so that you can use it on devices that do not provide Vulkan support.

For Vulkan application development, install vulkan-headers, and optionally vulkan-validation-layers, vulkan-man-pagesAUR and vulkan-tools (you can find the vulkaninfo, and vkcube tools in there).

To see which Vulkan implementations are currently installed on your system, use the following command:

To ensure that Vulkan is working with your hardware, install vulkan-tools and use the vulkaninfo command to pull up relevant information about your system. If you get info about your graphics card, you will know that Vulkan is working.

On systems with multiple GPUs you may need to force the usage of a specific GPU. vulkan-mesa-device-select is required for this to work. By setting MESA_VK_DEVICE_SELECT to vendorID:deviceID, you can choose the desired GPU.

To list the candidates, use:

Appending an ! at the end of the specified value enforces this behavior. See Vulkan mesa device select layer environment variables for more information.

You can install the software Vulkan rasterizer known as lavapipe, for example to debug hardware issues: vulkan-swrast (or lib32-vulkan-swrast for the 32-bit version).

The following example shows running vulkaninfo with the required environment variables to force a full software rendering for Vulkan and OpenGL (with __GLX_VENDOR_LIBRARY_NAME=mesa ensuring the command also works for PRIME users):

The Vulkan Hardware Database provides user reported GPU/driver combinations. Supplying own information is possible by using vulkan-caps-viewer-waylandAUR or vulkan-caps-viewer-x11AUR.

Invalid or contradictory environment variable values might cause Vulkan to fail, and inappropriate values can result in the use of a different GPU than intended on machines with multiple GPUs. Properly setting the variables can also help keep a secondary GPU powered down when it is not needed.

If your machine has multiple GPUs and Vulkan cannot see or use one of them, make sure it is not currently disabled by the BIOS/UEFI or in the kernel. See NVIDIA Optimus for an overview of the different methods of switching between GPUs.

This article or section needs expansion.

Example command to check the current status with optimus-manager-gitAUR:

See NVIDIA/Troubleshooting#GSP firmware.

Try to list both the intel_icd and primus_vk_wrapper configurations in VK_DRIVER_FILES

If after running vulkaninfo on AMD card from GCN1 or GCN2 family you got error message like:

Then check if you have correctly enable support for this models of graphics cards (AMDGPU#Enable Southern Islands (SI) and Sea Islands (CIK) support).

One of possibility to check if gpu drivers are correctly loaded is lspci -k, after running this command check kernel driver of your gpu. It should be amdgpu.

Some forum threads about this problem: [3] [4]

If you install cuda, you might find Vulkan applications, for example, Chromium, launch slowly. It's because nvidia-utils provides an Vulkan driver and Vulkan would try nvidia drivers before radeon drivers. To solve it, set the environment variable VK_DRIVER_FILES to /usr/share/vulkan/icd.d/radeon_icd.i686.json:/usr/share/vulkan/icd.d/radeon_icd.x86_64.json.

**Examples:**

Example 1 (unknown):
```unknown
$ ls /usr/share/vulkan/icd.d/
```

Example 2 (unknown):
```unknown
$ vulkaninfo
```

Example 3 (unknown):
```unknown
MESA_VK_DEVICE_SELECT
```

Example 4 (unknown):
```unknown
vendorID:deviceID
```

---

## Help talk:Reading

**URL:** https://wiki.archlinux.org/title/Help_talk:Reading

**Contents:**
  - Start a discussion about Help:Reading

Talk pages are where people discuss how to make content on ArchWiki the best that it can be. You can use this page to start a discussion with others about how to improve Help:Reading.

---

## Capabilities

**URL:** https://wiki.archlinux.org/title/Capabilities

**Contents:**
- Implementation
- Administration and maintenance
- Other programs that benefit from capabilities
- Useful commands
- Running a program with temporary capabilities
- systemd
- See also

Capabilities (POSIX 1003.1e, capabilities(7)) provide fine-grained control over superuser permissions, allowing use of the root user to be avoided. Software developers are encouraged to replace uses of the powerful setuid attribute in a system binary with a more minimal set of capabilities. Many packages make use of capabilities, such as CAP_NET_RAW being used for fping. This enables fping to be run by a normal user (as with the setuid method), while at the same time limiting the security consequences of a potential vulnerability in fping.

Capabilities are implemented on Linux using extended attributes (xattr(7)) in the security namespace. Extended attributes are supported by all major Linux file systems, including Ext2, Ext3, Ext4, Btrfs, JFS, XFS, and Reiserfs. The following example prints the capabilities of fping with getcap, and then prints the same data in its encoded form using getfattr:

Some programs copy extended attributes automatically, while others require a special flag. Examples for both classes are at extended attributes#Preserving extended attributes.

Capabilities are set by package install scripts on Arch, e.g. fping.install.

It is considered a bug if a package has overly permissive capabilities, so these cases should be reported rather than listed here. A capability essentially equivalent to root access (CAP_SYS_ADMIN) or trivially allowing root access (CAP_DAC_OVERRIDE) does not count as a bug since Arch does not support any MAC/RBAC systems.

The following packages do not have files with the setuid attribute but require root privileges to work. By enabling some capabilities, regular users can use the program without privilege elevation.

The +ep behind the capabilities indicate the capability sets effective and permitted, more information can be found at capabilities(7)  File capabilities.

Some packaged binaries, like mtr(8), are already configured with needed capabilities via .install files. There is no need to add capabilities manually as described above.

Find setuid-root files:

Find setgid-root files:

Using capsh(1) it is possible to run a program with some specific capabilities without modifying the extended attributes of the binary. The following example shows how to attach to a process using GDB using the CAP_SYS_PTRACE capability:

The -E is supplied to sudo above to pass the current user's login environment, e.g., the PATH variable and so on, to the child process(es).

An example of binding to a low port using netcat, in this case 123:

Both of the above examples are really just for illustrative purposes, as (on most systems) you would be able to attach debugger to a process owned by any user, or open a port < 1024 as the root user, regardless. The use of capsh may provide some security benefits, though, as capsh --user runs as the named user, with all the normal kernel capabilities (i.e., restrictions) in place.

Using AmbientCapabilities and CapabilityBoundingSet, it is possible to assign capabilities to systemd units, which is much more safe than setting capabilities on binaries. See systemd.exec(5).

**Examples:**

Example 1 (unknown):
```unknown
CAP_NET_RAW
```

Example 2 (unknown):
```unknown
$ getcap /usr/bin/fping
```

Example 3 (unknown):
```unknown
/usr/bin/fping cap_net_raw=ep
```

Example 4 (unknown):
```unknown
$ getfattr --dump --match="^security\\." /usr/bin/fping
```

---

## Linux firmware

**URL:** https://wiki.archlinux.org/title/Linux_firmware

**Contents:**
- Installation
- Tips and tricks
  - Detecting loaded firmware
- See also

Linux firmware is a collection of firmware binary blobs distributed alongside the kernel, necessary for partial or full functionality of certain hardware devices. These binary blobs were never permitted to include in a GPL'd work, but have been permitted to redistribute under separate cover.

Typical kinds of hardware requiring firmware:

Install the linux-firmware meta package to pull all commonly used firmware. This is the recommended way for most users. To save some space you could opt into installing firmware only for individual hardware vendors your system uses.

Primary packages pulled by linux-firmware:

Third-party packages:

Sometimes you want to know what firmware is loaded by your system, for debugging or to pick firmware packages to install. That could be achieved using dynamic debug:

**Examples:**

Example 1 (unknown):
```unknown
dyndbg="func fw_log_firmware_info +p"
```

Example 2 (unknown):
```unknown
# journalctl -kg 'loaded f'
```

Example 3 (unknown):
```unknown
Jan 01 00:00:00 example kernel: amdgpu 0000:03:00.0: Loaded FW: amdgpu/psp_13_0_0_sos.bin, sha256: SHA_sum
Jan 01 00:00:00 example kernel: amdgpu 0000:6d:00.0: Loaded FW: amdgpu/vcn_3_1_2.bin, sha256: SHA_sum

Jan 01 00:00:00 example kernel: bluetooth hci0: Loaded FW: mediatek/BT_RAM_CODE_MT7922_1_1_hdr.bin, sha256: SHA_sum
Jan 01 00:00:00 example kernel: mt7921e 0000:0a:00.0: Loaded FW: mediatek/WIFI_RAM_CODE_MT7922_1.bin, sha256: SHA_sum

```

---

## xmonad

**URL:** https://wiki.archlinux.org/title/Xmonad

**Contents:**
- Installation
- Starting
- Configuration
  - A base desktop configuration
- Exiting xmonad
- Tips and tricks
  - X-Selection-Paste
  - Keyboard shortcuts
  - Targeting unbound keys
  - Run X () actions by touching the edge of your screen with your mouse

This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.

xmonad is a tiling window manager for X. Windows are arranged automatically to tile the screen without gaps or overlap, maximizing screen use. Window manager features are accessible from the keyboard: a mouse is optional.

xmonad is written, configured and extensible in Haskell. Custom layout algorithms, key bindings and other extensions may be written by the user in configuration files.

Layouts are applied dynamically, and different layouts may be used on each workspace. Xinerama is fully supported, allowing windows to be tiled on several physical screens.

Install the xmonad package which provides a very basic configuration, ideally also install xmonad-contrib for most notably a more useful desktop configuration as well as additional tiling algorithms, configurations, scripts, etc.

Alternatively, install xmonad-gitAUR, the development version, with some additional dependencies; and likewise xmonad-contrib-gitAUR.

Run xmonad with xinit.

Alternatively, select Xmonad from the session menu in a display manager of choice.

Make sure you have the Xterm package installed or have changed the terminal emulator in the configuration. Otherwise you will not be able to do anything in xmonad.

Create the ~/.xmonad directory and the ~/.xmonad/xmonad.hs file and edit it as described below.

After changes to ~/.xmonad/xmonad.hs are made, use the Mod+q shortcut to recompile and have them take effect.

Because the xmonad configuration file is written in Haskell, non-programmers may have a difficult time adjusting settings. For detailed HOWTO's and example configurations, we refer you to the following resources:

The best approach is to only place your changes and customizations in ~/.xmonad/xmonad.hs and write it such that any unset parameters are picked up from the built-in def function.

This is achieved by writing an xmonad.hs like this:

This simply overrides the default terminal and borderWidth while leaving all other settings at their defaults (inherited from the XConfig value def).

As things get more complicated, it can be handy to call configuration options by function name inside the main function, and define these separately in their own sections of your ~/.xmonad/xmonad.hs. This makes large customizations like your layout and manage hooks easier to visualize and maintain.

The simple xmonad.hs from above could have been written like this:

Also, order at top level (main, myTerminal, myModMask etc.), or within the {} does not matter in Haskell, as long as imports come first.

The following is taken from the 0.9 configuration file template. It is an example of the most common functions one might want to define in their main do block.

The package itself also includes a xmonad.hs, which is the latest official example xmonad.hs that comes with the xmonad Haskell module as an example of how to override everything. This should not be used as a template configuration, but as examples of parts you can pick to use in your own configuration. It is located in an architecture and version dependant directory in /usr/share/ (e.g. find /usr/share -name xmonad.hs).

In xmonad-contrib is a better default configuration for average desktop uses. It also helps with problems in some modern programs like Chromium.

It can be added like so:

To end the current xmonad session, press Mod+Shift+Q. By default, Mod is the Alt key. To confirm exit each time,

The keyboard-centered operation in xmonad can be further supported with a keyboard shortcut for X-Selection-Paste.

Also, there exists a function pasteSelection in XMonad.Util.Paste that can be bound to a key using a line like:

Pressing the Insert key will now paste the mouse buffer in the active window.

Default keyboard shortcuts are listed xmonad(1).

If you use xmonad as a stand alone window manager, you can edit the xmonad.hs to add unbound keyboard keys. You just need to find the Xf86 name of the key (such as XF86PowerDown) and look it up in /usr/include/X11/XF86keysym.h. It will give you a keycode (like 0x1008FF2A) which you can use to add a line like the following into list of keybindings in your xmonad.hs:

You can also search for the Xf86 key name in Graphics.X11.ExtraTypes.XF86 module, and use its KeySym constant (such as xF86XK_PowerDown) instead of a keycode as shown in the previous example. You will also need to import the module in your xmonad.hs for the key constant to be available. See more elaborate example with multiple keys in format used by additionalKeys function:

With XMonad.Hooks.ScreenCorners, users can have KDE-like screen corners with XMonad.

Define a series of operations in startupHook:

Then add screenCornerEventHook to handleEventHook:

Finally add screenCornerLayoutHook:

By default, xmonad uses a set of 9 workspaces. You can change this by changing the workspaces parameter:

XMonad.Util.EZConfig provides a function checkKeymap to check for duplicate key bindings, otherwise the duplicates will be silently ignored.

Wrap your layouts with avoidStruts from XMonad.Hooks.ManageDocks for automatic dock/panel/trayer spacing:

If you ever want to toggle the gaps, this action can be added to your key bindings:

with XMonad.Actions.TagWindows, users can operate on windows having the same tags.

If your goal is to have equally sized gaps between individual windows and the screen, the following code will not work as expected:

This makes each window have its own spacing in each direction. If you have two windows side-by-side, the spacing in the middle will be combined, creating a gap that is twice as large as needed.

A workaround is to specify both a screen and a window spacing, but only use the top and left margins for the screen and bottom and right margins for the windows. To do this, change the above code into:

This article or section is a candidate for merging with xmobar.

xmobar is a light and minimalistic text-based bar, designed to work with xmonad. To use xmobar with xmonad, you will need two packages in addition to the xmonad package. These packages are xmonad-contrib and xmobar, or you can use xmobar-gitAUR.

Here we will start xmobar from within xmonad, which reloads xmobar whenever you reload xmonad.

Open ~/.xmonad/xmonad.hs in your favorite editor, and choose one of the two following options:

The xmobar action starts xmobar and returns a modified configuration that includes all of the options described in #More configurable.

As of xmonad(-contrib) 0.9, there is a new statusBar function in XMonad.Hooks.DynamicLog. It allows you to use your own configuration for:

The following is an example of how to use it:

The template and default xmobarrc contains this.

At last, open up ~/.xmobarrc and make sure you have StdinReader in the template and run the plugin. E.g.

Now, all you should have to do is either to start, or restart, xmonad.

Here are a few ways to do it,

If you are using xmonad-gitAUR, as of January of 2011, you can restart to another window manager from within xmonad. You just need to write a small script, and add stuff to your ~/.xmonad/xmonad.hs. Here is the script.

And here are the modifications you need to add to your ~/.xmonad/xmonad.hs:

You also need to add the following key binding:

Just remember to add a comma before or after and change the path to your actual script path. Now just Mod+q (restart xmonad to refresh the configuration), and then hit Mod+Shift+o and you should have Openbox running with the same windows open as in xmonad. To return to xmonad you should just exit Openbox. Here is a link to adamvo's ~/.xmonad/xmonad.hs which uses this setup Adamvo's xmonad.hs

The xmonad wiki has instructions on how to run xmonad inside KDE

It also might be a good idea to set a global keyboard shortcut in KDE to start xmonad in case it is accidentally killed or closed.

You might want to disable plasmashell (the KDE5 thing responsible for desktop background, taskbar, tray, etc.).

Then edit ~/.config/autostart/plasmashell.desktop and replace Exec=plasmashell with Exec=. The result will look like this:

Below are some example configurations from fellow xmonad users. Feel free to add links to your own.

Use xfceConfig instead of def after importing XMonad.Config.Xfce in ~/.xmonad/xmonad.hs, e.g. adapting the minimal configuration above:

Also add an entry to Settings > Session and Startup > Application Autostart that runs xmonad --replace.

xmonad should automatically create the xmonad-X86_64-linux file (in ~/.xmonad/). If this it not the case, grab a configuration file from the xmonad wiki or create your own. Put the .hs and all others files in ~/.xmonad/ and run this command from the directory:

Now you should see the file.

If you have problems, like Java application Windows not resizing, or menus immediately closing after you click, see Java#Gray window, applications not resizing with WM, menus immediately closing.

See Vim#Empty space at the bottom of gVim windows for a solution which makes the area match the background color.

You can also configure xmonad to respect size hints, but this will leave a gap instead. See the documentation on Xmonad.Layout.LayoutHints.

If Chrome fails to go fullscreen when F11 is pressed, you can use the XMonad.Hooks.EwmhDesktops extension found in the xmonad-contrib package. Simply add the import statement to your ~/.xmonad/xmonad.hs:

and then add ewmhFullscreen . ewmh to the appropriate place; for example:

After a recompile/restart of xmonad, Chromium should now respond to F11 (fullscreen) as expected.

Touchgg polls the window manager for the _NET_CLIENT_LIST (in order to fetch a list of windows it should listen for mouse events on.) By default, xmonad does not supply this property. To enable this, use the XMonad.Hooks.EwmhDesktops extension found in the xmonad-contrib package.

Users with a keyboard with azerty layout can run into issues with certain keybindings. Using the XMonad.Config.Azerty module will solve this.

If you do not need the capability to switch the display-setup in the gnome-control-center, just execute

as your user, to disable the xrandr plugin which grabs Super+p.

Chromium and Chrome browser windows will not have the defined border color per default but a blurred transparent one. This problem is known for a long time but easy to fix. Activating `Use system title bar and borders` in the browser options should fix it immediately.

A known issue with Virtualbox (Ticket #6479) can cause problems with the focused window border. A solution can be found by installing a compositing manager like xcompmgr which overrides the incorrect behavior of vboxvideo.

There seems to be some trouble with xmonad and Source engine based games like Half-Life. If they do not start or get stuck with a black screen, a workaround is to start them in windowed mode. To do so, right click on the game in your Steam library and choose properties, click on launch options and enter [3]:

Another solution is to float the window of the game using the manage hook. For example, the following line can be used for Half-Life:

This can also be worked around by making xmonad pay attention to EWMH hints and including its fullscreen hook [4]:

This has a few other effects and makes it behave more akin to fullscreen applications in other WMs.

The LibreOffice UI defaults to the gtk engine outside a desktop environment. This may cause problems with some xmonad configurations resulting in focus rapidly flicking between the LibreOffice main window and any open LibreOffice dialog window. Effectively locking the application. In this case the environment variable SAL_USE_VCLPLUGIN can be set to explicitly force LibreOffice to use another UI theme as outlined in LibreOffice#Theme For instance

to use the general (QT) UI.

IntelliJ IDEA has received better support for tiling window managers. But there are still some annoying issues, some of them have simple solutions:

The xmonad executable is located in ~/.xmonad/. After upgrading xmonad, an old executable might persist and need in that case be removed for xmonad to compile a new executable. Alternatively use xmonad --recompile.

The recompilation can be automated by adding a pacman hook like the following to /etc/pacman.d/hooks/xmonad.hook (you may have to first create the hooks directory):

Where YOUR_USERNAME is the username that you run xmonad from.

In the case that xmonad --recompile cannot find any modules at all (including XMonad itself), try regenerating the package database cache:

XMonad by default uses the font -misc-fixed-*-*-*-*-10-*-*-*-*-*-*-* [6]. If this font is missing those windows simply fail to render at all. Easiest fix is to install xorg-fonts-misc.

**Examples:**

Example 1 (unknown):
```unknown
xmonad --recompile
```

Example 2 (unknown):
```unknown
~/.xmonad/xmonad.hs
```

Example 3 (unknown):
```unknown
~/.xmonad/xmonad.hs
```

Example 4 (unknown):
```unknown
~/.xmonad/xmonad.hs
```

---

## Mobile broadband modem

**URL:** https://wiki.archlinux.org/title/ModemManager

**Contents:**
- Device identification
- Mode switching
  - From mass storage mode
  - From router mode
- Modem mode
  - Remove the PIN
    - Using mmcli
    - Using AT commands
  - Connection
    - ModemManager

This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.

A number of mobile phone carriers around the world offer internet access via mobile broadband (e.g. LTE, UMTS, EDGE, GSM, etc.).

This article focuses on mobile broadband modems in the format of portable USB devices and mini PCIe cards. For standalone mobile broadband routers, simply connect to them using an interface they provide (e.g. Ethernet or Wi-Fi).

Examine the output of:

which will show the vendor and product IDs of the device. Note that some devices will show two different product IDs at different times as explained below.

Often these devices will have two modes (1) USB flash memory storage (2) USB modem. The first mode, sometimes known as ZeroCD, is often used to deliver an internet communications program for another operating system and is generally of no interest to Linux users. Additionally some have a slot into which the user can insert an additional flash memory card.

A useful utility for switching these devices into modem mode is usb_modeswitch. It ships with udev rules /usr/lib/udev/rules.d/40-usb_modeswitch.rules that contain entries for many devices, which it will switch to modem mode upon insertion.

When a device is switched, its product ID may change to a different value. The vendor ID will remain unchanged. This can be seen in the output of lsusb.

Some devices are supported in the USB serial kernel module called option (named after the "Option" devices, but not limited to just those) and may be used without usb_modeswitch.

This article or section needs expansion.

Depending on the device, it may expose an Ethernet network interface or provide Wi-Fi. In that case you will need to have the interface up. If the device has a DHCP server, you can use a DHCP client to match it. Otherwise, you will have to have some knowledge about the network the device expects. Such information might be obtained from its behavior in another OS. Or by searching the web. Or from the drivers, and other information, stored in the initial USB flash memory storage (ZeroCD). Some Huawei HiLink devices, for example, sometime operate at 192.168.8.0/24, with a gateway at 192.168.8.1. They also might have a web interface at http://192.168.8.1.

In general, at this point you should note if mode switching left you with additional /dev/ttyUSB* serial device and a ww* network interface. You can do that with journalctl or by shell commands such as:

First of all use your SIM card in a normal phone and disable the PIN request if present. If the SIM card asks the PIN wvdial will not work.

Failing that, you can use mmcli (provided by modemmanager) or AT commands, to unlock the SIM card.

First, list the modems and find the modem's index:

Look for /org/freedesktop/ModemManager1/Modem/MODEM_INDEX.

Find the SIM card index:

Just as with the modem index, look for primary sim path: /org/freedesktop/ModemManager1/SIM/SIM_INDEX.

Remove the requirement for PIN:

Follow the instructions in https://unix.stackexchange.com/a/313878.

To connect to the mobile network, use one of the following methods.

This article or section needs expansion.

ModemManager is a system daemon which controls WWAN devices and connections.

Install the modemmanager and usb_modeswitch packages.

Start and enable ModemManager.service.

Use mmcli(1) to communicate with the modem.

The simplest way to establish a connection is to use mmcli's --simple-connect option.

First, list the modems and find the modem's index:

Look for /org/freedesktop/ModemManager1/Modem/MODEM_INDEX.

Next connect to the mobile network. For example:

Replace internet.myisp.example with your ISP's provided APN. If a user name and password is required, set them accordingly:

To disconnect from the mobile network run:

See also mmcli(1)  EXAMPLES.

NetworkManager uses ModemManager to work with mobile broadband modems. See NetworkManager#Mobile broadband support.

Install libmbim. To bring up the modem you can use mbim-network which is a wrapper for mbimcli calls. First create a profile for mbim-network.

Now connect to the network with:

Then follow Network configuration to bring up the ww* interface and get an IP address using DHCP.

pppd can be used to configure 3g connections. Step by step instruction is available on 3G and GPRS modems with pppd. Optionally, pppconfigAUR can be used to simplify the pppd configuration using dialog interface.

See main article: wvdial

Netctl can be used to establish a connection using a USB modem. An example configuration file provided by netctl is located at /etc/netctl/examples/mobile_ppp. Minimally you will probably have to specify

See the netctl article and netctl.profile(5) for more information.

Some ways to disable usb_modeswitch from operating on a device before the device was inserted, for example to be able to read the initial flash memory (ZeroCD), are:

Masking the udev rule the package is using can be achieved with

There are some useful commands:

Encode *100# to PDU format:

Decode AA180C3602 from PDU format:

Answer decoding (this example is balance response: 151.25):

Some operators return USSD result in PDU encoding, so you should check proper decoding method.

Frequently a 3G connection obtained via a mobile phone operator comes with restricted bandwidth, so that you are only allowed to use a certain bandwidth per time (e.g. 1GB per month). While it is quite straight-forward to know which type of network applications are pretty bandwidth extensive (e.g. video streaming, gaming, torrent, etc.), it may be difficult to keep an overview about overall consumed bandwidth.

A number of tools are available to help with that. Two console tools are vnstat, which allows to keep track of bandwith over time, and iftop to monitor bandwidth of individual sessions.

The internal web server found in some devices, such as some Huawei HiLink, might also show information about bandwidth usage.

This was tested on a Huawei EM770W (GTM382E) 3g card integrated into an Acer Aspire AS3810TG laptop. Install gnokiiAUR, then:

Usually the configuration directory is ~/.config/gnokii.

Edit ~/.config/gnokii/config as follows:

You may have to use a different port depending on your configuration, for example /dev/ttyUSB1 or something else:

You need to be part of the uucp group to use /dev/ttyUSB0.

Click on the "SMS" icon button, a window opens up. Then click: "messages->activate sms reading". Your messages will show up in the window.

A small command line script using gnokii to read SMS on your SIM card (not phone memory) without having to start a GUI:

Granted this does not work very well if your SMS contains the word "Text", but you may adapt the script to your liking.

Another option is to use mmcli, you can use simple bash script like this one[dead link 2024-10-12HTTP 404] that is also used to write messages or this one below:

Some Devices, such as some Huawei HiLink, include an email like web interface for SMS. It is included in the device internal web server, which is used for other purposes too.

You may need give permission by creating file with content like

Unplugging, and plugging, the device is sometimes used to restart the USB device. The following describes how to do that from the shell. Doing that from the shell might be useful, if, for example, the plug is at the rear side of the PC. The method described is not just for USB modems. It should be good for many other USB devices.

The important part is that the requirements are for the USB bus, and the port, the device is attached to. There could be one, or more, sub ports too. Suppose I obtained bus 2 and port 4, without sub ports, for my device from the output of lsusb -t. This information is also recorded in the journal. With

I can verify it is the intended device.

The following sequence will restart the device:

Some more comments are at, for example, https://askubuntu.com/questions/1036341/unplug-and-plug-in-again-a-usb-device-in-the-terminal.

This problem commonly occurs on some modems which locked by a mobile operator. You can successfully connect to the internet but after few minutes connection halts and your modem reboots. That happens because an operator built a some checks into modem firmware so a modem checks if a branded software is running on your pc, but usually that software is Windows-only, and obviously you do not use it. Fix (it works on ZTE-mf190 at least) is simple - send this command through serial port (use minicom or similar soft):

This command will delete a NODOWNLOAD.FLG file in the modem's filesystem - it will disable such checks.

Another possibility for such disconnections is to help the customer save bandwidth, which might be expensive. With Huawei HiLink devices with a web interface, there might be an option there to set a longer period of inactivity before the connection hangs up.

Someone claims that the connection speed under Linux is lower than Windows [3]. This is a short summary for possible solutions which are not fully verified.

In most of conditions, the low speed is caused by bad receiver signals and too many people in cell. But you still could use the following method to try to improve the connection speed:

It is advisable to see the baud rate set by the official modem application for Windows (possibly 9600 on Vista).

If you are getting low quality images while browsing the web over a mobile broadband connection with the hints shift+r improves the quality of this image and shift+a improves the quality of all images on this page, follow these instructions:

Edit /etc/tinyproxy/tinyproxy.conf and insert the following two lines:

Start tinyproxy.service

Configure your browser to use localhost:8888 as a proxy server and you are all done. This is especially useful if you are using, for example, Google Chrome which, unlike Firefox, does not allow you to modify the Pragma and Cache-Control headers.

In case ModemManager does not recognize the modem, check the unit status of ModemManager.service. If you get error messages such as Couldn't check support for device and not supported by any plugin, you may have to whitelist your device using the ModemManager filter rules.

The FCC lock is a software lock integrated in WWAN modules shipped by several different laptop manufacturers like Lenovo, Dell, or HP. This lock prevents the WWAN module from being put online until some specific unlock procedure (usually a magic command sent to the module) is performed.

Since release 1.18.4, the ModemManager daemon no longer automatically performs the FCC unlock procedure [4].

ModemManager will keep on providing support for the known FCC unlock procedures, but no longer automatically: the user must install and select the FCC unlock procedure needed in the specific laptop being used. This applies to: EM7355, MC8805, MC7355, EM7455, SDX55, EM120.

The modemmanager package ships several scripts installed under /usr/share/ModemManager/fcc-unlock.available.d/ and named as vid:pid with either the PCI or USB vendor and product IDs. However, they are not used if they are not in the /etc/ModemManager directory.

For each device the vid:pid can be found in the brackets at the end of the line:

To enable unlock script for the device it must be symlinked like so:

For a Quectel EM120 modem that would be:

See the ModemManager documentation for more information.

If NetworkManager persists on that the device (e.g. /dev/cdc-wdm0) is not available while ModemManager can use it, it could either be, that the device is blocked using a hardware switch, by rfkill or just NetworkManager believes that.

The wwan device should be listed as unblocked for both SOFT & HARD. If it is HARD blocked, a hardware switch blocks the device. If it is SOFT blocked, unblock it using:

If NetworkManager still declares the device not available, it could be that NetworkManager is not synced with rfkill. Check that using:

If WWAN is listed as disabled, enable it using:

This has been reported to happen on some LTE modems with buggy or incompatible firmware versions. In this scenario, when inspecting the bearer with:

It can be seen how the IPv4 configuration section shows no IP address, and may show dhcp as the method despite the associated interface (e.g. wwan0) not being dhcp-capable. In this cases, the modem firmware is not behaving correctly and it should be upgraded.

**Examples:**

Example 1 (unknown):
```unknown
$ lspci -nn
```

Example 2 (unknown):
```unknown
/usr/lib/udev/rules.d/40-usb_modeswitch.rules
```

Example 3 (unknown):
```unknown
/dev/ttyUSB*
```

Example 4 (unknown):
```unknown
$ ls /dev/ttyUSB*
$ ip link
```

---

## KeePass

**URL:** https://wiki.archlinux.org/title/KeePass

**Contents:**
- Installation
- Integration
  - Plugin installation in KeePass
  - Browser integration
    - keepassxc-browser for KeePassXC
    - keepassxc-browser for KeePass
    - KeePassRPC and Kee
    - Via autotype feature
  - Nextcloud
  - Yubikey

KeePass is an encrypted password database format. It is an alternative to online password managers and is supported on all major platforms.

There are two versions of the format: KeePass 1.x (Classic) and KeePass 2.x

There are three major implementations of KeePass available in the official repositories:

Other lesser-known alternatives can be found in the AUR:

Many plugins and extensions are available for integrating KeePass to other software. KeePassX and KeePassXC do not have a plugin interface, but KeePassXC has various integrations built-in.

KeePass is by default installed at /usr/share/keepass/. Copy plugin.plgx to a plugins sub-directory under the KeePass installation directory as demonstrated below:

keepassxc-browser is the browser extension of KeePassXCs built-in browser integration using native-messaging and transport encryption using libsodium. It was developed to replace KeePassHTTP, as KeePassHTTPs protocol has fundamental security problems.

The developers provide the browser extension on

Support for Firefox and Chromium forks is available. For librewolfAUR, open KeePassXC, go to Tools > Settings > Browser Integration > Advanced > Config Location:, and add ~/.librewolf/native-messaging-hosts.

The source code and an explanation how it works can be found on GitHub, the KeePassXC developers provide a configuration guide on their website.

This article or section is being considered for removal.

keepassxc-browser can also be used with KeePass through Keepass-natmsg Plugin from AUR (keepass-natmsgAUR) and is recommended as successor of KeePassHTTP.

Kee (GitHub repo) is a browser extension for Firefox and Chromium which integrates KeePass through KeePassRPC, a KeePass plugin from the same developers.

The KeePass plugin is available from GitHub or from the AUR (keepass-plugin-rpcAUR).

The browser extension can be found on GitHub, Firefox Add-ons and the chrome web store.

An alternative to having a direct channel between browser and KeePass(XC) is using the autotype feature.

To enable the autotype feature on Wayland, edit /usr/share/applications/org.keepassxc.KeePassXC.desktop and change the value of Exec to keepassxc -platform xcb. Alternatively, set the QT_QPA_PLATFORM=xcb environment variable before launching KeePassXC. However, native Wayland applications will not work with autotype. For example, autotype works when running Firefox without Wayland, but not with.

There are browser extensions which support this way by putting the page URL into the window name:

This article or section is being considered for removal.

YubiKey can be integrated with KeePass thanks to contributors of KeePass plugins. KeepassXC provides built-in support for Yubikey Challenge-Response without plugins.

KeePassXC offers SSH agent support, a similar feature is also available for KeePass using the KeeAgent plugin.

The feature allows to store SSH keys in KeePass databases, KeePassXC/KeeAgent acts as OpenSSH Client and dynamically adds and removes the key to the Agent.

The feature in KeePassXC is documented in its FAQ. First configure SSH agent to start on login and make sure the SSH_AUTH_SOCK variable is set. Then logout and log back in. Now, in KeePassXC settings, enable SSH agent integration. The SSH_AUTH_SOCK value exposed in the UI should correspond to what you configured earlier.

KeePassXC contains a Freedesktop.org Secret Service integration. It will allow external applications to use KeePassXC as an encrypted database (a.k.a. as a vault, wallet, or keyring) to store user credentials (e.g., messaging applications, games).

It can be enabled by going into the settings (under the Tools menu), and selecting which group(s) you want to share (for each database, open Database > Database Settings..., then go to the Secret Service Integration tab).

KeePassXC will refuse to enable its integration if it detects that another program (such as GNOME/Keyring) is already providing that service. You should first stop that program (for example, by stopping gnome-keyring-daemon.service user unit for gnome-keyring). Note that you will likely want to disable the program permanently, otherwise KeePassXC's integration will fail on the next reboot (for example, by disabling the gnome-keyring-daemon.socket of a systemd/User for gnome-keyring).

An application that requests access to the database will connect to KeePassXC through D-Bus, where KeePassXC will be "seen" just as GNOME libsecret by the application. The database that will be exposed can be stored anywhere on the disk, just like any other KeePassXC database, and the master password of this database will be the one to type when applications will want to retrieve a credential in the future.

This article or section is being considered for removal.

To ensure also dbus doesn't active gnome-keyring again (bypassing the systemd socket for some odd reason...) delete these files:

and edit your pacman config prevent it from recreating these conflicting files when reinstalling or updating packages:

KeePassXC will not be automatically started when an application requests secrets, which may cause them to break. A D-Bus auto-start file can be created:

If you are an avid user of clipboard managers, you may need to disable your clipboard manager before you launch Keepass and then re-start your clipboard manager afterwards.

KeePassXC implementations has the option to auto-clear the clipboard manager after an amount of time, enough to paste copied items.

To enable the dark theme for KeePass, install keepass-keethemeAUR. After installation, the plugin will get compiled upon starting KeePass. It can then be activated via Tools > Dark Theme, or by pressing Ctrl+t.

Without using specialized plugin, a KeePass database is well-suited to be synchronized through Syncthing. On conflicts, some applications provides a way to resolve them, such as the Merge from database feature of KeePassXC.

If the user interface elements are not scaled properly, see HiDPI#Qt 5 and upstream bug report.

Some options like Start minimized and locked may appear greyed-out. According to a discussion on SourceForge, since version 2.31, KeePass has disabled two options because of their broken behaviors on Mono.

To force these features to be enabled, launch KeePass with the -wa-disable:1418 argument.

In some desktop environments, the tray icon of KeePass may appear too big or too small due to Mono's bug, according to a bug report on SourceForge.

Keebuntu contains three plugins to provide desktop integration:

After installing one of these plugins, it is sometimes necessary to hide the original tray icon to prevent duplicate icons in the system tray.

First, check that the group under which your passwords are stored is exposed; the Tools > Settings menu contains a list of groups enabled for each database. If a database isn't exposing the proper group, select its tab, open Database > Database Settings..., then select the group in the Secret Service Integration tab).

Note that merging a database can cause it to stop exposing any groups.

If you are experiencing graphical glitches, install the qt5-wayland package. KeePassXC (as of version v2.7.7) still uses Qt5.

**Examples:**

Example 1 (unknown):
```unknown
keepassxc-cli
```

Example 2 (unknown):
```unknown
/usr/share/keepass/
```

Example 3 (unknown):
```unknown
plugin.plgx
```

Example 4 (unknown):
```unknown
# mkdir /usr/share/keepass/plugins
# cp plugin.plgx /usr/share/keepass/plugins
```

---

## Improving performance

**URL:** https://wiki.archlinux.org/title/Improving_performance

**Contents:**
- The basics
  - Know your system
  - Benchmarking
- Storage devices
  - Sector size
  - Partitioning
    - Multiple drives
      - An SSD as a cache for an HDD
    - Layout on HDDs
  - Choosing and tuning your filesystem

This article provides information on basic system diagnostics relating to performance as well as steps that may be taken to reduce resource consumption or to otherwise optimize the system with the end-goal being either perceived or documented improvements to a system's performance. See also Gaming#Improving performance for additional gaming and low latency specific advice.

The best way to tune a system is to target bottlenecks, or subsystems which limit overall speed. The system specifications can help identify them.

The effects of optimization are often difficult to judge. They can however be measured by benchmarking tools.

Check that your NVMe drives and Advanced Format hard disk drives are using the optimal logical sector size.

Make sure that your partitions are properly aligned.

If you have multiple disks available, you can set them up as a software RAID for serious speed improvements.

Creating swap on a separate disk can also help quite a bit, especially if your machine swaps frequently.

When forgoing hard disk drives is not an option, a solid state drive can be added as a caching layer to improve the read and/or write speeds and reduce the noise from random access. The options to accomplish this include LVM#Cache, Bcache and Bcachefs#SSD caching.

If using a traditional spinning HDD, your partition layout can influence the system's performance. Sectors at the beginning of the drive (closer to the outside of the disk) are faster than those at the end. Also, a smaller partition requires less movements from the drive's head, and so speed up disk operations. Therefore, it is advised to create a small partition (15-20GiB, more or less depending on your needs) only for your system, as near to the beginning of the drive as possible. Other data (pictures, videos) should be kept on a separate partition, and this is usually achieved by separating the home directory (/home) from the system (/).

Choosing the best filesystem for a specific system is very important because each has its own strengths. The File systems article provides a short summary of the most popular ones. You can also find relevant articles in Category:File systems.

The various *atime options can mitigate the performance penalty of strictatime.

Other mount options are filesystem specific, therefore see the relevant articles for the filesystems:

There are several key tunables affecting the performance of block devices, see sysctl#Virtual memory for more information.

The input/output (I/O) scheduler is the kernel component that decides in which order the block I/O operations are submitted to storage devices. It is useful to remind here some specifications of two main drive types because the goal of the I/O scheduler is to optimize the way these are able to deal with read requests:

If there are many processes making I/O requests to different storage parts, thousands of IOPS can be generated while a typical HDD can handle only about 200 IOPS. There is a queue of requests that have to wait for access to the storage. This is where the I/O schedulers plays an optimization role.

One way to improve throughput is to linearize access: by ordering waiting requests by their logical address and grouping the closest ones. Historically this was the first Linux I/O scheduler called elevator.

One issue with the elevator algorithm is that it is not optimal for a process doing sequential access: reading a block of data, processing it for several microseconds then reading next block and so on. The elevator scheduler does not know that the process is about to read another block nearby and, thus, moves to another request by another process at some other location. The anticipatory I/O scheduler overcomes the problem: it pauses for a few milliseconds in anticipation of another close-by read operation before dealing with another request.

While these schedulers try to improve total throughput, they might leave some unlucky requests waiting for a very long time. As an example, imagine the majority of processes make requests at the beginning of the storage space while an unlucky process makes a request at the other end of storage. This potentially infinite postponement of the process is called starvation. To improve fairness, the deadline algorithm was developed. It has a queue ordered by address, similar to the elevator, but if some request sits in this queue for too long then it moves to an "expired" queue ordered by expire time. The scheduler checks the expire queue first and processes requests from there and only then moves to the elevator queue. Note that this fairness has a negative impact on overall throughput.

The Completely Fair Queuing (CFQ) approaches the problem differently by allocating a timeslice and a number of allowed requests by queue depending on the priority of the process submitting them. It supports cgroup that allows to reserve some amount of I/O to a specific collection of processes. It is in particular useful for shared and cloud hosting: users who paid for some IOPS want to get their share whenever needed. Also, it idles at the end of synchronous I/O waiting for other nearby operations, taking over this feature from the anticipatory scheduler and bringing some enhancements. Both the anticipatory and the elevator schedulers were decommissioned from the Linux kernel replaced by the more advanced alternatives presented below.

The Budget Fair Queuing (BFQ) is based on CFQ code and brings some enhancements. It does not grant the disk to each process for a fixed time-slice but assigns a "budget" measured in number of sectors to the process and uses heuristics. It is a relatively complex scheduler, it may be more adapted to rotational drives and slow SSDs because its high per-operation overhead, especially if associated with a slow CPU, can slow down fast devices. The objective of BFQ on personal systems is that for interactive tasks, the storage device is virtually as responsive as if it was idle. In its default configuration it focuses on delivering the lowest latency rather than achieving the maximum throughput, which can sometimes greatly accelerate the startup of applications on hard drives.

Kyber is a recent scheduler inspired by active queue management techniques used for network routing. The implementation is based on "tokens" that serve as a mechanism for limiting requests. A queuing token is required to allocate a request, this is used to prevent starvation of requests. A dispatch token is also needed and limits the operations of a certain priority on a given device. Finally, a target read latency is defined and the scheduler tunes itself to reach this latency goal. The implementation of the algorithm is relatively simple and it is deemed efficient for fast devices.

While some of the early algorithms have now been decommissioned, the official Linux kernel supports a number of I/O schedulers. The Multi-Queue Block I/O Queuing Mechanism (blk-mq) maps I/O queries to multiple queues, the tasks are distributed across threads and therefore CPU cores. Within this framework the following schedulers are available:

To list the available schedulers for a device and the active scheduler (in brackets):

To list the available schedulers for all devices:

To change the active I/O scheduler to bfq for device sda, use:

The process to change I/O scheduler, depending on whether the disk is rotating or not can be automated and persist across reboots. For example the udev rules below set the scheduler to bfq for rotational drives, bfq for SSD/eMMC drives and none for NVMe drives:

Reboot or force udev#Loading new rules.

Each of the kernel's I/O scheduler has its own tunables, such as the latency time, the expiry time or the FIFO parameters. They are helpful in adjusting the algorithm to a particular combination of device and workload. This is typically to achieve a higher throughput or a lower latency for a given utilization. The tunables and their description can be found within the kernel documentation.

To list the available tunables for a device, in the example below sdb which is using deadline, use:

To improve deadline's throughput at the cost of latency, one can increase fifo_batch with the command:

When dealing with traditional rotational disks (HDDs) you may want to completely disable or lower power saving features, and check if the write cache is enabled.

See Hdparm#Power management configuration and Hdparm#Write cache.

Afterwards, you can make a udev rule to apply them on boot-up.

Avoiding unnecessary access to slow storage drives is good for performance and also increasing lifetime of the devices, although on modern hardware the difference in life expectancy is usually negligible.

The iotop package can sort by disk writes, and show how much and how frequently programs are writing to the disk. See iotop(8) for details.

Relocate files, such as your browser profile, to a tmpfs file system, for improvements in application response as all the files are now stored in RAM:

Refer to corresponding file system page in case there were performance improvements instructions, see the list at #Choosing and tuning your filesystem.

See Swap#Performance for details.

See Sysctl#Virtual memory for details.

See Core dump#Disabling automatic core dumps.

Many tasks such as backups do not rely on a short storage I/O delay or high storage I/O bandwidth to fulfill their task, they can be classified as background tasks. On the other hand quick I/O is necessary for good UI responsiveness on the desktop. Therefore it is beneficial to reduce the amount of storage bandwidth available to background tasks, whilst other tasks are in need of storage I/O. This can be achieved by making use of the Linux I/O scheduler BFQ, which allows setting different priorities for processes.

The I/O priority of a background process can be reduced to the "Idle" level by starting it with

See a short introduction to ionice and ionice(1) for more information.

For optimal performance, empty blocks of solid state drives should be discarded (a.k.a. trimmed) periodically to optimize random write speeds. See Solid state drive#TRIM for more information.

Overclocking improves the computational performance of the CPU by increasing its peak clock frequency. The ability to overclock depends on the combination of CPU model and motherboard model. It is most frequently done through the BIOS. Overclocking also has disadvantages and risks. It is neither recommended nor discouraged here.

Many Intel chips will not correctly report their clock frequency to acpi_cpufreq and most other utilities. This will result in excessive messages in dmesg, which can be avoided by unloading and blacklisting the kernel module acpi_cpufreq. To read their clock speed use i7z from the i7zAUR package. To check for correct operation of an overclocked CPU, it is recommended to do stress testing.

See CPU frequency scaling.

The default CPU scheduler in the mainline Linux kernel is EEVDF.

This article or section needs expansion.

Some applications such as running a TV tuner card at full HD resolution (1080p) may benefit from using a realtime kernel.

See also nice(1) and renice(1).

Ananicy CPP is a daemon, available as ananicy-cpp or ananicy-cpp-gitAUR, for auto adjusting the nice levels of executables. The nice level represents the priority of the executable when allocating CPU resources.

LimitCPU is a program to limit the CPU usage percentage of a specific process. After installing limitcpuAUR, you may limit the CPU usage of a process's PID using a scale of 0 to 100 times the number of CPU cores that the computer has. For example, with eight CPU cores the percentage range will be 0 to 800. Usage:

The purpose of irqbalance is to distribute hardware interrupts across processors on a multiprocessor system in order to increase performance. It can be controlled by the provided irqbalance.service.

Turning off CPU exploit mitigations may improve performance. Use below kernel parameter to disable them all:

The explanations of all the switches it toggles are given at kernel.org. You can use spectre-meltdown-checkerAUR or lscpu(1) (from util-linux) for vulnerability check.

Graphics performance may depend on the settings in xorg.conf(5); see the NVIDIA, AMDGPU and Intel articles. Improper settings may stop Xorg from working, so caution is advised.

The performance of the Mesa drivers can be configured via drirc. adriconf (Advanced DRI Configurator) is a GUI tool to configure Mesa drivers by setting options and writing them to the standard drirc file.

Hardware video acceleration makes it possible for the video card to decode/encode video.

As with CPUs, overclocking can directly improve performance, but is generally recommended against. There are several packages, such as rovclockAUR (ATI cards), rocm-smi-lib (recent AMD cards), nvclockAUR (old NVIDIA - up to Geforce 9), and nvidia-utils for recent NVIDIA cards.

See AMDGPU#Overclocking or NVIDIA/Tips and tricks#Enabling overclocking in nvidia-settings.

The PCI specification allows larger Base Address Registers (BARs) to be used for exposing PCI devices memory to the PCI Controller. This can result in a performance increase for video cards. Having access to the full video memory improves performance, but also enables optimizations in the graphics driver. The combination of resizable BAR, above 4G decoding and these driver optimizations are what AMD calls AMD Smart Access Memory, available at first on AMD Series 500 chipset motherboards, later expanded to AMD Series 400 and Intel Series 300 and later through UEFI updates. This setting may not be available on all motherboards, and is known to sometimes cause boot problems on certain boards.

If the BAR has a 256M size, the feature is not enabled or not supported:

To enable it, enable the setting named "Above 4G Decode" or ">4GB MMIO" in your motherboard settings. Verify that the BAR is now larger:

RAM can run at different clock frequencies and timings, which can be configured in the BIOS. Memory performance depends on both values. Selecting the highest preset presented by the BIOS usually improves the performance over the default setting. Note that increasing the frequency to values not supported by both motherboard and RAM vendor is overclocking, and similar risks and disadvantages apply, see #Overclocking.

This article or section is out of date.

If running off a slow writing medium (USB, spinning HDDs) and storage requirements are low, the root may be run on a RAM overlay ontop of read only root (on disk). This can vastly improve performance at the cost of a limited writable space to root. See liverootAUR.

Similar benefits (at similar costs) can be achieved using zswap or swap on zram. The two are generally similar in intent although not operation:

Because both options involve the swap subsystem, configuration that affects swap also affects these systems. For example, swappiness determines whether the kernel should prioritize dropping file cache or moving pages to swap when memory pressure is high. Because zswap intercepts the action of moving pages to swap and zram acts as the swap, the option would also determine how often these two mechanisms are used.

In the unlikely case that you have very little RAM and a surplus of video RAM, you can use the latter as swap. See Swap on video RAM.

This article or section needs expansion.

On traditional GNU/Linux system, especially for graphical workstations, when allocated memory is overcommitted, the overall system's responsiveness may degrade to a nearly unusable state before either triggering the in-kernel out-of-memory (OOM)-killer or a sufficient amount of memory got free (which is unlikely to happen quickly when the system is unresponsive, as you can hardly close any memory-hungry applications which may continue to allocate more memory). The behaviour also depends on specific setups and conditions, returning to a normal responsive state may take from a few seconds to more than half an hour, which could be a pain to wait in serious scenario like during a conference presentation.

While the behaviour of the kernel as well as the userspace things under low-memory conditions may improve in the future as discussed on kernel and Fedora mailing lists, users can use more feasible and effective options than hard-resetting the system or tuning the vm.overcommit_* sysctl parameters:

Sometimes a user may prefer OOM daemon to SysRq because with kernel OOM-killer you cannot prioritize the process to (or not) terminate. To list some OOM daemons:

**Examples:**

Example 1 (unknown):
```unknown
# hdparm -t /dev/sdX
```

Example 2 (unknown):
```unknown
direct rendering: Yes
```

Example 3 (unknown):
```unknown
$ glxinfo | grep "direct rendering"
```

Example 4 (unknown):
```unknown
strictatime
```

---

## Firefox/Privacy

**URL:** https://wiki.archlinux.org/title/Firefox/Privacy

**Contents:**
- Configuration
  - Tracking protection
  - Anti-fingerprinting
  - Change browser time zone
  - Change user agent and platform
  - WebRTC exposes LAN IP address
  - Disable HTTP referer
  - Disable connection tests
  - Disable telemetry
  - Enable "Do Not Track" header

This article overviews how to configure Firefox to enhance security and privacy.

The following are privacy-focused tweaks to prevent browser fingerprinting and tracking.

Firefox gained an option for Enhanced Tracking Protection. It can be enabled in different levels via the GUI Settings > Privacy & Security, or by setting about:config:

Apart from privacy benefits, enabling tracking protection may also reduce load time by 44%.

Note that this is not a replacement for ad blocking extensions such as uBlock Origin and it may or may not work with Firefox forks. If you are already running such an ad blocker with the correct lists, tracking protection might be redundant.

The Firefox tracking protection blocks a list of known "fingerprinters" when your privacy settings are set to Standard (the default) or Strict. Fingerprinting Protection is a different, experimental feature under heavy development in Firefox.

Mozilla has started an anti-fingerprinting project in Firefox, as part of a project to upstream features from Tor Browser. Many of these anti-fingerprinting features are enabled by this setting in the about:config:

For more information see: Firefox's protection against fingerprinting.

The time zone of your system can be used in browser fingerprinting. To set Firefox's time zone to UTC launch it as:

Or, set a script to launch the above (for example, at /usr/local/bin/firefox).

You can override Firefox's user agent with the general.useragent.override preference in about:config.

The value for the key is your browser's user agent. Select a known common one.

To change the platform for firefox, add the following string key in about:config:

Select a known common platform that corresponds with your user agent.

To prevent websites from getting your local IP address via WebRTC's peer-to-peer (and JavaScript), open about:config and set:

You can use this WebRTC test page and WebRTC IP Leak VPN / Tor IP Test to confirm that your internal/external IP address is no longer leaked.

HTTP referer is an optional HTTP header field that identifies the address of the previous webpage from which a link to the currently requested page was followed.

Set network.http.sendRefererHeader to 0 or 1, depending on your preferences.

By default Firefox attempts to connect to Amazon and/or Akamai servers at regular intervals, to test your connection. For example a hotel, restaurant or other business might require you to enter a password to access the internet. If such a Captive portal exists and is blocking traffic this feature blocks all other connection attempts. This may leak your usage habits.

To disable Captive Portal testing, in about:config set:

Set toolkit.telemetry.enabled to false and/or disable it under Preferences > Privacy & Security > Firefox Data Collection and Use.

Set privacy.donottrackheader.enabled to true or toggle it in Preferences > Privacy & Security > Tracking Protection

Firefox 60 introduced a feature called Trusted Recursive Resolver (TRR). It circumvents DNS servers configured in your system, instead sending all DNS requests over HTTPS to Cloudflare servers. While this is significantly more secure (as "classic" DNS requests are sent in plain text over the network, and everyone along the way can snoop on these), this also makes all your DNS requests readable by Cloudflare, providing TRR servers.

To enable Encrypted Client Hello (ECH) (formerly encrypted Server Name Indicator (eSNI)), so that nobody listening on the wire can see the server name you made a TLS connection to, set:

You may also need to set network.trr.mode to 2 or 3.

Set geo.enabled to false in about:config.

Safe Browsing offers phishing protection and malware checks, however it may send user information (e.g. URL, file hashes, etc.) to third parties like Google.

To disable the Safe Browsing service, in about:config set:

In addition disable download checking, by setting browser.safebrowsing.downloads.enabled to false.

WebGL is a potential security risk.[1] Set webgl.disabled to true in about:config if you want to disable it.

See Browser extensions#Privacy.

WebAssembly, also known as Wasm, is a relatively new language. Unlike JavaScript, Wasm executes pre-compiled code natively in browsers for high-performance simulations and applications. It has been criticized for hiding pathways for malware and as with JavaScript, can be used to track users. Tor Browser blocks both JavaScript and Wasm.

See NoScript in Browser extensions#Privacy to block JavaScript the way Tor Browser does, which enables quick access when needed. To disable Wasm, in about:config set:

Some extensions are hidden and installed by default in /usr/lib/firefox/browser/features. Many can be safely removed via rm extension-name.xpi. They might not be enabled by default and may have a menu option for enabling or disabling. Note that any files removed will return upon update of the firefox package. To keep these extensions removed, add the directories to NoExtract in /etc/pacman.conf. Some extensions include:

Firefox installations to paths such as the default release installed to /opt have system extensions installed at /firefox/firefox/browser/features.

This article or section is out of date.

Privacy can be boosted by reducing the amount of information you give to a single entity. For example, sending each new web search via a different, randomly selected proxy makes it near impossible for a single search engine to build a profile of you. We can do this using public instances (or sites) of Searx. Searx is an AGPL-3.0, open-source site-builder, that produces site, known as an 'instances'. Each public 'instance' can act as a middle-man between you and a myriad of different search engines.

From this list of public instances and others, bookmark as many Searx sites as you wish (if JavaScript is disabled you will need to enable it temporarily to load the list). For fast access to these bookmarks, consider adding SX1, SX2 ... SX(n) to the bookmark's Name field, with (n) being the number of searx instances you bookmark.

After this bookmarking, simply typing sx, a number and Enter in the URL bar will load an instance.

See https://www.privacyguides.org/en/search-engines/ for other options.

Invidious instances act as an alternative front-end to YouTube. They are websites built from open-source code. It has typically been difficult to limit the amount of information a user sent to YouTube (Google) in order to access content.

Benefits of using Invidious include:

Bookmark as many functioning invidious instances from the following lists as possible (here, here, here[dead link 2024-01-13HTTP 404]). Note that some of these instances may be hosted by Cloudflare.

You can change any YouTube video URL to an Invidious one by simply replacing the youtube.com part with the domain of the instance you want to use.

Network and system-wide policies may be established through the use of enterprise policies which both supplements and overrides user configuration preferences. For example, there is no documented user preference to disable the checking of updates for beta channel releases. However, there exists an enterprise policy which can be effectively deployed as a workaround. Single and/or multiple policies may be administered through policies.json as follows:

Verify that Enterprise Policies is set to Active in about:support and review release-specific policies in about:policies.

Files which constitute a Firefox profile can be stripped of certain metadata. For example, a typical prefs.js contains strings which identify the client and/or the user.

There are multiple approaches by which these strings can be reset with the caveat that a master prefs.js must first be created without such identifiers and synced into a working profile. The simplest solution is close Firefox before copying its prefs.js to a separate location:

Strip out any and all identfier strings and date codes by either setting them to 0 or removing the entries outright from the copied prefs.js. Sync the now sanitized prefs.js to the working profile as required:

A secondary privacy effect is also incurred which can be witnessed by examining the string results between a sanitized prefs.js versus a working prefs.js at Fingerprint JS API Demo.

Assuming that extensions are installed, the extensions.json file lists all profile extensions and their settings. Of note is the location of the user home directory where the .mozilla and extensions folder exist by default. Unwanted background updates may be disabled by setting applyBackgroundUpdates to the appropriate 0 value. Of minor note are installDate and updateDate. Bubblewrap can effectively mask the username and location of the home directory at which time the extensions.json file may be sanitized and modified to point to the sandboxed HOME location.

Removal of similar metadata from addonStartup.json.lz4 and search.json.mozlz4 can also be accomplished. mozlz4 is a command-line tool which provides compression/decompression support for Mozilla (non-standard) LZ4 files.

Telemetry related to crash reporting may be disabled by removing the following:

Those deleted files will be back after upgrading the package, add them to NoExtract for persistence.

For those who have opted to install Firefox manually from official Mozilla sources, the updater system may be disabled by removing updater in the firefox directory.

The file /usr/lib/firefox/omni.ja contains most of the default configuration settings used by Firefox. As an example, starting from Firefox 73, network calls to firefox.settings.services.mozilla.com and/or content-signature-2.cdn.mozilla.net cannot be blocked by extensions or by setting preference URLs to "");. Aside from using a DNS sinkhole or firewalling resolved IP blocks, one solution is to grep(1) through the extracted contents of omni.ja before removing all references to firefox.settings.services.mozilla.com and/or cdn.mozilla.net. Extraneous modules such as unused dictionaries and hyphenation files can also be removed in order to reduce the size of omni.ja for both security and performance reasons.

To repack/rezip, use the command zip -0DXqr omni.ja * and make sure that your working directory is the root directory of the files from the omni.ja file.

Several active projects maintain comprehensive hardened Firefox configurations in the form of a user.js config that can be dropped to Firefox profile directory:

**Examples:**

Example 1 (unknown):
```unknown
about:config
```

Example 2 (unknown):
```unknown
privacy.trackingprotection.enabled
```

Example 3 (unknown):
```unknown
about:config
```

Example 4 (unknown):
```unknown
privacy.resistFingerprinting
```

---

## Help:Reading

**URL:** https://wiki.archlinux.org/title/Starting/enabling

**Contents:**
- Organization
- Formatting
- Root, regular user or another user
- Append, add, create, edit
  - Make executable
- Source
- Installation of packages
  - Official packages
  - Arch User Repository
- Control of systemd units

Because the vast majority of the ArchWiki contains indications that may need clarification for users new to Arch Linux (or GNU/Linux in general), this rundown of basic procedures was written both to avoid confusion in the assimilation of the articles and to deter repetition in the content itself.

Most articles on the ArchWiki do not attempt to provide a holistic introduction to a single topic; they are instead written in adherence to the "Don't Repeat Yourself" principle, under the assumption that the user will seek out and read any supporting material that they do not yet understand. Where possible, such supporting material is indicated in the article via special formatting, see #Formatting.

Because of this organization, it may be necessary to examine several related sources in order to fully understand an ArchWiki article. In particular, users who are new to Arch (or GNU/Linux in general) should expect to end up reading a great number of articles even when solving simple problems. It is especially important to study the supporting material before seeking additional help from other users.

Some lines are written like so:

Others have a different prefix:

The numeral or hash sign (#) indicates that the command needs to be run as root, whereas the dollar sign ($) shows that the command should be run as a regular user.

When the commands need to run as a specific user, they will be prefixed by the username in square brackets, for example:

This means you should use a privilege elevation tool, e.g. with sudo:

A notable exception to watch out for:

In this example, the context surrounding the numeral sign communicates that this is not to be run as a command; it should be edited into a file instead. So in this case, the numeral sign denotes a comment. A comment can be explanatory text that will not be interpreted by the associated program. Bash scripts denotation for comments happens to coincide with the root PS1.

After further examination, "give away" signs include the uppercase character following the # sign. Usually, Unix commands are not written this way and most of the time they are short abbreviations instead of full-blown English words (e.g., Copy becomes cp).

Regardless, most articles make this easy to discern by notifying the reader:

Append to ~/path/to/file:

When prompted to append to, add to, create, or edit one or more files, it is implied that you should use one of the following methods.

To create or modify multiline files, it is suggested to use a text editor. For example, using the nano command to edit the file /etc/bash.bashrc is:

To create or overwrite a file from a string, it may be simpler to use output redirection. The following example creates or overwrites the contents of the file /etc/hostname with the text myhostname.

Output redirection can also be used to append a string to a file. The following example appends the text [custom-repo] to the file /etc/pacman.conf.

When prompted to create directories, use the mkdir command:

After creating a file, if it is meant to be run as a script (whether manually or called by another program), it needs to be set as executable, for example with:

See chmod. Some applications such as file managers may provide graphical interfaces to do the same.

Some applications, notably command-line shells, use scripts for their configuration: after modifying them, they must be sourced in order for the changes to be applied. In the case of bash, for example, this is done by running (you can also replace source with .):

When the wiki suggests modifying such a configuration script, it will not explicitly remind you to source the file, and only in some cases will it point to this section with a reminder link.

When an article invites you to install some packages in the conventional way, it will not indicate the detailed instructions to do so; instead, it will simply mention the names of the packages to be installed.

The subsections below give an overview of the generic installation procedures depending on the package type.

For packages from the official repositories, you will read something like:

This means that you have to run:

The pacman article contains detailed explanations to deal with package management in Arch Linux proficiently.

For packages from the Arch User Repository (AUR), you will read something like:

This means that in general you have to follow the foobarAUR link, download the PKGBUILD archive, extract it, verify the content and finally run, in the same folder:

The Arch User Repository article contains all the detailed explanations and best practices to deal with AUR packages.

When an article invites to start, enable, etc., some systemd unit (e.g. a service), it will not indicate the detailed instructions to do so, but instead you will read something like:

This means that you have to run:

A notable command that does not follow this exact pattern is systemctl daemon-reload which will be called without arguments.

The systemd#Using units section contains structured list of available actions (like start, enable, enable and start, etc.) with their corresponding systemctl commands.

It is important to remember that there are two different kinds of configurations on a GNU/Linux system. System-wide configuration affects all users. Since system-wide settings are generally located in the /etc directory, root privileges are required in order to alter them. For example, to apply a Bash setting that affects all users, /etc/bash.bashrc should be modified.

User-specific configuration affects only a single user. Dotfiles are used for user-specific configuration. For example, the file ~/.bashrc is the user-specific configuration file. The idea is that each user can define their own settings, such as aliases, functions and other interactive features like the prompt, without affecting other users' preferences.

Bash and other Bourne-compatible shells, such as Zsh, also source files depending on whether the shell is a login shell or an interactive shell. See Bash#Configuration files and Zsh#Startup/Shutdown files for details.

Some code blocks may contain so-called pseudo-variables, which, as the name says, are not actual variables used in the code. Instead they are generic placeholders and have to be manually replaced with system-specific configuration items before the code may be run or parsed. Common shells such as bash and zsh provide tab-completion to auto-complete parameters for common commands such as systemctl.

In the articles that comply with Help:Style/Formatting and punctuation, pseudo-variables are formatted in italics. For example:

In this case interface_name is used as a pseudo-variable placeholder in a systemd template unit. All systemd template units, identifiable by the @ sign, require a system-specific configuration item as argument. See systemd#Using units.

In this case the pseudo-variables are used to describe the parameters that must be substituted for them. Details on how to gather them are elaborated on in the section Securely wipe disk#Calculate blocks to wipe manually, which features the command.

This article or section needs expansion.

In case of file examples, pasting pseudo-variables in real configuration files might break the programs that use them.

In most cases, ellipses (...) are not part of the actual file content or code output, and instead represent omitted or optional text that is not relevant for the discussed subject.

For example HOOKS=(... encrypt ... filesystems ...) or:

Be aware though that, in a few instances, ellipses may be a meaningful part of the code syntax: attentive users should be able to recognize these cases by the context.

**Examples:**

Example 1 (unknown):
```unknown
# mkinitcpio -p linux
```

Example 2 (unknown):
```unknown
$ makepkg -s
```

Example 3 (unknown):
```unknown
sudo command
```

Example 4 (unknown):
```unknown
[postgres]$ initdb -D /var/lib/postgres/data
```

---

## Getting involved

**URL:** https://wiki.archlinux.org/title/Get_involved

**Contents:**
- Community
  - Post on the forums
  - Improve this wiki
  - Join the chatroom
  - Join the mailing lists
  - Artwork
- Packages
  - Report installed packages
  - Fix and report bugs
  - Inform about security issues

In evolutionary biology, cooperation describes interactions where an individual pays a small cost to yield a larger benefit to one or more others. If this costly contribution is reciprocated, everyone involved can benefit tremendously. This principle also applies to proactive members of the Arch community wanting to get involved and contribute to their favorite Linux distribution. Their participation benefits not only the community member and their fellow Archers, but all users of free and open source software.

This article describes how both new and experienced Arch users can contribute to the community. Note that this is not an exhaustive list. Before contributing, please get accustomed with the Code of conduct.

One of the easiest ways to get involved is participating in the Arch Linux Forums, which allows getting to know the community and help new users.

ArchWiki is a collaboratively maintained Arch Linux documentation. All users are encouraged to contribute.

You can help other users solve problems on the Arch IRC channels. It is of vital importance however, that you read the channel rules before participating. Further channels are available for specific topics.

Join the discussion on one or more of the public mailing lists. Make sure to stay on topic as provided in the list description.

Feel free to share wallpapers, splash screens, color palettes, widgets, themes, etc. with the community on the art subforum.

See also Arch Linux Art and Artwork.

pkgstats provides a systemd timer that sends a list of the packages installed on your system, along with the architecture and the mirrors you use, to the Arch Linux developers in order to help them prioritize their efforts and make the distribution even better. The information is sent anonymously and cannot be used to identify you. You can view the collected data at the Statistics page. More information is available in this forum thread.

Reporting and fixing bugs for Arch packages on GitLab is one of the possible ways to help the community.

However, ineffective use can be counter-productive. Please read the Bug reporting guidelines and General guidelines#Packaging merge requests.

New vulnerabilities are found all the time. Help the Arch Security Team keep track of new vulnerabilities.

Packages on the testing repositories need to be tried out and signed off before they are promoted to the main repositories. Help the Arch Testing Team test new packages.

Request features for Arch packages on GitLab. Before doing so, read Bug reporting guidelines#Bug or feature?, to make sure it is a valid feature request, and General guidelines#Packaging merge requests.

Request features for AUR packages on the corresponding AUR package pages.

The Arch User Repository contains community-made package scripts, allowing users to easily install software not part of the official repositories. Popular packages get included into the extra repository.

If you want to help maintain the AUR and packages in the extra repository, you can apply to become a Package Maintainer. See Package Maintainers#How do I become a Package Maintainer? for details.

If you wish to contribute to a worldwide spread mirror network and help deliver package updates to users, you can set up a mirror server and apply by following the DeveloperWiki:NewMirrors guidelines.

While Arch Linux only supports x86_64 as underlying processor architecture, the acceptance of RFC0032: Arch Linux Ports has opened the possibility for other architectures to eventually become supported. If this is something you want to help out with, take a look at the related projects on GitLab, and join the #archlinux-ports IRC channel and the arch-ports mailing list to participate in discussions.

There are regular events open to the community for bugfixing, cleanup, and other activities.

The Arch Linux distribution comprises of many components and each of the projects can be contributed to individually.

Discussion around the various projects (unless noted otherwise) takes place on the arch-projects mailing list and in the #archlinux-projects IRC channel on the Libera Chat network.

You can find out how to help sustaining server costs on the official Arch Linux donate page.

Arch's community maintains many projects. Feel free to include yours!

Arch-specific groups that you can engage in.

Community-developed software that focuses on Arch Linux.

The main motivation for your work on Arch should be helping the whole community, and not simply trying to become an Arch developer by any means.

Usually, new developers are picked by the existing developers as the workload increases. Sometimes they post a position and you can apply to fill it, but more often, they just invite somebody they know would be good at it and would fit in well with the rest of the team. Having a portfolio of Arch contributions is the best way to make it on the team.

Here is a list of things that you may do in order to gain some "popularity" towards Arch's developers:

---

## Getting involved

**URL:** https://wiki.archlinux.org/title/Getting_involved

**Contents:**
- Community
  - Post on the forums
  - Improve this wiki
  - Join the chatroom
  - Join the mailing lists
  - Artwork
- Packages
  - Report installed packages
  - Fix and report bugs
  - Inform about security issues

In evolutionary biology, cooperation describes interactions where an individual pays a small cost to yield a larger benefit to one or more others. If this costly contribution is reciprocated, everyone involved can benefit tremendously. This principle also applies to proactive members of the Arch community wanting to get involved and contribute to their favorite Linux distribution. Their participation benefits not only the community member and their fellow Archers, but all users of free and open source software.

This article describes how both new and experienced Arch users can contribute to the community. Note that this is not an exhaustive list. Before contributing, please get accustomed with the Code of conduct.

One of the easiest ways to get involved is participating in the Arch Linux Forums, which allows getting to know the community and help new users.

ArchWiki is a collaboratively maintained Arch Linux documentation. All users are encouraged to contribute.

You can help other users solve problems on the Arch IRC channels. It is of vital importance however, that you read the channel rules before participating. Further channels are available for specific topics.

Join the discussion on one or more of the public mailing lists. Make sure to stay on topic as provided in the list description.

Feel free to share wallpapers, splash screens, color palettes, widgets, themes, etc. with the community on the art subforum.

See also Arch Linux Art and Artwork.

pkgstats provides a systemd timer that sends a list of the packages installed on your system, along with the architecture and the mirrors you use, to the Arch Linux developers in order to help them prioritize their efforts and make the distribution even better. The information is sent anonymously and cannot be used to identify you. You can view the collected data at the Statistics page. More information is available in this forum thread.

Reporting and fixing bugs for Arch packages on GitLab is one of the possible ways to help the community.

However, ineffective use can be counter-productive. Please read the Bug reporting guidelines and General guidelines#Packaging merge requests.

New vulnerabilities are found all the time. Help the Arch Security Team keep track of new vulnerabilities.

Packages on the testing repositories need to be tried out and signed off before they are promoted to the main repositories. Help the Arch Testing Team test new packages.

Request features for Arch packages on GitLab. Before doing so, read Bug reporting guidelines#Bug or feature?, to make sure it is a valid feature request, and General guidelines#Packaging merge requests.

Request features for AUR packages on the corresponding AUR package pages.

The Arch User Repository contains community-made package scripts, allowing users to easily install software not part of the official repositories. Popular packages get included into the extra repository.

If you want to help maintain the AUR and packages in the extra repository, you can apply to become a Package Maintainer. See Package Maintainers#How do I become a Package Maintainer? for details.

If you wish to contribute to a worldwide spread mirror network and help deliver package updates to users, you can set up a mirror server and apply by following the DeveloperWiki:NewMirrors guidelines.

While Arch Linux only supports x86_64 as underlying processor architecture, the acceptance of RFC0032: Arch Linux Ports has opened the possibility for other architectures to eventually become supported. If this is something you want to help out with, take a look at the related projects on GitLab, and join the #archlinux-ports IRC channel and the arch-ports mailing list to participate in discussions.

There are regular events open to the community for bugfixing, cleanup, and other activities.

The Arch Linux distribution comprises of many components and each of the projects can be contributed to individually.

Discussion around the various projects (unless noted otherwise) takes place on the arch-projects mailing list and in the #archlinux-projects IRC channel on the Libera Chat network.

You can find out how to help sustaining server costs on the official Arch Linux donate page.

Arch's community maintains many projects. Feel free to include yours!

Arch-specific groups that you can engage in.

Community-developed software that focuses on Arch Linux.

The main motivation for your work on Arch should be helping the whole community, and not simply trying to become an Arch developer by any means.

Usually, new developers are picked by the existing developers as the workload increases. Sometimes they post a position and you can apply to fill it, but more often, they just invite somebody they know would be good at it and would fit in well with the rest of the team. Having a portfolio of Arch contributions is the best way to make it on the team.

Here is a list of things that you may do in order to gain some "popularity" towards Arch's developers:

---

## Kernel mode setting

**URL:** https://wiki.archlinux.org/title/KMS

**Contents:**
- Background
- Configuration
  - Late KMS start
  - Early KMS start
    - mkinitcpio
    - Booster
    - Dracut
- Troubleshooting
  - My fonts are too tiny
- Forcing modes and EDID

This article or section needs expansion.

Kernel Mode Setting (KMS) is a method for setting display resolution and depth in the kernel space rather than user space.

The Linux kernel's implementation of KMS enables native resolution in the framebuffer and allows for instant console (tty) switching. KMS also enables newer technologies (such as DRI2) which will help reduce artifacts and increase 3D performance, even kernel space power-saving.

Previously, setting up the video card was the job of the X server. Because of this, it was not easily possible to have fancy graphics in virtual consoles. Also, each time a switch from X to a virtual console was made (Ctrl+Alt+F2), the server had to give control over the video card to the kernel, which was slow and caused flickering. The same "painful" process happened when the control was given back to the X server (Alt+F7 when X runs in VT7).

With Kernel Mode Setting (KMS), the kernel is now able to set the mode of the video card. This makes fancy graphics during bootup, virtual console and X fast switching possible, among other things.

At first, note that for any method you use, you should always disable:

This article or section is out of date.

This article or section needs expansion.

Intel, Nouveau, ATI and AMDGPU drivers already enable KMS automatically for all chipsets, so you do not need to do anything.

The proprietary NVIDIA driver supports KMS (since 364.12), which has to be manually enabled.

KMS is typically initialized after the initramfs stage. However, it is possible to enable KMS already during the initramfs stage. Add the required module for the video driver to the initramfs configuration file:

Initramfs configuration instructions are slightly different depending on the initramfs generator you use.

For in-tree modules, make sure kms is included in the HOOKS array in /etc/mkinitcpio.conf (this is the default since mkinitcpio v33).

For out-of-tree modules, place the module names in the MODULES array. For example, to enable early KMS for the NVIDIA graphics driver:

If you are using the #Forcing modes and EDID method, you should embed the custom file into initramfs as well:

Then regenerate the initramfs.

If you use Booster, you can load required modules with this config change:

If you are using the #Forcing modes and EDID method, you should embed the custom file into your booster images as well:

Then regenerate the booster images.

Dracut enables early loading (at the initramfs stage, via modprobe) through its --force_drivers command or force_drivers+="" config entry line. For example:

See Linux console#Fonts for how to change your console font to a large font. The Terminus font (terminus-font) is available in many sizes, such as ter-132b which is larger.

Alternatively, disabling modesetting might switch to lower resolution and make fonts appear larger.

If your native resolution is not automatically configured or no display at all is detected, then your monitor might send none or just a skewed EDID file. The kernel will try to catch this case and will set one of the most typical resolutions.

In case you have the EDID file for your monitor, you merely need to explicitly enforce it (see below). However, most often one does not have direct access to a sane file and it is necessary to either extract an existing one and fix it or to generate a new one.

Generating new EDID binaries for various resolutions and configurations is possible during kernel compilation by following the upstream documentation (also see here for a short guide). Other solutions are outlined in details in this article. Extracting an existing one is in most cases easier, e.g. if your monitor works fine under Windows, you might have luck extracting the EDID from the corresponding driver, or if a similar monitor works which has the same settings, you may use get-edid(1) from the read-edid package. You can also try looking in /sys/class/drm/*/edid.

After having prepared your EDID, place it in a directory, e.g. called edid under /usr/lib/firmware/ and copy your binary into it.

To load it at boot, specify the following in the kernel command line:

In order to apply it only to a specific connector, use:

If you want to set multiple edid files, use:

If you are doing early KMS, you must include the custom EDID file in the initramfs, otherwise you will run into problems.

The value of the drm.edid_firmware parameter may also be altered after boot by writing to /sys/module/drm/parameters/edid_firmware:

This will only take effect for newly plugged in displays, already plugged-in screens will continue to use their existing EDID settings. For external displays, replugging them is sufficient to see the effect however.

To load an EDID after boot, you can use debugfs instead of a kernel command line parameter if the kernel is not in lockdown mode. This is very useful if you swap the monitors on a connector or just for testing. Once you have an EDID file as above, run:

If your monitor supports hotplugging, you can also trigger a hotplug to make the monitor use the new EDID you just loaded (e.g. into edid_override), so you don't have to physically replug the monitor nor reboot:

From the nouveau wiki:

You can override the modes of several outputs using video= several times, for instance, to force DVI to 1024x768 at 85 Hz and TV-out off:

To get the name and current status of connectors, you can use the following shell oneliner:

You may want to disable KMS for various reasons. To disable KMS, add nomodeset as a kernel parameter. See Kernel parameters for more info.

Along with the nomodeset kernel parameter, for an Intel graphics card, you need to add i915.modeset=0, and for an Nvidia graphics card, you need to add nouveau.modeset=0. For Nvidia Optimus dual-graphics system, you need to add all the three kernel parameters (i.e. "nomodeset i915.modeset=0 nouveau.modeset=0").

**Examples:**

Example 1 (unknown):
```unknown
Ctrl+Alt+F2
```

Example 2 (unknown):
```unknown
nvidia nvidia_modeset nvidia_uvm nvidia_drm
```

Example 3 (unknown):
```unknown
<video><model type='type'>
```

Example 4 (unknown):
```unknown
/etc/mkinitcpio.conf
```

---

## Default applications

**URL:** https://wiki.archlinux.org/title/Resource_opener

**Contents:**
- Background information
- Resource openers
  - xdg-open
  - perl-file-mimeinfo
  - mimeo
  - handlr
  - clifm
  - Minimalist replacements
    - run-mailcap

This article or section needs expansion.

Programs implement default application associations in different ways. While command-line programs traditionally use environment variables, graphical applications tend to use XDG MIME Applications through either the GIO API, the Qt API, or by executing /usr/bin/xdg-open, which is part of xdg-utils. Because xdg-open and XDG MIME Applications are quite complex, various alternative resource openers were developed. The following table lists example applications for each method.

Many desktop environments and graphical file managers provide a GUI for configuring default applications.

Programs sometimes need to open a file or a URI in the user's preferred application. To open a file in the user's preferred application the filetype needs to be detected (usually using filename extensions or magic numbers mapped to MIME types) and there needs to be an application associated with the filetype.

Heirloom UNIX programs used mime.types for MIME type detection and mailcap for application association.

xdg-open (part of xdg-utils) implements XDG MIME Applications and is used by many programs.

Because of the complexity of the xdg-utils version of xdg-open, it can be difficult to debug when the wrong default application is being opened. Because of this, there are many alternatives that attempt to improve upon it. Several of these alternatives replace the /usr/bin/xdg-open executable, thus changing the default application behavior of most applications. Others simply provide an alternative method of choosing default applications.

perl-file-mimeinfo provides the tools mimeopen(1p) and mimetype(1p). These have a slightly nicer interface than their xdg-utils equivalents:

Most importantly, xdg-utils programs will actually call file instead of mimetype for MIME type detection if it does not detect your desktop environment. This is important because file does not follow the XDG standard.

mimeoAUR provides the tool mimeo, which unifies the functionality of xdg-open and xdg-mime.

In the following example we see how to associate SVG files with Inkscape:

One can also find the path to the mimeapps.list file:

However a big difference with xdg-utils is that mimeo also supports custom "association files" that allow for more complex associations. For example, passing specific command line arguments based on a regular expression match:

xdg-utils-mimeoAUR patches xdg-utils so that xdg-open falls back to mimeo if no desktop environment is detected.

handlr-regex, written in Rust, provides the functionality of xdg-open and xdg-mime with a streamlined interface. handlr-regex is a fork of handlrAUR with regex support.

Compared to xdg-utils, it includes:

To use handlr as a replacement for xdg-open, shadow it with following script:

Lira, clifm's built-in resource opener, can be used as a standalone resource opener via the --open command line option. The configuration file (~/.config/clifm/profiles/PROFILE_NAME/mimelist.clifm) supports regular expressions for both MIME types and file names (or file extensions). A few examples:

The following packages conflict with and provide xdg-utils because they provide their own /usr/bin/xdg-open script.

If you want to use one of these resource openers while still being able to use xdg-utils, install them manually in a PATH directory before /usr/bin.

**Examples:**

Example 1 (unknown):
```unknown
/usr/bin/xdg-open
```

Example 2 (unknown):
```unknown
gio mime mimetype
```

Example 3 (unknown):
```unknown
/usr/bin/xdg-open
```

Example 4 (unknown):
```unknown
xdg-mime query default  mimetype
```

---

## PekWM

**URL:** https://wiki.archlinux.org/title/PekWM

**Contents:**
- Installation
- Starting
- Configuring PekWM
  - Menus
    - MenuMaker
    - Manually
  - Hotkeys
  - Mouse
  - Autostart
  - Variables

pekwm is a X window manager written by Claes Nsten. The code is based on the aewm++ window manager, but it has evolved enough that it no longer resembles aewm++ at all. It also has an expanded feature-set, including window grouping (not unlike to pwm, or even fluxbox), auto properties, xinerama and keygrabber that supports keychains, and much more.

Install the pekwm package.

Run pekwm with xinit.

The main configuration file is stored in the file ~/.pekwm/config. It controls the workspace and viewports settings, the menu and harbour behaviour, window edge resistance, and more. There is an example file with complete documentation in the pekwm documentation.

PekWM comes with pre-created menus by default stored in ~/.pekwm/menu. These do not reflect an existing system and as such are likely to be inaccurate. These should be seen as an example only.

One way to automatically set up menus for your installed applications is menumaker. To set up menus of all your installed applications run it with the following command:

To see a full list of options, run mmaker --help

Now you can modify the menu file by hand, or simply regenerate the list whenever you install new software.

The menu file is ~/.pekwm/menu. The syntax is fairly straightforward; a simple entry has the following structure:

A submenu has the following syntax:

To add a separator line to the menu, use the following:

PekWM also supports dynamic menus. These are menu entries or submenus that display the output of a run script every time the entry or submenu is accessed. Check the exact syntax each menu requires, as they may vary.

You can find dynamic menus for Gmail and network connections, and one to display the time and date.

There used to be a project called pekwm_menu_tools which aimed to be a set of useful applications for generating dynamic menus for pekwm.

The hotkey settings are stored in ~/.pekwm/keys. This file controls all the keyboard bindings and keychains used in PekWM. You can add keyboard bindings to launch programs or to perform actions in PekWM, such as show a menu, move a window, switch desktops, etc. For a full list of pekwm's actions, see the documentation.

You can have more than one action assigned to one key combination. To do so, just separate the actions by a semicolon. Here is an example:

When you press Ctrl+Alt+R Pekwm will display on the screen the text 'Reconfiguring' (osdctl -s 'Reconfiguring') and reconfigure (Reload). (Note that this requires osdsh to be installed)

The next example will bind a media key to lower the volume:

You can also do "chains" of keys, so for example the code

Would make it so that if you first press Ctrl+Alt+c and then q you move the active window to the top left corner of the screen, and if you press Ctrl+Alt+c and then w you move the window to the top center edge.

The Mouse settings are stored in ~/.pekwm/mouse. This file is also rather self-explanatory in its layout. For example:

means that if you release button 1 (usually left mouse button) over the frame title of a window the window will be "Raised" above the other windows and it will become the focused window.

One of the things PekWM is set up to do by default is to focus windows when the mouse moves over them (as opposed to the "click to focus" style). This is one thing that quite a few users would like to change to the more "traditional" way. To change this, look for the following lines in the file and do what they say (there are quite a few of the first, but only one occurrence of the second):

The autostart file is ~/.pekwm/start. If you would like to display a wallpaper or launch a panel whenever Pekwm is started, you can add entries for these things in that file. Note, though, that these applications are run every time Pekwm is started -- including when you run Restart in the root menu. The commands are executed only after Pekwm is started.

To add an application, use the following structure:

The & at the end is crucial, or anything after it will not be run. Here is an example:

Before you can use this file, you will have to make it executable.

The Variables file contains the general variables used in PekWM, the default entry should explain it quite clearly:

Whenever the variable $TERM is used in any of PekWM's configuration files, the command xterm -fn fixed +sb -bg white -fg black will be run. For example changing it to:

would mean that urxvt would be loaded for terminal commands.

If you would like certain applications to open on certain workspaces, have a certain title, skip the (window) menus, or be automatically tabbed together, you can specify all that here. It is probably the most confusing configuration file in PekWM, but it is also the most powerful file. The amount of things that can be set in this file are far too great to fit here, but it is explained in detail in the autoproperties page of the documentation. The default ~/.pekwm/autoproperties file also contains a crash course to autopropping.

A collection of themes for pekwm is available on the official homepage [1], a more comprehensive list of themes is available at Box-Look.org but they are not always verified against the current version of pekwm.

To install a theme, extract the theme archive to one of the theme paths:

With the release pekwm 0.2.0 a background setting application is included named pekwm_bg.

Setting a scaled background image:

Setting 3 horizontal lines:

pekwm 0.1.X releases did not come with a background setting application and required you to use a separate program to set a desktop wallpaper. See List of applications/Other#Wallpaper setters.

Edit ~/.pekwm/config and look for the line:

Try setting the environment variable GDK_CORE_DEVICE_EVENTS. See pekwm issue #4.

**Examples:**

Example 1 (unknown):
```unknown
~/.pekwm/config
```

Example 2 (unknown):
```unknown
~/.pekwm/menu
```

Example 3 (unknown):
```unknown
mmaker --no-desktop pekwm
```

Example 4 (unknown):
```unknown
mmaker --help
```

---

## Uniform look for Qt and GTK applications

**URL:** https://wiki.archlinux.org/title/Uniform_look_for_Qt_and_GTK_applications

**Contents:**
- Overview
- Styles for both Qt and GTK
  - Themes originally Qt based for GTK programs
    - Breeze
  - Themes originally GTK based for Qt programs
    - Adwaita
    - GTK themes ported to Kvantum
      - Theme configuration
- Theme engines
  - QGtk3Style

Qt and GTK based programs both use a different widget toolkit to render the graphical user interface. Each come with different themes, styles and icon sets by default, among other things, so the "look and feel" differ significantly. This article will help you make your Qt and GTK applications look similar for a more streamlined and integrated desktop experience.

To get a similar look between the toolkits, you will most likely have to modify the following:

You can choose various approaches:

There are widget style sets available for the purpose of integration, where builds are written and provided for both Qt and GTK, all major versions included. With these, you can have one look for all applications regardless of the toolkit they had been written with.

These are themes originally created for a Qt environment, but was later ported to GTK.

Breeze is the default Qt style of KDE Plasma. It can be installed with the breeze package and the breeze-gtk package for GTK 2 and GTK 3.

Once installed, you can use one of the many GTK configuration tools to change the GTK theme.

If running KDE Plasma, install kde-gtk-config, log-out and log-in again, and then go to System Settings > Colors & Themes > Application Style > Configure GNOME/GTK Application Style. Fonts, icon themes, cursors, and widget styles set in System Settings affect GTK settings automatically; only the GTK theme should be set manually using the previously mentioned module.

If you are not running KDE Plasma, you can use qt5ct-kdeAUR and qt6ct-kdeAUR to apply breeze to qt applications, you can also change the color scheme to be Breeze-Dark.

These are themes originally created for a GTK environment, but was later ported to Qt.

This article or section is out of date.

Adwaita is the default GNOME theme. The GTK 3 version is included in the gtk3 package, while the GTK 2 version is in gnome-themes-extra-gtk2AUR. adwaita-qt is a Qt port of the Adwaita theme. Unlike #QGtkStyle, which mimics the GTK 2 theme, it provides a native Qt style made to look like the GTK 3 Adwaita. It can be installed with the adwaita-qt5-gitAUR and adwaita-qt6-gitAUR packages for the Qt 5 and 6 respectively.

To set the Qt style, set the following environment variable: QT_STYLE_OVERRIDE=adwaita. Alternatively, use qt5ct package. For more detailed instructions, see Qt#Configuration of Qt 5/6 applications under environments other than KDE Plasma.

Kvantum (kvantum) is an SVG-based style customizer for Qt6 that comes with a variety of built-in styles, including versions of some of popular GTK themes such as Adapta, Arc, Ambiance, Libadwaita and Materia. More themes can be found on the KDE Store. For Qt5 you additionally need the kvantum-qt5 package.

Kvantum works as a Qt style instead of a Qt platform theme. To set Kvantum for all Qt applications, set it in qt6ct for Qt6 or qt5ct for Qt5 respectively, or use the environment variable QT_STYLE_OVERRIDE=kvantum.

To configure a theme variant for Kvantum, such as KvLibadwaita, edit the configuration file:

or use the kvantummanager GUI.

A theme engine can be thought of as a thin layer API which translates themes (excluding icons) between one or more toolkits. These engines add some extra code in the process and it is arguable that this kind of a solution is not as elegant and optimal as using native styles.

This is a platform theme built into qt5-base starting with version 5.7.0 [2] and qt6-base. It can be used to style Qt5 and Qt6 applications according to current GTK3 style. It can be enabled by setting the following environment variable: QT_QPA_PLATFORMTHEME=gtk3. For users of Adwaita it can be used together with #QAdwaitaDecorations for a complete look.

This Qt style uses GTK 2 to render all components to blend in with GNOME and similar GTK based environments. Beginning with Qt 4.5, this style is included in Qt. It requires gtk2AUR to be installed and configured.

This is the default Qt4 style in Cinnamon, GNOME and Xfce, and the default Qt5 style in Cinnamon, GNOME, MATE, LXDE and Xfce. In other environments:

For full uniformity, make sure that the configured GTK theme supports both GTK 2 and GTK 3. If your preferred theme has inconsistent rendering after configuring Qt to use GTK2, install gtk-theme-switch2AUR and choose a theme. You should also make sure that the preferred theme is installed in /usr/share/themes as $XDG_DATA_HOME/themes directory is not being scanned for active GTK 2 theme.

QAdwaitaDecorations is Qt decoration plugin implementing Adwaita-like client-side decorations for Wayland. It can be installed with the qadwaitadecorations-qt5AUR and qadwaitadecorations-qt6AUR packages. After installing, set QT_WAYLAND_DECORATION=adwaita to environment variable.

QWhiteSurGtkDecorations is Qt decoration plugin implementing WhiteSur-gtk-like client-side decorations for Wayland. It can be installed with the qwhitesurgtkdecorations-qt5AUR and qwhitesurgtkdecorations-qt6AUR packages. After installing, set QT_WAYLAND_DECORATION=whitesur-gtk to environment variable.

If you are running Plasma, run kde-gtk-config and select the icon-theme under System Settings > Application Style > GTK.

If you are using GNOME, run dconf-editor and change the icon-theme key under org > gnome > desktop > interface to your preferred icon theme.

If you are not using a desktop environment, for example if you are running a minimal system with i3-wm, install dconf-editor and set the icon-theme as explained above. You might also have to set the value of DESKTOP_SESSION in your profile. See Environment variables#Defining variables for the possible ways to obtain the desired result.

To have GNOME/GTK applications display with a KDE/Plasma title bar and frame, disable client-side decorations as described in GTK#Client-side decorations.

See Font configuration#LCD filter.

To have the same file dialog across applications in KDE Plasma, you can use XDG Desktop Portals.

Install xdg-desktop-portal and xdg-desktop-portal-kde as a first step.

Historically, setting the environment variable GTK_USE_PORTAL=1 was sufficient for most applications. Some newer GTK applications may use GDK_DEBUG=portals instead (see GTK NEWS), while others (like Betterbird 128.5.2esr-bb19, as noted in the discussion) still require the original variable. You may need to test which variable works with your specific applications, or use both to ensure maximum compatibility.

This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.

Since xdg-desktop-portal 1.18.0, direct configuration of the portal system is recommended.

This can be configured per-user or system-wide.

User-specific configuration

Create the configuration directory:

Append the following to ~/.config/xdg-desktop-portal/portals.conf:

System-wide configuration

As root, create the directory:

Create the file /etc/xdg/xdg-desktop-portal/portals.conf with the following content:

This ensures that the portal service uses the correct backend, regardless of the current desktop environment.

User-specific configuration

Create the override directory:

Append the following to ~/.config/systemd/user/xdg-desktop-portal.service.d/override.conf:

System-wide configuration

As root, create the directory:

Create the file /etc/systemd/user/xdg-desktop-portal.service.d/override.conf with the following content:

Reload the systemd user daemon and restart the portal service:

This configuration method is independent of the current desktop environment and is considered more robust against future changes than relying solely on environment variables.

Not all GTK applications support KDE file dialogs correctly:

There are still lots of GTK applications that do not implement portal properly (abandoned applications, or authors are focused on other tasks). To simplify file picking from such applications, you can at least synchronize bookmarks from dolphin to nautilus. Use this command:

Alternatively, use bookmarksync-gitAUR for that purpose. There you can manually edit and sync bookmarks to both sides.

In addition to the environment variable approach, some GTK applications have their own settings for forcing use of XDG Portals:

If the style or theme engine you set up is not showing in your GTK applications then it is likely your GTK settings files are not being loaded for some reason. You can check where your system expects to find these files by doing the following:

Usually the expected files should be ~/.gtkrc for GTK1 and ~/.gtkrc2.0 or ~/.gtkrc2.0-kde for GTK 2.x.

Try to run this to fix this issue:

To further integrate Plasma settings on GTK applications, one may want to install gnome-settings-daemon, gsettings-desktop-schemas and gsettings-qt5. This will offer proper Qt bindings for GTK.

When kde-gtk-config breaks and the "Application Style > GTK" menu is missing from System Settings, it is possible to choose GTK configuration tools like lxappearance and nwg-look to be able to configure GTK 2 and GTK 3 styles. lxappearance is desktop independent even if it comes from the LXDE project (it does not require other parts of the LXDE desktop). nwg-look is a GTK3 settings editor, designed to work properly in wlroots-based Wayland environment. The look and feel is strongly influenced by LXAppearance, but nwg-look is intended to free the user from a few inconveniences.

Check the section Mismatched folder view background colors for how to deal with weird coloring.

Follow XDG Desktop Portal#Force desktop environment with XDG_CURRENT_DESKTOP=KDE.

Be sure to have kvantum-qt5 installed.

**Examples:**

Example 1 (unknown):
```unknown
QT_STYLE_OVERRIDE=adwaita
```

Example 2 (unknown):
```unknown
QT_STYLE_OVERRIDE=kvantum
```

Example 3 (unknown):
```unknown
/etc/xdg/Kvantum/kvantum.kvconfig (or ~/.config/Kvantum/kvantum.kvconfig)
```

Example 4 (unknown):
```unknown
...
theme=KvLibadwaita
...
```

---

## fprint

**URL:** https://wiki.archlinux.org/title/Fprint

**Contents:**
- Prerequisites
- Installation
- Configuration
  - Login configuration
  - Create fingerprint signature
  - Restrict enrolling
- Troubleshooting
  - No devices available
  - gdm hangs when revealing login prompt after suspend
  - Unexpected error while suspending device

From the fprint homepage:

The idea is to use the built-in fingerprint reader in some notebooks for login using PAM. This article will also explain how to use regular password for backup login method (solely fingerprint scanner is not recommended due to numerous reasons).

You can check if your device is supported by checking the list of supported devices or the list of unsupported devices. To check which one you have, type:

The lsusb tool is available inside the usbutils package.

Install the fprintd package. imagemagick might also be needed.

Some devices require a different fork of libfprint, not (yet?) merged with the main libfprint:

This list is not exhaustive. See many more forks for other devices in the AUR.

Upstream recommends using S2Idle sleep instead of S3 but depending on your device, S3 might work as well.

Add pam_fprintd.so as sufficient to the top of the auth section of /etc/pam.d/system-local-login:

This tries to use fingerprint login first, and if it fails or if it finds no fingerprint signatures in the given user's home directory, it proceeds to password login.

Put this right before auth sufficient pam_fprintd.so to disallow fingerprint with sudo/su when it can't show a prompt.

You can also modify other files in /etc/pam.d/{login,su,sudo,gdm,lightdm} in the same way. For example /etc/pam.d/polkit-1 for polkit based authentication (GNOME & many other desktop environments). If /etc/pam.d/polkit-1 does not exist, copy it from /usr/lib/pam.d/polkit-1.

KDE already has fingerprint authentication configured in /etc/pam.d/kde-fingerprint so you do not need to edit that file. For a minimal working setup, which asks for your fingerprint first and requires password authentification if it fails on KDE Plasma, it suffices to change the following lines to /etc/pam.d/system-auth:

Adding pam_fprintd.so as sufficient to any configuration file in /etc/pam.d/ when a fingerprint signature is present will only prompt for fingerprint authentication. This prevents the use of a password if you cannot Ctrl+c fingerprint authentication (due to the lack of a shell). In order to use either a password or a fingerprint in a graphical interface, add the following line to the top of any files required:

This will prompt for a password; pressing Enter on a blank field will proceed to fingerprint authentication.

If you want to prompt for fingerprint and password input at the same time, you can use pam-fprint-grosshackAUR. This may be needed for some graphical programs which do not allow blank password input, such as Gnome's built-in polkit agent. To use this package, add the following lines to the top of any files required:

You will need to have an authentication agent running before being able to enroll.

To add a signature for a finger, run:

or create a new signature for all fingers:

You will be asked to scan the given finger. Swipe your right index finger five times. After that, the signature is created in /var/lib/fprint/.

You can also enroll without an authentication agent:

To verify the newly created fingerprint, use:

For more information, see fprintd(1).

This article or section is a candidate for merging with polkit.

By default every user is allowed to enroll new fingerprints without prompting for the password or the fingerprint. You can change this behavior using polkit rules.

There are two locations that contains the polkit configuration files:

In the following example only root can enroll fingerprints:

If your supported device cannot be found or is claimed to be already open (in use), check the fprintd.service logs in the journal.

You may find log entries like:

Ensure your device's firmware is up to date with Fwupd.

This issue is described in libfprint repository. The developers answer is:

The proposed fix is to create:

Or execute straight away:

Then it should not try to initialize the device.

This issue is described in libfprint repository:

After adding fingerprint rules to Linux PAM, fingerprint recognition only works for SDDM and sudo, but not for polkit.

Copy the 50-default.rules file to /etc/polkit-1/rules.d/ and change group name wheel to your user group.

Use the following environment variable and command line flag:

Create the following udev rule which enables USB device persistence, replacing the ID with the one for your fingerprint reader which you can find using lsusb:

Fprintd waits 30 seconds after a successful login before quitting, so sleeping during that time period may cause fprintd to break. If that happens, create and enable the following systemd service:

Some touch-based fingerprint readers generate images too small for fprint's algorithm to work properly. A common workaround for those is swiping instead of touching the sensor, but the speed at which a good image is generated may vary. Some sensors require a slower swipe and some a faster one. Here are some tips regarding what a good image should look like.

If you want to practice with different speeds to see which generates a better image, try dumping the images with the script examples/img-capture and comparing it with the examples from above (you will need to compile libfprint from source).

See also https://gitlab.freedesktop.org/libfprint/libfprint/-/issues/174

If you get an error like EnrollStart failed: GDBus.Error:net.reactivated.Fprint.Error.PermissionDenied: Not Authorized: net.reactivated.fprint.device.enroll, a simple workaround is to run the enroll command as root:

sudo fprintd-enroll "$(whoami)"

It is important to pass the name of a regular user, otherwise the fingerprint will be associated with the root user, which is counterproductive to most usecases.

If you get an "enroll-duplicate" error, that finger has already been enrolled as root. You can either enroll a different finger or remove that finger from the root user with fprintd-delete.

After this you should be able to run fprintd-verify as a regular user with no permission errors.

**Examples:**

Example 1 (unknown):
```unknown
pam_fprintd.so
```

Example 2 (unknown):
```unknown
/etc/pam.d/system-local-login
```

Example 3 (unknown):
```unknown
/etc/pam.d/system-local-login
```

Example 4 (unknown):
```unknown
auth      sufficient pam_fprintd.so
auth      include   system-login
...
```

---

## Repository

**URL:** https://wiki.archlinux.org/title/Repositories

Repository may refer to:

---

## Arch compared to other distributions

**URL:** https://wiki.archlinux.org/title/Arch_compared_to_other_distributions

**Contents:**
- Source-based
  - CRUX
  - LFS
  - Gentoo Linux
  - GNU Guix System
- General
  - Debian
  - Fedora
  - Slackware
- Beginner-friendly

This page attempts to draw a comparison between Arch Linux and other notable GNU/Linux distributions and UNIX-like operating systems. The summaries that follow are brief descriptions that may help a person decide if Arch Linux will suit their needs. Although reviews and descriptions can be useful, first-hand experience is invariably the best way to compare distributions.

For a more complete comparison, see Wikipedia:Comparison of operating systems and Wikipedia:Comparison of Linux distributions.

In all of the following, only Arch Linux is compared with other distributions. Community ports that support architectures other than x86_64 can be found listed among the Arch-based distributions.

Source-based distributions are highly portable, giving the advantage of controlling and compiling the entire OS and applications for a particular machine architecture and usage scheme, with the disadvantage of the time-consuming nature of source compilation. The Arch base and all packages are only compiled for the x86_64 architecture.

These distributions offer a broad range of advantages and strengths, and can be made to serve most operating system uses.

Sometimes called "newbie distros", the beginner-friendly distributions share a lot of similarities, though Arch is quite different from them. Arch may be a better choice if you want to learn about GNU/Linux by building up from a small base, as an installation of Arch installs few packages in comparison. Specific differences between distributions are described below.

openSUSE was born from the original SUSE Linux and is sponsored by SUSE (the makers of SUSE Enterprise Linux). SUSE Enterprise Linux Desktop (SLED) is based on openSUSE Tumbleweed and shares a common codebase with openSUSE Leap

Mandriva Linux (formerly Mandrake Linux) was created in 1998 with the goal of making GNU/Linux easy to use for everyone.

Arch takes a simpler approach than Mandriva or Mageia, being text-based and relying on more manual configuration, and is aimed at intermediate to advanced users.

**Examples:**

Example 1 (unknown):
```unknown
/gnu/store/
```

---

## General purpose mouse

**URL:** https://wiki.archlinux.org/title/General_purpose_mouse

**Contents:**
- Installation
- Configuration
  - QEMU or VirtualBox
- See also

GPM, short for General Purpose Mouse, is a daemon that provides mouse support for Linux virtual consoles.

Install the gpm package. For touchpad support on a laptop, please see Laptop#Touchpad.

The -m parameter precedes the declaration of the mouse to be used. The -t parameter precedes the type of mouse. To get a list of available types for the -t option, run gpm with -t help.

The gpm package needs to be started with a few parameters. These parameters can be recorded by creating the file /etc/conf.d/gpm, or used when running gpm directly. The gpm.service includes the parameters for USB mice (ExecStart=/usr/bin/gpm -m /dev/input/mice -t imps2).

Obviously, it should be edited, preferably in a systemd friendly manner, if there is another mouse type, and the service is used.

Once a suitable configuration has been found, start and enable the gpm.service.

For more information see gpm(8).

The default mouse emulated by QEMU and VirtualBox has severe problems in both gpm and x with positioning and clicking. The position becomes unsynchronized with the host, so there are areas that cannot be hovered over without repeatedly exiting and re-entering the window. Clicks register in a different location than the cursor was showing at.

Both QEMU and VirtualBox solve this problem by providing emulation for a USB tablet, which gives absolute positioning. (libvirt uses this automatically.)

However, the gpm only knows how to use the emulated mouse in relative positioning mode, so these problems remain. Attempting to use other types via -t fail to get it working properly.

gpm-vmAUR includes a several year old pull request to add USB tablet support for VirtualBox (which also works under QEMU) and modifies the gpm.service file to use it by default.

You may need to change which event is used. (Giving gpm the original -m /dev/input/mice will not work.) By default:

You can determine the event to use by installing evtest and running:

If you need to give gpm additional options, you can set additional_args in /etc/gpm-vm.conf.

Once a suitable configuration has been found, start and enable the gpm.service.

**Examples:**

Example 1 (unknown):
```unknown
# gpm -m /dev/input/mice -t help
```

Example 2 (unknown):
```unknown
/etc/conf.d/gpm
```

Example 3 (unknown):
```unknown
gpm.service
```

Example 4 (unknown):
```unknown
ExecStart=/usr/bin/gpm -m /dev/input/mice -t imps2
```

---

## mpv

**URL:** https://wiki.archlinux.org/title/Mpv

**Contents:**
- Installation
  - Front ends
- Configuration
  - General settings
    - Subtitle configurations
    - High quality configurations
    - Custom profiles
      - Automatic profiles
  - Key bindings
  - Additional configuration files

mpv is a media player based on MPlayer and the now unmaintained mplayer2. It supports a wide variety of video file formats, audio and video codecs, and subtitle types. A detailed (although admittedly incomplete) list of differences between mpv and the aforementioned players can be found here.

Install the mpv package.

See List of applications/Multimedia#mpv-based.

mpv comes with good all-around defaults that should work well on computers with weaker/older video cards. However, if you have a computer with a more modern video card, mpv allows you to do a great deal of configuration to achieve better video quality (limited only by the power of your video card). To do this, one only needs to create a few configuration files (as they do not exist by default).

To help you get started, mpv provides sample configuration files with default settings. Copy them to use as a starting point:

mpv.conf contains the majority of mpv's settings, input.conf contains key bindings. Read through both of them to get an idea of how they work and what options are available.

Add the following settings to ~/.config/mpv/mpv.conf.

Enable fuzzy searching:

Bold the subtitles to increase readability:

By default, mpv utilizes settings that balance quality and performance. Additionally, two predefined profiles are available: fast for maximum performance and high-quality for superior rendering quality. You can apply a specific profile using the --profile=name option and inspect its contents using --show-profile=name.

Live statistics showing how well mpv is performing can be brought up with the i key. It is very useful for making sure that your hardware can keep up with your configuration and for comparing different configurations.

These last two options are a little more complicated. video-sync=display-resample makes it so that if audio and video go out of sync, then instead of dropping video frames, it will resample the audio (a slight change in audio pitch is often less noticeable than dropped frames). The mpv wiki has an in depth article on it titled Display Synchronization. interpolation makes motion appear smoother on your display by changing the way that frames are shown so that the source framerate jives better with your display's refresh rate (not to be confused with SVP's technique which actually converts video to 60fps). The mpv wiki has an in depth article on it titled Interpolation though it is also commonly known as smoothmotion.

Beyond this, there is still a lot you can do, but things become more complicated, require more powerful video cards, and are in constant development. As a brief overview, it is possible to load special shaders that perform exotic scaling and sharpening techniques including some that actually use deep neural networks trained on images (for both real world and animated content). To learn more about this, take a look around the mpv wiki, particularly the user shader's section.

There are also plenty of other options you may find desirable as well. It is worthwhile taking a look at mpv(1). It is also helpful to run mpv from the command line to check for error messages about the config.

In mpv.conf it is possible to create profiles which are essentially just "groups of options" with which you can:

Creating a profile is easy. The area at the top of mpv.conf is called the top level, any options you write there will kick into effect once mpv is started. However, once you define a profile by writing its name in brackets, every option you write below it (until you define a new profile) is considered part of that profile. Here is an example mpv.conf:

There are only two lines within the top level area and there are two separate profiles defined below it. When mpv starts, it sees the first line, loads the options in myprofile2 (which means it loads the options in high-quality and log-file=~~/log) finally it loads ontop=yes and finishes starting up. Note, myprofile1 is never loaded because it is never called in the top level area.

Alternatively, one could call mpv from the command line with:

and it would ignore all options except the ones for myprofile1.

Certain types of profiles will be loaded automatically based on either the file extension or the protocol used.

These profiles will be loaded for all files with a matching file extension (for all .mkv and .gif files respectively):

This profile will be loaded automatically whenever any http or https streams are played (e.g. mpv https://example.com/video.mp4):

Run mpv --list-protocols to see the different protocols supported by mpv.

Key bindings are fairly straightforward given the examples in /usr/share/doc/mpv/input.conf and mpv(1)  COMMAND INTERFACE.

Add the following examples to ~/.config/mpv/input.conf:

For an attempt to reproduce MPC-HC key bindings in mpv, see [1].

In addition there are a few more configuration files and directories that can be created, among which:

See mpv(1)  FILES for information on other files and directories.

mpv has a large variety of scripts that extend the functionality of the player. To this end, it has internal bindings for both Lua and JavaScript.

Scripts are typically installed by putting them in the ~/.config/mpv/scripts/ directory (you may have to create it first). After that they will be automatically loaded when mpv starts (this behavior can be altered with other mpv options). Some scripts come with their own installation and configuration instructions, so make sure to have a look. In addition some scripts are old, broken, and unmaintained.

JavaScript (ES5 via MuJS) has been supported as an mpv scripting language since 2014. Currently only a few scripts are available, but documentation exists at mpv(1)  JAVASCRIPT for anyone interested in making their own.

To get started, drop a script with a .js extension in the mpv scripts directory, e.g.:

For more details, e.g. on using require to load CommonJS modules, see mpv(1)  CommonJS modules and require(id).

The development of mpv's Lua scripts is documented in mpv(1)  LUA SCRIPTING with examples in TOOLS/lua, which are installed to /usr/share/mpv/scripts.

For example, you can enable the builtin script to automatically crop videos with black bars:

mpv-ytdlautoformat is a Lua script to auto change ytdl-format for Youtube and Twitch or the domains you desire, to 480p or the quality you desire.

mpv-webm (or simply webm) is a very easy to use Lua script that allows one to create WebM files while watching videos. It includes several features and does not have any extra dependencies (relies entirely on mpv).

ytdl-preload is a Lua script to preload the next ytdl-link in your playlist.

The C plugin mpv-mpris allows other applications to integrate with mpv via the MPRIS protocol. For example, with mpv-mpris installed, kdeconnect can automatically pause video playback in mpv when a phone call arrives. Another example is buttons (play\pause etc) on bluetooth audio-devices.

To use the plugin, install mpv-mpris.

Vapoursynth is an alternative to AviSynth that can be used on Linux and allows for Video manipulation via python scripts. Vapoursynths python scripts can be used as video filters for mpv.

The mpv package now enables Vapoursynth support by default.

SmoothVideoProject SVP is a program that is primarily known for converting video to 60fps. It is free [as in beer] and full featured for 64bit Linux (costs money for Windows and OS X and is incompatible with 32bit Linux).

It has three main features and each one can be disabled/enabled as one chooses (you are not forced to use motion interpolation).

Once you have mpv compiled with Vapoursynth support, it is fairly easy to get SVP working with mpv. Simply install svp-binAUR, open the SVP program to let it assess your system performance (you may want to close other programs first so that it gets an accurate reading), and finally add the following mpv profile to your mpv.conf[3]:

Then, in order to use SVP, you must have the SVP program running in the background before opening the file using mpv with that profile. Either do:

or set profile=svp in the top-level portion of the mpv configuration.

If you want to use hardware decoding, you must use a copy-back decoder since normal decoders are not compatible with Vapoursynth (choose a hwdec option that ends in -copy). For instance:

Either way, hardware decoding is discouraged by mpv developers and is not likely to make a significant difference in performance.

See Hardware video acceleration.

Hardware accelerated video decoding is available via the --hwdec=API option. For a list of all supported APIs and other required options, see mpv(1)  hwdec.

To make it permanent (for example when playing videos from a desktop environment), add it to the configuration file:

To allow CPU processing with video filters, choose a *-copy API.

Use the keyboard shortcut Ctrl+h while a video is running to toggle hardware decoding.

To troubleshoot hardware acceleration, adjusting the logging levels (see mpv(1)  msg-level) may be necessary. For instance, --msg-level=vd=v,vo=v,vo/gpu/vaapi-egl=trace enables the following:

You can cycle between aspect ratios using Shift+a.

You can ignore the aspect ratio using --keepaspect=no. To make the option permanent, add the line keepaspect=no to the configuration file.

Run mpv with --wid=0. mpv will draw to the window with a window ID of 0.

To show the application window even for audio files when launching mpv from the command line, use the --force-window option. To make the option permanent, add the line force-window=yes to the configuration file.

To disable video output when launching from command line, use the --vid=no option, or its alias, --no-video.

Set volume-max=value in your configuration file to a reasonable amount, such as volume-max=150, which then allows you to increase your volume up to 150%, which is more than twice as loud. Increasing your volume too high will result in clipping artefacts. Additionally (or alternatively), you can utilize dynamic range compression with af=acompressor.

Run the following command to get a list of available audio output devices

Then add one to ~/.config/mpv/mpv.conf. For example:

To enable HD audio codecs like TrueHD and DTS-MA to passthrough to an AV receiver, add the following to ~/.config/mpv/mpv.conf

This article or section needs expansion.

Different sources may have different or inconsistent loudness, so mpv users may need to configure automatic volume normalization. For example:

This binds the key n to cycle the audio filter settings (af) through the specified values:

Audio filtering in mpv is provided by the FFmpeg backend. See Wikipedia:EBU R 128 and ffmpeg loudnorm filter for details.

See also upstream issues [5] and [6] which mention different options.

This blog post introduces the music.lua script, which shows how Lua scripts can be used to improve mpv as a music player.

By default, you can save the position and quit by pressing Shift+q. The shortcut can be changed by setting quit_watch_later in the key bindings configuration file.

To automatically save the current playback position on quit, start mpv with --save-position-on-quit, or add save-position-on-quit to the configuration file.

A playlist could simply be a list of files, see mpv(1)  playlist. To play a playlist and remember its position:

With the option --pause mpv will start in paused state and --reset-on-next-file=pause will reset the pause mode when switching to the next file.

mpv does not support DVD menus. To start the main stream with the longest title of a video DVD, use the command:

An optional title specifier is a number (starting at 0) which selects between separate video streams on the DVD:

DVDs which have been copied on to a local file system (by e.g. the dvdbackup tool) are accommodated by specifying the path to the local copy: --dvd-device=PATH.

See the following desktop file example for playing DVDs from a local file system:

By replacing the Exec line with

the mpv player will queue DVD title 0 to 9 in the playlist, which allows the user to play the titles consecutively or jump forward and backward in the DVD titles with the mpv GUI.

Install libdvdcss, to fix the error:

Since version 0.21.0, mpv has replaced the on-screen controls by a bottombar. In case you want on-screen controls back, you can edit the mpv configuration as described here.

The screenshot template option can include the precise timecode (HH:MM:SS.mmm) of the screenshoted frame. The meaningful filename makes it easy to know the origin of the screenshot. It can be set like this:

This expands to filename - [HH:MM:SS.mmm] (number).jpg. Example result:

A bonus is it sorts nicely because alphabetically, the timecode is sorted within the episode number.

See mpv(1)  screenshot-template for more information.

An example of creating a single screenshot, by using a start time (HH:MM:SS):

Screenshots will be saved in /path/to/screenshot.png.

If yt-dlp or youtube-dlAUR is installed, mpv can directly open a Twitch livestream.

Alternatively, see Streamlink#Twitch.

Another alternative based on Livestreamer is this Lua script: https://gist.github.com/ChrisK2/8701184fe3ea7701c9cc

The default --ytdl-format is bestvideo+bestaudio/best. For youtube videos that have 4K resolutions available, this may mean that your device will struggle to decode 4K VP9 encoded video in software even if the attached monitor is much lower resolution.

Setting the right youtube-dl format selectors can fix this easily though. In the following configuration example, only videos with a vertical resolution of 1080 pixels or less will be considered.

If you wish to avoid a certain codec altogether because you cannot hardware-decode it, you can add this to the format selector. For example, we can additionally choose to ignore VP9 as follows:

If you prefer best quality open codecs (VP9 and Opus), use:

To find and stream audio from your terminal emulator with yta search terms, put the following function in your .bashrc:

If youtube-dlAUR or yt-dlp is installed and KDE Plasma is being used, it is possible to create a custom action in the KDE clipboard to conveniently play links from video sharing sites.

Now, you can play video links from your clipboard in mpv by pressing Ctrl+Alt+r and selecting mpv from the context menu. You may need to go to Advanced Settings and remove Firefox from the section Disable Actions for Windows of Type WM_CLASS.

If you are having trouble with mpv's playback (or if it is flat out failing to run) then the first three things you should do are:

If mpv runs but it just does not run well then a fourth thing that might be worth taking a look at is the live statistics (with i) to see exactly how it is performing.

mpv defaults to using the OpenGL video output device setting on hardware that supports it. In cases such as trying to play video back on a 4K display using an Intel HD4XXX series card or similar, you will find video playback unreliable, jerky to the point of stopping entirely at times and with major tearing when using any OpenGL output setting. If you experience any of these issues, using the XV (Xorg only) video output device may help:

It is possible to increase playback performance even more (especially on lower hardware), but this decreases the video quality dramatically in most cases.

The following options may be considered to increase the video playback performance:

As described in mpv(1)  Window, mpv by default disables any active window compositor while in fullscreen mode. This is done to prevent potential performance issues during playback.

For window compositors such as KWin or Mutter, it can be advantageous to disable window compositing even while in windowed mode. This can be achieved by using setting the x11-bypass-compositor=yes option.

There are two disadvantages to disabling compositing:

To sidestep these issues, you can try keeping your compositor enabled with x11-bypass-compositor=no

Spin the mouse wheel over the volume icon.

mpv may not suspend GNOME's Power Saving Settings if using Wayland resulting in screen saver turning off the monitor during video playback. A workaround is to add gnome-session-inhibit to the beginning of the Exec= line in mpv.desktop.

In order to inhibit the screensaver only during playback, use mpv_inhibit_gnomeAUR. Alternatively, a mpv lua script based on gnome-session-inhibit may be used.

See GNOME/Troubleshooting#Cursor size or theme issues on Wayland.

While using VAAPI hardware acceleration on AMD GPUs on versions v0.34.1 and older, you may see a persistent error message saying Cannot load libcuda.so.1. This can be suppressed by setting gpu-hwdec-interop=vaapi.

Related bug reports: Github issue #9691, Github issue #8765

This issue has been fixed upstream in pull request #9842.

If mpv crashes or fails to play audio on systems where PipeWire is masked, reporting no outputs or broken pipe, set the --ao option to match your environment. Set it in mpv.conf for persistent configuration.

If mpv will not play a DVD from file in plain VIDEO_TS/VOB structure, there could be a problem with the restore playback position function. Try either cleaning .config/mpv/watch_later folder, or start mpv with the no-resume-playback option and/or set the save-position-on-quit=no option.

If video is stuttering with PulseAudio, try the pulse-latency-hacks option discussed at mpv(1)  --pulse-latency-hacks:

**Examples:**

Example 1 (unknown):
```unknown
~/.config/mpv/
```

Example 2 (unknown):
```unknown
XDG_CONFIG_HOME
```

Example 3 (unknown):
```unknown
$ cp -r /usr/share/doc/mpv/ ~/.config/
```

Example 4 (unknown):
```unknown
~/.config/mpv/mpv.conf
```

---

## Android

**URL:** https://wiki.archlinux.org/title/Android

**Contents:**
- Synchronization
  - All-in-one
  - Synchronized notifications
  - Transferring files
- App development
  - Android Studio
  - SDK packages
    - Android Emulator
    - Other SDK packages in the AUR
    - Making /opt/android-sdk group-writeable

Plug your phone to your computer using a USB cable adapted for your device. Make sure this USB cable has a data line (not all cables do).

On your smartphone, you should see the charging icon. If this is the case, go to your notifications, scroll to the bottom until you see the notification saying that your phone is connected in Charging mode.

Click on the notification, and then set it to File transfer mode, MTP or something similar.

You should now see your phone being detected by most desktop environments.

There are various applications to transfer files, synchronize notifications and more.

The officially supported way to build Android apps is to use #Android Studio.[1]

Android Studio is the official Android development environment based on IntelliJ IDEA. It provides integrated Android developer tools for development and debugging.

You can install it with the android-studioAUR package. For the Beta branch, install the android-studio-betaAUR package. For the Canary branch, install the android-studio-canaryAUR package.

Android Studio creates a .android directory in home directory. To reset Android Studio, this directory can be removed.

The Android Studio Setup Wizard installs the required #SDK packages and places the SDK by default in ~/Android/Sdk.

To build apps from the command-line (using e.g. ./gradlew assembleDebug) set the ANDROID_HOME environment variable to your SDK location.

Android Studio has experimental Wayland support since 2024.2, you can enable it by going to Menu -> Help -> Edit Custom VM Options and adding -Dawt.toolkit.name=WLToolkit as a parameter.

Android SDK packages can be installed directly from upstream using #Android Studio's SDK Manager or the sdkmanager command line tool (part of the Android SDK Tools). Some Android SDK packages are also available as AUR packages, they generally install to /opt/android-sdk/.

The required SDK packages are:

The android-tools package provides adb, #fastboot, e2fsdroid and mke2fs.android from the SDK Platform-Tools along with mkbootimg and ext2simg.

The Android Emulator is available as the emulator SDK package, the android-emulatorAUR package, and there is also a dummy package for it: android-emulator-dummyAUR.

To run the Android Emulator you need an Intel or ARM System Image. You can install them through the AUR, with the sdkmanager or using Android Studio's AVD Manager.

If Wayland is used, make sure to read Wayland#Qt, as it might be the case that the emulator complains about Wayland.

The Android Support Library is now available online from Google's Maven repository. You can also install it offline through the extras;android;m2repository SDK package (also available as android-support-repositoryAUR).

The factual accuracy of this article or section is disputed.

The AUR packages install the SDK in /opt/android-sdk/. This directory has root permissions, so keep in mind to run sdk manager as root. If you intend to use it as a regular user, create the android-sdk users group, add your user.

Set an access control list to let members of the newly created group write into the android-sdk folder. As running sdkmanager can also create new files, set the ACL as default ACL. the X in the default group entry means "allow execution if executable by the owner (or anyone else)"

Re-login or as <user> log your terminal in to the newly created group:

The AUR packages install the SDK in /opt/android-sdk/. This directory has root permissions, so to use it as your regular user, you should have another directory with write permissions.

To do so, you should use fuse-overlayfs, a FUSE package around overlayfs. This will allow to access read-only /opt/android-sdk/ directory and write modifications without copying the data (Copy-On-Write conception).

First, create directories to host the overlay:

Then mount your overlay and export the Android home variable:

You can now use any Android tool (`sdk-manager` for instance) with your copy-on-write setup.

You can unmount it as with any other FUSE fs:

Android Studio is the official Android development environment based on IntelliJ IDEA. Alternatively, you can use Netbeans with the NBAndroid-V2. All are described below.

If you prefer using Netbeans as your IDE and want to develop Android applications, use NBAndroid-V2 .

Install android-sdkAUR package and follow the instructions from the NBANDROID README.

It is possible to write flutter applications for Android and iOS using (Neo)vim like an IDE. Install coc using a Vim plugin manager. Also install the coc-flutter extension for autocompletion (like in Android Studio) and to load the code into an Android emulator.

To develop a mobile flutter application using Emacs, as the official instruction at flutter.dev suggests, install lsp-dart.

Marvin is a tool which helps beginners set up an Android development environment. Installing marvin_dscAUR helps you set up the following things: JDK, Android SDK, IDE(s), and AVD.

Please note that these instructions are based on the official AOSP build instructions. Other Android-derived systems such as LineageOS will often require extra steps.

To build AOSP 13 you need a TTF font installed (e.g. ttf-dejavu) and the dependencies of the aosp-develAUR metapackage.

Additionally, LineageOS (as well as other many Android distributions like ArrowOS,PixelExperience etc) requires the following dependencies of the lineageos-develAUR metapackage.

The required JDK version depends on the Android version you are building:

Set JAVA_HOME to avoid this requirement and match the Arch Linux installation path. Example:

Install the repo package.

Create a directory to build.

This will clone the repositories. You only need to do this the first time you build Android, or if you want to switch branches.

The -c switch will only sync the branch which is specified in the manifest, which in turn is determined by the branch specified with the -b switch, or the default branch set by the repository maintainer.

Wait a long time. Just the uncompiled source code, along with the .repo and .git directories that are used to keep track of it, are very large. As of Android 10, at least 250 GB of free disk space is required.

This should do what you need for AOSP:

If you run lunch without arguments, it will ask what build you want to create. Use -j with a number between one and two times number of cores/threads.

The build takes a very long time.

When finished, run/test the final image(s).

To create an image that can be flashed it is necessary to:

This will create a zip image under out/target/product/hammerhead (hammerhead being the device name) that can be flashed.

In some cases, you want to return to the stock Android after flashing custom ROMs to your Android mobile device. For flashing instructions of your device, please use XDA forums.

Fastboot (as well as ADB) is included in the android-tools package.

Samsung devices cannot be flashed using Fastboot tool. Alternatives are Heimdall and Odin (by using Windows and VirtualBox).

To download original Samsung firmware, a platform independent script, samloader can be used.

Heimdall is a cross-platform open-source tool suite used to flash firmware (also known as ROMs) onto Samsung mobile devices and is also known as an alternative to Odin. It can be installed as heimdall, however, it is no longer maintained: an actively maintained fork can be installed as heimdall-grimler-gitAUR.

The flashing instructions can be found on Heimdall's GitHub repository or on XDA forums.

It is also possible to restore firmware (Android) on the Samsung devices using Odin, but inside the VirtualBox.

Arch Linux (host) preparation:

Windows (guest) preparation:

Check if configuration is working:

which means that your device is visible to Odin & Windows operating system and is ready to be flashed.

There are several projects and methods which support running Android on Arch Linux (or other distributions). As listed below:

Make sure you have exported the variable ANDROID_HOME as explained in #Android Studio.

If you try to run an AVD (Android Virtual Device) under x86_64 Arch and get the error above, install the lib32-gcc-libs package from the multilib repository.

Most probably the debugger wants to step into the Java code. As the source code of Android does not come with the Android SDK, this leads to an error. The best solution is to use step filters to not jump into the Java source code. Step filters are not activated by default. To activate them: Window > Preferences > Java > Debug > Step Filtering. Consider to select them all. If appropriate you can add the android.* package. See Use Step Filters.

If that does not work, then try this:

Sometimes, beginning to load an AVD will cause an error message similar to this to be displayed, or the loading process will appear to finish but no AVD will load and no error message will be displayed.

The AVD loads an incorrect version of libstdc++, you can remove the folder libstdc++ from ~/.android-sdk/emulator/lib64 (for 64-bit) or ~/.android-sdk/emulator/lib (for 32-bit) , e.g.:

Note that in versions before Android Studio 3.0, this directory was in a different location:

Alternatively you can set and export ANDROID_EMULATOR_USE_SYSTEM_LIBS in ~/.profile as:

Reference: Android Studio user guide

Fix for the .desktop file might be achieved by using env command, prefixing the Exec line Desktop entries#Modify environment variables

Here is the full error:

You can try to install glxinfo (mesa-utils) but if your computer has enough power you could simply use software to render graphics. To do so, go to Tools > Android > AVD Manager, edit the AVD (click the pencil icon), then select Software - GLES 2.0 for Emulated Performance > Graphics.

In xfwm4, the vertical toolbar buttons window that is on the right of the emulator takes focus from the emulator and consumes keyboard events. (bug report)

You can use the workaround described in [2]:

When using Tiled Window Manager like dwm, Android Emulator will shake and blink. You can use the workaround described in krohnkite issue 72 (window floating is induced by Alt+f in dwm).

When using Nouveau drivers try to disable gpu hardware acceleration.

In some devices it can only be done by editing $HOME/.avd/device_name.avd/config.ini.[3]

There is an issue where no emulator-window shows up after starting a virtual device in android-studio. If this applies to you, launch the emulator from the console and inspect its output:

If on any line, it says anything similar to:

you may try disabling IPv6:

If this solves the issue and the virtual device shows up in android-studio, you may consider a permanent change:

If running Wayland, a graphical error like The emulator process for AVD Pixel_4_API_33 has terminated may be worked around by forcing X11 with QT_QPA_PLATFORM=xcb.

If you get the errors:

You might be able to solve it by restarting the adb server:

Alternatively, make sure you have installed the Android udev rules. See #Fastboot.

Since SDK packages from the AUR are installed to /opt/android-sdk which is owned by root, the sdkmanager --licenses command for accepting the licenses must also be ran as root. If this script is not ran as root, accepting the licenses will silently fail.

You can confirm the licenses were successfully accepted by checking if the license appears in the /opt/android-sdk/licenses directory.

This error arises on devices that require a vendor directory to be populated before breakfast will succeed, as noted in the warning section. Extract proprietary blobs and continue.

(hardware/qcom/sm7250/display/composer/../common.mk: error: "vendor.qti.hardware.display.composer-service (EXECUTABLES android-arm64) missing libthermalclient (SHARED_LIBRARIES andr oid-arm64)" You can set ALLOW_MISSING_DEPENDENCIES=true in your environment if this is intentional, but that may defer real problems until later in the build.)

You either have not extracted proprietary blobs, the image you extracted from did not have all of them, or the extraction script did not work correctly. Either find a different image to extract blobs from, use a different extraction script or use blobs from themuppets

e2fsprogs disabled orphan_file in version 1.47.0, so it either must be downgraded for the old command to succeed, or the option orphan_file should be removed from /etc/mke2fs.conf on the machine you're building on, since the bundled mke2fs reads the host's config file.

To downgrade: e2fsprogs 1.46.0 on arch linux archive

if you scroll up a bit, see: Sum of sizes in google_dynamic_partitions_partition_list is 4883337216, which is greater than google_dynamic_partitions_size (4873781248) [0224/122117.280262:FATAL:generate_delta_main.cc(619)] Check failed: payload_config.target.ValidateDynamicPartitionMetadata().

Some users had success with setting the option export WITH_GMS=true, GMS being Google Mobile Services, more commonly - GApps. This option may or may not be suitable to your needs. Some strongly discourage using this option even as a fix to this error.

Issue seems to come from changes being made, resulting in larger file sizes. To leave more space for the partitions, it is recommended to lower the parameter BOARD_SYSTEMIMAGE_PARTITION_RESERVED_SIZE := 17523507 by an amount at least equivalent to the difference between the numbers you get in the error. This option is set in a makefile, for example in lineage/device/google/blueline/BoardConfigLineage.mk or, if you don't see it there, in a file it includes, for example google devices have common parameters in device/google/redbull/BoardConfigLineage.mk

**Examples:**

Example 1 (unknown):
```unknown
_JAVA_AWT_WM_NONREPARENTING=1
```

Example 2 (unknown):
```unknown
~/Android/Sdk
```

Example 3 (unknown):
```unknown
./gradlew assembleDebug
```

Example 4 (unknown):
```unknown
Menu -> Help -> Edit Custom VM Options
```

---

## OpenPGP

**URL:** https://wiki.archlinux.org/title/OpenPGP

**Contents:**
- Software
  - Applications
  - Stateless OpenPGP (SOP)
  - Libraries
- Hardware security device support
- Standardization
- PGP Public Key Infrastructure
  - Keyserver
  - Web Key Directory
  - Explicit Authentication

OpenPGP is an open standard for cryptographic operations. It is a system based on well-understood cryptographic building blocks. OpenPGP supports the secure delivery of files and messages between a sender and a recipient. It also addresses identities and their verification.

A number of e-mail clients implement OpenPGP features, such as Thunderbird.

General-purpose OpenPGP commandline tools include gnupg and sequoia-sq.

Stateless OpenPGP defines a generic stateless command-line interface for dealing with OpenPGP messages, known as SOP. It aims for a minimal, well-structured API covering OpenPGP object security.

For many use-cases SOP offers all required functionality and can be used as a vendor-agnostic, stand-alone alternative to tools such as GnuPG (see the dedicated IETF draft for details).

Many SOP implementations exist and are cross-tested in an interoperability test suite.

A number of libraries exist for various programming languages.

OpenPGP private keys can be securely handled on specialized hardware devices. The OpenPGP card standard defines a smart card application for this purpose. This standard is implemented on many devices, including most models of Nitrokey and YubiKey.

Users can use these smartcards with software such as GnuPG or OpenPGP-card-tools.

The standardization of OpenPGP takes place in the context of the IETF OpenPGP working group.

The most recent and widely adopted IETF ratified standard for OpenPGP is RFC 9580. This standard defines formats of what is colloquially referred to as "OpenPGP version 4", as well as the new "OpenPGP version 6" format. RFC 9580 specifies the use of modern cryptographic mechanisms following best practices, including AEAD.

Future work will center around topics such as post-quantum cryptography (PQC) and forward secrecy (see Charter 04 of the IETF OpenPGP working group for details).

The OpenPGP ecosystem has developed several mechanisms to deal with public keys. This public key infrastructure (aka PGPKI) deals with two separate concerns:

To communicate with a peer, a copy of the peer's certificate is needed. Obtaining a copy can be achieved with one of the distribution models described in the following sections (see #Keyserver and #Web Key Directory).

An OpenPGP certificate contains identity claims. Since OpenPGP is a decentralized system with no central authorities, identity claims are issued by the certificate holder and can be independently verified. Depending on the threat model, different methods of identity verification (aka authentication) are appropriate.

For some low-risk purposes it may be acceptable to ignore authentication and rely on trust on first use (TOFU).

A slightly higher level of assurance of authenticity is achieved by relying on validating keyservers or Web Key Directory.

One of the highest levels of authenticity assurance can be achieved by explicit authentication, for example by manually verifying the OpenPGP fingerprint of a certificate.

Key server implementations of the OpenPGP HTTP Keyserver Protocol offer varying feature sets while providing users access to OpenPGP certificates.

Some keyserver instances synchronize OpenPGP certificates amongst each other, forming a pool of hosts that serve the same key material.

Most keyservers accept OpenPGP certificates without authentication or validation. However, some newer implementations enforce the validation of User IDs by sending a verification e-mail to addresses connected to an uploaded certificate.

All keyservers in the below table are GDPR compliant, as they provide a necessary public service for the ecosystem, allow the removal of personal data and/ or enforce opt-in of its publication.

Third-party identity certifications are not distributed by all keyservers.

Web Key Directory (WKD) is a mechanism to distribute OpenPGP certificates for a given domain based on well-known URIs.

With WKD a web server provides access to a well-known directory structure with normalized file names which allows lookup by e-mail address.

According to GnuPG's wiki this mechanism is supported by several mail hosting providers.

A WKD directory structure can be created from an OpenPGP public keyring and exposed by a web server. The setup can be checked for correctness using https://metacode.biz/openpgp/web-key-directory.

Explicit authentication may mean manually verifying the OpenPGP fingerprints of a certificate with its owner over a secure channel (e.g. in person).

A variation on explicit authentication is the Web of Trust, which encodes manual authentication as machine readable artifacts: OpenPGP certifications (aka signatures). Both direct certifications as well as indirect paths using delegation via trusted introducers can be modeled.

On Arch Linux a set of main signing keys acts as trust anchor for the project. These keys are used to certify the OpenPGP certificates of all package maintainers and developers. Anyone is able to verify the authenticity of each packager by relying on the main signing keys as trust anchors. For Arch Linux users this can be achieved by installing archlinux-keyring and using pacman-key(8).

Alternatively, the OpenPGP fingerprints of the main signing keys can be manually configured as trust anchors by any OpenPGP user, e.g. after verifying them on https://archlinux.org/master-keys/. All relevant certificates can be obtained either through Arch Linux's own web key directory, releases of the archlinux-keyring project or from keyservers.

This article or section needs expansion.

---

## xbindkeys

**URL:** https://wiki.archlinux.org/title/Xbindkeys

**Contents:**
- Installation
- Configuration
  - Audio control
  - Backlight control
  - GUI method
- Identifying keycodes
- Making changes permanent
- Simulating multimedia keys
- Mouse chording
- Troubleshooting

xbindkeys is a program that allows to bind commands to certain keys or key combinations on the keyboard. It works with multimedia keys and is independent of the window manager and desktop environment.

Install the xbindkeys package.

Create a blank ~/.xbindkeysrc, or you can create a sample file:

Now you can either edit ~/.xbindkeysrc to set keybindings, or you can do that with the #GUI method.

Here is an example configuration file that binds Fn key combos on a laptop to pactl commands that adjust audio, such as sound volume and mute status. Note that pound (#) symbols can be used to create comments.

For alternative commands to control volume, see PulseAudio#Keyboard volume control or ALSA#Keyboard volume control.

Keybindings for backlight control can be defined using the XF86MonBrightnessUp and XF86MonBrightnessDown keys. See Backlight#Backlight utilities for the available backlight control utilities.

For graphical configuration install the xbindkeys_config-gtk2AUR package and run:

To find the keycodes for a particular key, enter the following command:

or the following to grab multiple keys:

A blank window will pop up. Press the key(s) to which you wish to assign a command and xbindkeys will output a handy snippet that can be entered into ~/.xbindkeysrc. For example, while the blank window is open, press Alt+o to get the following output (results may vary):

To use this output, copy either one of the last two lines to ~/.xbindkeysrc and replace "(Scheme function)" with the command you wish to perform.

To identify mouse buttons, xev can be used, see [1].

Once you are done configuring your keys, edit your xprofile or xinitrc file (depending on your window manager) and place

before the line that starts your window manager or DE.

The XF86Audio* and other multimedia keys (see LQWiki:XF86 keyboard symbols) are pretty-much well-recognized by the major DEs. For keyboards without such keys, you can simulate their effect with other keys

However, to actually call the keys themselves you can use tools like xdotool and xmacroAUR. Unfortunately since you would already be holding down some modifier key (Super or Shift, for example), X will see the result as Super-XF86AudioLowerVolume which will not do anything useful. Here is a script based on xmacro and xmodmap from the xorg-xmodmap package for doing this[2].

This works for calling XF86AudioLowerVolume once (assuming you are using Super+minus), but repeatedly calling it without releasing the Super key (like tapping on a volume button) does not work. If you would like it to work that way, add the following line to the bottom of the script.

With this modified script, if you press the key combination fast enough your Super_L key will remain 'on' till the next time you hit it, which may result in some interesting side-effects. Just tap it again to remove that state, or use the original script if you want things to 'just work' and do not mind not multi-tapping on volume up/down.

These instructions are valid for pretty much any one of the XF86 multimedia keys (important ones would be XF86AudioRaiseVolume, XF86AudioLowerVolume, XF86AudioPlay, XF86AudioPrev, XF86AudioNext).

By dedicating one button on the mouse as a "chording" key (much like the Shift key on a keyboard), it is possible to use xbindkeys to configure your mouse to perform more actions than would otherwise be possible. This requires the use of Scheme rather than the simplified xbindkeys syntax.

With this function defined, you can now configure some chorded commands:

This defines "button 10" as as chording key on your mouse. When button 10 is pressed down, the function will create bindings for the buttons defined inside the block. When button 10 is released, those bindings will be removed. So for example: with button 10 held down, pressing and releasing button 1, and then releasing button 10, will result in a virtual "button 8" (back) event being generated.

If, for any reason, a hotkey you already set in ~/.xbindkeysrc does not work, open up a terminal and type the following:

By pressing the non-working key, you will be able to see any error xbindkeys encounter (e.g: mistyped command/keycode,...).

If the command for a keybind works via the xdotool in command line, but not when activated by the hotkey try adding "+ Release" to the hotkey (especially notable on GNOME):

This will make the F7 key play/pause audio. Where the "xdotool" command would work in commandline, if the "+ Release" is removed it will fail with xbindkeys.

**Examples:**

Example 1 (unknown):
```unknown
~/.xbindkeysrc
```

Example 2 (unknown):
```unknown
$ xbindkeys -d > ~/.xbindkeysrc
```

Example 3 (unknown):
```unknown
~/.xbindkeysrc
```

Example 4 (unknown):
```unknown
killall -HUP xbindkeys
```

---

## MySQL

**URL:** https://wiki.archlinux.org/title/MySQL

**Contents:**
- Installation
  - Graphical tools
  - Console tools
  - Programmatic access
  - Docker
- Troubleshooting
  - Cannot connect to local MySQL server through socket

MySQL is a widely spread, multi-threaded, multi-user SQL database, developed by Oracle.

Arch Linux favors MariaDB, a community-developed fork of MySQL, aiming for drop-in compatibility.

Oracle's MySQL was dropped to the AUR: mysqlAUR.

Another fork aiming to be fully compatible is Percona Server, available as percona-server.

The InnoDB storage engine by Oracle was also forked by Percona as XtraDB. The fork is used by both MariaDB and Percona Server.

For tools supporting multiple DBMSs, see List of applications/Documents#Database tools.

A docker image is available: https://hub.docker.com/_/mysql

If running a new container fails to start, you may need to increase the ulimit. See GitHub issue for more information.

---

## Kernel

**URL:** https://wiki.archlinux.org/title/Linux

**Contents:**
- Officially supported kernels
- Compilation
  - kernel.org kernels
  - Unofficial kernels
- Troubleshooting
  - Kernel panics
    - Examine panic message
      - QR code on a blue screen
      - Console way
      - Example scenario: bad module

According to Wikipedia:

Arch Linux is based on the Linux kernel. There are various alternative Linux kernels available for Arch Linux in addition to the latest stable kernel. This article lists some of the options available in the repositories with a brief description of each. There is also a description of patches that can be applied to the system's kernel. The article ends with an overview of custom kernel compilation with links to various methods.

Kernel packages are installed under the /usr/lib/modules/ path and subsequently used to copy the vmlinuz executable image to /boot/. [1] When installing a different kernel or switching between multiple kernels, you must configure your boot loader to reflect the changes. For downgrading the kernel to an older version, see Downgrading packages#Downgrading the kernel.

Community support on forum and bug reporting is available for officially supported kernels.

Following methods can be used to compile your own kernel:

Some of the listed packages may also be available as binary packages via Unofficial user repositories.

Many of these unofficial kernels contain features that need to be enabled manually. Try reading the documentation in the patches themselves (many already include changes to the Documentation/ directory in the kernel source) or searching up the name of the patchset on the web.

A kernel panic occurs when the Linux kernel enters an unrecoverable failure state. The state typically originates from buggy hardware drivers resulting in the machine being deadlocked, non-responsive, and requiring a reboot. Just prior to deadlock, a diagnostic message is generated, consisting of: the machine state when the failure occurred, a call trace leading to the kernel function that recognized the failure, and a listing of currently loaded modules. Thankfully, kernel panics do not happen very often using mainline versions of the kernel--such as those supplied by the official repositories--but when they do happen, you need to know how to deal with them.

If a kernel panic occurs very early in the boot process, you may see a message on the console containing Kernel panic - not syncing:, but once systemd is running, kernel messages will typically be captured and written to the system log. However, when a panic occurs, the diagnostic message output by the kernel is almost never written to the log file on disk because the machine deadlocks before system-journald gets the chance.

Since linux 6.10 (for drm_panic), the kernel will display a panic as a QR code (by default) in a blue screen. The stack trace is visible at the URL given by the QR code. For Arch Linux, it is a link to https://panic.archlinux.org. The URL contains various information and the stack trace compressed by gzip and encoded in the URL fragment which is not transferred to the server (it is processed on the client side).

An example panic with a link and screenshot can be seen in a forum post.

You can revert to the old behavior by passing the parameter panic_screen=kmsg to the drm kernel module (or drm.panic_screen=kmsg as kernel parameter) to display the stack trace in a console.

The "old" style way of viewing the crash on the console as it happens is still available (without resorting to setting up a kdump crashkernel). Boot with the following kernel parameters and attempting to reproduce the panic on tty1:

It is possible to make a best guess as to what subsystem or module is causing the panic using the information in the diagnostic message. In this scenario, we have a panic on some imaginary machine during boot. Pay attention to the lines highlighted in bold:

We can surmise then, that the panic occurred during the initialization routine of module firewire_core as it was loaded. (We might assume then, that the machine's firewire hardware is incompatible with this version of the firewire driver module due to a programmer error, and will have to wait for a new release.) In the meantime, the easiest way to get the machine running again is to prevent the module from being loaded. We can do this in one of two ways:

This article or section is out of date.

The factual accuracy of this article or section is disputed.

You will need a root shell to make changes to the system so the panic no longer occurs. If the panic occurs on boot, there are several strategies to obtain a root shell before the machine deadlocks:

See General troubleshooting#Debugging regressions.

Try linux-mainlineAUR to check if the issue is already fixed upstream. The pinned comment also mentions a repository which contains already built kernels, so it may not be necessary to build it manually, which can take some time.

It may also be worth considering trying the LTS kernel (linux-lts) to debug issues which did not appear recently. Older versions of the LTS kernel can be found in the Arch Linux Archive.

If the issue still persists, bisect the linux-gitAUR kernel and report the bug in accordance to the kernel process for reporting regressions. Depending on the Bugtracker (B:) entry in the MAINTAINERS file this then entails opening an issue via the subsystems mailing lists, Kernel Bugzilla, or in other issue trackers like the DRM Gitlab. It is important to try the "vanilla" version without any patches to make sure it is not related to them. If a patch causes the issue, report it to the author of the patch.

You can shorten kernel build times by building only the modules required by the local system using modprobed-db, or by make localmodconfig. Of course you can completely drop irrelevant drivers, for example sound drivers to debug a network problem.

**Examples:**

Example 1 (unknown):
```unknown
/usr/lib/modules/
```

Example 2 (unknown):
```unknown
/proc/config.gz
```

Example 3 (unknown):
```unknown
CONFIG_IKCONFIG_PROC
```

Example 4 (unknown):
```unknown
Documentation/
```

---

## C

**URL:** https://wiki.archlinux.org/title/C

**Contents:**
- Useful tools
  - Static code analyzers
- Alternative compilers
- Alternative libc implementations
- Libraries
- See also

The Linux kernel and the GNU userland are written primarily in C.

Arch Linux uses the GNU C Library (glibc) as the C standard library; it is a dependency of the base meta package.

You can use the GNU toolchain or the LLVM toolchain to develop software in C, C++ or Objective-C.

See also Wikipedia:List of compilers#C compilers.

---

## PostgreSQL

**URL:** https://wiki.archlinux.org/title/PostgreSQL

**Contents:**
- Installation
- Initial configuration
- Create your first database/user
- Familiarize with PostgreSQL
  - Access the database shell
- Optional configuration
  - Restricts access rights to the database superuser by default
  - Require password for login
  - Configure PostgreSQL to be accessible exclusively through UNIX Sockets
  - Configure PostgreSQL to be accessible from remote hosts

PostgreSQL is an open source, community driven, standard compliant object-relational database system.

Install the postgresql package. It will also create a system user called postgres.

You can now switch to the postgres user using a privilege elevation program.

Before PostgreSQL can function correctly, the database cluster must be initialized:

Where -D is the default location where the database cluster must be stored (see #Change default data directory if you want to use a different one). initdb accepts a number of extra arguments:

This article or section needs expansion.

Many lines should now appear on the screen with several ending by ... ok:

If these are the kind of lines you see, then the process succeeded. Return to the regular user using exit.

Finally, start and enable the postgresql.service.

Become the postgres user. Add a new database role/user using the createuser command:

Create a new database over which the above user has read/write privileges using the createdb command (execute this command from your login shell if the database user has the same name as your Linux user, otherwise add -O database-username to the following command):

Become the postgres user. Start the primary database shell, psql, where you can do all your creation of databases/tables, deletion, set permissions, and run raw SQL commands. Use the -d option to connect to the database you created (without specifying a database, psql will try to access a database that matches your username).

Some helpful commands:

Connect to a particular database:

List all users and their permission levels:

Show summary information about all tables in the current database:

Exit/quit the psql shell:

There are of course many more meta-commands, but these should help you get started. To see all meta-commands run:

The PostgreSQL database server configuration file is postgresql.conf. This file is located in the data directory of the server, typically /var/lib/postgres/data. This folder also houses the other main configuration files, including the pg_hba.conf which defines authentication settings, for both local users and other hosts ones.

The defaults pg_hba.conf allow any local user to connect as any database user, including the database superuser. This is likely not what you want, so in order to restrict global access to the postgres user, change the following line:

You might later add additional lines depending on your needs or software ones.

Edit /var/lib/postgres/data/pg_hba.conf and set the authentication method for each user (or all to affect all users) to scram-sha-256:

Restart postgresql.service, and then re-add each user's password using ALTER USER user WITH ENCRYPTED PASSWORD 'password';.

When initially creating the cluster, append -c listen_addresses='' to the initdb command.

For an existing cluster, edit postgresql.conf and in the connections and authentication section set:

This will disable network listening completely. After this you should restart postgresql.service for the changes to take effect.

In the connections and authentications section, set the listen_addresses line to your needs:

You can use '*' to listen on all available addresses.

Then add a line like the following to the authentication config:

where ip_address is the IP address of the remote client.

See the documentation for pg_hba.conf.

The factual accuracy of this article or section is disputed.

After this you should restart postgresql.service for the changes to take effect.

For troubleshooting take a look in the server log file:

PostgreSQL offers a number of authentication methods. If you would like to allow users to authenticate with their system password, additional steps are necessary. First you need to enable PAM for the connection.

For example, the same configuration as above, but with PAM enabled:

The PostgreSQL server is however running without root privileges and will not be able to access /etc/shadow. We can work around that by allowing the postgres group to access this file:

The default directory where all your newly created databases will be stored is /var/lib/postgres/data. To change this, follow these steps:

Create the new directory and make the postgres user its owner:

Become the postgres user, and initialize the new cluster:

Edit postgresql.service to create a drop-in file and override the Environment and PIDFile settings. For example:

If you want to use /home directory for default directory or for tablespaces, add one more line in this file:

When creating a new database (e.g. with createdb blog) PostgreSQL actually copies a template database. There are two predefined templates: template0 is vanilla, while template1 is meant as an on-site template changeable by the administrator and is used by default. In order to change the encoding of a new database, one of the options is to change on-site template1. To do this, log into PostgreSQL shell (psql) and execute the following:

First, we need to drop template1. Templates cannot be dropped, so we first modify it so it is an ordinary database:

The next step is to create a new database from template0, with a new default encoding:

Now modify template1 so it is actually a template:

Optionally, if you do not want anyone connecting to this template, set datallowconn to FALSE:

Now you can create a new database:

If you log back in to psql and check the databases, you should see the proper encoding of your new database:

This article or section is out of date.

If your database files reside on a file system without checksumming, its data is suspectible to silent data corruption due to bit rot and broken hardware. While those events are rare, you might want to enable PostgreSQL's built-in data checksumming if you care about data integrity. This feature must be enabled on the cluster level, not per-database or per-table.

For tools supporting multiple DBMSs, see List of applications/Documents#Database tools.

It is recommended to set up backups for databases containing valuable data. See the Backup and Restore chapter in the PostgreSQL documentation. There is also a list of backup tools in the PostgreSQL wiki, though it may not be up-to-date or complete. Remember that a backup system cannot be trusted unless you perform a test restore from time to time!

This article or section needs expansion.

Upgrading major PostgreSQL versions (e.g. version 14.x to version 15.y) requires some extra maintenance.

Get the currently used database version via

To ensure you do not accidentally upgrade the database to an incompatible version, it is recommended to skip updates to the PostgreSQL packages.

Minor version upgrades are safe to perform. However, if you do an accidental upgrade to a different major version, you might not be able to access any of your data. Always check the PostgreSQL home page to be sure of what steps are required for each upgrade. For a bit about why this is the case, see the versioning policy.

There are two main ways to upgrade your PostgreSQL database. Read the official documentation for details.

The pg_upgrade utility attempts to copy over as much compatible data as possible between clusters and upgrading everything else. It is generally the fastest method to upgrade most instances, although it requires access to binaries for both source and target PostgreSQL versions. Read the pg_upgrade(1) man page to understand what actions it performs. For non-trivial instances (e.g. with streaming replication or log-shipping), read the upstream documentation first.

For those wishing to use pg_upgrade, a postgresql-old-upgrade package is available that will always run one major version behind the real PostgreSQL package. This can be installed side-by-side with the new version of PostgreSQL. To upgrade from older versions of PostgreSQL there are AUR packages available, e.g. postgresql-12-upgradeAUR. (You must use the pg_upgrade version packaged with the PostgreSQL version you are upgrading to.)

Note that the database cluster directory does not change from version to version, so before running pg_upgrade, it is necessary to rename your existing data directory and migrate into a new directory. The new database cluster must be initialized using the same parameters as the old one.

When you are ready to begin the upgrade:

You could also do something like this (after the upgrade and install of postgresql-old-upgrade).

Stop postgresql.service

Start postgresql.service

If you are using PostgresSQL on a local machine for development and it seems slow, you could try turning synchronous_commit off in the configuration. Beware of the caveats, however.

The cause in this case is mostly the existing package is not compiled for the newer version (and it may be up-to-date), the solution is rebuilding the package either manually or waiting for an update to the extension package.

This is caused because the old version of postgres from the package postgresql-old-upgrade does not have the required extensions (.so files) in its lib directory. The current solution is dirty, and might cause a lot of problems so keep a backup of the database just in case. Basically backup /usr/lib/postgresql/ or individual .so files to a separate, temporary location, upgrade postgresql-old-upgrade, postgresql etc. and then restore the previously backed up files into /opt/pgsql-XX/lib/ (remember to replace XX with the major version of postgresql-old-upgrade).

For example, for vectorchord

To know the exact files to copy, check the content of the package of the extension using:

You might see something like this:

That means collation provider library (glibc or icu) was updated which might have made some indexes invalid. So that means need to reindex those databases.

You can do that with:

Repeat this above for all other databases by replacing postgres with respective DB name.

**Examples:**

Example 1 (unknown):
```unknown
[postgres]$ initdb -D /var/lib/postgres/data
```

Example 2 (unknown):
```unknown
--locale=locale
```

Example 3 (unknown):
```unknown
--encoding=encoding
```

Example 4 (unknown):
```unknown
[postgres]$ psql -l
```

---

## Help:Reading

**URL:** https://wiki.archlinux.org/title/Start

**Contents:**
- Organization
- Formatting
- Root, regular user or another user
- Append, add, create, edit
  - Make executable
- Source
- Installation of packages
  - Official packages
  - Arch User Repository
- Control of systemd units

Because the vast majority of the ArchWiki contains indications that may need clarification for users new to Arch Linux (or GNU/Linux in general), this rundown of basic procedures was written both to avoid confusion in the assimilation of the articles and to deter repetition in the content itself.

Most articles on the ArchWiki do not attempt to provide a holistic introduction to a single topic; they are instead written in adherence to the "Don't Repeat Yourself" principle, under the assumption that the user will seek out and read any supporting material that they do not yet understand. Where possible, such supporting material is indicated in the article via special formatting, see #Formatting.

Because of this organization, it may be necessary to examine several related sources in order to fully understand an ArchWiki article. In particular, users who are new to Arch (or GNU/Linux in general) should expect to end up reading a great number of articles even when solving simple problems. It is especially important to study the supporting material before seeking additional help from other users.

Some lines are written like so:

Others have a different prefix:

The numeral or hash sign (#) indicates that the command needs to be run as root, whereas the dollar sign ($) shows that the command should be run as a regular user.

When the commands need to run as a specific user, they will be prefixed by the username in square brackets, for example:

This means you should use a privilege elevation tool, e.g. with sudo:

A notable exception to watch out for:

In this example, the context surrounding the numeral sign communicates that this is not to be run as a command; it should be edited into a file instead. So in this case, the numeral sign denotes a comment. A comment can be explanatory text that will not be interpreted by the associated program. Bash scripts denotation for comments happens to coincide with the root PS1.

After further examination, "give away" signs include the uppercase character following the # sign. Usually, Unix commands are not written this way and most of the time they are short abbreviations instead of full-blown English words (e.g., Copy becomes cp).

Regardless, most articles make this easy to discern by notifying the reader:

Append to ~/path/to/file:

When prompted to append to, add to, create, or edit one or more files, it is implied that you should use one of the following methods.

To create or modify multiline files, it is suggested to use a text editor. For example, using the nano command to edit the file /etc/bash.bashrc is:

To create or overwrite a file from a string, it may be simpler to use output redirection. The following example creates or overwrites the contents of the file /etc/hostname with the text myhostname.

Output redirection can also be used to append a string to a file. The following example appends the text [custom-repo] to the file /etc/pacman.conf.

When prompted to create directories, use the mkdir command:

After creating a file, if it is meant to be run as a script (whether manually or called by another program), it needs to be set as executable, for example with:

See chmod. Some applications such as file managers may provide graphical interfaces to do the same.

Some applications, notably command-line shells, use scripts for their configuration: after modifying them, they must be sourced in order for the changes to be applied. In the case of bash, for example, this is done by running (you can also replace source with .):

When the wiki suggests modifying such a configuration script, it will not explicitly remind you to source the file, and only in some cases will it point to this section with a reminder link.

When an article invites you to install some packages in the conventional way, it will not indicate the detailed instructions to do so; instead, it will simply mention the names of the packages to be installed.

The subsections below give an overview of the generic installation procedures depending on the package type.

For packages from the official repositories, you will read something like:

This means that you have to run:

The pacman article contains detailed explanations to deal with package management in Arch Linux proficiently.

For packages from the Arch User Repository (AUR), you will read something like:

This means that in general you have to follow the foobarAUR link, download the PKGBUILD archive, extract it, verify the content and finally run, in the same folder:

The Arch User Repository article contains all the detailed explanations and best practices to deal with AUR packages.

When an article invites to start, enable, etc., some systemd unit (e.g. a service), it will not indicate the detailed instructions to do so, but instead you will read something like:

This means that you have to run:

A notable command that does not follow this exact pattern is systemctl daemon-reload which will be called without arguments.

The systemd#Using units section contains structured list of available actions (like start, enable, enable and start, etc.) with their corresponding systemctl commands.

It is important to remember that there are two different kinds of configurations on a GNU/Linux system. System-wide configuration affects all users. Since system-wide settings are generally located in the /etc directory, root privileges are required in order to alter them. For example, to apply a Bash setting that affects all users, /etc/bash.bashrc should be modified.

User-specific configuration affects only a single user. Dotfiles are used for user-specific configuration. For example, the file ~/.bashrc is the user-specific configuration file. The idea is that each user can define their own settings, such as aliases, functions and other interactive features like the prompt, without affecting other users' preferences.

Bash and other Bourne-compatible shells, such as Zsh, also source files depending on whether the shell is a login shell or an interactive shell. See Bash#Configuration files and Zsh#Startup/Shutdown files for details.

Some code blocks may contain so-called pseudo-variables, which, as the name says, are not actual variables used in the code. Instead they are generic placeholders and have to be manually replaced with system-specific configuration items before the code may be run or parsed. Common shells such as bash and zsh provide tab-completion to auto-complete parameters for common commands such as systemctl.

In the articles that comply with Help:Style/Formatting and punctuation, pseudo-variables are formatted in italics. For example:

In this case interface_name is used as a pseudo-variable placeholder in a systemd template unit. All systemd template units, identifiable by the @ sign, require a system-specific configuration item as argument. See systemd#Using units.

In this case the pseudo-variables are used to describe the parameters that must be substituted for them. Details on how to gather them are elaborated on in the section Securely wipe disk#Calculate blocks to wipe manually, which features the command.

This article or section needs expansion.

In case of file examples, pasting pseudo-variables in real configuration files might break the programs that use them.

In most cases, ellipses (...) are not part of the actual file content or code output, and instead represent omitted or optional text that is not relevant for the discussed subject.

For example HOOKS=(... encrypt ... filesystems ...) or:

Be aware though that, in a few instances, ellipses may be a meaningful part of the code syntax: attentive users should be able to recognize these cases by the context.

**Examples:**

Example 1 (unknown):
```unknown
# mkinitcpio -p linux
```

Example 2 (unknown):
```unknown
$ makepkg -s
```

Example 3 (unknown):
```unknown
sudo command
```

Example 4 (unknown):
```unknown
[postgres]$ initdb -D /var/lib/postgres/data
```

---

## General guidelines

**URL:** https://wiki.archlinux.org/title/General_guidelines

**Contents:**
- Forums
  - How to post
  - Pasting pictures and code
  - Life is a two-way street
  - Product recommendation requests
  - Old threads/"necro-bumping"
  - No power-posting/empty posts
  - Bumping
  - Cross-posting
  - Thread hijacking

In addition to the code of conduct, each of the fora has its own specific guidelines summarized in the following subsections.

Guidelines specific to the Arch forums.

Use [code] tags when pasting console snippets. Use a pastebin client when posting large amounts of code. Do not use pastebin.comit is blocked for some users and has a history of annoying issues (JavaScript, adverts, poor formatting, etc). For non-English locale users: Prepend LC_ALL=C.UTF-8 to posted commands so that the output will be in English. Do not post full screen pictures; use links to the images instead, optionally with thumbnails. Any image with dimensions greater than 250250px or over 50 KiB in size will be removed. Do not post screenshots of text output; post the actual text.

A simple, yet profound and undeniable truth. Ensure your thread includes details and information that others will find useful. Share your findings with the community. Share your failures as well. Posting the equivalent of "Nevermind, I fixed it." in your thread or deleting your own posts for similar reasons is not only selfish and useless to the community, but a complete waste of resources and everyone's time. Also, demanding help or showing rude impatience is unwelcome here. Arch is provided by a community of volunteers. Arch users are strongly encouraged to do research, make an effort, report back in the thread, help others, get involved, and contribute to the community.

Do not be a "help vampire".

Threads seeking advice about computer product recommendations are discouraged. Such topics, like the technology they discuss, quickly become obsolete and are unlikely to provide any lasting benefit to the wider community. You are expected to be able to do your own research and draw your own conclusions about which product best suits your individual requirements.

Do your part to keep the forums tidy. As the wiki is where Arch is documented, posting in old threads ("necrobumping") is generally discouraged in the technical issue subforums, since it can potentially create disjointed "zombie" information; outdated posts with data which is no longer relevant due to Arch's rolling nature, combined with more recent posts reflecting more current circumstance.

Power-posting is best described as posting empty and worthless messages. It is not tolerated. People may have two reasons to do this: to increase their post count meaninglessly, or to lend support to an idea as if it were a vote. Examples of power-posting include, but are not limited to, replying with "+1", "lol", "me too", "I agree", or ":)".

When posting or replying to messages, make sure you have something to say. These empty posts clutter up threads and discussion, invalidate the 'Show New Posts' function, and waste bandwidth and server space.

Threads that degenerate into a series of "+1/-1" or "me too/I agree/I disagree" will be locked. Individual power posts may also be deleted.

Posting a single word or useless message (bumping) to attract attention to your thread is not allowed. Do your own research, continue to troubleshoot, post the results, and be patient with the community. If people are reading your thread without answering or offering help, you may try supplying more details, or ask to be pointed in the right direction. Often, the reason for posts remaining unanswered is due in large part to the sparse details in the original post itself, or, the obvious availability of solutions in the wiki, on the forum or on the web, and the community's unwillingness to point out the obvious.

Cross-posting is posting the same question multiple times in different subfora (for example, posting in both Newbie Corner and Installation), or posting slight variants of the question in the same or different subfora. This is a waste of resources and is not permitted. Any cross-posted topic will be immediately locked and marked for deletion.

Thread hijacking is the process of replying to an existing thread with a different topic. This is generally discouraged. It is better to start a new thread if you have a problem that is related to an existing posted issue but clearly different. Posts that hijack a serious thread with off-topic discussion are also discouraged.

Threads that are locked/closed because they are either already documented on the boards or Wiki or are inconsistent with the Arch Way will be moved to Dust/troll-bin. After a period of five days, the thread will be eligible for deletion at the discretion of the staff. The Moderator responsible will clearly mark the thread as "Binned" or "For deletion."

Guidelines for the mailing lists. See also Mailing list posting style.

There is never an excuse for top posting. Do not do it.

Only quote the necessary elements from a previous email (also known as interleaved quoting). Bulk quoting quickly bloats threads and reduces the legibility while simultaneously increasing the cognitive load on the entire list. Prune all of the redundant material and just reply to the relevant quoted material.

Plain text is the Unix and email standard. HTML is unnecessary and, for those using command line clients, unwelcome. Keep your line lengths reasonable: 72 characters is considered the default to wrap at.

https://useplaintext.email/ provides instructions for configuring email clients to use plain text.

When replying to a mailing list thread, make sure to reply to the mailing list instead of the author of the original email. Most email clients should provide a "reply to mailing list" feature among the reply options. If not, manually input the mailing list email address in the To field.

For mailing lists that do not require subscribing, such as arch-mirrors-announce and aur-requests, use reply to all and make sure the mailing list is among the recipients.

Guidelines for the Arch User Repository can be found at AUR submission guidelines.

All Arch IRC channels are on the Libera Chat IRC network. Users on Libera Chat must follow the network policy and Libera Chat channel guidelines.

The official language of the #archlinux channel is English. If you need help in another language, search international arch channels.

Guidelines for the wiki can be found in ArchWiki:Contributing.

Guidelines for reporting issues in the archlinux/packaging/packages GitLab namespace can be found in Bug reporting guidelines.

The following guidelines apply when submitting merge requests for projects in the archlinux/packaging/packages GitLab namespace.

**Examples:**

Example 1 (unknown):
```unknown
LC_ALL=C.UTF-8
```

Example 2 (unknown):
```unknown
program &> program-output.txt
```

Example 3 (unknown):
```unknown
/query phrik help command
```

---

## File permissions and attributes

**URL:** https://wiki.archlinux.org/title/File_permissions

**Contents:**
- Viewing permissions
  - Examples
- Changing permissions
  - Text method
    - Text method shortcuts
    - Copying permissions
  - Numeric method
  - Bulk chmod
- Changing ownership
- Access Control Lists

File systems use permissions and attributes to regulate the level of interaction that system processes can have with files and directories.

Use the ls command's -l option to view the permissions (or file mode) set for the contents of a directory, for example:

The first column is what we must focus on. Taking an example value of drwxrwxrwx+, the meaning of each character is explained in the following tables:

Each of the three permission triads (rwx in the example above) can be made up of the following characters:

See info Coreutils -n "Mode Structure" and chmod(1) for more details.

Let us see some examples to clarify:

Archie has full access to the Documents directory. They can list, create files and rename, delete any file in Documents, regardless of file permissions. Their ability to access a file depends on the file's permissions.

Archie has full access except they can not create, rename, delete any file. They can list the files and (if the file's permissions allow it) may access an existing file in Documents.

Archie can not do ls in the Documents directory but if they know the name of an existing file then they may list, rename, delete or (if the file's permissions allow it) access it. Also, they are able to create new files.

Archie is only capable of (if the file's permissions allow it) accessing those files the Documents directory which they know of. They can not list already existing files or create, rename, delete any of them.

You should keep in mind that we elaborate on directory permissions and it has nothing to do with the individual file permissions. When you create a new file it is the directory that changes. That is why you need write permission to the directory.

Let us look at another example, this time of a file, not a directory:

Here we can see the first letter is not d but -. So we know it is a file, not a directory. Next the owner's permissions are rw- so the owner has the ability to read and write but not execute. This may seem odd that the owner does not have all three permissions, but the x permission is not needed as it is a text/data file, to be read by a text editor such as Gedit, EMACS, or software like R, and not an executable in its own right (if it contained something like python programming code then it very well could be). The group's permissions are set to r--, so the group has the ability to read the file but not write/edit it in any way  it is essentially like setting something to read-only. We can see that the same permissions apply to everyone else as well.

chmod is a command in Linux and other Unix-like operating systems that allows to change the permissions (or access mode) of a file or directory.

To change the permissions  or access mode  of a file, use the chmod command in a terminal. Below is the command's general structure:

Where who is any from a range of letters, each signifying who is being given the permission. They are as follows:

The permissions are the same as discussed in #Viewing permissions (r, w and x).

Now have a look at some examples using this command. Suppose you became very protective of the Documents directory and wanted to deny everybody but yourself, permissions to read, write, and execute (or in this case search/look) in it:

Before: drwxr-xr-x 6 archie web 4096 Jul 5 17:37 Documents

After: drwx------ 6 archie web 4096 Jul 6 17:32 Documents

Here, because you want to deny permissions, you do not put any letters after the = where permissions would be entered. Now you can see that only the owner's permissions are rwx and all other permissions are -.

This can be reverted with:

Before: drwx------ 6 archie web 4096 Jul 6 17:32 Documents

After: drwxr-xr-x 6 archie web 4096 Jul 6 17:32 Documents

In the next example, you want to grant read and execute permissions to the group, and other users, so you put the letters for the permissions (r and x) after the =, with no spaces.

You can simplify this to put more than one who letter in the same command, e.g:

Now let us consider a second example, suppose you want to change a foobar file so that you have read and write permissions, and fellow users in the group web who may be colleagues working on foobar, can also read and write to it, but other users can only read it:

Before: -rw-r--r-- 1 archie web 5120 Jun 27 08:28 foobar

After: -rw-rw-r-- 1 archie web 5120 Jun 27 08:28 foobar

This is exactly like the first example, but with a file, not a directory, and you grant write permission (just so as to give an example of granting every permission).

The chmod command lets add and subtract permissions from an existing set using + or - instead of =. This is different from the above commands, which essentially re-write the permissions (e.g. to change a permission from r-- to rw-, you still need to include r as well as w after the = in the chmod command invocation. If you missed out r, it would take away the r permission as they are being re-written with the =. Using + and - avoids this by adding or taking away from the current set of permissions).

Let us try this + and - method with the previous example of adding write permissions to the group:

Before: -rw-r--r-- 1 archie web 5120 Jun 27 08:28 foobar

After: -rw-rw-r-- 1 archie web 5120 Jun 27 08:28 foobar

Another example, denying write permissions to all (a):

Before: -rw-rw-r-- 1 archie web 5120 Jun 27 08:28 foobar

After: -r--r--r-- 1 archie web 5120 Jun 27 08:28 foobar

A different shortcut is the special X mode: this is not an actual file mode, but it is often used in conjunction with the -R option to set the executable bit only for directories, and leave it unchanged for regular files, for example:

It is possible to tell chmod to copy the permissions from one class, say the owner, and give those same permissions to group or even all. To do this, instead of putting r, w, or x after the =, put another who letter. e.g:

Before: -rw-r--r-- 1 archie web 5120 Jun 27 08:28 foobar

After: -rw-rw-r-- 1 archie web 5120 Jun 27 08:28 foobar

This command essentially translates to "change the permissions of group (g=), to be the same as the owning user (=u). Note that you cannot copy a set of permissions as well as grant new ones e.g.:

In that case chmod throw an error.

chmod can also set permissions using numbers.

Using numbers is another method which allows you to edit the permissions for all three owner, group, and others at the same time, as well as the setuid, setgid, and sticky bits. This basic structure of the code is this:

Where xxx is a 3-digit number where each digit can be anything from 0 to 7. The first digit applies to permissions for owner, the second digit applies to permissions for group, and the third digit applies to permissions for all others.

In this number notation, the values r, w, and x have their own number value:

To come up with a 3-digit number you need to consider what permissions you want owner, group, and all others to have, and then total their values up. For example, if you want to grant the owner of a directory read write and execution permissions, and you want group and everyone else to have just read and execute permissions, you would come up with the numerical values like so:

This is the equivalent of using the following:

To view the existing permissions of a file or directory in numeric form, use the stat(1) command:

Where the %a option specifies output in numeric form.

Most directories are set to 755 to allow reading, writing and execution to the owner, but deny writing to everyone else, and files are normally 644 to allow reading and writing for the owner but just reading for everyone else; refer to the last note on the lack of x permissions with non executable files: it is the same thing here.

To see this in action with examples consider the previous example that has been used but with this numerical method applied instead:

Before: -rw-r--r-- 1 archie web 5120 Jun 27 08:28 foobar

After: -rw-rw-r-- 1 archie web 5120 Jun 27 08:28 foobar

If this were an executable the number would be 774 if you wanted to grant executable permission to the owner and group. Alternatively if you wanted everyone to only have read permission the number would be 444. Treating r as 4, w as 2, and x as 1 is probably the easiest way to work out the numerical values for using chmod xxx filename, but there is also a binary method, where each permission has a binary number, and then that is in turn converted to a number. It is a bit more convoluted, but here included for completeness.

Consider this permission set:

If you put a 1 under each permission granted, and a 0 for every one not granted, the result would be something like this:

You can then convert these binary numbers:

The value of the above would therefore be 775.

Consider we wanted to remove the writable permission from group:

The value would therefore be 755 and you would use chmod 755 filename to remove the writable permission. You will notice you get the same three digit number no matter which method you use. Whether you use text or numbers will depend on personal preference and typing speed. When you want to restore a directory or file to default permissions e.g. read and write (and execute) permission to the owner but deny write permission to everyone else, it may be faster to use chmod 755/644 filename. However if you are changing the permissions to something out of the norm, it may be simpler and quicker to use the text method as opposed to trying to convert it to numbers, which may lead to a mistake. It could be argued that there is not any real significant difference in the speed of either method for a user that only needs to use chmod on occasion.

You can also use the numeric method to set the setuid, setgid, and sticky bits by using four digits.

For example, chmod 2777 filename will set read/write/executable bits for everyone and also enable the setgid bit.

Generally directories and files should not have the same permissions. If it is necessary to bulk modify a directory tree, use find to selectively modify one or the other.

To chmod only directories to 755:

To chmod only files to 644:

chown changes the owner of a file or directory, which is quicker and easier than altering the permissions in some cases.

Consider the following example, making a new partition with GParted for backup data. Gparted does this all as root so everything belongs to root by default. This is all well and good but when it comes to writing data to the mounted partition, permission is denied for regular users.

As you can see the device in /dev is owned by root, as is the mount location (/media/Backup). To change the owner of the mount location one can do the following:

Before: drwxr-xr-x 5 root root 4096 Jul 6 16:01 Backup

After: drwxr-xr-x 5 archie root 4096 Jul 6 16:01 Backup

Now the partition can have data written to it by the new owner, archie, without altering the permissions (as the owner triad already had rwx permissions).

Access Control Lists provides an additional, more flexible permission mechanism for file systems by allowing to set permissions for any user or group to any file.

The umask utility is used to control the file-creation mode mask, which determines the initial value of file permission bits for newly created files.

Apart from the file mode bits that control user and group read, write and execute permissions, several file systems support file attributes that enable further customization of allowable file operations.

The e2fsprogs package contains the programs lsattr(1) and chattr(1) that list and change a file's attributes, respectively.

These are a few useful attributes. Not all filesystems support every attribute.

See chattr(1) for a complete list of attributes and for more info on what each attribute does.

For example, if you want to set the immutable bit on some file, use the following command:

To remove an attribute on a file just change + to -.

See Extended attributes.

Use the --preserve-root flag to prevent chmod from acting recursively on /. This can, for example, prevent one from removing the executable bit systemwide and thus breaking the system. To use this flag every time, set it within an alias. See also [1].

**Examples:**

Example 1 (unknown):
```unknown
$ ls -l /path/to/directory
```

Example 2 (unknown):
```unknown
total 128
drwxr-xr-x 2 archie archie  4096 Jul  5 21:03 Desktop
drwxr-xr-x 6 archie archie  4096 Jul  5 17:37 Documents
drwxr-xr-x 2 archie archie  4096 Jul  5 13:45 Downloads
-rw-rw-r-- 1 archie archie  5120 Jun 27 08:28 customers.ods
-rw-r--r-- 1 archie archie  3339 Jun 27 08:28 todo
-rwxr-xr-x 1 archie archie  2048 Jul  6 12:56 myscript.sh
```

Example 3 (unknown):
```unknown
drwxrwxrwx+
```

Example 4 (unknown):
```unknown
info ls -n "What information is listed"
```

---

## Device file

**URL:** https://wiki.archlinux.org/title/Device_file

**Contents:**
- Block devices
  - Block device names
    - SCSI
    - NVMe
    - MMC
    - SCSI optical disc drive
    - virtio-blk
    - Partition
  - Utilities
    - lsblk

This article or section needs expansion.

On Linux they are in the /dev directory, according to the Filesystem Hierarchy Standard.

On Arch Linux the device nodes are managed by udev.

A block device is a special file that provides buffered access to a hardware device. For a detailed description and comparison of virtual file system devices, see Wikipedia:Device file#Block devices.

The beginning of the device name specifies the kernel's used driver subsystem to operate the block device.

Storage devices, like hard disks, SSDs and flash drives, that support the SCSI command (SCSI, SAS, UASP), ATA (PATA, SATA) or USB mass storage connection are handled by the kernel's SCSI driver subsystem. They all share the same naming scheme.

The name of these devices starts with sd. It is then followed by a lower-case letter starting from a for the first discovered device (sda), b for the second discovered device (sdb), and so on.

The name of storage devices, like SSDs, that are attached via NVM Express (NVMe) starts with nvme. It is then followed by a number starting from 0 for the device controller, nvme0 for the first discovered NVMe controller, nvme1 for the second, and so on. Next is the letter "n" and a number starting from 1 expressing the namespace on a controller, i.e. nvme0n1 for first discovered namespace on first discovered controller, nvme0n2 for second discovered namespace on first discovered controller, and so on.

This article or section needs expansion.

SD cards, MMC cards and eMMC storage devices are handled by the kernel's mmc driver and name of those devices start with mmcblk. It is then followed by a number starting from 0 for the device, i.e. mmcblk0 for first discovered device, mmcblk1 for second discovered device and so on.

The name of optical disc drives (ODDs), that are attached using one of the interfaces supported by the SCSI driver subsystem, start with sr. The name is then followed by a number starting from 0 for the device, ie. sr0 for the first discovered device, sr1 for the second discovered device, and so on.

Udev also provides /dev/cdrom that is a symbolic link to /dev/sr0. The name will always be cdrom regardless of the drive's supported disc types or the inserted media.

The name of drives attached to a virtio block device (virtio-blk) interface start with vd. It is then followed by a lower-case letter starting from a for the first discovered device (vda), b for the second discovered device (vdb), and so on.

Partition device names are a combination of the drive's device name and the partition number assigned to them in the partition table, i.e. /dev/drivepartition. For drives whose device name ends with a number, the drive name and partition number is separated with the letter "p", i.e. /dev/driveppartition.

The util-linux package provides the lsblk(8) utility which lists block devices, for example:

In the example above, only one device is available (sda), and that device has three partitions (sda1 to sda3), each with a different file system.

You can use the -o/--output option to enable a specific list of output columns:

The above is based on the options provided by the -f/--fs argument with removal of UUID and addition of partition label and disk size, which are useful when identifying multiple disks. See lsblk --help for a full list of supported columns.

This article or section needs expansion.

wipefs can list or erase file system, RAID or partition-table signatures (magic strings) from the specified device to make the signatures invisible for libblkid(3). It does not erase the file systems themselves nor any other data from the device.

See wipefs(8) for more information.

For example, to erase all signatures from the device /dev/sdb and create a signature backup ~/wipefs-sdb-offset.bak file for each signature:

Device nodes that do not have a physical device.

**Examples:**

Example 1 (unknown):
```unknown
/dev/nvme0n1
```

Example 2 (unknown):
```unknown
/dev/nvme2n5
```

Example 3 (unknown):
```unknown
/dev/mmcblkXboot{0,1}
```

Example 4 (unknown):
```unknown
/dev/mmcblkXrpmb
```

---

## IceWM

**URL:** https://wiki.archlinux.org/title/IceWM

**Contents:**
- Installation
- Starting
- Configuration
  - Autostart
  - Generating menu entries
  - Themes
  - Desktop icons
- Tips and tricks
  - Compositing
- Troubleshooting

According to Wikipedia:

Install the icewm package.

With xinit run icewm, or icewm-session to also run icewmbg and icewmtray.

Configuration changes from the defaults can be made either system wide (in /etc/icewm/) or on a user-specific basis (in ~/.config/icewm/).

To change your icewm configuration from the default, copy the default configuration files from /usr/share/icewm/ to ~/.config/icewm/, for example:

The startup script is not provided by the icewm package so you will need to create it yourself, add the commands for the programs that you wish to start with the IceWM session and make it executable.

Below is an example of an IceWM startup script which starts network-manager-applet and XScreenSaver within the IceWM session:

menumaker from the official repositories is a Python script that automatically populates your applications menu based on what is installed in your system. Although this may result in a menu filled with many unwanted applications, it may still be preferable to manually editing the menu configuration file. When running MenuMaker, use the -f flag to overwrite an existing menu file:

You can avoid populating your menu with terminal based applications such as alsamixer(1) by running the following switches with the mmaker command: --no-legacy and --no-debian. For example:

Alternatively, you can generate a menu using xdg-menu. See the xdg-menu#IceWM section.

A small number of themes are included in the icewm package. These can supplemented by the themes available from the icewm-extra-themesAUR package. Many more themes can be downloaded from box-look.org.

A file manager such as PCManFM or roxAUR can manage the wallpaper and add desktop icons. Alternatively, you could install Idesk, a small program that can also add icons to the desktop.

IceWM is not a compositing window manager. If you need compositing with IceWM, you have the option of using a standalone composite manager such as Xcompmgr or Picom.

If you are using IceWM with Intel graphics you may find that the start menu in your taskbar has no icon. This is due to a recent change in the xf86-video-intel driver which means that the new, but rather unstable, SNA acceleration backend is used by default. To fix the start menu issue (and other possible graphical glitches) you need to switch back to the older UXA backend. See the following article: Intel graphics#AccelMethod.

If you use PCManFM to manage the desktop you may find that the IceWM logout button no longer works. As a workaround, you can define a logout command. This should allow you to logout whilst PCManFM is managing the desktop. To do this, uncomment LogoutCommand:

Shutdown and reboot commands will be ignored if a logout command has been defined. If you want shutdown and reboot options in the logout menu then you must not define a logout command.

If you have defined shutdown and reboot commands (such as systemctl poweroff and systemctl reboot) and you have not defined a logout command but you still find that there are no shutdown or reboot options in the logout menu then update to icewm 1.3.8-2. See FS#37884 for more information.

**Examples:**

Example 1 (unknown):
```unknown
icewm-session
```

Example 2 (unknown):
```unknown
icewm-session
```

Example 3 (unknown):
```unknown
/etc/icewm/
```

Example 4 (unknown):
```unknown
~/.config/icewm/
```

---

## Default applications

**URL:** https://wiki.archlinux.org/title/Default_applications

**Contents:**
- Background information
- Resource openers
  - xdg-open
  - perl-file-mimeinfo
  - mimeo
  - handlr
  - clifm
  - Minimalist replacements
    - run-mailcap

This article or section needs expansion.

Programs implement default application associations in different ways. While command-line programs traditionally use environment variables, graphical applications tend to use XDG MIME Applications through either the GIO API, the Qt API, or by executing /usr/bin/xdg-open, which is part of xdg-utils. Because xdg-open and XDG MIME Applications are quite complex, various alternative resource openers were developed. The following table lists example applications for each method.

Many desktop environments and graphical file managers provide a GUI for configuring default applications.

Programs sometimes need to open a file or a URI in the user's preferred application. To open a file in the user's preferred application the filetype needs to be detected (usually using filename extensions or magic numbers mapped to MIME types) and there needs to be an application associated with the filetype.

Heirloom UNIX programs used mime.types for MIME type detection and mailcap for application association.

xdg-open (part of xdg-utils) implements XDG MIME Applications and is used by many programs.

Because of the complexity of the xdg-utils version of xdg-open, it can be difficult to debug when the wrong default application is being opened. Because of this, there are many alternatives that attempt to improve upon it. Several of these alternatives replace the /usr/bin/xdg-open executable, thus changing the default application behavior of most applications. Others simply provide an alternative method of choosing default applications.

perl-file-mimeinfo provides the tools mimeopen(1p) and mimetype(1p). These have a slightly nicer interface than their xdg-utils equivalents:

Most importantly, xdg-utils programs will actually call file instead of mimetype for MIME type detection if it does not detect your desktop environment. This is important because file does not follow the XDG standard.

mimeoAUR provides the tool mimeo, which unifies the functionality of xdg-open and xdg-mime.

In the following example we see how to associate SVG files with Inkscape:

One can also find the path to the mimeapps.list file:

However a big difference with xdg-utils is that mimeo also supports custom "association files" that allow for more complex associations. For example, passing specific command line arguments based on a regular expression match:

xdg-utils-mimeoAUR patches xdg-utils so that xdg-open falls back to mimeo if no desktop environment is detected.

handlr-regex, written in Rust, provides the functionality of xdg-open and xdg-mime with a streamlined interface. handlr-regex is a fork of handlrAUR with regex support.

Compared to xdg-utils, it includes:

To use handlr as a replacement for xdg-open, shadow it with following script:

Lira, clifm's built-in resource opener, can be used as a standalone resource opener via the --open command line option. The configuration file (~/.config/clifm/profiles/PROFILE_NAME/mimelist.clifm) supports regular expressions for both MIME types and file names (or file extensions). A few examples:

The following packages conflict with and provide xdg-utils because they provide their own /usr/bin/xdg-open script.

If you want to use one of these resource openers while still being able to use xdg-utils, install them manually in a PATH directory before /usr/bin.

**Examples:**

Example 1 (unknown):
```unknown
/usr/bin/xdg-open
```

Example 2 (unknown):
```unknown
gio mime mimetype
```

Example 3 (unknown):
```unknown
/usr/bin/xdg-open
```

Example 4 (unknown):
```unknown
xdg-mime query default  mimetype
```

---

## Core utilities

**URL:** https://wiki.archlinux.org/title/Dmesg

**Contents:**
- Essentials
  - Preventing data loss
- Nonessentials
- Alternatives
  - cat alternatives
  - cd alternatives
  - date alternatives
  - cp alternatives
  - ls alternatives
  - find alternatives

Core utilities are the basic, fundamental tools of a GNU/Linux system. This article provides an incomplete overview of them, links their documentation and describes useful alternatives. The scope of this article includes, but is not limited to, the GNU Core Utilities. Most core utilities are traditional Unix tools and many were standardized by POSIX but have been developed further to provide more features.

Most command-line interfaces are documented in man pages, utilities by the GNU Project are documented primarily in Info manuals, some shells provide a help command for shell builtin commands. Additionally most utilities print their usage when run with the --help flag.

The following table lists some important utilities which Arch Linux users should be familiar with. See also intro(1).

rm, mv, cp and shell redirections happily delete or overwrite files without asking. rm, mv, and cp all support the -i flag to prompt the user before every removal / overwrite. Some users like to enable the -i flag by default using aliases. Relying upon these shell options can be dangerous, because you get used to them, resulting in potential data loss when you use another system or user that does not have them. The best way to prevent data loss is to create backups.

This table lists core utilities that often come in handy.

The moreutils package provides useful tools like sponge(1) that are missing from the GNU coreutils.

Alternative core utilities are provided by the following packages:

See also Bash#Auto "cd" when entering just a path and Zsh#Remembering recent directories.

This article or section is a candidate for moving to List of applications/Other.

Using rsync#As cp/mv alternative allows you to resume a failed transfer, to show the transfer status, to skip already existing files and to make sure of the destination files integrity using checksums.

For graphical file searchers, see List of applications/Utilities#File searching.

While diffutils does not provide a word-wise diff, several other programs do:

See also List of applications/Utilities#Comparison, diff, merge.

These tools aim to replace grep for code search. They do recursive search by default, skip binary files and respect .gitignore.

This article or section needs expansion.

See also: dd and ddrescue

This subsection lists dd implementations whose interface and default behaviour is mostly compliant with the POSIX specification of dd(1p).

The GNU implementation of dd found in coreutils also conforms to POSIX. This subsection lists its forks.

This subsection lists dd alternatives that do not conform to POSIX (in terms of the JCL-resembling command-line syntax and default behaviour).

This subsection lists forks of bufferAUR, a general-purpose I/O buffering utility similar to dd but has a dynamic-sized buffer. It supports blockwise I/O and can be used when dumping from/to an LTO-tape to avoid shoe shining.

See also List of applications/Utilities#Disk usage display.

Many common packages already install most popular POSIX utilities as dependencies, but the posix metapackage can be installed to ensure all of them being always present.

Beside mandatory utilities, there are also metapackages for some of the optional categories:

Some commands (arch, kill, etc.) are missing from coreutils or taken from other packages. To complete them for compatibility, install uutils-coreutils and do:

**Examples:**

Example 1 (unknown):
```unknown
--color-words
```

Example 2 (unknown):
```unknown
# ln -sf /usr/bin/uu-coreutils /usr/local/bin/arch
# echo -e "#compdef arch=uu-arch\n_uu-arch" > /usr/local/share/zsh/site-functions/_arch
# echo "complete -c arch -w uu-arch" > /usr/local/share/fish/vendor_completions.d/arch.fish
```

---

## X resources

**URL:** https://wiki.archlinux.org/title/Xresources

**Contents:**
- Installation
- Configuration
  - Basic syntax
  - Wildcard matching
  - Comments
  - Include files
  - Default settings
  - Samples
- Usage
  - Load resource file

X resources file is a user-level configuration dotfile, typically located at ~/.Xresources. It can be used to set configuration parameters for X client applications. Among other things, it can be used to:

Install the xorg-xrdb package for X server resource database utility and xorg-docs for X.org documentations.

X(7)  RESOURCES and XrmGetDatabase(3)  FILE SYNTAX provide detailed information on X resources mechanism and file syntax.

~/.Xresources is a conventional file name, xrdb does not claim it. You can use any other file names, like ~/.config/X11/Xresources and ~/.config/X11/Xresources.d/application-name (also see #Samples and #Include files).

The syntax of an X resources file is a sequence of resource lines as follows:

application_name and Class substrings will never contain a dot (.), the resourceName substring may contain a dot. For example, Dialog.bodyFont is a XScreenSaver internal resource that is specified to set the body font and fallback font:

Question mark (?) and asterisk (*) can be used as wildcards, making it easy to write a single rule that can be applied to many different applications or elements. ? is used to match any single component name, while * is used to represent any number of intervening components including none.

Using the previous example, if you want to apply the same font to all programs (not just XScreenSaver) that contain the class name Dialog which contains the resource name headingFont, you could write:

If you want to apply this same rule to all programs that contain the resource headingFont, regardless of its class, you could write:

See XrmGetResource(3)  MATCHING RULES for more information.

Lines starting with an exclamation mark (!) are ignored, for example:

The two-character sequence \newline (backslash followed by newline), which allows a value to be broken across multiple lines, is also recognized inside the comments. In the following sample all four lines are commented out, despite only one exclamation mark is used:

To spread resource configuration across multiple files (e.g. to use its own file for each application), use C preprocessor #include directive:

If files that are referenced with #include are not reachable from the applied configuration file directory, you need to pass a directory to search for:

To see the default settings for your installed X11 applications, look in /usr/share/X11/app-defaults/.

Detailed information on program-specific resources is usually provided in the man page for the program. xterm(1)  RESOURCES is a good example, as it contains a list of X resources and their default values.

To see the currently loaded resources:

Resources are stored in the X server, so have to only be read once. They are also accessible to remote X11 clients (such as those forwarded over SSH).

Load a resource file (such as the conventional .Xresources), replacing any current settings:

Load a resource file, and merge with the current settings:

If you are using a copy of the default xinitrc as your .xinitrc it already merges ~/.Xresources.

If you are using a custom one, add:

If you want to get the value of a resource (for example if you want to use it in a bash script) you can use xgetresAUR:

Display managers such as GDM may use the --nocpp argument for xrdb.

It is not rare for xrdb -query to output nothing. Try following #Load resource file and #xinitrc from above. And note some of the files mentioned there could be empty.

**Examples:**

Example 1 (unknown):
```unknown
~/.Xresources
```

Example 2 (unknown):
```unknown
~/.Xresources
```

Example 3 (unknown):
```unknown
~/.config/X11/Xresources
```

Example 4 (unknown):
```unknown
~/.config/X11/Xresources.d/application-name
```

---

## chrony

**URL:** https://wiki.archlinux.org/title/Chrony

**Contents:**
- Installation
- Configuration
  - NTP Servers
    - Offline computers
    - Using NTS servers
  - Real-Time Clock
    - Example: intermittently running desktops
  - Other interesting options
- Usage
  - Starting chronyd

This article describes how to set up and run chrony, an alternative NTP client and server that is roaming friendly and designed specifically for systems that are not online all the time.

Install the chrony package.

The smallest useful configuration file (using IP addresses instead of a hostname) would look something like:

Refer to /usr/share/doc/chrony/README, which will point you to the right answer to any doubts you could still have. Documentation is also available online. See also the related man pages: chronyc(1), chrony.conf(5), chronyd(8).

The first thing you define in your /etc/chrony.conf is the servers your machine will synchronize to. NTP servers are classified in a hierarchical system with many levels called strata: the devices which are considered independent time sources are classified as stratum 0 sources; the servers directly connected to stratum 0 devices are classified as stratum 1 sources; servers connected to stratum 1 sources are then classified as stratum 2 sources and so on.

It has to be understood that a server's stratum cannot be taken as an indication of its accuracy or reliability. Typically, stratum 2 servers are used for general synchronization purposes: if you do not already know the servers you are going to connect to, you should use the pool.ntp.org servers (alternate link) and choose the server pool that is closest to your location.

The following lines tells chrony to pick 4 sources from the NTP pool (chrony has special handling of pools, so as to not confuse its tracking of server-side drift), and use a burst behavior on startup:

If your computer is not connected to the internet on startup, it is recommended to use the offline option, to tell Chrony not to try and connect to the servers, until it has been given the go:

It may also be a good idea to either use IP addresses instead of host names, or to map the hostnames to IP addresses in your /etc/hosts file, as DNS resolving will not be available until you have made a connection.

Since version 4.0 [1], chrony supports Network Time Security (NTS), a cryptographically secured variety of NTP. To use it, add an NTS-secured server, and specify nts at the end, like so:

You can find a list of all known NTS-supporting servers here.

During boot the initial time is read from the hardware real-time clock (RTC) and the system time is then set, and synchronised over a period of minutes once the chrony daemon has been running for a while. If the hardware clock is out of sync then the initial system time can be some minutes away from the true time. Chrony.conf has three different mechanisms for handling the RTC:

In addition, rtconutc describes whether RTC runs on UTC.

An intermittently running desktop would require the use of rtcfile to keep track of RTC error. A machine running Arch Linux for five years, accumulated a 300 s error within the RTC. After a reboot it took chrony a long time to adjust this difference using the above configuration. If we go for the below instead:

This keeps, interestingly, the RTC still out-of-date, but after each re-start, chrony adjusts the accumulated error of the RTC and the system time is quite synchronous to NTP even shortly after a start.

RTC remains out-of-date because we forgot to add the rtcautotrim line telling chrony to adjust the RTC. If we do add it, both the RTC and the system time will become correct.

The package provides chronyd.service and chrony-wait.service, see systemd for details.

If you are connected to the internet, run:

You may also be interested in the activity option to display status:

Chrony should now connect to the configured time servers and update your clock if needed. To tell chrony that you are not connected to the Internet anymore, execute the following:

The online/offline status can be automatically handled by dispatcher services for networkmanager and connman, see below.

To check which NTP servers chrony is actually using, and how precise they are, you can use chronyc -N 'sources -a -v':

If you have specified your pools as offline in chrony.conf, you need to tell chrony that the network status has changed.

You can either use chronyc to notify chrony that your network configuration has changed, or you can use a dispatcher for your relevant network configuration manager.

chronyd can go into online/offline mode along with a network connection through the use of NetworkManager's dispatcher scripts. Create a symlink using the shipped upstream NetworkManager dispatcher:

You can alternatively install networkmanager-dispatcher-chronyAUR from the AUR.

Install netctl-dispatcher-chronyAUR from the AUR. This adds a hook to netctl which is run automatically for any connection.

Create the following hook:

See dhcpcd-run-hooks(8)

**Examples:**

Example 1 (unknown):
```unknown
/etc/chrony.conf
```

Example 2 (unknown):
```unknown
server 1.2.3.4 offline
server 5.6.7.8 offline
server 9.10.11.12 offline
driftfile /var/lib/chrony/drift
rtconutc

# This sets the rtc, but throws out RTC drift tracking. rtcfile is the recommended alternative: see "Real-Time Clock" below.
rtcsync
```

Example 3 (unknown):
```unknown
/usr/share/doc/chrony/README
```

Example 4 (unknown):
```unknown
/etc/chrony.conf
```

---

## ACPI modules

**URL:** https://wiki.archlinux.org/title/ACPI_modules

**Contents:**
- Which modules are available?
- How to select the correct ones
- Getting information
- Troubleshooting
  - DSDT fix
  - ACPI fix for notebooks
  - Boot-looping
- See also

ACPI modules are kernel modules for different ACPI parts. They enable special ACPI functions or add information to /proc or /sys. These information can be parsed by acpid for events or other monitoring applications.

This is a small list and summary of ACPI kernel modules:

A complete list for your running kernel can be obtained with the following command:

You have to try yourself which module works for your machine using modprobe yourmodule, then check if the module is supported on your hardware by using dmesg. It may help to add a grep text search to narrow your results:

You can load the module at boot to make the change permanent for the working ones.

To read out battery information, you can simply install the package acpi and run acpi -i.

Using /proc to store ACPI information has been discouraged and deprecated since Linux 2.6.24. The same data is available in /sys now, and interested parties can (should) subscribe to ACPI events from the kernel via netlink. For example, for battery:

If problems with power management persist despite having loaded the proper modules, a Linux-unfriendly DSDT might be the cause.

Sometimes you see "ACPI: EC: input buffer is not empty, aborting transaction". This is a problem with ACPI, more specifically an incompatibility of the BIOS. There may be four ways to solve this issue:

Some notebooks or motherboard may have boot issues, such as powering off during the transition from boot loader to OS due to bad ACPI firmware implementation. The following steps provide several kernel parameters, to be tested in order:

**Examples:**

Example 1 (unknown):
```unknown
$ ls -l /usr/lib/modules/$(uname -r)/kernel/drivers/acpi
```

Example 2 (unknown):
```unknown
total 112
-rw-r--r-- 1 root root  2808 Aug 29 23:58 ac.ko.gz
-rw-r--r-- 1 root root  3021 Aug 29 23:58 acpi_ipmi.ko.gz
-rw-r--r-- 1 root root  3354 Aug 29 23:58 acpi_memhotplug.ko.gz
-rw-r--r-- 1 root root  4628 Aug 29 23:58 acpi_pad.ko.gz
drwxr-xr-x 2 root root  4096 Aug 29 23:59 apei
-rw-r--r-- 1 root root  7120 Aug 29 23:58 battery.ko.gz
-rw-r--r-- 1 root root  3700 Aug 29 23:58 button.ko.gz
-rw-r--r-- 1 root root  2181 Aug 29 23:58 container.ko.gz
-rw-r--r-- 1 root root  1525 Aug 29 23:58 custom_method.ko.gz
-rw-r--r-- 1 root root  1909 Aug 29 23:58 ec_sys.ko.gz
-rw-r--r-- 1 root root  2001 Aug 29 23:58 fan.ko.gz
-rw-r--r-- 1 root root  1532 Aug 29 23:58 hed.ko.gz
-rw-r--r-- 1 root root  3241 Aug 29 23:58 pci_slot.ko.gz
-rw-r--r-- 1 root root 17742 Aug 29 23:58 processor.ko.gz
-rw-r--r-- 1 root root  3073 Aug 29 23:58 sbshc.ko.gz
-rw-r--r-- 1 root root  7098 Aug 29 23:58 sbs.ko.gz
-rw-r--r-- 1 root root  6311 Aug 29 23:58 thermal.ko.gz
-rw-r--r-- 1 root root  8891 Aug 29 23:58 video.ko.gz
```

Example 3 (unknown):
```unknown
CONFIG_ACPI_*=y
```

Example 4 (unknown):
```unknown
modprobe yourmodule
```

---

## bspwm

**URL:** https://wiki.archlinux.org/title/Bspwm

**Contents:**
- Installation
- Starting
- Configuration
  - Note for multi-monitor setups
  - Rules
  - Panels
    - Using lemonbar
    - Using polybar
  - Scratchpad
    - Using pid

bspwm is a tiling window manager that represents windows as the leaves of a full binary tree. bspwm supports multiple monitors and is configured and controlled through messages. EWMH is partially supported.

Install bspwm for the window manager itself and sxhkd for the X hotkey daemon.

Run bspwm using xinit.

The example configuration is located in /usr/share/doc/bspwm/examples/.

Copy/install bspwmrc from there into ~/.config/bspwm/ and sxhkdrc into ~/.config/sxhkd/.

The file bspwmrc needs to be executable since the default example is simply a shell script that in turn configures bspwm via the bspc command.

These two files are where you will be setting wm settings and keybindings, respectively.

See the bspwm(1) and sxhkd(1) manuals for detailed documentation.

The example bspwmrc configures ten desktops on one monitor like this:

You will need to change this line and add one for each monitor, similar to this:

You can use xrandr -q or bspc query -M --names to find the monitor names. Note that if monitor names contain special characters, the entire name needs to be escaped by a percentage sign bspc monitor %DVI-I-1.5 -d I II III IV to get identified correctly.

The total number of desktops were maintained at ten in the above example. To address each desktop, you should refer to the exact desktop names (defined by the bspc monitor command above) in your sxhkdrc like this:

Note: In default config, sxhkd refers to desktops by number. But those change each time the monitor setup is changed.

Desktops in bspwm are activated by their index. Unless explicitly set, bspwm will automatically choose the display order, which determines the desktop index. If you encounter issues with desktops being on the wrong display, you should explicitly set the display order. similar to this:

If you want to automate your monitor-screens setup in case you periodically change monitor layouts, you need to use a conditional structure to check if a specific monitor is connected and configure it appropriately; you can place the following script in place of the standard bspc monitor -d I II III IV V VI VII VIII IX X command:

The previous script uses xrandr to set the virtual display configuration and bspc to assign the appropriate desktops layout.

Note that you should first set your preferred monitor layout with arandr, then export the xrandr configuration script and then copy the contents inside the script in place of the xrandr command.

bspwm can apply rules to windows based on the class name, which is the second string within the WM_CLASS property specified by ICCM. To determine the class name, you can:

There are two ways to configure window rules (as of cd97a32).

The first is by using the built in rule command, as shown in the example bspwmrc:

The second option is to use an external rule command. This is more complex, but can allow you to craft more complex window rules. See these examples for a sample rule command.

An example panel for lemonbarAUR is provided in the examples folder on the GitHub page. You might also get some insights from the lemonbar wiki page. The panel will be executed by placing panel & in your bspwmrc. Check the optdepends in the bspwm package for dependencies that may be required.

To display system information on your status bar you can use various system calls. This example will show you how to edit your panel to get the volume status on your BAR:

Next, we will have to make sure it is called and redirected to $PANEL_FIFO:

Polybar can be used by adding polybar example & to your bspwmrc configuration file, where example is the name of the bar.

You can emulate a dropdown terminal.

First create a file called /usr/local/bin/scratch:

The terminal application is alacritty. Modify the following class specification and command execution in above script for other terminals.

For toggling the scratchpad, modify ~/.config/sxhkd/sxhkdrc:

Modify polybar config with:

And create a file $XDG_CONFIG_HOME/polybar/scratchpads_status.sh with:

In this example we are going to use termite with a custom class name as our dropdown terminal. It does not have to be termite.

First create a file in your path with the following content and make it executable. In this example let us call it scratchpad.sh:

Then add this to your bspwm config.

To toggle the window a custom rule in sxhkd is necessary. Give as parameter the custom class name.

For a scratch-pad which can use any window type without pre-defined rules, see: [3]

For a more sophisticated scratchpad script that supports many terminals out of the box and has flags for doing things like optionally starting a tmuxinator/tmux session, turning any window into a scratchpad on the fly, and automatically resizing a scratchpad to fit the current monitor see tdrop-gitAUR.

Since the bspwmrc is a shell script, it allows you to do things like these:

Here is how to setup the desktop 3 to have only floating windows. It can be useful for GIMP or other applications with multiple windows.

Put this script somewhere in your $PATH and call it from .xinitrc or similar (with a & at the end):

Bspwm does not handle any keyboard input and instead provides the bspc program as its interface.

For keyboard shortcuts you will have to setup a hotkey daemon like sxhkd (sxhkd-gitAUR for the development version).

See Cursor themes#Change X shaped default cursor

This can happen if you are using GTK3 applications and usually for dialog windows. Create or add the following:

(source: Bspwm forum thread)

If you have problems, like Java application Windows not resizing, or menus immediately closing after you click, see Java#Gray window, applications not resizing with WM, menus immediately closing.

Furthermore, some applications based on Java can not display any window content at all (e.g. Intellij IDEs like PyCharm, CLion, etc). A solution is to install wmname and add the following line in your ~/.config/bspwm/bspwmrc:

Additionally, these errors can often be solved by setting an environment variable for the JVM within bspwmrc or a shell rc like ~/.bashrc, since BSPWM is a non reparenting WM.

If you use fish, you will find that you are unable to switch desktops. This is because bspc's use of the ^ character is incompatible with fish. You can fix this by explicitly telling sxhkd to use bash to execute commands:

Alternatively, the ^ character may be escaped with a backslash in your sxhkdrc file.

sxhkd uses the shell set in the SHELL environment variable in order to execute commands. fish can have long initialization time due to large or improper configuration files, thus all sxhkd commands can take much longer to execute than with other shells. To fix this without changing your default SHELL you can make tell sxhkd explicitly to use bash, or another faster shell to execute commands (for example, sh):

Either you try to use the same key twice, or you start sxhkd twice. Check bspwmrc and ~/.profile or ~/.bash_profile for excessive commands starting sxhkd.

**Examples:**

Example 1 (unknown):
```unknown
/usr/share/doc/bspwm/examples/
```

Example 2 (unknown):
```unknown
~/.config/bspwm/
```

Example 3 (unknown):
```unknown
~/.config/sxhkd/
```

Example 4 (unknown):
```unknown
$ install -Dm755 /usr/share/doc/bspwm/examples/bspwmrc ~/.config/bspwm/bspwmrc
$ install -Dm644 /usr/share/doc/bspwm/examples/sxhkdrc ~/.config/sxhkd/sxhkdrc
```

---

## Font configuration

**URL:** https://wiki.archlinux.org/title/Font_configuration

**Contents:**
- Font paths
- Fontconfig configuration
  - Presets
  - Anti-aliasing
  - Hinting
    - Byte-Code Interpreter (BCI)
    - Autohinter
    - Hintstyle
  - Subpixel layout
  - Subpixel rendering

Fontconfig is a library designed to provide a list of available fonts to applications, as well as configuration for how fonts get rendered.

The FreeType library renders fonts based on this configuration. The freetype2 font rendering package includes the bytecode interpreter (BCI) enabled for better font rendering, especially with an LCD monitor. See #Fontconfig configuration and Font configuration/Examples.

Although Fontconfig is used often in modern Unix and Unix-like operating systems, some applications rely on the original method of font selection and display, the X Logical Font Description (XLFD).

For fonts to be known to applications, they must be cataloged for easy and quick access.

The font paths known to Fontconfig by default are: /usr/share/fonts/, ~/.local/share/fonts (and ~/.fonts/, now deprecated). Fontconfig will scan these directories recursively. For ease of organization and installation, it is recommended to use these font paths when adding fonts.

To see a list of known Fontconfig fonts:

See fc-list(1) for more output formats.

Check for Xorg known font paths by reviewing its log:

Keep in mind that Xorg does not search recursively through the /usr/share/fonts/ directory like Fontconfig does. To add a path, the full path must be used:

For more details about Xorg configuration read Xorg#Configuration. If you want font paths to be set on a per-user basis, you can add and remove font paths from the default by adding the following line(s) to ~/.xinitrc:

To see a list of fonts known by Xorg use xlsfonts, from the xorg-xlsfonts package.

Fontconfig configuration is documented in the fonts-conf(5) man page.

Configuration can be done per-user through $XDG_CONFIG_HOME/fontconfig/fonts.conf (usually $HOME/.config/fontconfig/fonts.conf), and globally with /etc/fonts/local.conf. The settings in the per-user configuration have precedence over the global configuration. Both these files use the same syntax.

Fontconfig gathers all its configurations in a central file (/etc/fonts/fonts.conf). This file is replaced during Fontconfig updates and should not be edited. Fontconfig-aware applications source this file to know available fonts and how they get rendered; simply restarting such applications is sufficient to load the new configuration. This file is a conglomeration of rules from the global configuration (/etc/fonts/local.conf), the configured presets in /etc/fonts/conf.d/, and the user configuration file ($XDG_CONFIG_HOME/fontconfig/fonts.conf). fc-cache can be used to rebuild Fontconfig configuration, although changes will only be visible in newly launched applications.

Fontconfig configuration files use XML format and need these headers:

The configuration examples in this article omit these tags.

There are presets installed in the directory /usr/share/fontconfig/conf.avail. They can be enabled by creating symbolic links to them, both per-user and globally, as described in /etc/fonts/conf.d/README. These presets will override matching settings in their respective configuration files.

For example, to enable sub-pixel RGB rendering globally:

To do the same but instead for a per-user configuration:

Font rasterization converts vector font data to bitmap data so that it can be displayed. The result can appear jagged due to aliasing. Anti-aliasing can be used to increase the apparent resolution of font edges. Anti-aliasing is enabled by default. To disable it:

Font hinting (also known as instructing) is the use of mathematical instructions to adjust the display of an outline font so that it lines up with a rasterized grid (i.e. the pixel grid of the display). Its intended effect is to make fonts appear more crisp so that they are more readable. Fonts will line up correctly without hinting when displays have around 300 DPI.

Using BCI hinting, instructions in TrueType fonts are rendered according to FreeTypes's interpreter. BCI hinting works well with fonts with good hinting instructions. Hinting is enabled by default. To disable it:

The autohinter attempts to do automatic hinting, disregarding any hinting information embedded in the font. Originally, it was the default because TrueType2 fonts were patent-protected, but now that these patents have expired, there is very little reason to use it. It does work better with fonts that have broken or no hinting information, but it will be strongly sub-optimal for fonts with good hinting information. Generally, common fonts are of the latter kind, so the autohinter will not be useful. The autohinter is disabled by default. To enable it:

Hintstyle is the amount of font reshaping done to line up to the grid. Hinting values are: hintnone, hintslight, hintmedium, and hintfull. hintslight will make the font more fuzzy to line up to the grid but will be better in retaining font shape (see [2]), while hintfull will be a crisp font that aligns well to the pixel grid but will lose a greater amount of font shape. hintslight implicitly uses the autohinter in a vertical-only mode in favor of font-native information for non-CFF (.otf) fonts.

hintslight is the default setting. To change it:

Fontconfig will need to know your subpixel layout to be able to display your fonts correctly. Use the Subpixel layout monitor test (part of The Lagom LCD monitor test pages) to find out your subpixel arrangement.

Subpixel geometry is configured via the rgba property, which could be either:

There is no support for unusual subpixel layouts such as "Pentile" and "RGBY", occasionally found on TV, OLED and mobile screens. For these devices, it is best to give up subpixel rendering and rely on greyscale by using the none option.

Subpixel rendering is a technique to improve sharpness of font rendering by effectively tripling the horizontal (or vertical) resolution through the use of subpixels. On Windows machines, this technique is called ClearType.

FreeType2 provides two different types of subpixel rendering, called Harmony and ClearType (FT_CONFIG_OPTION_SUBPIXEL_RENDERING) [3]. Starting from FreeType 2.10.3, Arch Linux enables ClearType subpixel rendering by default [4].

An LCD filter is recommended when ClearType subpixel rendering is enabled. The Harmony subpixel rendering does not require setting an LCD filter and with default LCD geometry, it is equivalent to ClearType with the lcdlight filter [5]. See the following section on how to enable an LCD filter and its benefits.

When using ClearType subpixel rendering, you should enable the LCD filter, which is designed to reduce color fringing. This is described under LCD filtering in the FreeType 2 API reference. Different options are described under FT_LcdFilter, and are illustrated by this LCD filter test page.

The lcddefault filter will work for most users. Other filters are available that can be used in special situations: lcdlight; a lighter filter ideal for fonts that look too bold or fuzzy, lcdlegacy, the original Cairo filter; and lcdnone to disable it entirely.

If the available built-in LCD filters are not satisfactory, it is possible to tweak the font rendering very specifically by building a custom freetype2 package and modifying the hardcoded filters. The Arch build system can be used to build and install packages from source.

Checkout the freetype2 PKGBUILD and download/extract the build files. Arch build system#Retrieve PKGBUILD source lists some of the methods.

Enable ClearType subpixel rendering by editing the file src/freetype-VERSION/include/freetype/config/ftoption.h and uncommenting the FT_CONFIG_OPTION_SUBPIXEL_RENDERING macro.

Then, edit the file src/freetype-VERSION/src/base/ftlcdfil.c and look up the definition of the constant default_weights:

This constant defines a low-pass filter applied to the rendered glyph. Modify it as needed. (reference: freetype list discussion) Save the file, build and install the custom package:

Restart X. The lcddefault filter should now render fonts differently.

Some fonts may not look good with BCI hinting. It can be disabled for just those fonts:

A reliable way to set a default or fallback font is to add an XML fragment to perform a match test. With the "binding" attribute, for example, the following setting will fall back to Gentium in place of Georgia:

In the above, the "compare" attribute can be "eq" (i.e., exactly equal to Georgia), "contains" (e.g., matching either Georgia or Georgia Pro), or other values. See fonts-conf(5).

An alternate approach is to use <alias> to set the "preferred" font. Fonts matching the <family> element are edited to prepend the list of <prefer>ed families before the matching <family>. The following example will fall back to Bitstream Vera Sans when Helvetica is called:

<alias> can also be used to specify fallback fonts when some glyphs are missing. For example, many versions of Helvetica Neue do not include Greek characters. A user might have Helvetica Neue installed and want to use it for Latin characters, and fall back to FreeSans for Greek characters due to its similarity to Helvetica. However, the same user might have set another font Noto Sans as the default sans-serif font. The following will allow this to be achieved:

The above is not needed if the user simply wants to fall back to default fonts whenever glyphs are missing.

The element <selectfont> is used in conjunction with the <acceptfont> and <rejectfont> elements to selectively whitelist or blacklist fonts from the resolve list and match requests. The simplest and most typical use case is to reject a font that the user needs installed, but is getting matched for a generic font query that is causing problems with user interfaces.

First, obtain the Family name as listed in the font itself:

Then, use that Family name in a <rejectfont> stanza:

Typically, when both elements are combined, <rejectfont> is first used on a more general matching glob to reject a large group (such as a whole directory), then <acceptfont> is used after it to whitelist individual fonts out of the larger blacklisted group.

Bitmap fonts are sometimes used as fallbacks for missing fonts, which may cause text to be rendered pixelated or too large. Use the 70-no-bitmaps-except-emoji.conf preset to disable this behavior.

To disable embedded bitmap for all fonts:

To disable embedded bitmap fonts for a specific font:

If embedded bitmaps are disabled for all fonts, they can still be enabled for a specific font in case it does not function without embedded bitmaps. E.g. for Noto Color Emoji:

To disable scaling of bitmap fonts (which often makes them blurry), remove /etc/fonts/conf.d/10-scale-bitmap-fonts.conf. Note this can break the scaling of emoji fonts such as Segoe UI Emoji, making them huge.

To re-enable scaling of bitmap fonts, re-create the symbolic link:

FreeType has the ability to automatically create italic and bold styles for fonts that do not have them, but only if explicitly required by the application. Given programs rarely send these requests, this section covers manually forcing generation of missing styles.

Start by editing /usr/share/fonts/fonts.cache-1 as explained below. Store a copy of the modifications on another file, because a font update with fc-cache will overwrite /usr/share/fonts/fonts.cache-1.

Assuming the Dupree font is installed:

Duplicate the line, change style=Regular to style=Bold or any other style. Also change slant=0 to slant=100 for italic, weight=80 to weight=200 for bold, or combine them for bold italic:

Now add necessary modifications to $XDG_CONFIG_HOME/fontconfig/fonts.conf:

Fontconfig processes files in /etc/fonts/conf.d in numerical order. Therefore, the rules of 01-aaa.conf and 02-bbb.conf will have the same effect as a single 01-aaabbb.conf file first containing the rules of 01-aaa then the ones of 02-bbb.

Usually, that means the files with the smaller prefix will have higher precedence. However, the Fontconfig syntax is flexible and allows a new rule to take precedence over an existing rule. Therefore, it is recommended to #Query the current settings to test the result of the rule interactions.

The factual accuracy of this article or section is disputed.

Note that the user's rules defined in $XDG_CONFIG_HOME/fontconfig/fonts.conf and in the directory $XDG_CONFIG_HOME/fontconfig/conf.d are loaded via the file /etc/fonts/conf.d/50-user.conf and typically take precedence over the rules defined in files starting with a higher number.

To find out what settings are in effect, use fc-match --verbose. eg.

Look up the meaning of the numbers at fonts-conf(5) Eg. 'hintstyle: 3' means 'hintfull'

Some applications like URxvt will ignore Fontconfig settings. You can work around this by using ~/.Xresources, but it is not as flexible as Fontconfig. Example (see #Fontconfig configuration for explanations of the options):

Make sure the settings are loaded properly when X starts with xrdb -q (see X resources for more information).

See HiDPI for instructions on handling high or mixed DPI displays: using a DPI setting which is not matching the physical hardware can lead to fuzzy font display.

Some scalable fonts have embedded bitmap versions which are rendered instead, mainly at smaller sizes. Using Metric-compatible fonts as replacements can improve the rendering in these cases.

You can also force using scalable fonts at all sizes by disabling embedded bitmap, sacrificing some rendering quality.

Some applications or desktop environments may override default Fontconfig hinting and anti-aliasing settings. This may happen with GNOME 3, for example while you are using Qt applications like vlc or smplayer. Use the specific configuration program for the application in such cases. For GNOME, try gnome-tweaks.

For instance, under GNOME it sometimes happens that Firefox applies full hinting even when it is set to "none" in GNOME's settings, which results in sharp and widened fonts. In this case you would have to add hinting settings to your fonts.conf file:

In this example, hinting is set to grayscale.

In some desktop environments, especially outside GNOME and Plasma, some GTK applications could not read font configuration properly. In order to solve this issue, install xsettingsd or xsettingsd-gitAUR and execute it at every system startup. See also Xsettingsd and xsettingsd wiki for more information. It can be configured with the following common configuration:

If that is not working in some other applications, you could install xorg-xrdb and provide the following configuration:

Then you can execute the script xrdb -merge ~/.Xresources at every system startup to apply the options. See also X resources and #Applications without Fontconfig support.

GTK4 and libadwaita programs ignore font hinting settings. To remedy this, create or modify the following configuration:

See also GTK documentation, GTK issue 3787, and this related note.

If the following command

then the bitmap font provided by xorg-fonts-75dpi is likely to be embedded into PDFs generated by "Print to File" or "Export" in various applications. The bitmap font was probably installed as a consequence of installing the whole xorg group (which is usually NOT recommended). To solve the pixelized font problem, you can uninstall the package. Install gsfonts (Type 1) or tex-gyre-fonts (OpenType) for corresponding free substitute of Helvetica (and other PostScript/PDF base fonts).

You may also experience similar problem when you open a PDF which requires Helvetica but does not have it embedded for viewing.

Some users are reporting problems (FS#52502) with bitmap fonts having changed names after upgrading freetype2 to version 2.7.1, creating havoc in terminal emulators and several other programs such as dwm or dmenu by falling back to another (different) font. This was caused by the changes to the PCF font family format, which is described in their release notes [6]. Users transitioning from the old format might want to create a font alias to remedy the problems, like the solution which is described in [7], given here too:

Assume we want to create an alias for terminus-font, which was renamed from Terminus to xos4 Terminus in the previously described freetype2 update:

This article or section is out of date.

Everything should now work as it did before the update, the font alias should not be in effect, but make sure to either reload .Xresources or restart the display server first so the affected programs can use the alias.

Since Pango 1.44, the underscore characters disappear with certain font sizes when using the DejaVu Sans Mono font. A workaround is to use Liberation Mono as the monospace font, see #Set default or fallback fonts.

freetype2-demos provides tools for debugging FreeType font configuration. ftview is a GUI in which you can tweak font rendering settings with a live preview. For example:

Some applications (e.g. Chromium/Electron) do not apply gamma correction properly, some have it disabled on certain scenarios (grayscale) which cause small text on dark background to be blurry and unreadable text on <=1080p screens. It is a long standing issue for Chromium/Electron, a workaround is to enable stem darkerning with the FREETYPE_PROPERTIES="cff:no-stem-darkening=0 autofitter:no-stem-darkening=0" environment variable.

You can gather your effective font configuration with font-config-info-gitAUR. It queries information from several GTK sources, X resources, XSETTINGS protocol and fontconfig.

**Examples:**

Example 1 (unknown):
```unknown
/usr/share/fonts/
```

Example 2 (unknown):
```unknown
~/.local/share/fonts
```

Example 3 (unknown):
```unknown
$ fc-list ':' file
```

Example 4 (unknown):
```unknown
$ grep /fonts ~/.local/share/xorg/Xorg.0.log
```

---

## Webcam setup

**URL:** https://wiki.archlinux.org/title/Webcam_setup

**Contents:**
- Loading
- Configuration
  - Command line
  - Graphical
  - Persisting configuration changes
  - Disable internal laptop webcam
  - Disable webcam completely
- Applications
  - FFmpeg
  - Firefox

This is a guide to setting up your webcam. Most probably your webcam will work out of the box. Permissions to access video devicese.g. /dev/video0are handled by udev, there is no configuration necessary.

Most recent webcams are USB video device class (UVC) compliant and are supported by the generic uvcvideo kernel driver module. To check that your webcam is recognized, see the journal just after you plug the webcam in. You should see something like this:

Some pre-UVC webcams are also supported via the gspca kernel driver module. See the gspca cards list for a non-exhaustive list of supported devices under this framework.

Otherwise, if your webcam is not supported by the kernel drivers, an external driver is necessary:

If you want to configure brightness, color and other webcam parameters (e.g. in the case when out-of-the-box colors are too bluish/reddish/greenish) you may use a variety of applications. Some specific webcams such as the Logitech Brio or the Razer Kiyo Pro might require a specific application for some of their specific options such as HDR. Changing any settings in an application that configures V4L settings will generally change those settings for all applications using those cameras unless they override those settings themselves.

v4l-utils installs a command line tool, v4l2-ctl.

To list all video devices:

To list the configurable settings of a video device:

You can change the settings of the video device by doing the following:

Alternatively, you can use cameractrls:

For generic graphical webcam configuration tools you can use either qv4l2 from v4l-utils or guvcview. In addition to this, cameractrls contains cameractrlsgtk4 which allows you to configure some camera-specific features for the Logitech Brio as well as the Razer Kiyo Pro on top of supporting all the other V4L options.

Configuration made via V4L2 does not persist after the webcam is disconnected and reconnected. It is possible to use v4l2-ctl with udev rules in order to set some configuration each time a particular camera is connected.

For example, to set a default zoom setting on a particular Logitech webcam each time it is connected, add a udev rule like this:

The above rule is valid for the Logitech C920 HD Pro Webcam - hardware ID 046d:0892.

The device's vendor id and product id can be found using lsusb. These are unique per product and do not change unless the product gets a new hardware revision.

To find udev attributes like the product name and serial, see udev#List the attributes of a device. It is also possible to set a static name for a video device).

Sometimes we might want to disable a laptop's internal webcam so that only the one attached via USB is showing. This can be done with a udev rule. First we will need the device's vendor id and the product id:

Then we add new udev rule to remove the device as soon as it is detected:

To disable webcam completely, you can blacklist the uvcvideo kernel module. For example:

See also List of applications/Multimedia#Webcam.

See v4l2loopback#Use cases for various examples, where FFmpeg is used to output video to a v4l2 device, which can be used as a webcam.

For laptops without a webcam, an IP camera can be used as an alternative to droidcam which does not keep the extra webcam device hanging around. For android, something like IP webcam can be hosted on the phone, then use the IP camera as a video input for the laptop. First, install linux-headers and v4l2loopback-dkms, then connect to the video source as /dev/video0 using v4l2loopback with 192.168.1.xxx being the IP address of the phone:

Firefox may use not the highest resolution possible by default, leading to 4:3 image being captured from a 16:9 capable webcam. You can force specific resolution in about:config via media.navigator.video.default_height and media.navigator.video.default_width settings.

For example, to force 1920x1080 captured output, set media.navigator.video.default_width to 1920.

To use MPlayer to take snapshots from your webcam run this command from the terminal:

From here you have to press s to take the snapshot. The snapshot will be saved in the current folder as shotXXXX.png. If you want to record video continuous:

Press Ctrl+c to end the recording.

To play video with MPlayer using MJPEG as the pixelformat instead of the default, which in most cases is YUYV, you can run the following:

To use mpv to take snapshots from your webcam, run this command from the terminal:

From here you have to press s to take the snapshot. The snapshot will be saved in your current folder as mpv-shotNNNN.jpg.

To use MJPEG as the pixelformat instead of the default, which in most cases is YUYV, you can run the following instead:

In some cases this can lead to drastic improvements in quality and performance (5FPS -> 30FPS for example).

To adjust webcam settings, including the resolution, see the mpv documentation.

VLC can also be used to view and record your webcam. In VLC's Media menu, open the Capture Device... dialog and enter the video and audio device files. Or from the command line, for example:

This will make VLC mirror your webcam.

Note that by default this will not display the video. In order to see what you are recording, you need to add the display as a destination to the argument (which will slow down the operation):

This is a basic Video4Linux2 device viewer, and although it is intended for use with TV tuner cards, it works well with webcams. It will display what your webcam sees in a window.

Install xawtvAUR and run it with:

In case of error see #xawtv with NVIDIA card.

If you are using an NVIDIA graphics card, and get an error like:

you should instead run it as xawtv -nodga.

Under certain configurations, the Microsoft lifecam studio/cinema may request too much usb bandwidth and fail see Uvcvideo FAQ. In this case, change the buffering by loading the uvcvideo driver with quirks=0x80. Add it to /etc/modprobe.d/uvcvideo.conf :

When running multiple webcams on a single USB bus, they may saturate the bandwidth of the USB bus and not work properly. You can diagnose this with the usbtop tool from the usbtopAUR package.

If your video stream is inverted, you can make a new virtual video camera which inverts the inverted video. You need to install v4l-utils and also v4l2loopback-dkms. Create the virtual video camera:

Check the name of the newly created camera:

Then you can run ffmpeg to read from your actual webcam (here /dev/video0) and invert it and feed it to the virtual camera:

Here vflip inverts the video stream vertically. Use hflip to invert it horizontally.

Note that the format argument yuv420p might be needed to avoid an error, otherwise there might not be any video stream and a black screen will be shown [2]. In other words:

You can then use the "Dummy" camera in your applications instead of the "Integrated" camera.

If you experience images being too bright, too dark, too exposed or any other, you can install v4l2ucpAUR to tweak your image output.

Install libwebcam-gitAUR to enable autofocus control. After the installation, disconnect and reconnect your camera to trigger the newly added udev rules.

Creality webcam is based on the Fullhan FH8852 chip used in many cheap 2MP cameras (USB ID 1d6c:0103), but its firmware requires a special sequence to enable it, otherwise you'll get a black screen. Trying to grab a frame using FFmpeg seems to properly initialize the device, making it work on other apps. You might have to try the following on both /dev/video0 and /dev/video2, and see which one produces a correct /tmp/test_img.jpg image:

Once done, the camera should work on web browser, applications like Cheese, etc.

**Examples:**

Example 1 (unknown):
```unknown
/dev/video0
```

Example 2 (unknown):
```unknown
kernel: sn9c102: V4L2 driver for SN9C10x PC Camera Controllers v1:1.24a
usb 1-1: SN9C10[12] PC Camera Controller detected (vid/pid 0x0C45/0x600D)
usb 1-1: PAS106B image sensor detected
usb 1-1: Initialization succeeded
usb 1-1: V4L2 device registered as /dev/video0
usb 1-1: Optional device control through 'sysfs' interface ready
usbcore: registered new driver sn9c102
```

Example 3 (unknown):
```unknown
$ v4l2-ctl --list-devices
```

Example 4 (unknown):
```unknown
$ v4l2-ctl -d /dev/video0 --list-ctrls
```

---

## Kernel

**URL:** https://wiki.archlinux.org/title/Kernels

**Contents:**
- Officially supported kernels
- Compilation
  - kernel.org kernels
  - Unofficial kernels
- Troubleshooting
  - Kernel panics
    - Examine panic message
      - QR code on a blue screen
      - Console way
      - Example scenario: bad module

According to Wikipedia:

Arch Linux is based on the Linux kernel. There are various alternative Linux kernels available for Arch Linux in addition to the latest stable kernel. This article lists some of the options available in the repositories with a brief description of each. There is also a description of patches that can be applied to the system's kernel. The article ends with an overview of custom kernel compilation with links to various methods.

Kernel packages are installed under the /usr/lib/modules/ path and subsequently used to copy the vmlinuz executable image to /boot/. [1] When installing a different kernel or switching between multiple kernels, you must configure your boot loader to reflect the changes. For downgrading the kernel to an older version, see Downgrading packages#Downgrading the kernel.

Community support on forum and bug reporting is available for officially supported kernels.

Following methods can be used to compile your own kernel:

Some of the listed packages may also be available as binary packages via Unofficial user repositories.

Many of these unofficial kernels contain features that need to be enabled manually. Try reading the documentation in the patches themselves (many already include changes to the Documentation/ directory in the kernel source) or searching up the name of the patchset on the web.

A kernel panic occurs when the Linux kernel enters an unrecoverable failure state. The state typically originates from buggy hardware drivers resulting in the machine being deadlocked, non-responsive, and requiring a reboot. Just prior to deadlock, a diagnostic message is generated, consisting of: the machine state when the failure occurred, a call trace leading to the kernel function that recognized the failure, and a listing of currently loaded modules. Thankfully, kernel panics do not happen very often using mainline versions of the kernel--such as those supplied by the official repositories--but when they do happen, you need to know how to deal with them.

If a kernel panic occurs very early in the boot process, you may see a message on the console containing Kernel panic - not syncing:, but once systemd is running, kernel messages will typically be captured and written to the system log. However, when a panic occurs, the diagnostic message output by the kernel is almost never written to the log file on disk because the machine deadlocks before system-journald gets the chance.

Since linux 6.10 (for drm_panic), the kernel will display a panic as a QR code (by default) in a blue screen. The stack trace is visible at the URL given by the QR code. For Arch Linux, it is a link to https://panic.archlinux.org. The URL contains various information and the stack trace compressed by gzip and encoded in the URL fragment which is not transferred to the server (it is processed on the client side).

An example panic with a link and screenshot can be seen in a forum post.

You can revert to the old behavior by passing the parameter panic_screen=kmsg to the drm kernel module (or drm.panic_screen=kmsg as kernel parameter) to display the stack trace in a console.

The "old" style way of viewing the crash on the console as it happens is still available (without resorting to setting up a kdump crashkernel). Boot with the following kernel parameters and attempting to reproduce the panic on tty1:

It is possible to make a best guess as to what subsystem or module is causing the panic using the information in the diagnostic message. In this scenario, we have a panic on some imaginary machine during boot. Pay attention to the lines highlighted in bold:

We can surmise then, that the panic occurred during the initialization routine of module firewire_core as it was loaded. (We might assume then, that the machine's firewire hardware is incompatible with this version of the firewire driver module due to a programmer error, and will have to wait for a new release.) In the meantime, the easiest way to get the machine running again is to prevent the module from being loaded. We can do this in one of two ways:

This article or section is out of date.

The factual accuracy of this article or section is disputed.

You will need a root shell to make changes to the system so the panic no longer occurs. If the panic occurs on boot, there are several strategies to obtain a root shell before the machine deadlocks:

See General troubleshooting#Debugging regressions.

Try linux-mainlineAUR to check if the issue is already fixed upstream. The pinned comment also mentions a repository which contains already built kernels, so it may not be necessary to build it manually, which can take some time.

It may also be worth considering trying the LTS kernel (linux-lts) to debug issues which did not appear recently. Older versions of the LTS kernel can be found in the Arch Linux Archive.

If the issue still persists, bisect the linux-gitAUR kernel and report the bug in accordance to the kernel process for reporting regressions. Depending on the Bugtracker (B:) entry in the MAINTAINERS file this then entails opening an issue via the subsystems mailing lists, Kernel Bugzilla, or in other issue trackers like the DRM Gitlab. It is important to try the "vanilla" version without any patches to make sure it is not related to them. If a patch causes the issue, report it to the author of the patch.

You can shorten kernel build times by building only the modules required by the local system using modprobed-db, or by make localmodconfig. Of course you can completely drop irrelevant drivers, for example sound drivers to debug a network problem.

**Examples:**

Example 1 (unknown):
```unknown
/usr/lib/modules/
```

Example 2 (unknown):
```unknown
/proc/config.gz
```

Example 3 (unknown):
```unknown
CONFIG_IKCONFIG_PROC
```

Example 4 (unknown):
```unknown
Documentation/
```

---

## cwm

**URL:** https://wiki.archlinux.org/title/Cwm

**Contents:**
- Installation
- Configuration
  - Window groups
  - Moving windows
- See also

cwm is an X11 window manager with a focus on getting out of your way so you can be productive. It was originally derived from evilwm, but the codebase has since been re-written from scratch.

cwm is developed as part of the OpenBSD base system. A portable version which runs on Linux is also available.

Install one of the following packages:

cmw is configured by editing ~/.cwmrc. There is no default cwmrc file; all defaults  including the keybinds  are defined in conf.c. cwm(1) lists the default keybinds; cwmrc(5) lists all configuration directives.

You can remove all default keybinds with unbind-key all and unbind-mouse all though.

cwm lacks traditional workspaces; instead you can assign windows to a group. This is a more flexible approach, as two or more groups can be displayed at the same time, and is similar or identical to the workspace feature of many tiling window managers.

For example one might put a chat/irc application in group 4, and then assign a key to toggle the visibility of that group (bind-key <k> group-toggle 4) to display that group in addition to whatever other windows/groups might be displayed.

You can also use bind-key <k> group-only <n> to show only windows from that group, hiding everything else.

The default for new windows is to not put them in any group, meaning they will always be displayed (what many WMs call sticky windows). However by enabling sticky group mode with sticky yes windows will be assigned the currently selected group by default. You can also use the autogroup directory to automatically group windows.

There is no action to move windows to pre-defined locations; but you can get around this with xdotool; put this cwm-w-mv script in your PATH:

And then bind it in cwm with something like:

This will make Mod4 (Windows key) plus hjkl or the arrow keys move the window to the side.

**Examples:**

Example 1 (unknown):
```unknown
unbind-key all
```

Example 2 (unknown):
```unknown
unbind-mouse all
```

Example 3 (unknown):
```unknown
bind-key <k> group-toggle 4
```

Example 4 (unknown):
```unknown
bind-key <k> group-only <n>
```

---

## VLC media player

**URL:** https://wiki.archlinux.org/title/VLC

**Contents:**
- Installation
  - Optional dependencies
- Language
- Skins
- Web interface
- Tips and tricks
  - Twitch.tv streaming over VLC
  - Playing streamed content from a local DLNA server
  - Control using hotkeys or cli
  - Preventing multiple instances

From the project home page:

Install the vlc split package, that depends on a minimal subset of VLC plugins. See the optional dependencies for more functionality.

Certain codecs require additional VLC plugins (e.g. H.264), thus installing vlc-plugin-ffmpeg is recommended. This would use the available libav (FFmpeg) dynamic libraries for decoding; codec support depends on FFmpeg compile configurations.

vlc-plugins-all includes all VLC plugins, but the plugin packages may be installed separately (see dependencies).

VLC does not offer an option to change language in its Preferences menu. But you can use the LANGUAGE environment variable. For instance, modify the vlc.desktop desktop entry to change:

to switch VLC interface to French.

VLC can be "skinned" for a different look and feel. You can get skins at the skins website.

To install a skin download it and move it to ~/.local/share/vlc/skins2/.

Open up VLC, click Tools > Preferences. When the preferences window opens up you should be in the "Interface" tab

Choose the "Use custom skin" radio button, and select the downloaded skin.

Restart VLC for the change to take effect.

Run VLC with the parameter --extraintf=http to use both the desktop and web interface. The --http-host parameter specifies the address to bind to, which is localhost by default. To set a password, use --http-password, otherwise VLC will not allow you to log in.

To enable the web interface from the graphical interface, navigate to View > Add Interface > Web Interface. Set the password via Tools > Preferences > Show settings: All > Interface > Main interfaces > Lua > Lua HTTP > Password.

VLC defaults to port 8080: http://localhost:8080

See Streamlink#Twitch.

If you find that trying to play UPnP/DLNA content (by going to View > Playlist > Local Network > Universal Plug'n'Play), that vlc fails to see the DLNA server on the local network, then make sure that the firewall is not blocking port 1900 UDP. It is essential that this port is open in order to play local UPnP/DLNA content.

Install openbsd-netcat.

Get script at the archived CrunchBang forums.

Follow instructions in script to setup a socket for VLC.

Either run the script from the command line or register the script with keyboard shortcuts through your desktop.

Alternatively, you can use MPRIS to interact with VLC.

It is also possible to start vlc with an ncurses interface:

see the documentation for more information

By default, VLC opens a new instance of the program every time it is launched with one or more files. This can be annoying if you use VLC to play something like your music collection. You can open multiple files at once by selecting them in the file manager, or disable this behavior from the menu: check Tools > Preferences (set Show settings to Simple) > Interface > Playlist and Instances > Allow only one instance, and also check Enqueue items into playlist in one instance mode which keeps current file playing and adds any newly opened files to the current playlist.

There is also an option called Use only one instance when started from file manager  when this is enabled, all files opened via the file manager will be played in a single instance. The Enqueue items into playlist in one instance mode option still applies.

See Hardware video acceleration.

VLC automatically tries to use an available API, but you can override it by going to Tools > Preferences > Input & Codecs and choosing the suitable option under Hardware-accelerated decoding, e.g. Video Acceleration (VA) API for VA-API or Video Decode and Presentation API for Unix (VDPAU) for VDPAU.

VLC's web interface can be started from systemd. First, you need to create a default user:

Now create the systemd service file:

Start and enable vlc.service. Log in to http://yourmachine:8090/ with no username and with the password you put in the service file.

Starting with 3.0 release (Vetinari branch), VLC can stream to Chromecast devices on the same network.

Then, edit the file /etc/nsswitch.conf and change the hosts line to include mdns_minimal [NOTFOUND=return] before resolve and dns:

Install vlc-pause-click-pluginAUR, which allows you to click on the video inside VLC's window, and it will be paused or resumed. It is a commonly expected behavior.

It is not activated after installation, you need to manually enable it in settings as described in https://github.com/nurupo/vlc-pause-click-plugin#usage.

Now and then VLC will have some issues with configuration even in minor releases. Before making bug reports, remove or rename your configuration located at ~/.config/vlc and confirm whether the issue is still there.

If using a ffmpeg variant from the AUR, be sure that you have upgraded it as well. Pacman will not upgrade it when necessary and a mismatch will break VLC.

When starting VLC you can get a segfault, and ruling out general factors such as Microcode, a possible workaround to this is running the following:

If VLC can open and play audio files, but closes when you play a video with a segfault, then Hardware video acceleration was wrongly configured, making VLC unable to refer to graphics devices. It occurs especially when you are using different graphic cards on one computer.

This can happen under XFCE, there will be no more icons in dropdown menus, like the PCI card icon.

Execute these commands to reactivate these icons:

See Hardware video acceleration#Failed to open VDPAU backend.

Since your system probably does not support VDPAU you should tell VLC to use VA-API instead, see #Hardware video acceleration.

The factual accuracy of this article or section is disputed.

Manually set VLC to use the VA-API acceleration backend in Settings -> Input/Codecs. VLC seems to select VDPAU by default, which is broken for many people.

If VLC does not play any videos or audio files over SFTP make sure you have sshfs installed.

If it refuses to play any media files containing spaces via SFTP and always asks for authentication change the Exec line in the vlc.desktop file to:

Due to app store licensing restrictions, VLC's iOS and tvOS apps use an incomplete ssh implementation. openssh needs a config change to be compatible. Create a file /etc/ssh/sshd_config.d/vlc.conf with contents:

Then, run systemctl reload sshd.

To be able to play DVDs via Media > Open Disc, VLC optional dependencies vlc-plugin-dvd and libdvdcss need to be installed.

Other plugins such as vlc-plugin-dca may be required for certain DVDs. Alternatively, install vlc-plugins-extra, which will install the above two plugins, as well as others.

If you have all of the above packages and still cannot open DVDs, try deleting your ~/.dvdcss folder. This will force a re-download of your CSS keys, which may fix the problem.

If some RTP, RTSP, DVB-T streams or Blu-rays look like they are endlessly buffering or silently do not load, without giving an error message in the logs (e.g. IPTV from French FAI Free) install vlc-plugin-aribb24.

vlc-gitAUR builds with Wayland support by default. Set the QT_QPA_PLATFORM=wayland environment variable to enable Wayland. See Wayland#Qt for more info.

Note that although the --enable-wayland build flag is used in the vlc PKGBUILD (which currently uses VLC version 3.0), Xwayland is used anyway, as Wayland support on VLC 3 is broken. Video output in VLC may be cropped or otherwise malformed unless xorg-xwayland is installed.

With the following command VLC 3 will start using Wayland:

This works because VLC 3 checks for the environment variable DISPLAY on startup and will use Wayland if it is not set. Globally removing DISPLAY from the environment is not recommended, because some older Applications still rely on this variable.

When connection to RTSP stream ends up with the failed to setup rtsp session error, install vlc-plugin-live555.

Install the optional dependency vlc-plugin-aribb24.

When playing a video encoded with HEVC (H265), users may encounter a total system freeze and are unable to do anything or shutdown.

This can be fixed by changing the settings for Open GL/GLES hardware converter to something other than Automatic (e.g. VDPAU OpenGL surface converter or VA-API OpenGL surface converter for Wayland).

See https://gitlab.freedesktop.org/drm/amd/-/issues/2113#note_1602599

Set Tools > Preferences (with Simple view) > Audio > Output module" to the audio server currently in use. (For PipeWire, you need to install vlc-plugin-pipewireAUR, otherwise only compatibility layers can be used.)

Even if you have the plugin to decode subtitles, you will not see them on the screen (and there will not be any error on the Messages/Console), if you are missing the vlc-plugin-freetype, which is not included in vlc-plugins-base but vlc-plugins-extra.

**Examples:**

Example 1 (unknown):
```unknown
# pacman -S vlc-plugin-ffmpeg
```

Example 2 (unknown):
```unknown
vlc.desktop
```

Example 3 (unknown):
```unknown
Exec=/usr/bin/vlc %U
```

Example 4 (unknown):
```unknown
Exec=env LANGUAGE=fr /usr/bin/vlc %U
```

---

## X resources

**URL:** https://wiki.archlinux.org/title/X_resource

**Contents:**
- Installation
- Configuration
  - Basic syntax
  - Wildcard matching
  - Comments
  - Include files
  - Default settings
  - Samples
- Usage
  - Load resource file

X resources file is a user-level configuration dotfile, typically located at ~/.Xresources. It can be used to set configuration parameters for X client applications. Among other things, it can be used to:

Install the xorg-xrdb package for X server resource database utility and xorg-docs for X.org documentations.

X(7)  RESOURCES and XrmGetDatabase(3)  FILE SYNTAX provide detailed information on X resources mechanism and file syntax.

~/.Xresources is a conventional file name, xrdb does not claim it. You can use any other file names, like ~/.config/X11/Xresources and ~/.config/X11/Xresources.d/application-name (also see #Samples and #Include files).

The syntax of an X resources file is a sequence of resource lines as follows:

application_name and Class substrings will never contain a dot (.), the resourceName substring may contain a dot. For example, Dialog.bodyFont is a XScreenSaver internal resource that is specified to set the body font and fallback font:

Question mark (?) and asterisk (*) can be used as wildcards, making it easy to write a single rule that can be applied to many different applications or elements. ? is used to match any single component name, while * is used to represent any number of intervening components including none.

Using the previous example, if you want to apply the same font to all programs (not just XScreenSaver) that contain the class name Dialog which contains the resource name headingFont, you could write:

If you want to apply this same rule to all programs that contain the resource headingFont, regardless of its class, you could write:

See XrmGetResource(3)  MATCHING RULES for more information.

Lines starting with an exclamation mark (!) are ignored, for example:

The two-character sequence \newline (backslash followed by newline), which allows a value to be broken across multiple lines, is also recognized inside the comments. In the following sample all four lines are commented out, despite only one exclamation mark is used:

To spread resource configuration across multiple files (e.g. to use its own file for each application), use C preprocessor #include directive:

If files that are referenced with #include are not reachable from the applied configuration file directory, you need to pass a directory to search for:

To see the default settings for your installed X11 applications, look in /usr/share/X11/app-defaults/.

Detailed information on program-specific resources is usually provided in the man page for the program. xterm(1)  RESOURCES is a good example, as it contains a list of X resources and their default values.

To see the currently loaded resources:

Resources are stored in the X server, so have to only be read once. They are also accessible to remote X11 clients (such as those forwarded over SSH).

Load a resource file (such as the conventional .Xresources), replacing any current settings:

Load a resource file, and merge with the current settings:

If you are using a copy of the default xinitrc as your .xinitrc it already merges ~/.Xresources.

If you are using a custom one, add:

If you want to get the value of a resource (for example if you want to use it in a bash script) you can use xgetresAUR:

Display managers such as GDM may use the --nocpp argument for xrdb.

It is not rare for xrdb -query to output nothing. Try following #Load resource file and #xinitrc from above. And note some of the files mentioned there could be empty.

**Examples:**

Example 1 (unknown):
```unknown
~/.Xresources
```

Example 2 (unknown):
```unknown
~/.Xresources
```

Example 3 (unknown):
```unknown
~/.config/X11/Xresources
```

Example 4 (unknown):
```unknown
~/.config/X11/Xresources.d/application-name
```

---

## Bug reporting guidelines

**URL:** https://wiki.archlinux.org/title/Bug_reporting_guidelines

**Contents:**
- Before reporting
  - Search for duplicates
  - Upstream or Arch?
  - Bug or feature?
    - Reasons for not being a bug
    - Reasons for not being a feature
  - Gather useful information
- Opening a bug
  - Creating an account
  - Where to open the bug

Opening (and closing) bug reports on the Arch Linux bug tracker in GitLab is one of many possible ways to help the community. However, poorly-formed bug reports can be counter-productive. When bugs are incorrectly reported, developers waste time investigating and closing invalid reports. This document will guide anyone wanting to help the community by efficiently reporting and hunting bugs.

See also: How to Report Bugs Effectively by Simon Tatham

Preparing a detailed and well-formed bug report is not difficult, but requires an effort on behalf of the reporter. The work done before reporting a bug is arguably the most useful part of the job. Unfortunately, few people take the time to do this work properly.

The following steps will guide you in preparing your bug report or feature request.

If you encounter a problem or desire a new feature, there is a high probability that someone else already had this problem or idea. If so, an appropriate bug report may already exist. In this case, please do not create a duplicate report; see #Following up on bugs instead.

Search thoroughly for existing information, including:

Arch Linux is a GNU/Linux distribution. Arch developers and Package Maintainers are responsible for compiling, packaging, and distributing software from a wide range of sources. Upstream refers to these sources  the original authors or maintainers of software that is distributed in Arch Linux. For example, the popular Firefox web browser is developed by the Mozilla Project.

If Arch is not responsible for a bug, the problem will not be solved by reporting the bug to Arch developers. Responsibility for a bug is said to lie upstream when it is not caused through the distribution's porting and integration efforts.

By reporting bugs upstream, you will not only help Archers and Arch developers, but you will also help other people in the free software community as the solution will trickle down to all distributions.

Once you have reported a bug upstream or have found other relevant information from upstream, it might be helpful to post this in the Arch bug tracker, so both Arch developers and users are made aware of it.

So what is Arch Linux responsible for?

If a bug/feature is not under Arch's responsibility, report it upstream. See also #Reasons for not being a bug.

Here is a list of useful information that should be mentioned in your bug report:

If you have to paste a lot of text, like the output of dmesg, or an Xorg log, is it preferred to save it as a file on your computer and attach it to your bug report. Attaching a file rather than using a pastebin to present relevant information is preferable in general due to the fact that pastebined content may suffer by expired links or any other potential problems. Attaching a file guarantees the provided information will always be available.

When you are sure it is a bug or a feature and you gathered all useful information, then you are ready to open a bug report or feature request.

You have to create an account on Arch's GitLab, which manages its login via Arch Linux SSO.

Once you have determined your feature or bug is related to Arch and not an upstream issue, you will need to file your problem in the correct project. This is most easily done via Add new bug on the respective packages page on archlinux.org.

You can alternatively also get to the correct page using pkgctl from devtools:

Problems with packages in the AUR are not reported in the bug tracker. The AUR allows you to add comments to a package, which you should use to report bugs to the package maintainer.

Please write a concise and descriptive title for your bug/feature request.

Here is a list of recommendations:

This article or section is out of date.

Choosing a critical severity will not help to solve the bug faster. It will only make truly critical problems less visible and probably make the developer assigned to your bug a bit less open to fixing it.

Here is a general usage of severities:

This is maybe one of the most difficult parts of bug reporting. You have to choose from the section #Gather useful information which information you will add to your bug report. This will depend on which your problem is. If you do not know what the relevant pieces of information are, do not be shy: it is better to give more information than needed than not enough.

A good tutorial on reporting bugs can be found at https://www.chiark.greenend.org.uk/~sgtatham/bugs.html.

However, developers or bug hunters will ask you for more information if needed. Fortunately after a few bug reports you will know what information should be given.

Short information can be inlined in your bug report, whereas long information (logs, screenshots...) should be attached.

Do not think the work is done once you have reported a bug!

Developers or other users will probably ask you for more details and give you some potential fixes to try. Without continued feedback, bugs cannot be closed and the end result is both angry users and developers.

You can vote for your favourite bugs via reactions on the issue. The number of reactions indicates to the developers how many people are impacted by the bug without creating too much noise. However, this is not a very effective way of getting the bug solved. Much more important would be posting any additional information you know about the bug if you were not the original reporter.

Watching a bug is important: you will receive an email when new comments are added or the bug status has changed. This can be done via the "..." menu in the upper right corner by toggling the Notification switch there. If you opened a issue or commented on it you will automatically be subscribed to changes.

People will take the time to look at your bug report and will try to help you. You need to continue to help them resolve your bug. Not answering to their questions will keep your bug unresolved and likely hamper enthusiasm to fix it.

Please take the time to give people more information if requested and try the solutions proposed.

Developers or bug hunters will close your bug if you do not answer questions after a few weeks or a month.

Sometimes, a bug is only present in certain version(s) of a given package, and the bug is fixed in a new version of the package. If this is the case, say so in the bug report comments, and request that the bug be closed.

Sometimes people report a bug but do not notify when they have solved it on their own, leaving people searching for a solution that has already been found. Please close the bug if you found a solution, and give the solution in the bug report comments.

During its life, a bug may go through several states:

The Bug Wranglers are responsible for dispatching bugs and setting their status together with the help of the respective maintainers of the package the bug was opened against.

**Examples:**

Example 1 (unknown):
```unknown
pacman -Qi package_name
```

Example 2 (unknown):
```unknown
/var/log/messages
```

Example 3 (unknown):
```unknown
~/.local/share/xorg/Xorg.0.log
```

Example 4 (unknown):
```unknown
/var/log/Xorg.0.log
```

---

## Icons

**URL:** https://wiki.archlinux.org/title/Icons

**Contents:**
- Installation
  - Icons and emblems
  - Mime type icons
  - Icon themes
    - From a package
    - Manually
- fstab / gvfs

The freedesktop project provides the Icon Theme Specification, which applies to most linux desktop environments and tries to unify the look of a whole bunch of icons in an icon-theme. Freedesktop also provides the Icon Naming Specification, which defines a standard naming scheme for icons believed to be installed on any system. The default theme hicolor should include them all.

To append a custom icon to an existing icon theme xdg-icon-resource can be used. This will resize and copy the icon to $HOME/.local/share/icons/. With this method, custom emblems can also be added. Examples:

File managers do not rely on the traditional mime type which file --mime outputs. Instead definitions from /usr/share/mime/ are used. Calling an icon according to the definition found there and copying it to ~/.local/share/icons/ will cause the file manager to display the custom mime type icon. This command illustrates the method to create a custom icon for the keepass database files (.kdb):

Rename your icon according to this output:

If you cannot find a package for the icon theme you are looking for, you will need to install it manually.

According to this document file managers using GVFS (like GNOME Files or Thunar) can display icons for custom locations, like NFS shares. All you need are some extended mount options inside /etc/fstab with icon names supported by your selected icon theme:

**Examples:**

Example 1 (unknown):
```unknown
xdg-icon-resource
```

Example 2 (unknown):
```unknown
$HOME/.local/share/icons/
```

Example 3 (unknown):
```unknown
$ xdg-icon-resource install --size 128 --context emblems archuser-example.png # add as emblem
$ xdg-icon-resource install --size 128 archuser-example.png # add as normal icon
```

Example 4 (unknown):
```unknown
file --mime
```

---

## Wine

**URL:** https://wiki.archlinux.org/title/Wine

**Contents:**
- Installation
  - Optional dependencies
    - Sound
      - MIDI support
    - Other dependencies
  - In-prefix dependencies
  - Third-party applications
- Configuration
  - WINEPREFIX
  - Fonts

Wine is a compatibility layer capable of running Microsoft Windows applications on Unix-like operating systems.

Wine does not use emulation, binary translation, or virtualization to operate. Instead, Wine provides an implementation of the Win32 API to applications that require it. As Wine's implementation of the Win32 API will naturally differ from what is provided by Microsoft Windows, applications may suffer behavioral, compatibility, or performance penalties when using Wine.

Wine can be installed either through the wine (development), wine-stableAUR (stable) or wine-staging (testing) package. Wine Staging is a patched version of Wine, which contains bug fixes and features that have not been integrated into the stable or development branch yet.

If using wine-stableAUR, see #Using 32-bit Wine builds for additional requirements.

Wine has numerous optional dependencies, which may not be required for basic applications, but should be installed to provide functionality such as sounds, 3D graphics, video playback, etc.

By default sound issues may arise when running Wine applications. Ensure only one sound device is selected in winecfg.

MIDI was a quite popular system for video games music in the 90s. If you are trying out old games, it is not uncommon that the music will not play out of the box. Wine has excellent MIDI support. However you first need to make it work on your host system, as explained in MIDI. Last but not least you need to make sure Wine will use the correct MIDI output, though by default, no extra configuration may be required.

For MIDI tracks to play in-game, install Microsoft's General MIDI DLS Collection, DirectSound and DirectMusic using winetricks gmdls dsound directmusic or by searching for them on the dependency manager if using Bottles.

Some applications may require additional packages [1].

Aside from system dependencies, many programs require additional fonts and DLLs to be installed to the Wine prefix [2]. To satisfy these dependencies you can use Winetricks, a primitive "package manager" where each verb either installs something or applies a configuration tweak. There are two ways to use Winetricks:

Due to conflicts between dependencies, you may not be able to create the "perfect" Windows installation that can run everything [3] [4]. Rather, you should treat prefixes as disposable (unless they contain important configurations or data) and use separate prefixes for programs with different dependencies. You can use the #WINEPREFIX environment variable to control which prefix the verbs act on.

Determining the verbs required by a program needs can require much trial and error. See the Bottles dependency page for some of the more common dependencies, as well as the following program-specific resources:

If you find yourself spending a lot of time managing prefixes for games, it may be easier to use a third-party application that handles it for you.

These have their own communities and websites, and are not supported by the main Wine community. See Wine Wiki for more details.

Configuring Wine is typically accomplished using:

See Programs for the full list.

By default, Wine stores its configuration files and installed Windows programs in ~/.wine. This directory is commonly called a "Wine prefix" or "Wine bottle". It is created/updated automatically whenever you run a Windows program or one of Wine's bundled programs such as winecfg. The prefix directory also contains a tree which your Windows programs will see as C: (the C-drive).

You can override the location Wine uses for a prefix with the WINEPREFIX environment variable. This is useful if you want to use separate configurations for different Windows programs. The first time a program is run with a new Wine prefix, Wine will automatically create a directory with a bare C-drive and registry.

For example, if you run one program with env WINEPREFIX=~/.win-a wine program-a.exe, and another with env WINEPREFIX=~/.win-b wine program-b.exe, the two programs will each have a separate C-drive and separate registries.

To create a default prefix without running a Windows program or other GUI tool you can use:

If Wine applications have unreadable or missing fonts, you may not have any fonts installed. To easily link all of the system fonts so they are accessible from wine:

Wine uses FreeType to render fonts, and FreeType's defaults changed a few releases ago. Try using the following environment variable when running programs in Wine:

Another possibility is to install Microsoft's TrueType fonts into your wine prefix. If this does not help, try running winetricks corefonts first, then winetricks allfonts as a last resort.

After running such programs, kill all Wine servers and run winecfg. Fonts should be legible now.

If the fonts look somehow smeared, run the following command to change a setting in the Wine registry:

For high resolution displays, you can adjust dpi values in winecfg.

See also Font configuration#Applications without Fontconfig support.

A good way to improve wine font rendering is to enable cleartype font smoothing. To enable "Subpixel smoothing (ClearType) RGB":

If you have installed winetricks, there is a simpler way to do this:

For more information, check the original answer

When a Windows application installer creates a shortcut Wine creates a .desktop file instead. The default locations for those files in Arch Linux are:

By default, installation of Wine does not create desktop menus/icons for the software which comes with Wine (e.g. for winecfg, winebrowser, etc). This can be achieved by installing wine-installerAUR or wine-installer-gitAUR meta-package (the latter has no additional dependencies), otherwise these instructions will add entries for these applications.

First, install a Windows program using Wine to create the base menu. After the base menu is created, you can create the following files in ~/.local/share/applications/wine/:

And create the following file in ~/.config/menus/applications-merged/:

If these settings produce a ugly/non-existent icon, it means that there are no icons for these launchers in the icon set that you have enabled. You should replace the icon settings with the explicit location of the icon that you want. Clicking the icon in the launcher's properties menu will have the same effect. A great icon set that supports these shortcuts is gnome-colors-icon-themeAUR.

Menu entries created by Wine are located in ~/.local/share/applications/wine/Programs/. Remove the program's .desktop entry to remove the application from the menu.

In addition to remove unwanted extensions binding by Wine, execute the following commands: [6]

Sometimes you should also remove wine-*.menu files from /.config/menus/ to completely remove items from Wine submenu in KDE.

A similar to XP-looking theme can be downloaded. To install it, see this upstream wiki article. Lastly, use winecfg to select it.

Wine staging users may instead want to try enabling the option Enable GTK3 Theming under the Staging section of winecfg for a theme that matches the current GTK theme.

In order to use your installed printers (both local and network) with wine applications (e.g. MS Word), install the libcups package, reboot wine (wineboot) and restart your wine application.

After installation, the gnutls package may need to be installed for applications making TLS or HTTPS connections to work.

For ICMP (ping), Wine may need the network access as described in the WineHQ FAQ:

If issues arise after this (such as an unhandled exception or privileged instruction), remove via:

See Wine User's Guide for general information on Wine usage.

See Wine Application Database (AppDB) for additional information on specific Windows applications in Wine.

By default, Wine runs on Wayland through Xwayland, providing a satisfactory experience for most users. As of version 9.0rc1, Wine has made substantial progress on merging native Wayland support, now making it suitable for some use cases.

To experiment with the native Wayland driver added in recent Wine versions, you can follow these steps:

If the second step makes Wine stop working, check to see if your Wine version is built with support for the new Wayland driver.

Stopping started executables, wine with Ctrl+Z or wineconsole with Ctrl+C, might leave processes running in the background. See for example:

All running wine and wineconsole processes are stopped at once using the wineserver -k command. For example:

This command is WINEPREFIX-dependent, so when using a custom Wine prefix, run:

An equivalent command to gracefully finish both executables in the above example is:

Upstream Wine supports three ways of running 32-bit Windows applications on a 64-bit system:

Since Wine 10.8-2, Arch Linux enables the new WoW64 mode. Most 32-bit Windows applications will install and run without any additional steps needed.

While the new WoW64 mode will work for most applications, it has a few limitations:

As a workaround, wine32AUR is an alternate wine package that provides 32-bit builds of wine. The wine-stableAUR package also currently provides a 32-bit build. These packages require the host system to have 32-bit versions of libraries installed for Wine to be able to run 32-bit applications. Some common 32-bit libraries are listed below. When installing other libraries listed on this page (e.g. those listed in #Other dependencies), you should also install the corresponding lib32- package if you are running a 32-bit application.

You need to install the 32-bit version of your graphics driver. Please install the package that is listed in the OpenGL (multilib) column in the table in Xorg#Driver installation.

A good sign that your drivers are inadequate or not properly configured is when Wine reports the following in your terminal window:

Install the correct packages for the audio driver you want to use:

If winecfg still fails to detect the audio driver (Selected driver: (none)), configure it via the registry. For example, in a case where the microphone was not working in a 32-bit Windows application on a 64-bit stock install of wine-1.9.7, this provided full access to the sound hardware (sound playback and mic): open regedit, look for the key HKEY_CURRENT_USER > Software > Wine > Drivers, and add a string called Audio and give it the value alsa. Also, it may help to recreate the prefix.

If supported, you can use WINEARCH with WINEPREFIX to make separate win32 and win64 (old WoW64) environments:

You can also use WINEARCH in combination with other Wine programs, such as winetricks (using Steam as an example):

In order to see the architecture of an existing prefix you can check its registry file. The command below reads the system registry of the ~/.wine prefix and returns #arch=win32 or #arch=win64 depending on the architecture type:

Often you may need to run .exe's to patch game files, for example a widescreen mod for an old game, and running the .exe normally through Wine might yield nothing happening. In this case, you can open a terminal and run the following command:

Then navigate to the directory and run the .exe file from there.

Winetricks is a script to allow one to install base requirements needed to run Windows programs. Installable components include DirectX 9.x, MSXML (required by Microsoft Office 2007 and Internet Explorer), Visual Runtime libraries and many more.

Install the winetricks package (or alternatively winetricks-gitAUR). Then run it with:

For using GUI you can install either zenity(GTK) or kdialog(Qt).

CSMT is a technology used by Wine to use a separate thread for the OpenGL calls to improve performance noticeably. Since Wine 3.2, CSMT is enabled by default.

Note that CSMT may actually hurt performance for some applications - if this is the case, disable it by running wine regedit and set the DWORD value for HKEY_CURRENT_USER -> Software > Wine > Direct3D > csmt to 0x00 (disabled).

Some games might have an OpenGL mode which may perform better than their default DirectX mode. While the steps to enable OpenGL rendering is application specific, many games accept the -opengl parameter.

You should of course refer to your application's documentation and Wine's AppDB for such application specific information.

DXVK is an implementation of DirectX 8, 9, 10, and 11 over Vulkan. It beats the WineD3D driver in performance and compatibility for most games. It does not support DirectX 12, see #VKD3D-Proton instead. DXVK and VKD3D-Proton can and should be installed alongside each other to be able to support all DirectX versions.

To install the latest version, use #Winetricks:

You can also specify a version to install. For example, to install a DXVK version with relaxed requirements, use:

Alternatively, install dxvk-mingwAUR or dxvk-binAUR. Then run the following command to activate it in your Wine prefix (by default ~/.wine):

While using DXVK with a dual graphics setup, Wine prefers the dedicated GPU. On laptops for power saving, this can be overridden:

VKD3D-Proton is a fork of VKD3D which aims to implement the full Direct3D 12 API using Vulkan. The project serves as the development effort for Direct3D 12 support in Proton improving performance and compatibility for DirectX 12 games.

To install the latest version, use #Winetricks:

Alternatively, install vkd3d-proton-mingwAUR or vkd3d-proton-binAUR. Then run the following command to activate it in your Wine prefix (by default ~/.wine):

Some games heavily use windows sync objects to run multi-threaded workloads, Wine is able to provide a user space implementation through wineserver, however most of the time the default implementation have major performance impact in CPU bound scenarios.

Currently there are 3 options available to improve the performance, and you should use only one at same time:

To enable ESync, export the following environment variable before running Wine:

Or for FSync with patched Wine:

NTSync does not require setting an environment variable, instead it will automatically be used if the ntsync kernel module is loaded. wine and wine-staging ship with a file to load the module at boot, otherwise you can manually create:

MangoHud can shows the absence or presence of ESync, FSync or NTSync in games if you have enabled an indication of winesync in its config file.

By default, Wine takes over as the default application for a lot of formats. Some (e.g. vbs or chm) are Windows-specific, and opening them with Wine can be a convenience. However, having other formats (e.g. gif, jpeg, txt, js) open in Wine's bare-bones simulations of Internet Explorer and Notepad can be annoying.

Wine's file associations are set in ~/.local/share/applications/ as wine-extension-extension.desktop files. Delete the files corresponding to the extensions you want to unregister. Or, to remove all wine extensions:

Next, remove the old cache:

And, update the cache:

Please note Wine will still create new file associations and even recreate the file associations if the application sets the file associations again.

This method prevents the creation of filetype associations but retains the creation of XDG .desktop files (that you might see e.g. in menus).

If you want to stop wine from creating filetype associations via winecfg you have to uncheck the "Manage File Associations" checkbox under the Desktop Integration tab. See Wine FAQ

To make the same change via registry add the string Enable with value N under:

You might have to create the key FileOpenAssociations first!

To make this change via the command-line, run the following command:

If you want to apply this by default for new Wine prefixes, edit /usr/share/wine/wine.inf and add this line for example under the [Services] section:

To prevent a package upgrade from overriding the modified file, create a pacman hook to make the change automatically:

See Pacman#Hooks for more information.

The wine package installs a binfmt file which will allows you to run Windows programs directly, e.g. ./myprogram.exe will launch as if you had typed wine ./myprogram.exe. Service starts by default on boot, if you have not rebooted after installing Wine you can start systemd-binfmt.service to use it right away.

Installing libxinerama might fix dual-head issues with wine (for example, unclickable buttons and menus of application in the right most or bottom most monitor, not redrawable interface of application in that zone, dragging mouse cursor state stucked after leaving application area).

To burn CDs or DVDs, you will need to load the sg kernel module.

Some applications will check for the disc to be in drive. They may check for data only, in which case it might be enough to configure the corresponding path as being a CD-ROM drive in winecfg. However, other applications will look for a name and/or a serial number, in which case the image has to be mounted with these special properties.

Some virtual drive tools do not handle these metadata, like fuse-based virtual drives (Acetoneiso for instance). CDemu will handle it correctly.

Wine features an embedded FPS monitor which works for all graphical applications if the environment variable WINEDEBUG=fps is set. This will output the framerate to stdout. You can display the FPS on top of the window thanks to osd_cat from the xosd package. See winefps.sh for a helper script.

It may be desirable to run Wine under a specifically created user account in order to reduce concerns about Windows applications having access to your home directory.

First, create a user account for Wine:

Now switch to another TTY and start your X WM or DE as you normally would or keep reading...

Afterwards, in order to open Wine applications using this new user account you need to add the new user to the X server permissions list:

Finally, you can run Wine via the following command, which uses env to launch Wine with the environment variables it expects:

It is possible to automate the process of running Windows applications with Wine via this method by using a shell script as follows:

Wine applications can then be launched via:

In order to not be asked for a password each time Wine is run as another user the following entry can be added to the sudoers file: mainuser ALL=(wineuser) NOPASSWD: ALL. See Sudo#Configuration for more information.

It is recommended to run winecfg as the Wine user and remove all bindings for directories outside the home directory of the Wine user in the "Desktop Integration" section of the configuration window so no program run with Wine has read access to any file outside the special user's home directory.

Keep in mind that audio will probably be non-functional in Wine programs which are run this way if PulseAudio is used. See PulseAudio/Examples#Allowing multiple users to share a PulseAudio daemon for information about allowing the Wine user to access the PulseAudio daemon of the principal user.

To prevent Wine from writing its temporary files to a physical disk, one can define an alternative location, like tmpfs. Remove Wine's default directory for temporary files and creating a symlink:

If Gecko and/or Mono are not present on the system nor in the Wine prefix, Wine will prompt to download them from the internet. If you do not need Gecko and/or Mono, you might want to disable this dialog, by setting the WINEDLLOVERRIDES environment variable to mscoree=d;mshtml=d.

For security reasons it may be useful to remove the preinstalled Wine bindings so Windows applications cannot be launched directly from a file manager or from the browser (Firefox offers to open EXE files directly with Wine!). If you want to do this, you may add the following NoExtract lines in /etc/pacman.conf:

Every time Wine creates (or updates) a prefix it will set its own bundled apps like Notepad and Winebrowser as the default text editor and web browser accordingly.

A way to work around this undesirable behavior is by using this environment variable:

If you need professional audio support under wine you can use wineasioAUR which provides an ASIO interface for wine that you can then use with JACK.

In order to use wineasio you must add yourself to the realtime user group.

Next you need to register wineasio in your desired wine prefix. Register the 32-bit and/or 64-bit version as needed:

If you run a text mode (Command User Interface) executable without X installed, these errors might appear while starting the executable:

This is because wine by default starts explorer.exe. Even wineconsole starts explorer.exe /desktop according to ps output.

Starting explorer including systray can be disabled with this environment setting:

Depending on your CUI program, you might be able to use it with lowest memory footprint by disabling services.exe too:

See Wine User's Guide and Wine FAQ (especially its Troubleshooting section) for general tips.

Also refer to the Wine AppDB for an advice on specific applications.

Each Wine prefix has a lot of persistent state, between the installed programs and the registry. The first step to troubleshooting issues with program installation should be to either create an isolated prefix, or clear the default prefix via rm -rf ~/.wine. The latter will delete any of the programs and settings you have added to the default prefix.

You might get the following error when running wine:

This is caused by the syscall to mmap2 failing:

This is a known bug in the kernel.

Changing the vm.mmap_min_addr sysctl value from the default of 65536 seems to fix the problem:

If you use Wine under Xwayland, you can activate the option for "Emulating a virtual desktop" in the Graphics Tab in winecfg, to avoid problems with:

If disabling the Virtual Desktop left you unable to interact with the winecfg window with mouse & keyboard anymore, you can explicitly start winecfg on a Virtual Desktop anyway and reenable it with:

When starting GUI windows (eg. winecfg) with Wayland and none are displayed with these errors in console:

You may try setting the DISPLAY variable to :1:

This could be caused by the window manager not switching focus. In the Graphics tab of winecfg, disable the 'Allow the window manager...' options, or set windowed mode with 'Emulate a virtual desktop'.

If the keyboard does not work after unfocusing the application, try editing the registry:

Some older games and applications assume that the current working directory is the same as that which the executable is in. Launching these executables from other locations will prevent them from starting correctly. Use cd path_containing_exe before invoking Wine to rule this possibility out.

**Examples:**

Example 1 (unknown):
```unknown
winetricks gmdls dsound directmusic
```

Example 2 (unknown):
```unknown
winetricks verb_name
```

Example 3 (unknown):
```unknown
wine control
```

Example 4 (unknown):
```unknown
env WINEPREFIX=~/.win-a wine program-a.exe
```

---

## man page

**URL:** https://wiki.archlinux.org/title/Man

**Contents:**
- Installation
- Accessing man pages
- Searching manuals
- Page width
- Reading local man pages
  - Conversion to HTML
    - mandoc
    - man2html
    - man -H
    - roffit

man pagesabbreviation for "manual pages"are the form of documentation that is available on almost all UNIX-like operating systems, including Arch Linux. The command used to display them is man(1).

In spite of their scope, manual pages are designed to be self-contained documents, consequently limiting themselves to referring to other manual pages when discussing related subjects. This is in sharp contrast with the hyperlink-aware Info documentsGNU attempt at replacing the traditional manual page format.

man-db implements man on Arch Linux, and less is the default pager used with man. mandoc can also be used.

man-pages provides both the Linux and the POSIX.1 man pages [1].

Some localized man pages are also available:

You can also search for all of the available localized man pages on the official repositories and on the AUR.

To read a man page, simply enter:

Manuals are sorted into several sections. Each section has an intro, such as intro(1), intro(2) and so on. For a full listing see man-pages(7)  Sections of the manual pages.

Man pages are usually referred to by their name, followed by their section number in parentheses. Often there are multiple man pages of the same name, such as man(1) and man(7). In this case, give man the section number followed by the name of the man page, for example:

to read the man page on /etc/passwd, rather than the passwd utility.

Or equivalently, the man page followed by the section number, separated by a period:

Man pages can be searched when the exact name of a page is not known using any of the following equivalent commands:

expression is interpreted as a regular expression by default.

To search for keywords in whole page texts, use the -K option instead.

One-line descriptions of man pages can be displayed using the whatis command. For example, for a brief description of the man page sections about ls, type:

The man page width is controlled by the MANWIDTH environment variable.

If the number of columns in the terminal is too small (e.g. the window width is narrow), the line breaks will be wrong. This can be very disturbing for reading. You can fix this by setting the MANWIDTH on man invocation. With Bash, that would be:

You can use some applications to view man pages:

Using browsers such as lynx and Firefox to view man pages allows users to reap info pages' main benefit of hyperlinked text. Alternatives include the following:

Install the mandoc package. To convert a page, for example free(1):

Now open the file called free.html in your favourite browser.

First, install man2html from the official repositories.

Now, convert a man page:

Another use for man2html is exporting to raw, printer-friendly text:

The man-db implementation also has the ability to do this on its own:

This will read your BROWSER environment variable to determine the browser. You can override this by passing the binary to the -H option.

First install roffitAUR.

To convert a man page:

man pages have always been printable: they are written in troff(1), which is fundamentally a typesetting language. Therefore, you can easily convert man pages to any of the formats supported as output devices by groff, which is used by man-db. For a list of output devices, see the -T option in groff(1) (or mandoc(1) if you use the mandoc package).

This will produce a PDF file:

Caveats: Fonts are generally limited to Times at hardcoded sizes. Some man pages were specifically designed for terminal viewing, and will not look right in PS or PDF form.

For an alternative interface for reading manual pages, one that supports modern features such as hyperlinks and history, install qmanAUR or qman-gitAUR. You can now use qman in the place of man:

For more information and troubleshooting, see the project's GitHub page.

Note that while man-pages provides man pages for POSIX.1-2017 (see [2]), an official online reference also exists:

There is also a comparison table of the online databases.

Here follows a non-exhaustive list of noteworthy pages that might help you understand a lot of things more in-depth. Some of them might serve as a good reference (like the ASCII table).

More generally, have a look at category 7 (miscellaneous) pages:

Arch Linux specific pages:

This article or section needs expansion.

**Examples:**

Example 1 (unknown):
```unknown
$ man page_name
```

Example 2 (unknown):
```unknown
$ man 5 passwd
```

Example 3 (unknown):
```unknown
/etc/passwd
```

Example 4 (unknown):
```unknown
$ man passwd.5
```

---

## Qt

**URL:** https://wiki.archlinux.org/title/Qt

**Contents:**
- Installation
- Known issues
- Default Qt toolkit
- Configuration
  - Styles in Qt 5
  - Qt Style Sheets
  - GTK and Qt
  - Configuration of Qt 5/6 applications under environments other than KDE Plasma
- Development
  - Supported platforms

Qt is a cross-platform application and widget toolkit that uses standard C++ but makes extensive use of a special code generator (called the Meta Object Compiler, or moc) together with several macros to enrich the language. Some of its more important features include:

The Qt framework is the basis of the KDE software community, as well as other important open source and proprietary applications such as VLC, VirtualBox, Mathematica and many others.

Qt 6.x and 5.x are available in the official repositories. Legacy versions of Qt (4.x and 3.x) are available from the AUR. They can be installed with the following packages:

This article or section is being considered for removal.

To define the default Qt toolkit, you can create the QT_SELECT environment variable. For example, to use Qt n, set QT_SELECT=n.

This article or section is out of date.

This article or section is out of date.

Qt 5 decides the style to use based on what desktop environment is used:

To force a specific style, you can set the QT_STYLE_OVERRIDE environment variable. Specifically, set it to gtk2 if you want to use the GTK theme (Note: you will need to install the Qt style plugins mentioned below to get the GTK style). Qt 5 applications also support the -style flag, which you can use to launch a Qt 5 application with a specific style.

The following styles are included in Qt 5: Fusion, Windows. Others can be installed separately:

An interesting way of customizing the look and feel of a Qt application is using Style Sheets, which are just simple CSS files. Using Style Sheets, one can modify the appearance of every widget in the application.

To run an application with a different style just execute:

For more information on Qt Style Sheets see the official documentation or other tutorials. As an example Style Sheet see this Dolphin modification.

If you have GTK and Qt applications, their looks might not exactly blend in very well. If you wish to make your GTK styles match your Qt styles please read Uniform look for Qt and GTK applications.

Unlike Qt 4, Qt 5 does not ship a qtconfig utility to configure fonts, icons or styles. Instead, it will try to use the settings from the running desktop environment. In KDE Plasma or GNOME this works well, but in other less popular desktop environments or window managers it can lead to missing icons in Qt 5 applications. One way to solve this is to fake the running desktop environment by setting XDG_CURRENT_DESKTOP=KDE or GNOME, and then using the corresponding configuration application to set the desired icon set.

Another solution is provided by the qt5ct/qt6ct packages, which provide a QPA independent of the desktop environment and a configuration utility. After installing package, run qt5ct/qt6ct to set an icon theme, and set the environment variable QT_QPA_PLATFORMTHEME=qt5ct:qt6ct so that the settings are picked up by Qt applications. Alternatively, use --platformtheme qt5ct as argument to the Qt 5 application.

qt5ct-kdeAUR and qt6ct-kdeAUR provide patched qt5ct/qt6ct with better integration to KDE applications, including KDE QML applications and can read and apply KColorSchemes.

If the errors below are received, and some icons still do not appear in some of the applications, install oxygen and oxygen-icons:

Qt supports most platforms that are available today, even some of the more obscure ones, with more ports appearing every once in a while. For a more complete list see the Qt Wikipedia article.

First of all, you need an Android SDK and NDK from AUR or using Android Studio.

SDK requires OpenJDK too. Different Qt versions have different version requirements, check here for detail.

Next you are going to need Qt 5 for Android. You can install it from AUR as described below or build it yourself, you can find build instructions on Qt wiki page.

In case of problems you may want to visit known issues.

This article or section is out of date.

Alternatively, you can use the official Qt installer.

The following are official Qt tools:

When using KDE and/or any other Qt desktop environment debug info may be frequently logged in the systemd journal.

Set QT_LOGGING_RULES as environment variable to change this behaviour, e.g. to completely disable logging:

To disable only debug logging, use QT_LOGGING_RULES="*.debug=false".

Since Qt 5.1 SVG support has moved into a module. Since qt5-base does not depend on qt5-svg it may happen that the qt5-base is installed but not qt5-svg. This results in deceptive icon theme behaviour. Since SVG is not supported the icons are silently skipped and the icon theme may seem to be unused. Installing qt5-svg explicitly solves the problem.

As the user theme file ($XDG_CONFIG_HOME/Trolltech.conf), are not read by other accounts, the selected theme will not apply to X applications run as root. Possible solutions include:

If pure Qt 4 (non-KDE) applications do not stick with your selected Qt 4 style, then you will probably need to tell Qt 4 how to find KDE's styles (Oxygen, Phase etc.). You just need to set the environment variable QT_PLUGIN_PATH. E.g.:

qtconfig-qt4 should then be able to find your kde styles and everything should look nice again!

Alternatively, you can symlink the Qt 4 styles directory to the KDE4 styles one:

If you get an error similar to

then you are most likely using a Qt 5 platform theme or style plugin which has not been recompiled against the latest version of Qt 5. These usually use Qt private headers which means they depend on an exact version of Qt and not just a matching soname. Figure out which theme/style you are using by checking the QT_STYLE_OVERRIDE and QT_QPA_PLATFORMTHEME environment variables, and rebuild the AUR package that provides it.

Create a file with such content [1]:

If you have set the right keyboard configuration and dead keys are working in GTK apps (or other widget toolkits) but not in KDE or any Qt apps, then you might not have the proper compose file loaded in your Xorg session.

A way to confirm that is to:

To fix this, first identify your locale. Then, if your locale doesn't have its own folder in /usr/share/X11/locale/, eg. fr_FR.UTF-8, look for it in the compose.dir mapping file to find the corresponding compose file (eg. en_US.UTF-8/Compose):

Now create or edit ~/.XCompose to include this compose file:

Finally, restart your Qt app, dead keys should be working and qt.xkb.compose: failed to create compose table error should have disappeared whenever you debug with QT_LOGGING_RULES=qt.xkb.compose.debug=true.

The ~/.config/QtProject.conf file has been filled with garbage data and weighs hundreds of MiB: remove it. [2][3]

Qt introduced text-input-v3 for Wayland platform. However, the implementation is not perfect and may cause serious issues preventing the usage of input method.

Set the following environment variable globally if your Wayland compositor supports text-input-v2:

This article or section is being considered for removal.

Qt does not support automatically looking up the best font for emojis, and therefore the user must manually add a color emoji font as a fallback. [4]

To do this, save the contents of this gist to /etc/fonts/conf.d/75-noto-color-emoji.conf. This adds noto-fonts-emoji as a fallback, though "Noto Color Emoji" can be replaced with a different font family if the user desires.

Afterwards, relaunch the Qt applications affected to load the new font settings.

Some Qt apps, like keepassxc, crash [5] or suffer scaling issues [6] on Wayland.

To fix that, install qt5-wayland or qt6-wayland as explained on the Qt section of the Wayland page.

Some Qt 5 apps like keepassxc, fails to use the Global Menu of KDE [7].

To fix that, install plasma5-integration as explained on the Troubleshooting section of the KDE page.

Qt 6 apps shouldn't pose a problem since plasma-integration is pulled by plasma-workspace.

**Examples:**

Example 1 (unknown):
```unknown
QT_SELECT=n
```

Example 2 (unknown):
```unknown
QT_STYLE_OVERRIDE
```

Example 3 (unknown):
```unknown
$ qt_application -stylesheet style.qss
```

Example 4 (unknown):
```unknown
XDG_CURRENT_DESKTOP=KDE
```

---

## hdparm

**URL:** https://wiki.archlinux.org/title/Hdparm

**Contents:**
- Installation
- Usage
  - Disk info
  - Benchmarking
  - Power management configuration
  - Write cache
  - Power off a hard disk drive
- Tips and tricks
  - Querying the status of the disk without waking it up
  - Persistent configuration using udev rule

hdparm and sdparm are command line utilities to set and view hardware parameters of hard disk drives. hdparm can also be used as a simple benchmarking tool.

Originally, hdparm was created for IDE disks and sdparm for SCSI disks. Since approximately 2010 onward, storage device interfaces are an enhanced mixture of both IDE and SCSI, hdparm and sdparm complement each other.

Install the hdparm package. For use with SCSI devices, install the sdparm package.

To get information about hard disks, run the following:

hdparm can be used for Benchmarking#hdparm.

Modern hard drives support numerous power management features. The most common ones are summarized in the following table. See hdparm(8) for the complete list.

To query current value of -B, pass the parameter without a value:

To apply different value, for example set APM to 127:

Write caching is the process of temporarily caching files in the drive's embedded memory before writing them to the disk, which is essentially a performance boost. Write cache is a feature provided by most hard drives, and it is enabled by default in most cases. To check if that's the case, run:

If it is disabled, one may enable it with:

Conversely, to disable it, use:

The factual accuracy of this article or section is disputed.

For an example of forcing disk write caches to be persistently disabled on a machine, we can create a local script /usr/local/sbin/write-cache-disabler that uses hdparm and lsblk(8) to periodically probe all attached disks and apply the desired setting. This will disable write caches on boot when the service is first ran, and can catch within 30 seconds any infrequent cases that re-enable the write cache, i.e. disk additions and system suspensions.

Make sure the script is executable with chmod u+x /usr/local/sbin/write-cache-disabler. We can then run this daemon under systemd by creating a local unit:

Enable write-cache-disabler.service to start it at boot.

A typical usage case, where such a feature is looked for, is with disks connected to a cheap external USB/SATA/FireWire enclosure, or bridge. If it does not properly issue a stop command to the drive when turning off the power switch, the drive is forced to do an emergency head retract. Regularly doing that will, sooner or later, break the drive. One solution is, after one is sure the data has been written to the media, to run a command to power off the drive:

Invoking hdparm with the query option is known to wake-up some drives. In this case, consider smartctl provided by smartmontools to query the device which will not wake up a sleeping disk. For example:

To make the setting persistent across reboot or sleep, one can use a udev rule:

Because a disk device can be assigned randomly to a changing /dev/sdX, the disk can also be identified by its serial as explained in Udev#Identifying a disk by its serial.

Systems with multiple hard drives can apply the rule in a flexible way according to some criteria. For example, to apply power-saving settings to all rotational drives (hard disk with rotational head, excluding in particular solid state drives), use the following rule:

If the configuration is lost after system suspension/hibernation, it can be reapplied using systemd-sleep.

Put a script into /usr/lib/systemd/system-sleep/ and make it executable:

A device which is rarely needed can be put to sleep directly at the end of the boot process. This does not work with the above udev rule because it happens too early. In order to issue the command when the boot is completed, just create a systemd service and enable it:

Some drives do not support spin down via hdparm. A diagnostic error message similar to the following is a good indication this is the case:

For some other drives, the hdparm command is acknowledged but the drive do not respect the parameters (either APM or spin down timer). This was observed with a Toshiba P300 (model HDWD120) HDD.

Such drives can be spun down using hd-idleAUR which ships with a systemd service. One need to edit /etc/conf.d/hd-idle and the HD_IDLE_OPTS value, then start and enable hd-idle.service.

Example using a 10 min idle time for /dev/sda and a 1 min idle time for /dev/disk/by-uuid/01CF0AC9AA5EAF70:

the leading -i 0 parameter indicates that hd-idle is disabled on other drives.

Western Digital hard drives have a special idle3 timer which controls how long the drive waits before positioning its heads in their park position and entering a low power consumption state. There are different ways to amend the idle3 state:

The APM level may get reset after a suspend requiring it to be re-executed after each resume. This can be automated with the following systemd unit (adapted from a forum thread):

Alternatively, create a hook in /usr/lib/systemd/system-sleep.

**Examples:**

Example 1 (unknown):
```unknown
# hdparm -I /dev/sda
```

Example 2 (unknown):
```unknown
# hdparm -B /dev/sda
```

Example 3 (unknown):
```unknown
# hdparm -B 127 /dev/sda
```

Example 4 (unknown):
```unknown
# hdparm -W /dev/sdX
```

---

## Keyboard input

**URL:** https://wiki.archlinux.org/title/Keysym

**Contents:**
- Identifying scancodes
  - Using evtest
  - Using showkey
  - Using dmesg
- Identifying keycodes
  - Identifying keycodes in console
  - Identifying keycodes in Xorg
  - Identifying keycodes in Wayland
- Tips and tricks
  - Configuration of VIA compatible keyboards

Prerequisite for modifying the key mapping is knowing how a key press results in a symbol:

Most of your keys should already have a keycode, or at least a scancode. Keys without a scancode are not recognized by the kernel; these can include additional keys from "gaming" keyboards, etc.

In Xorg, some keysyms (e.g. XF86AudioPlay, XF86AudioRaiseVolume etc.) can be mapped to actions (i.e. launching an external application). See Keyboard shortcuts#Xorg for details.

In Linux console, some keysyms (e.g. F1 to F246) can be mapped to certain actions (e.g. switch to other console or print some sequence of characters). See Console keyboard configuration#Creating a custom keymap for details.

The most reliable way to obtain a scancode is to reference the MSC_SCAN evdev event produced when the key is pressed [1]. There are multiple evdev API testers, but the most straightforward is evtest(1) from the evtest package:

Use the "value" field of MSC_SCAN. This example shows that NumLock has scancode 70053 and keycode 69.

The traditional way to get a scancode is to use the showkey(1) utility. showkey waits for a key to be pressed, or exits if no keys are pressed within 10 seconds. For showkey to work you need to be in a virtual console, not in a graphical environment or logged in via a network connection. Run the following command:

and try to push keyboard keys; you should see scancodes being printed to the output.

You can get the scancode of a key by pressing the desired key and looking at the output of dmesg. For example, if you get:

then the scancode you need is 0xa0.

The Linux keycodes are defined in /usr/include/linux/input-event-codes.h (see the KEY_ variables).

The keycodes for virtual console are reported by the showkey(1) utility. showkey waits for a key to be pressed and if none are, in a span of 10 seconds, it quits. To execute showkey, you need to be in a virtual console, not in a graphical environment. Run the following command:

and try to push keyboard keys; you should see keycodes being printed to the output.

This article or section needs expansion.

The keycodes used by Xorg are reported by a utility called xev(1), which is provided by the xorg-xev package. Of course to execute xev, you need to be in a graphical environment, not in the console.

With the following command you can start xev and show only the relevant parts:

Xbindkeys is another wrapper to xev that reports keycodes.

If you press a key and nothing appears in the terminal, it means that either the key does not have a scancode, the scancode is not mapped to a keycode, or some other process is capturing the keypress. If you suspect that a process listening to X server is capturing the keypress, you can try running xev from a clean X session:

Although xev works through xwayland, you can also use wev to access keycodes under pure Wayland.

For example, this command lets you retrieve only key names and their UTF-8 equivalent:

VIA is a program to remap keys directly into compatible keyboards. In case you have one of those, in order for the keyboard to be picked up by the browser and configure it online, you need to add a custom udev rule changing the permissions of devices accessed through the hidraw driver.

Create the following udev rule:

Then reload the rules to take effect.

**Examples:**

Example 1 (unknown):
```unknown
XF86AudioPlay
```

Example 2 (unknown):
```unknown
XF86AudioRaiseVolume
```

Example 3 (unknown):
```unknown
# evtest /dev/input/event12
```

Example 4 (unknown):
```unknown
...
Event: time 1434666536.001123, type 4 (EV_MSC), code 4 (MSC_SCAN), value 70053
Event: time 1434666536.001123, type 1 (EV_KEY), code 69 (KEY_NUMLOCK), value 0
Event: time 1434666536.001123, -------------- EV_SYN ------------
```

---

## fish

**URL:** https://wiki.archlinux.org/title/Fish

**Contents:**
- Installation
- System integration
  - Setting fish as default shell
  - Setting fish as interactive shell only
    - Modify .bashrc to drop into fish
    - Use terminal emulator options
    - Use terminal multiplexer options
- Configuration
  - Web interface
  - Command completion

fish, the friendly interactive shell, is a commandline shell intended to be interactive and user-friendly.

fish is intentionally not fully POSIX compliant, it aims at addressing POSIX inconsistencies (as perceived by the creators) with a simplified or a different syntax. This means that even simple POSIX compliant scripts may require some significant adaptation or even full rewriting to run with fish.

Install the fish package.

Once installed, simply type fish to drop into the fish shell.

Documentation can be found by typing help from fish; it will be opened in a web browser. It is recommended to read at least the "Syntax overview" section, since fish's syntax is different from many other shells.

One must decide whether fish is going to be the default user's shell, which means that the user falls directly in fish at login, or whether it is used in interactive terminal mode as a child process of the current default shell, here we will assume the latter is Bash. To elaborate on these two setups:

If you decide to set fish as the default user shell, the first step is to set the shell of this particular user to /usr/bin/fish. This can be done by following the instructions in Command-line shell#Changing your default shell.

The next step is to port the current needed actions and configuration performed in the various Bash initialization scripts, namely /etc/profile, ~/.bash_profile, /etc/bash.bashrc and ~/.bashrc, into the fish framework.

In particular, the content of the $PATH environment variable, once directly logged under fish, should be checked and adjusted to one's need. In fish, $PATH is defined as a global environment variable: it has a global scope across all functions, it is lost upon reboot and it is an environment variable which means it is exported to child processes. The recommended way of adding additional locations to the path is by calling the fish_add_path command from config.fish. For example:

These three locations will be prepended to the path.

Not setting fish as system wide or user default allows the current Bash scripts to run on startup. It ensures the current user's environment variables are unchanged and are exported to fish which then runs as a Bash child. Below are several ways of running fish in interactive mode without setting it as the default shell.

Keep the default shell as Bash and simply add the line exec fish to the appropriate Bash#Configuration files, such as .bashrc. This will allow Bash to properly source /etc/profile and all files in /etc/profile.d. Because fish replaces the Bash process, exiting fish will also exit the terminal. Compared to the following options, this is the most universal solution, since it works both on a local machine and on a SSH server.

Another option is to open your terminal emulator with a command line option that executes fish. For most terminals this is the -e switch, so for example, to open gnome-terminal using fish, change your shortcut to use:

With terminal emulators that do not support setting the shell, for example lilyterm-gitAUR, it would look like this:

Also, depending on the terminal, you may be able to set fish as the default shell in either the terminal configuration or the terminal profile.

To set fish as the shell started in tmux, put this into your ~/.tmux.conf:

Whenever you run tmux, you will be dropped into fish.

The configuration file runs at every login and is located at ~/.config/fish/config.fish. Adding commands or functions to the file will execute/define them when opening a terminal, similar to .bashrc. Note that whenever a variable needs to be preserved, it should be set as universal rather than defined in the aforementioned configuration file.

The user's functions are located in the directory ~/.config/fish/functions under the filenames function_name.fish.

The fish terminal colors, prompt, functions, variables, history, bindings and abbreviations can be set with the interactive web interface:

It may fail to start if IPv6 has been disabled. See [1] and IPv6#Disable IPv6.

fish can generate autocompletions from man pages. Completions are written to ~/.cache/fish/generated_completions (controlled by XDG_CACHE_HOME environment variable) and can be generated by calling:

You can also define your own completions in ~/.config/fish/completions/. See /usr/share/fish/completions/ for a few examples.

Context-aware completions for Arch Linux-specific commands like pacman, pacman-key, makepkg, pbget, pacmatic are built into fish, since the policy of the fish development is to include all the existent completions in the upstream tarball. The memory management is clever enough to avoid any negative impact on resources.

fish does not implement Bash style history substitution (e.g. sudo !!), and the developers recommend in the fish faq to use the interactive history recall interface instead: the Up arrow recalls whole past lines and Alt+Up (or Alt+.) recalls individual arguments, while Alt+s prepends sudo to the existing line.

However some workarounds are described in the fish wiki: while not providing complete history substitution, some functions replace !! with the previous command or !$ with the previous last argument.

By default, fish prints a greeting message at startup. To disable it, run once:

This clears the universal fish_greeting variable, shared with all fish instances and which is preserved upon restart of the shell.

If su starts with Bash because Bash is the target user's (root if no username is provided) default shell, one can define a function to redirect it to fish whatever the user's shell:

Add the following to the bottom of your ~/.config/fish/config.fish.

For those running fish in interactive mode, replace status is-login with status is-interactive in the above code.

If you would like fish to display the branch and dirty status when you are in a git directory, you can define the following fish_prompt function:

To color the hostname in the prompt dynamically whenever connected through SSH, add the following lines in either the fish_prompt function or the fish configuration file, here using the red color:

Note that the default fish prompt colors the hostname yellow when connected through SSH; additional modification to the prompt is not required for this functionality.

In fish, eval (ssh-agent) generate errors due to how variables are set. To work around this, use the csh-style option -c:

Because keychain --eval uses the SHELL environment variable to format output, it should work with fish as the default shell. However, if using fish as an interactive shell only, this variable must be set:

Fish includes a "command not found" hook that will automatically search the official repositories, when entering an unrecognized command. This hook will be run using pkgfile, falling back to pacman -F if it is not installed.

Since 3.2.2, "command not found" will not fallback to pacman -F by default due to its bad performance.

If the delay this behavior introduces is undesirable, this hook can be overridden by redefining fish_command_not_found so that it only prints an error message:

To make this change permanent, the funcsave built-in can be used:

fish terminates any jobs put into the background when fish terminates. To keep a job running after fish terminates, first use the disown builtin. For example, the following starts firefox in the background and then disowns it:

This means firefox will not be closed when the fish process is closed. See disown(1) in fish for more details.

To quickly make a persistent alias, one can simply use the method showed in this example:

alias supports the -s/--save option since fish version 3.0:

This will create the function:

and will set the alias as a persistent shell function. To see all functions and/or edit them, one can simply use fish_config and look into the Function tab in the web configuration page.

For more detailed information, see alias - create a function  fish-shell.

Unlike bash, fish does not source /etc/profile on a tty login. If you need to source this file for the environment variables appended and declared in the /etc/profile.d directory, you can add the following to your config:

This allows you to run fish as your login shell, while still having all the environment variables you would typically have in a bash login session.

**Examples:**

Example 1 (unknown):
```unknown
/usr/bin/fish
```

Example 2 (unknown):
```unknown
/etc/profile
```

Example 3 (unknown):
```unknown
~/.bash_profile
```

Example 4 (unknown):
```unknown
/etc/bash.bashrc
```

---

## dd

**URL:** https://wiki.archlinux.org/title/Dd

**Contents:**
- Installation
- Disk cloning and restore
  - Cloning a partition
  - Cloning an entire hard disk
  - Backing up the partition table
  - Create disk image
  - Restore system
- Backup and restore MBR
  - Remove boot loader
- Extra scenario in addition to disk-related scenes

This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.

dd is a core utility whose primary purpose is to copy a file and optionally convert it during the copy process.

Similarly to cp, by default dd makes a bit-to-bit copy of the file, but with lower-level I/O flow control features.

For more information, see dd(1) or the full documentation.

dd is part of the GNU coreutils. For other utilities in the package, please refer to Core utilities.

The dd command is a simple, yet versatile and powerful tool. It can be used to copy from source to destination, block-by-block, regardless of their filesystem types or operating systems. A convenient method is to use dd from a live environment, as in a Live CD.

From physical disk /dev/sda, partition 1, to physical disk /dev/sdb, partition 1:

From physical disk /dev/sda to physical disk /dev/sdb:

This will clone the entire drive, including the partition table, boot loader, all partitions, UUIDs, and data.

See fdisk#Backup and restore partition table or gdisk#Backup and restore partition table.

Boot from a live medium and make sure no partitions are mounted from the source hard drive.

Then mount the external hard drive and backup the drive:

If necessary (e.g. when the resulting files will be stored on a FAT32 file system), split the disk image into multiple parts (see also split(1)):

If there is not enough disk space locally, you may send the image through ssh:

Finally, save extra information about the drive geometry necessary in order to interpret the partition table stored within the image. The most important of which is the sector size.

To restore your system:

When the image has been split, use the following instead:

Before making changes to a disk, you may want to backup the partition table and partition scheme of the drive. You can also use a backup to copy the same partition layout to numerous drives.

The MBR is stored in the first 512 bytes of the disk. It consists of 4 parts:

To save the MBR as mbr_file.img:

You can also extract the MBR from a full dd disk image:

To restore (be careful, this destroys the existing partition table and with it access to all data on the disk):

If you only want to restore the boot loader, but not the primary partition table entries, just restore the first 440 bytes of the MBR:

To restore only the partition table, one must use:

To erase the MBR bootstrap code (may be useful if you have to do a full reinstall of another operating system), only the first 440 bytes need to be zeroed:

This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.

As some readers might have already realised, the dd(1) core utility has a different command-line syntax compared to other utilities. Moreover, while supporting some unique features not found in other commodity utilities, several default behaviours it has are either less-ideal or potentially error-prone if applied to specific scenarios. For that reason, users may want to use some alternatives that are better in some aspects in lieu of the dd core utility.

That said, it is still worth to note that since dd is a core utility, which is installed by default on Arch and many other systems, it is preferable to some alternatives or more specialised utilities if it is inconvenient to install a new package on your system.

To cover the two aspects that are addressed above, this section is dedicated to summarising the features of the dd(1) core utility that are rarely found in other commodity utilities  in a form that resembles the pacman/Rosetta article but with the quantity of examples being cut down to examine the features of dd (as denoted by i.e. or To-clause in "Tip:" box under subsection), either in practice or pseudocode.

It is not an uncommon practice to use dd as a feature-limited binary file patcher in an automated shell script as it supports:

Here is an example to modify the timestamp part of the first member in a cpio(5)  Portable ASCII Format archive, which starts at the 49th byte of the file (or with an offset of 0x30 if you prefer hex notation):

To read the filesystem volume label of an VFAT image file, which should be in total length of 11 bytes that padded by ASCII spaces, with an offset of 0x047:

In the following example, to avoid unnecessary long-lasting TCP connection on input end if the output end blocks longer than expected, one may put a dd between two commands with an output block size certainly larger than input while still reasonably smaller than available memory:

It is a general practice to use dd in a data streaming shell script for limiting total length of data that a piped command may consume. For example, to inspect an ustar header block (tar(5)  POSIX ustar Archives) using a shell script function in a streaming manner:

See USB flash installation medium#Using basic command line utilities for examples of commodity utilities include the potential least adapted dd for that case.

Files created with dd can end up with a smaller size than requested if a full input block is not available for the moment, as per documentation:

On Linux, the underlying read(2) system call may returns early (i.e. partial read) when reading from a pipe(7), or when reading a device file like /dev/urandom and /dev/random (e.g. due to hardcoded limitation of underlying kernel device driver) Which make the total size of copied data smaller than expected when bs in conjunction of count=n option is used, where n limited the maximum number of (potential partial) input block(s) to copy to output.

It is possible, but not guaranteed, that dd will warn you about such kind of issue:

The solution is to do as the warning says, add iflag=fullblock option in addition to the input file option to the dd command. For example, to create a new file filled up with random data in total length of 40 mebibytes:

When reading from a pipe, an alternative to iflag=fullblock is to limit bs to the PIPE_BUF constant value as defined in linux/limits.h to make the pipe(7) I/O atomic. For example, to prepare a text file filled up will random alphanumeric string in total length of 5 mebibytes:

Since the output file is not a pipe, one may prefer to use ibs and obs options to set block size separately for the (input) pipe and the (output) on-disk file. For example, to set a more efficient block size for output file:

The total transferred bytes count readout may be greater than actual if an error is encountered on writing to output (i.e. partial write, caused by e.g. SIGPIPE, faulty medium, or accidentally disconnected the target network block device), like in following proof of concept where the second dd obviously will not read more than 512200 bytes, but the first dd instance still report an inaccurate bytes count 512400 bytes:

When resuming an interrupted transfer like the above PoC, it is recommended to only rely on the readout of number of whole output blocks already copied, as denoted by the number before "+" sign.

**Examples:**

Example 1 (unknown):
```unknown
status=progress
```

Example 2 (unknown):
```unknown
# dd if=/dev/sda1 of=/dev/sdb1 bs=64M status=progress
```

Example 3 (unknown):
```unknown
# dd if=/dev/sda of=/dev/sdb bs=64M status=progress
```

Example 4 (unknown):
```unknown
status=progress
```

---

## Audit framework

**URL:** https://wiki.archlinux.org/title/Audit

**Contents:**
- Installation
- Adding rules
  - Audit files and directories access
  - Audit syscalls
  - Filter unwanted messages
- Search the logs
  - Using pid
  - Using keys
  - Look for abnormalities
- Which files or syscalls are worth-auditing?

The Linux audit framework provides a CAPP-compliant (Controlled Access Protection Profile) auditing system that reliably collects information about any security-relevant (or non-security-relevant) event on a system. It can help you track actions performed on a system.

Linux audit helps make your system more secure by providing you with means to analyze what is happening on your system in great detail. It does not, however, provide additional security itselfit does not protect your system from code malfunctions or any kind of exploits. Instead, Audit is useful for tracking these issues and helps you take additional security measures to prevent them.

The audit framework works by listening to the event reported by the kernel and logging them to a log file.

In-kernel audit support is available in all officially supported kernels. For custom kernels CONFIG_AUDIT should be enabled. Userspace support is provided by the audit package that is already installed as a dependency of base packages.

Audit can be enabled at boot-time by setting audit=1 as kernel parameter. This will ensure that all processes that run before the audit daemon starts are marked as auditable by the kernel. Not doing that will make a few processes impossible to properly audit. See auditd(8).

Audit framework is composed of the auditd daemon, responsible for writing the audit messages that were generated through the audit kernel interface and triggered by application and system activity. Start/enable auditd.service to activate the daemon.

This daemon can be controlled by several commands and files:

Before adding rules, you must know that the audit framework can be very verbose and that each rule must be carefully tested before being effectively deployed. Indeed, just one rule can flood all your logs within a few minutes.

The most basic use of the audit framework is to log the access to the files you want. To do this, you must place a watch on a file or a directory using the option -w followed by a path. The most basic rule to set up is to track accesses to the passwd file:

You can track access to a folder with:

The first rule keeps track of every read r , write w , execution x , attribute change a to the file /etc/passwd. The second one keeps track of any access to the /etc/security/ folder.

You can list all active rules with:

You can delete all rules with:

Once you validate the rules, you can add them to a .rules file in /etc/audit/rules.d/:

The audit framework allows you to audit the syscalls performed with the -a option.

A security related rule is to track the chmod(2) syscall, to detect file ownership changes:

For a list of all syscalls: syscalls(2)

A lot of rules and possibilities are available, see auditctl(8) and audit.rules(7).

In order to prevent noisy audit messages from flooding system logs you may add rules to exclude some of them:

Remember to verify changes (fix as necessary) and regenerate /etc/audit/audit.rules as follows:

The audit framework provides some tools to ease the use and the research of events happening on a system.

You can search events related to a particular pid using ausearch:

This command will show you all the events logged according to your rules related to PID 1 (i.e. systemd).

One of the great features of the audit framework is its ability to use keys to manage events, such a usage is recommended.

You can use the -F key= option in your rules to be able to find related events easily:

Then, if you search for events with the key KEY_pwd, ausearch will display only event related to the file /etc/passwd.

The aureport tool can be used to quickly report any abnormal event performed on the system, it includes network interfaces used in promiscous mode, process or thread crashing or exiting with ENOMEM error etc.

The easiest way to use aureport is:

aureport can be used to generate custom reports, see aureport(8).

Keep in mind that each audit rule added will generate logs, so you must be ready to treat this amount of information. Basically, each security-related event/file must be monitored, like ids, ips, anti-rootkits etc. On the other side, it is totally useless to track every write syscall, the smallest compilation will fill your logs with this event.

More complex set of rules can be set up, performing auditing on a very fine-grained base. If you want to do so, see auditctl(8).

The audit framework has a plugin system which provides the possibility to send local logfiles to a remote auditd.

To send your logfiles to a remote host you need the audisp-remote plugin which comes automatically with the audit package. Activate the plugin:

and set the remote host where the logs should be send to:

To make audit listen for remote audispds you just need to set the tcp options:

Now you can view the logs of all configured hosts in the logfiles of the receiving auditd.

Send SIGUSR1 to the audit daemon:

For users not having enabled auditd, using kernel debug messages higher than loglevel=4 can result in audit flooding security notices on top of virtual terminal.

These messages can be silenced by enabling auditd.service.

Alternative solutions are:

See the systemd issue 15324 on GitHub for the details.

**Examples:**

Example 1 (unknown):
```unknown
CONFIG_AUDIT
```

Example 2 (unknown):
```unknown
systemd-journald-audit.socket
```

Example 3 (unknown):
```unknown
auditd.service
```

Example 4 (unknown):
```unknown
/etc/audit/audit.rules
```

---

## Cutefish

**URL:** https://wiki.archlinux.org/title/Cutefish

**Contents:**
- Installation
- Starting
  - Graphically
  - Manually
- Configuration
  - Keybindings
  - Terminal colorscheme
- See also

Cutefish is a Desktop Environment written originally for the CutefishOS project that has a focus on simplicity, beauty, and practicality. It is written using the Qt5 framework to provide a simple, universal look and feel.

Cutefish can be installed with the cutefish group.

Cutefish can either be started with a display manager or manually from the console. The display manager included with Cutefish is SDDM.

The cutefish-session can be called from your xinitrc or directly on the startx command line.

Cutefish configuration can be done mainly in the settings application pinned on the dock.

Cutefish does not natively support keybindings, but using the sxhkd daemon we can add custom keybindings.

It can be started through a custom session:

Cutefish Terminal does not natively have a built-in color scheme switcher. We can however edit the source code to include our own colorscheme.

Start with retrieving the PKGBUILD for cutefish-terminal with the ABS.

Edit the theme, either in the building step or manually, which is either Cutefish-Dark.colorscheme or Cutefish-Light.colorscheme in /path/to/the/sources/qmltermwidget/lib/color-schemes/.

To avoid repeating this step at every update, it would be prudent to create a patch to reuse it afterwards.

**Examples:**

Example 1 (unknown):
```unknown
sddm.service
```

Example 2 (unknown):
```unknown
cutefish-session
```

Example 3 (unknown):
```unknown
/usr/bin/cutefish-session-sxhkd
```

Example 4 (unknown):
```unknown
#!/bin/bash
nohup sxhkd & 
cutefish-session
```

---

## Xhost

**URL:** https://wiki.archlinux.org/title/Xhost

**Contents:**
- Installation
- Usage
- The 'cannot connect to X server :0.0' output
- See also

From Xhost man page (boldface added):

See xhost(1) for the full info.

Install the xorg-xhost package.

To provide access to an application running with sudo or su to the graphical server (aka your X session aka your computer screen), open a terminal and type as your normal user:

To get things back to normal, with controlled access to the X screen:

xhost + will disable X authentication entirely. Do not do that unless you really know what are you doing.

The above command xhost + will get you rid of that output, albeit momentarily; one way of getting permanently rid of this issue, among many, is to add

to your ~/.bashrc file. This way, each time you fire up the terminal, the command gets executed. If you do not yet have a .bashrc file in your home directory, it is OK to create one with just this line in it. If you do not add > /dev/null then each time you fire a terminal, you will see a non-disruptive message saying: access control disabled, clients can connect from any host, which is your confirmation that you can now run your_software as root without issue.

**Examples:**

Example 1 (unknown):
```unknown
$XAUTHORITY
```

Example 2 (unknown):
```unknown
$ xhost +SI:localuser:username
```

Example 3 (unknown):
```unknown
xhost + > /dev/null
```

Example 4 (unknown):
```unknown
> /dev/null
```

---

## IBus

**URL:** https://wiki.archlinux.org/title/IBus

**Contents:**
- Installation
- Integration
- Configuration
  - GNOME
  - Other desktop environments
- Tips and tricks
  - Emoji input
  - Unicode input
  - Tray icon color
  - Layout switcher display delay

IBus (Intelligent Input Bus) is an input method framework, a type of application that allows for easily switching between different keyboard layouts. When combined with an input method editor, it also allows for typing non-Latin characters using a keyboard that does not natively support them.

Install the ibus package.

Additionally, see Input method#List of available input method editors for a comprehensive list of available input method editors.

Set at least the following environment variables:

To launch IBus on user login, create an autostart entry with the following command:

On the next login IBus will start along with the user session.

See Locale for help with adding non-Latin language support to your system.

See Fonts#Non-latin scripts for a non-exhaustive list of available non-Latin fonts.

GNOME uses IBus by default, so you can simply go to Settings > Keyboard > Input Sources and add a keyboard layout for the language of your choice.

Some non-Latin languages (e.g. Simplified Chinese) need to enable the Show Extended Input Sources option in GNOME Tweaks.

To launch the IBus preferences window, you can:

The points of interest here are the keyboard shortcut for Next input method (which is the one you will want to use instead of the default shortcut provided by your desktop environment) and the Input Method tab where you can add or remove the different keyboard layouts (which is where you will want to do this instead of your desktop environment's default layout manager).

IBus supports the input of emoji icons. Type Super+. and you will see the input prompt change to an underlined e character. You can then type the symbol or name of the emoji you want (e.g. :) or face) and press Space to render it. If you are satisfied with the result press Enter to submit it and exit emoji input mode, or press Space for a second time to open a dialog where you can further customize your desired emoji.

See ibus-emoji(7) for more information.

IBus supports the input of complex Unicode characters. Type Ctrl+Shift+u and you will see the input prompt change to an underlined u character. You can then type the code of the Unicode character you want and press Space or Enter to render and submit it.

By default, IBus uses a dark blue color to display the language symbol of the currently active layout (e.g. EN). The color value is stored in a gsettings schema, so if you wish to change it you can run the following command:

The string 'COLOR' should conform to the following guidelines:

The RGBA value can be

When pressing the Next input method hotkey, IBus displays a small dialog to signify the layout switch. By default this dialog is displayed 400ms after pressing the key, but this value can be changed by the user, with some interesting choices being '0' to display the dialog immediately without any delay, or a negative value (e.g. '-1') to switch the layouts without showing the window at all (which may be useful if you only use two layouts and simply switch from one to the other).

The value is stored in a gsettings schema, so if you wish to change it you can run the following command:

To show the currently stored value, run the following command:

The string 'VALUE' should conform to the following guidelines:

Set popup delay milliseconds to show IME switcher window. The default is 400.

For some reason, the IBus preferences GUI will not let you use a subset of modifier key combinations (e.g. Alt+Shift_R) as the layout switching hotkey. However, IBus stores the hotkey as a string in a gsettings schema, so you can still use such combinations by directly editing that string:

The string 'VALUE' should be any valid modifier key combination written in a format that IBus recognizes, e.g. <Alt>Shift_R.

For rxvt-unicode to work correctly with IBus, you may need to add the following lines to ~/.Xresources:

If using pyenv to manage python versions and configuration, ibus dependencies may not be installed on the set global python version. This can happen if the global version is not the one managed by the system (to which dependencies that ibus depends on, such as gi are installed). This may result in a running but non-functional ibus setup (ibus is in the system tray, but preferences can not be opened, etc). One way to verify this issue is to attempt to run ibus-setup and see if any missing dependency errors arise.

To fix this issue, change your pyenv global version to the system version:

If you find your settings are being removed consistently after restart (input methods being reverted), you can confirm the engines set to be preloaded and the order in which they will be loaded:

To ensure anthy gets loaded in this case, add it to the org.freedesktop.ibus.preload-engines list:

Changes your changes should be reflected immediately, and should persist across restarts.

A possible fix is to set the GTK_IM_MODULE environment variable to xim instead of ibus:

In certain games, especially those running via Proton, keeping a key pressed for several seconds might result in that key "sticking" (continuing to be registered as pressed) until it is pressed and released again. Limited testing [1] has found that simply killing the IBus daemon is sufficient to resolve the issue.

**Examples:**

Example 1 (unknown):
```unknown
/etc/environment
```

Example 2 (unknown):
```unknown
GTK_IM_MODULE=ibus
QT_IM_MODULE=ibus
XMODIFIERS=@im=ibus
```

Example 3 (unknown):
```unknown
ibus-daemon -rxRd
```

Example 4 (unknown):
```unknown
ibus@$DISPLAY.service
```

---

## Unbound

**URL:** https://wiki.archlinux.org/title/Unbound

**Contents:**
- Installation
- Configuration
  - Local DNS server
  - Root hints
  - DNSSEC validation
    - Testing validation
  - Forwarding queries
    - Allow local network to use DNS
      - Using openresolv
      - Exclude local subnets from answers

Unbound is a validating, recursive, and caching DNS resolver. According to Wikipedia:

Install the unbound package.

Additionally, the expat package is required for #DNSSEC validation.

This article or section needs expansion.

A default configuration is already included at /etc/unbound/unbound.conf. The following sections highlight different settings for the configuration file. See unbound.conf(5) for other settings and more details.

Unless otherwise specified, any options listed in this section are to be placed under the server section in the configuration like so:

If you want to use unbound as your local DNS server, set your nameserver to the loopback addresses ::1 and 127.0.0.1 in /etc/resolv.conf:

Make sure to protect /etc/resolv.conf from modification as described in Domain name resolution#Overwriting of /etc/resolv.conf.

Then run resolvconf -u to generate /etc/resolv.conf.

See Domain name resolution#Lookup utilities on how to test your settings.

Check specifically that the server being used is ::1 or 127.0.0.1 after making permanent changes to resolv.conf.

You can now setup unbound such that it is #Forwarding queries, perhaps all queries, to the DNS servers of your choice.

For recursively querying a host that is not cached as an address, the resolver needs to start at the top of the server tree and query the root servers, to know where to go for the top level domain for the address being queried. Unbound comes with default builtin hints. Therefore, if the package is updated regularly, no manual intervention is required. Otherwise, it is good practice to use a root-hints file since the builtin hints may become outdated.

First point unbound to the root.hints file:

Then, put a root hints file into the unbound configuration directory. The simplest way to do this is to run the command:

When actually using this file, and not the builtin hints, it is a good idea to update root.hints every six months or so in order to make sure the list of root servers is up to date. This can be done manually or by using a systemd timer. See #Roothints systemd timer for an example.

To use DNSSEC validation, the following setting for the server trust anchor should be under server::

This setting is already enabled in the default configuration file.

/etc/unbound/trusted-key.key is copied from /etc/trusted-key.key, which is provided by the dnssec-anchors dependency, whose PKGBUILD generates the file with unbound-anchor(8).

DNSSEC validation will only be done if the DNS server being queried supports it. If general #Forwarding queries have been set to DNS servers that do not support DNSSEC, their answers, whatever they are, should be considered insecure since no DNSSEC validation could be performed.

To test if DNSSEC is working, after starting unbound.service, do:

The response should be the IP address with the word (secure) next to it.

Here the response should include (BOGUS (security failure)).

Additionally you can use drill to test the resolver as follows:

The first command should give an rcode of SERVFAIL. The second should give an rcode of NOERROR.

If you only want to forward queries to an external DNS server, skip ahead to #Forward all remaining requests.

If your network manager supports openresolv, you can configure it to provide local DNS servers and search domains to Unbound:

Run resolvconf -u to generate the file.

Configure Unbound to read the openresolv's generated file and allow replies with private IP address ranges[1]:

Additionally you may want to disable DNSSEC validation for private DNS namespaces (see RFC 6762 Appendix G):

Will be useful to exclude local networks from DNS answers because it would protect against DNS rebinding attacks. By default this feature is not active but you can add any subnet you want in configuration file:

You can add all private and link-local subnets by this strings:

Note that Unbound may have adresses from excluded subnets in answers if they belong to domains from private-domain or specifed by local-data, so you need to define private-domain how described at #Using openresolv to able query local domains adresses.

To include a local DNS server for both forward and reverse local addresses a set of lines similar to these below is necessary with a forward and reverse lookup (choose the IP address of the server providing DNS for the local network accordingly by changing 10.0.0.1 in the lines below):

This line above is important to get the reverse lookup to work correctly.

You can set up the localhost forward and reverse lookups with the following lines:

If your network manager supports openresolv, you can configure it to provide upstream DNS servers to Unbound.

Run resolvconf -u to generate the file.

Finally configure Unbound to read the openresolv's generated file[2]:

To use specific servers for default forward zones that are outside of the local machine and outside of the local network add a forward zone with the name . to the configuration file. In this example, all requests are forwarded to Google's DNS servers:

To use DNS over TLS, you will need to enable the tls-system-cert option, allow unbound to forward TLS requests and also specify any number of servers that allow DNS over TLS.

For each server you will need to specify the connection port using @ and its domain name with #. The domain name is required for TLS authentication and also allows setting stub-zones and using the unbound-control forward control command with domain names. There should not be any spaces in the forward-addr specification.

You can specify the interfaces to answer queries from by IP address. The default, is to listen on localhost.

To listen on all interfaces, use the following:

To control which systems can access the server by IP address, use the access-control option:

action can be one of deny (drop message), refuse (polite error reply), allow (recursive ok), or allow_snoop (recursive and nonrecursive ok). By default everything is refused except for localhost.

Start/enable the unbound.service systemd service.

unbound ships with the unbound-control utility which enables us to remotely administer the unbound server. It is similar to the pdnsd-ctl command of pdnsd.

Before you can start using it, the following steps need to be performed:

1) Firstly, you need to run the following command

which will generate a self-signed certificate and private key for the server, as well as the client. These files will be created in the /etc/unbound directory.

2) After that, edit /etc/unbound/unbound.conf and put the following contents in that. The control-enable: yes option is necessary, the rest can be adjusted as required.

Some of the commands that can be used with unbound-control are:

Please refer to unbound-control(8) for a detailed look at the operations it supports.

To blacklist a domain, use local-zone: "domainname" always_refuse.

Save the blacklist as a separate file (e.g. /etc/unbound/blacklist.conf) for ease of management and include it from /etc/unbound/unbound.conf. For example:

The factual accuracy of this article or section is disputed.

For users who wish to run both a validating, recursive, caching DNS server as well as an authoritative DNS server on a single machine then it may be useful to refer to the wiki page NSD which gives an example of a configuration for such a system. Having one server for authoritative DNS queries and a separate DNS server for the validating, recursive, caching DNS functions gives increased security over a single DNS server providing all of these functions. Many users have used Bind as a single DNS server, and some help on migration from Bind to the combination of running NSD and Bind is provided in the NSD wiki page.

It is also possible to change the configuration files and interfaces on which the server is listening so that DNS queries from machines outside of the local network can access specific machines within the LAN. This is useful for web and mail servers which are accessible from anywhere, and the same techniques can be employed as has been achieved using bind for many years, in combination with suitable port forwarding on firewall machines to forward incoming requests to the right machine.

Here is an example systemd service and timer that update root.hints monthly using the method in #Root hints:

Start/enable the roothints.timer systemd timer.

unbound supports prefetching where cached DNS entries are automatically updated before they expire to keep the cache always up to date. To quote the unbound.conf(5) man page, turning it on gives about 10 percent more traffic and load on the machine, but popular items do not expire from the cache. This is particularly useful on mobile links with high RTT.

To enable prefetching, add this under the server section:

In March 2020, RFC 8767 was published that specifies when and how a resolver can serve stale data from its cache. If the data is unable to be authoritatively refreshed when the TTL expires, the record MAY be used as though it is unexpired. Since version 1.6.0, Unbound has the ability to answer with expired records.

To enable serving expired records, add this under the server section:

unbound.conf(5)  outgoing~2 mentions:

and some sources suggest that the num-threads parameter should be set to the number of cpu cores. The sample unbound.conf.example file merely has:

However it is not possible to arbitrarily increase num-threads above 1 without causing unbound to start with warnings in the logs about exceeding the number of file descriptors. In reality for most users running on small networks or on a single machine it should be unnecessary to seek performance enhancement by increasing num-threads above 1. If you do wish to do so then refer to official documentation and the following rule of thumb should work:

Set the outgoing-range to as large a value as possible, see the sections in the referred web page above on how to overcome the limit of 1024 in total. This services more clients at a time. With 1 core, try 950. With 2 cores, try 450. With 4 cores try 200. The num-queries-per-thread is best set at half the number of the outgoing-range.

Because of the limit on outgoing-range thus also limits num-queries-per-thread, it is better to compile with libevent, so that there is no 1024 limit on outgoing-range. If you need to compile this way for a heavy duty DNS server then you will need to compile the programme from source instead of using the unbound package.

Without a storage backend configured, unbound will start with a completely empty cache on every boot and every service (re)start. With an empty cache the first request will fire of a relatively large amount of queries to upstream/remote DNS servers. That number of requests will quickly hit an unbound quotum. The default unbound 1.22.0 configuration limits the number of upstream queries to 128. Even a future release with a default of 200 will not solve this issue completely.

The result is that your first DNS lookup will fail, and the second lookup succeeds. With error logging enabled, you will see a message like:

In debug mode the log will show something like:

Change your unbound.conf and restart unbound, to increase the default quotum to a more relaxed value according to other users experiencing this issue, for example:

Even with a 1 Gbit/s FTTH connection, the default unbound configuration might lead to timeouts on recursive lookups. In debug mode, log will show:

In case you want to wait a little longer for an answer then 1.9 seconds, the default time-out, change your unbound.conf:

**Examples:**

Example 1 (unknown):
```unknown
/etc/unbound/unbound.conf
```

Example 2 (unknown):
```unknown
/etc/unbound/unbound.conf
```

Example 3 (unknown):
```unknown
server:
  ...
  setting: value
  ...
```

Example 4 (unknown):
```unknown
/etc/resolv.conf
```

---

## BusyBox

**URL:** https://wiki.archlinux.org/title/BusyBox

**Contents:**
- Installation
- Usage
  - getty
  - mdev
  - runit
- See also

BusyBox provides many common UNIX utilities in a single small executable for embedded systems. The package includes runit; see runit for more information.

Install the busybox package.

Busybox commands are symbolic links to /usr/bin/busybox and thus take very little space. This is especially interesting for low-footprint systems.

The gettys are defined in the file /etc/inittab. By default, getty is started on ttys 1 through 4.

In order to enable/disable gettys:

Just replace tty2 with the tty you want getty to start on. If you want init to ask you before starting getty, then replace respawn with askfirst.

**Examples:**

Example 1 (unknown):
```unknown
/usr/bin/busybox
```

Example 2 (unknown):
```unknown
/etc/inittab
```

Example 3 (unknown):
```unknown
/etc/inittab
```

Example 4 (unknown):
```unknown
tty2::respawn:/sbin/agetty -8 -s 38400 tty2 linux
```

---

## Input remap utilities

**URL:** https://wiki.archlinux.org/title/Input_remap_utilities

**Contents:**
- Utilities
  - evremap
  - evdevremapkeys
  - evsieve
  - kbct
  - keyd
  - Input Remapper
  - makima
  - wtype
  - Other

In an X or Wayland environment, most remapping tasks can be completed with local XKB configuration, see X keyboard extension#Local XKB folder. For remapping keys in the linux console, see Linux console/Keyboard configuration. For keyboards or devices that report incorrect keycodes, see Map scancodes to keycodes.

Some specialized configurations or behaviors use software daemons to translate input events. This page lists software that you can use to reconfigure input events from keyboards, mice and other hardware.

evremap (evremap-gitAUR)  A keyboard input remapper for Linux/Wayland systems. This tool can do remap in a following way: remap the CapsLock key so that it produces Ctrl when held, but Esc if tapped and remap n keys to m keys. E.g.: F3 to Ctrl+c, and Alt+Left to Home.

After installation, create a config file /etc/evremap.toml (example from repo), or edit evremap.service to point to your config. Then start the service.

evdevremapkeys (evdevremapkeys-gitAUR)  A daemon to remap key events on linux input devices. This tool can remap keyboard and mouse events. It can map to repeated actions (for example, do double click) and can generate them while button is pressed (for example, generate wheel up events while holding back button).

It is also possible to remap combo to combo, but this is not yet merged, and is available in pronobis fork. See here.

evsieve (evsieveAUR)  A low-level utility that can read events from Linux event devices (evdev) and write them to virtual event devices (uinput), performing simple manipulations on the events along the way. Works on Wayland. Evsieve is particularly intended to be used in conjunction with the evdev-passthrough functionality of Qemu.

kbct (kbct-gitAUR)  Keyboard Customization Tool for Linux. Despite its name, also supports mouse events. This tool allows you to map an event (keyboard or mouse button) to another event. You can define multiple "layers"  lists of maps depending of which modifier you press with an input key. Unfortunately, currently the kbct does not allow you to generate multi-button event. See [1].

After installation, edit /etc/kbct/config.yml as required, then start kbct.service.

keyd (keyd)  A key remapping daemon for Linux using a flexible system-wide daemon which remaps keys using kernel level input primitives (evdev, uinput). Keyd works in both graphical environments, like X11 and Wayland, and the Linux virtual console. Read the project's README for more information about how keyd compared to similar software.

Input Remapper (input-remapper-gitAUR)  A utility that provides a GUI and a CLI to configure input devices remappings. Works with both X and Wayland.

makima (makima-binAUR)  Linux daemon to remap and create macros for keyboards, mice and controllers. Uses the evdev interface. Configured per device by placing device_name.toml files in ~/.config/makima, and also supports per application configuration with device_name::window_class.toml (X11, Sway, Hyprland and Niri only).

wtype (wtype)  xdotool type for Wayland (requires compositor support for the virtual keyboard protocol).

With evsieve --input /dev/input/event* --print you can see all events that are emitted on your computer.

Another approach of testing keyboard buttons is with online websites. Most of such testers cannot distinguish between left and right modifiers. One example that can is: https://stendec.io/

**Examples:**

Example 1 (unknown):
```unknown
/etc/evremap.toml
```

Example 2 (unknown):
```unknown
evremap.service
```

Example 3 (unknown):
```unknown
/etc/kbct/config.yml
```

Example 4 (unknown):
```unknown
kbct.service
```

---

## Compiz

**URL:** https://wiki.archlinux.org/title/Compiz

**Contents:**
- Installation
  - 0.9 series
  - 0.8 series
  - Extras
- Starting
  - Enabling important plugins
    - Window decoration
  - Compiz startup
    - Fusion Icon
  - Autostarting Compiz in a desktop environment

According to Wikipedia:

There are two versions of Compiz available, the 0.8 series which is written in C and the 0.9 series which is a complete re-write of Compiz in C++. Both series are actively developed. Compiz 0.9 is developed by the Compiz Maintainers on Launchpad whilst Compiz 0.8 is developed by the Compiz Reloaded project on GitLab. The two series cannot be installed side by side.

Before starting Compiz, you should activate some plugins to provide basic window manager behaviour or else you will have no ability to drag, scale or close any windows. Important plugins are listed below:

To be able to switch to different viewports you will need to enable one of the following:

The window decorator is the program which provides windows with borders. Unlike window managers such as Kwin or Xfwm which provide just one decorator, users of Compiz have a choice of three: GTK Window Decorator, KDE Window Decorator and Emerald. The GTK Window Decorator and the KDE Window Decorator are included in the Compiz source and can be optionally compiled whilst building Compiz. Emerald, on the other hand, is a separate, standalone decorator. The Window Decoration plugin in CCSM must be ticked otherwise no window decorator will be started.

The window decorator that will be started is specified under CCSM -> Effects -> Window Decoration -> Command. The default command is compiz-decorator which is a script which will attempt to locate the emerald and gtk-window-decorator executables (and also the kde4-window-decorator executable if you are using Compiz 0.9). It will then start the first decorator that it finds, according to the search order and conditions (such as session detection) specified in the script. Note that the script provided by Compiz 0.8 differs significantly from the one provided by Compiz 0.9 so the behavior may be different.

The compiz-decorator command can be replaced with one of the executables listed above. If you find that your preferred decorator is not being started, try appending the --replace switch to the command, for example: emerald --replace.

You can start Compiz using the following command:

See compiz --help for more options.

To start Compiz using Fusion Icon, execute the command below:

To ensure that fusion-icon then starts Compiz, right click on the icon in the panel and go to select window manager. Choose Compiz if it is not selected already.

See Desktop environment#Custom window manager.

A standalone Compiz session can be started from a display manager. For most display managers - LightDM for example - all that is required is to create a .desktop file in /usr/share/xsessions that executes compiz (with command line options if needed) or fusion-icon. See the article for your display manager. See Desktop entries for information on creating a .desktop file.

One way in which you could start programs with your Compiz session, when it is started from a display manager, is to use an xprofile file. Another option is for the .desktop file in /usr/share/xsessions to not execute compiz directly but to execute a script which starts the programs you wish to start and also starts Compiz.

A Compiz session can be started with startx. Define either compiz or fusion-icon in your .xinitrc file. See the xinitrc article for more details.

See Allow users to shutdown#Using systemd-logind: you can assign a keyboard shortcut to systemctl comands using the Commands plugin in CCSM.

You can switch back to your desktop environment's default window manager with the following command:

using kwin, metacity or xfwm4 for example instead of wm_name.

Enable the Gnome Compatibility plugin in CCSM.

There are two ways to enable MATE Panel's run dialog in Compiz. You can either:

Map the command below to the Alt+F2 key combination using the Commands plugin in CCSM.

When Compiz is used in an Xfce session, the run dialog (provided by xfce4-appfinder) should work without intervention. If you are using Xfce Appfinder in a standalone Compiz session, map the command below to the Alt+F2 key combination using the Commands plugin in CCSM.

Map the command for a run dialog of choice to the Alt+F2 key combination using the Commands plugin in CCSM.

Start CCSM and navigate to the Window Decoration plugin. Then in the Decoration Windows field, change any to !state=maxvert. [3]

A possible issue with GLX_EXT_texture_from_pixmap on ATI cards is that the card can only render it indirectly. If so, you have to pass the option to your libgl as shown below:

Use the following command to start Compiz (this command must be used every time).

Firstly, ensure that your window decorator settings are configured correctly - see #Window decoration. If window borders still do not start try adding Option "AddARGBGLXVisuals" "True" and Option "DisableGLXRootClipping" "True" to your "Screen" section in /etc/X11/xorg.conf.d/20-nvidia.conf. If window borders still do not load and you have used other Options elsewhere in /etc/X11/xorg.conf.d/ try commenting them out and using only the aformentioned ARGBGLXVisuals and GLXRootClipping Options.

If you receive a blank screen with a responsive cursor upon resume, try disabling sync to vblank. To do so, open CCSM, navigate to the OpenGL plugin and untick the Sync to VBlank option.

NVIDIA and Intel chips: If everything is configured correctly but you still have poor performance with some effects, try disabling CCSM > General Options > Display Settings > Detect Refresh Rate and instead choose a value manually.

NVIDIA chips only: The inadequate refresh rate with Detect Refresh Rate may be due to an option called DynamicTwinView being enabled by default which plays a factor in accurately reporting the maximum refresh rate that your card and display support. You can disable DynamicTwinView by adding the following line to the "Device" or "Screen" section of your /etc/X11/xorg.conf.d/20-nvidia.conf, and then restarting your computer:

To fix this behaviour create the file below:

If you experience video tearing when using Compiz, try enabling the Workarounds plugin in CCSM. Once enabled, ensure that the following options are enabled in Workarounds: Force complete redraw on initial damage, Force full screen redraws (buffer swap) on repaint.

If you are using Intel graphics and the workaround above does not fix the video tearing, see Intel graphics#Tearing.

Also see, #Poor performance from capable graphics cards.

If you get an output like this from the command line:

the problem is with the permission on ~/.config/compiz/. To fix it, use:

If Compiz replaces Xfwm4 at login, this can cause the Alt+F4 keybinding to become non-functional. To avoid this issue, ensure that only Compiz is started at login - see Xfce#Use a different window manager.

You may find that Emerald crashes when selecting certain themes (especially themes that use the legacy engine). If this occurs, select another theme in Emerald Theme Manager and then run the command emerald --replace.

You may find that the system bell (such as the drip sound played when pressing backspace at the beginning of a line in GNOME or MATE Terminal) will not sound if Compiz is running. See the following upstream bug report.

PulseAudio users, as a workaround, can force PulseAudio to handle the system bell, see PulseAudio#X11 Bell Events.

If you are using the GSettings backend, you may find that Compiz crashes if you try to enable the Gnome Compatibility plugin. In order to enable this plugin whilst using the GSettings backend you need to open CCSM and navigate to Preferences. Under the header Integration untick the box labelled Enable integration into the desktop environment. After unticking this option, you should find it possible to enable the Gnome Compatibility plugin.

You may find that certain windows (such as a Chromium window) will lose focus when unminimised. See the following upstream bug report. One possible solution is to enable the Keep previews of minimized windows option, located within the Workarounds plugin.

You may find that popout windows for panels which are placed at the bottom of the screen are offset by a few pixels so that the window appears to float above the panel. This problem is known to affect Xfce and KDE and may affect other desktops as well. Listed below are a number of workarounds that might fix some cases.

For more information, see the following upstream bug report.

You may find that the Alt-Tab switcher (provided by the staticswitcher or switcher plugins) has a completely transparent background when using Emerald as well. This can make it hard to differentiate window thumbnails from the desktop background behind them. As of Compiz 0.9.12 (revision 3975) a workaround is available. In CCSM, navigate to Application Switcher or Static Application Switcher depending on which plugin you are using. For the former, the Background settings are located under General and for the latter the settings are located under Appearance. Once you have found the settings, ensure that the Set background color box is ticked. The default is a dark grey which can be optionally changed.

Alternatively, use GTK Window Decorator instead of Emerald or use a different window switcher altogether such as the shift switcher. Note that even if you are using the GTK Window Decorator, you can still change the background color as described above.

See Cursor themes#Change X shaped default cursor.

Some plugins that were popular in Compiz 0.8 were disabled in Compiz versions 0.9.8 and above in order to complete OpenGL ES support. A few of the disabled plugins have since been re-enabled; for instance, the Animations Add-On plugin was re-enabled for the Compiz 0.9.13.0 release. Other currently-disabled plugins that receive patches for this issue may well be re-enabled in future releases. For more information, see the Compiz 0.9.8 release notes.

Likewise, Compiz Plugins Unsupported (a package which includes plugins such as Atlantis) is unavailable in recent versions of Compiz 0.9. It has not been developed for the Compiz 0.9 series since Compiz 0.9.5 and no longer builds successfully.

When Compiz is used with Xfce Panel 4.11 and above, the workspace pager will use the width of only one workspace but will divide this space into ever smaller bars, according to how many viewports Compiz specifies. This issue can be fixed by replacing xfce4-panel with xfce4-panel-compizAUR which incorporates a patch for this issue. For more information, see the following upstream bug report.

The D-Bus plugin will cause Compiz to crash if enabled in conjunction with certain other plugins such as the Cube plugin. See the following upstream bug report.

Only a few taskbars are compatible with Compiz's viewports. Incompatible panels and docks may display issues such as showing all window buttons in all workspaces or the workspace pager may only show one workspace available. The panels listed below are known to be compatible:

**Examples:**

Example 1 (unknown):
```unknown
emerald --replace
```

Example 2 (unknown):
```unknown
compiz --replace ccp
```

Example 3 (unknown):
```unknown
$ compiz --replace
```

Example 4 (unknown):
```unknown
compiz --help
```

---

## TLP

**URL:** https://wiki.archlinux.org/title/TLP

**Contents:**
- Installation
  - Radio Device Wizard (tlp-rdw)
  - Front end
  - ThinkPads only
    - Before Sandy Bridge (until 2010)
- Configuration
  - USB autosuspend
  - Force battery (BAT) configuration
  - Bumblebee with NVIDIA driver
  - PRIME with NVIDIA driver

TLP is a feature-rich command line utility for Linux, saving laptop battery power without the need to delve deeper into technical details.

TLP's default settings are already optimized for battery life and implement Powertop's recommendations out of the box, so additional configuration is not needed. Also, TLP is completely customizable, which means you can get even more power savings or meet your exact requirements.

TLP intentionally excludes some settings from the project, notably Fan speed control and Backlight.

Install the tlp package. Installing the optional dependencies may help provide additional power saving.

Enable/start tlp.service.

One should also mask the service systemd-rfkill.service and socket systemd-rfkill.socket to avoid conflicts and ensure proper operation of TLP's radio device switching options.

When using the Radio Device Wizard (tlp-rdw), it is required to use NetworkManager and enabling NetworkManager-dispatcher.service.

See TLP settings for details.

Controlling the charge thresholds using D-Bus without root privileges is possible using threshyAUR and its example Qt user interface threshy-guiAUR. Note: threshy is no longer maintained by its developer.

For ThinkPads before model year 2011, the tp_smapi kernel module is required. See tp_smapi#Installation for kernel-specific installation instructions.

The configuration file is located at /etc/tlp.conf and provides a largely optimized power saving by default. For a full explanation of options see: TLP settings. Instead of editing this file directly, you can also place files in /etc/tlp.d/, for example /etc/tlp.d/00-enable-wifi-at-startup.conf with the desired changes. If the same parameter is defined in both places, the value set in /etc/tlp.conf takes precedence.

When starting TLP with the default configuration, some USB devices such as audio DACs will be powered down when running on battery due to TLP's autosuspend feature. Some devices such as keyboards and scanners are blacklisted from autosuspend by default.

You may simply want to disable USB autosuspend entirely with the following setting:

Or blacklist specific devices from being auto-suspended. See the TLP documentation on USB devices for details.

When no power supply can be detected, the setting for AC will be used on devices like desktops and embedded hardware.

You may want to force the battery (BAT) settings when using TLP on these devices to enable more power saving:

If you are running Bumblebee with NVIDIA driver, you need to disable power management for the GPU in TLP in order to make Bumblebee control the power of the GPU.

Depending on the driver(s) that you are using, blacklist one or more of them, preventing TLP from managing their power state:

If you are running PRIME with NVIDIA driver, do not disable power management for the GPU in TLP. Instead, you may want to enable power management all the time to prevent your laptop from heating by adding the NVIDIA card PCI ID (lspci -d 10de::03xx) to the variable RUNTIME_PM_ENABLE:

Enabling runtime power management for PCI(e) bus devices while on AC may improve power saving on some laptops. This is enabled by default on battery, but not on AC. To enable on AC, set:

By default, TLP stops the wifi radio from automatically powering on. Although definitely power-saving, most users might find this behavior undesirable. To make WiFi start on boot, set the following:

TLP provides several command line tools. See TLP commands.

For debugging you can display information about the currently used Mode(AC/BAT) and applied configurations:

See also the upstream troubleshooting guide.

If your bluetooth headphones suddenly stop working and you see this error from dmesg, it may be caused by TLP suspending your device. Add device ID to USB_DENYLIST in /etc/tlp.conf:

Get the device ID for your bluetooth device from lsusb -v. Restart TLP and the bluetooth service.

**Examples:**

Example 1 (unknown):
```unknown
tlp.service
```

Example 2 (unknown):
```unknown
systemd-rfkill.service
```

Example 3 (unknown):
```unknown
systemd-rfkill.socket
```

Example 4 (unknown):
```unknown
NetworkManager-dispatcher.service
```

---

## echinus

**URL:** https://wiki.archlinux.org/title/Echinus

**Contents:**
- Installing
- Configuring
  - Rules
- Starting echinus
- Using echinus
  - Panels & Pagers
- See also
  - Screenshots

echinus is a simple and lightweight tiling and floating window manager for X11. Started as a dwm fork with easier configuration, echinus became a full-featured reparenting window manager with EWMH support.

Unlike dwm, echinus does not need to be re-compiled after changes have been made to the config. It supports Xft (freetype) out of the box and has the option of configurable titlebars.

Install echinusAUR. You might also want to install ouricoAUR, a lightweight EWMH taskbar, originally designed for echinus, and dmenu.

After successfully installing, copy all files from /etc/xdg/echinus to ~/.echinus(for user).

echinus is configured in one simple text file, in Xresources format: ~/.echinus/echinusrc. Details for all of the configuration options are in /usr/share/doc/echinus/README. A section of a sample configuration file looks like:

Rules can be set up to spawn applications in specific tags. The following rule, for example, would open firefox in the "web" tag:

Opening applications in a terminal requires that you explicitly set the -title tag when spawning them so that echinus can manage them:

Similarly, when spawning dmenu, you will need to declare the requisite properties, like so:

To start echinus with startx or the SLiM login manager, simply append the following to ~/.xinitrc:

After making changes to echinusrc, you can reload the configuration without recompiling by restarting echinus, with Alt+Shift+q. This keybinding, and any other, can be customized to suit your preference or muscle-memory.

Further details for manipulating windows are in the manpage and the README.

echinus supports some parts of EWMH - the following are known to work:

**Examples:**

Example 1 (unknown):
```unknown
/etc/xdg/echinus
```

Example 2 (unknown):
```unknown
~/.echinus/echinusrc
```

Example 3 (unknown):
```unknown
/usr/share/doc/echinus/README
```

Example 4 (unknown):
```unknown
Echinus*selected.border: #404040
Echinus*selected.button: #d3d7cf
Echinus*selected.bg: #262626
Echinus*selected.fg: #d3d7cf
```

---

## MIDI

**URL:** https://wiki.archlinux.org/title/MIDI

**Contents:**
- MIDI file
- General MIDI sound bank
  - List of SoundFonts
- Hardware playback
  - SB Audigy 1 - Emu10k1 WaveTable
- Software playback
  - Synthesizers
    - FluidSynth
    - TiMidity++
    - WildMIDI

MIDI itself, which stands for "Musical Instrument Digital Interface", is just a protocol and standard for communication between musical instruments and any device that understands the language. It can be used to control an array of synthesizers, make a tin can sound like a drum, or even operate industrial equipments.

The scope of this article, however, will mainly focus on the usage of MIDI in computer systems for playback of files that contain MIDI data. These files usually come with the .mid extension, and were hugely popular in the golden days of multimedia computing to share music. In professional music composition/arrangement, it still plays a vital role.

Without going into the details of what the format is composed of, you just need to understand that a MIDI file eg. foobar.mid does not contain any digital audio data, hence no "PCM stream". It is a common misconception that MIDI is a sound file format, and as such you usually see people complaining that music player applications cannot play the file. Here is a very beginner-friendly outline of what a MIDI/MID file contains:

In order for such a file to be useful, there needs to be an "engine" that can translate the data to music. This engine will have a "tone generator", and this is what we call a "synthesizer". So any player that can play back a MIDI file without MIDI-capable hardware (your computer's sound device), has a synthesizer built-in or uses an external one. A typical keyboard (not the thing you are typing on) is actually made up of two components - a MIDI "controller" (the keys) and a synthesizer (tone generator/module; the thing that makes sound).

So up to this point, you should be able to understand that:

General MIDI (GM) is a specification to standardise numerous MIDI-related matters, particularly that of instruments layout in a collection of sounds. A "soundbank" which is GM-compatible means that it meets the criteria of General MIDI, and as long as the MIDI file is also GM-compatible (as in nothing extraordinary is defined - such as introducing a new instrument or having one in a different location of the sound bank), the playback will be as intended since the sound bank has the correct instrument/handler for the MIDI message/event. One of the most popular sound bank format is SoundFont, particularly SF2. Another popular format is Gravis UltraSound (GUS) patch files.

There are a number of additional SoundFonts in the AUR: search for soundfont-. See also the FluidSynth Wiki.

This article or section needs expansion.

If you simply need to play a MIDI file on a MIDI-capable hardware device (e.g. a hardware synthesizer), you can use the aplaymidi command from alsa-utils.

To get the list of the available MIDI ports, use the command

Then to play a MIDI file, specify it along with an available port of the preferred MIDI device that you got from the output of the previous command. For example like this:

First, make sure that the Synth mixer control is not muted and that Audigy Analog/Digital output Jack is set to [Off].

To check and adjust them, use alsamixer or your mixer of choice.

Next, build and install the awesfxAUR package from the AUR. Then, load a SoundFont file on the Emux WaveTable, like so:

The .sf2 file can be any SoundFont. If you have access to 2GMGSMT.SF2 on Windows, you can use that one.

You should be all set now. To play your .mid files with aplaymidi, you will have to do as follows:

Get a list of the available MIDI ports by running

Then, pick an available "Emu10k1 WaveTable" MIDI port, in this case 29:0, and specify it as such:

"Why can I play MIDI with Windows Media Player, then?"

Well, because Windows has a default software synthesizer which acts globally. Even then, it lacks the quality which should be expected of modern computers. If there were a way to do it on Linux, you would be able to play back MIDI from any player too. Perhaps a MIDI server (which will hold a synthesizer of choice) that sits within the sound server, like Phonon or PulseAudio. Nevertheless, nothing of this sort has been implemented and you can only play MIDI with a player, or sound server, that has a plug-in to source a synthesizer, or has a synthesizer itself.

MIDI player and a daemon adding MIDI support to ALSA. It supports SoundFonts only. See FluidSynth.

MIDI to WAVE converter and player. It supports both SoundFonts and Gravis UltraSound patch files. See Timidity++.

WildMIDI is a simple software MIDI player. It uses Gravis UltraSound patch files to convert MIDI files into audio. SoundFonts are not supported yet.[1] In order to use it, a configuration file wildmidi.cfg(5) is needed which points to the GUS patches:

The configuration file format is mostly compatible with TiMidity++.

You can simply use WildMIDI to play MIDI files:

To convert MIDI files to WAV format:

See wildmidi(1) for more options.

MIDI files can be played in GNOME Videos and all other players using GStreamer as backend after having installed gst-plugins-bad and a SoundFont (soundfont-fluid for example). It uses FluidSynth as synthesizer and picks up the first usable SoundFont from the /usr/share/soundfonts/ directory.

The FluidSynth plugin provides MIDI playback support for VLC using FluidSynth as synthesizer. A SoundFont needs to be installed, and VLC will detect it automatically. If multiple SoundFonts are installed, you can choose one in VLC preferences (Tools > Preferences): you have to show all settings. Then, go to Input/Codecs > Audio codecs > FluidSynth.

And, if you installed e.g. Fluid, set the location to /usr/share/soundfonts/FluidR3_GM.sf2.

The AMIDI-Plug from the audacious-plugins package provides MIDI playback support for Audacious using FluidSynth as synthesizer. You can specify the SoundFont to use for playback in the settings of its MIDI output plugin (File > Preferences > Plugins > Input > AMIDI-Plug > Preferences).

deadbeefAUR player is able to play MIDI files via its WildMidi player plugin. It does not support SoundFonts, just Gravis UltraSound patch files. You can specify the configuration file location in DeaDBeeF by going to Edit > Preferences > Plugins > WildMidi Player > Configure.

Drumstick MIDI File Player is able to play MIDI files using FluidSynth as synthesizer. Install dmidiplayerAUR.

**Examples:**

Example 1 (unknown):
```unknown
# FOOBAR.MID
Note ON
  Use Instrument #1
  Play Note C1
  Set Volume at 100
  Set Pitch at 50
```

Example 2 (unknown):
```unknown
$ aplaymidi -l
```

Example 3 (unknown):
```unknown
$ aplaymidi -p 24:0 midi_file.mid
```

Example 4 (unknown):
```unknown
$ asfxload /path/to/any/file.sf2
```

---

## Linux firmware

**URL:** https://wiki.archlinux.org/title/Firmware

**Contents:**
- Installation
- Tips and tricks
  - Detecting loaded firmware
- See also

Linux firmware is a collection of firmware binary blobs distributed alongside the kernel, necessary for partial or full functionality of certain hardware devices. These binary blobs were never permitted to include in a GPL'd work, but have been permitted to redistribute under separate cover.

Typical kinds of hardware requiring firmware:

Install the linux-firmware meta package to pull all commonly used firmware. This is the recommended way for most users. To save some space you could opt into installing firmware only for individual hardware vendors your system uses.

Primary packages pulled by linux-firmware:

Third-party packages:

Sometimes you want to know what firmware is loaded by your system, for debugging or to pick firmware packages to install. That could be achieved using dynamic debug:

**Examples:**

Example 1 (unknown):
```unknown
dyndbg="func fw_log_firmware_info +p"
```

Example 2 (unknown):
```unknown
# journalctl -kg 'loaded f'
```

Example 3 (unknown):
```unknown
Jan 01 00:00:00 example kernel: amdgpu 0000:03:00.0: Loaded FW: amdgpu/psp_13_0_0_sos.bin, sha256: SHA_sum
Jan 01 00:00:00 example kernel: amdgpu 0000:6d:00.0: Loaded FW: amdgpu/vcn_3_1_2.bin, sha256: SHA_sum

Jan 01 00:00:00 example kernel: bluetooth hci0: Loaded FW: mediatek/BT_RAM_CODE_MT7922_1_1_hdr.bin, sha256: SHA_sum
Jan 01 00:00:00 example kernel: mt7921e 0000:0a:00.0: Loaded FW: mediatek/WIFI_RAM_CODE_MT7922_1.bin, sha256: SHA_sum

```

---

## Kernel

**URL:** https://wiki.archlinux.org/title/Linux-pf

**Contents:**
- Officially supported kernels
- Compilation
  - kernel.org kernels
  - Unofficial kernels
- Troubleshooting
  - Kernel panics
    - Examine panic message
      - QR code on a blue screen
      - Console way
      - Example scenario: bad module

According to Wikipedia:

Arch Linux is based on the Linux kernel. There are various alternative Linux kernels available for Arch Linux in addition to the latest stable kernel. This article lists some of the options available in the repositories with a brief description of each. There is also a description of patches that can be applied to the system's kernel. The article ends with an overview of custom kernel compilation with links to various methods.

Kernel packages are installed under the /usr/lib/modules/ path and subsequently used to copy the vmlinuz executable image to /boot/. [1] When installing a different kernel or switching between multiple kernels, you must configure your boot loader to reflect the changes. For downgrading the kernel to an older version, see Downgrading packages#Downgrading the kernel.

Community support on forum and bug reporting is available for officially supported kernels.

Following methods can be used to compile your own kernel:

Some of the listed packages may also be available as binary packages via Unofficial user repositories.

Many of these unofficial kernels contain features that need to be enabled manually. Try reading the documentation in the patches themselves (many already include changes to the Documentation/ directory in the kernel source) or searching up the name of the patchset on the web.

A kernel panic occurs when the Linux kernel enters an unrecoverable failure state. The state typically originates from buggy hardware drivers resulting in the machine being deadlocked, non-responsive, and requiring a reboot. Just prior to deadlock, a diagnostic message is generated, consisting of: the machine state when the failure occurred, a call trace leading to the kernel function that recognized the failure, and a listing of currently loaded modules. Thankfully, kernel panics do not happen very often using mainline versions of the kernel--such as those supplied by the official repositories--but when they do happen, you need to know how to deal with them.

If a kernel panic occurs very early in the boot process, you may see a message on the console containing Kernel panic - not syncing:, but once systemd is running, kernel messages will typically be captured and written to the system log. However, when a panic occurs, the diagnostic message output by the kernel is almost never written to the log file on disk because the machine deadlocks before system-journald gets the chance.

Since linux 6.10 (for drm_panic), the kernel will display a panic as a QR code (by default) in a blue screen. The stack trace is visible at the URL given by the QR code. For Arch Linux, it is a link to https://panic.archlinux.org. The URL contains various information and the stack trace compressed by gzip and encoded in the URL fragment which is not transferred to the server (it is processed on the client side).

An example panic with a link and screenshot can be seen in a forum post.

You can revert to the old behavior by passing the parameter panic_screen=kmsg to the drm kernel module (or drm.panic_screen=kmsg as kernel parameter) to display the stack trace in a console.

The "old" style way of viewing the crash on the console as it happens is still available (without resorting to setting up a kdump crashkernel). Boot with the following kernel parameters and attempting to reproduce the panic on tty1:

It is possible to make a best guess as to what subsystem or module is causing the panic using the information in the diagnostic message. In this scenario, we have a panic on some imaginary machine during boot. Pay attention to the lines highlighted in bold:

We can surmise then, that the panic occurred during the initialization routine of module firewire_core as it was loaded. (We might assume then, that the machine's firewire hardware is incompatible with this version of the firewire driver module due to a programmer error, and will have to wait for a new release.) In the meantime, the easiest way to get the machine running again is to prevent the module from being loaded. We can do this in one of two ways:

This article or section is out of date.

The factual accuracy of this article or section is disputed.

You will need a root shell to make changes to the system so the panic no longer occurs. If the panic occurs on boot, there are several strategies to obtain a root shell before the machine deadlocks:

See General troubleshooting#Debugging regressions.

Try linux-mainlineAUR to check if the issue is already fixed upstream. The pinned comment also mentions a repository which contains already built kernels, so it may not be necessary to build it manually, which can take some time.

It may also be worth considering trying the LTS kernel (linux-lts) to debug issues which did not appear recently. Older versions of the LTS kernel can be found in the Arch Linux Archive.

If the issue still persists, bisect the linux-gitAUR kernel and report the bug in accordance to the kernel process for reporting regressions. Depending on the Bugtracker (B:) entry in the MAINTAINERS file this then entails opening an issue via the subsystems mailing lists, Kernel Bugzilla, or in other issue trackers like the DRM Gitlab. It is important to try the "vanilla" version without any patches to make sure it is not related to them. If a patch causes the issue, report it to the author of the patch.

You can shorten kernel build times by building only the modules required by the local system using modprobed-db, or by make localmodconfig. Of course you can completely drop irrelevant drivers, for example sound drivers to debug a network problem.

**Examples:**

Example 1 (unknown):
```unknown
/usr/lib/modules/
```

Example 2 (unknown):
```unknown
/proc/config.gz
```

Example 3 (unknown):
```unknown
CONFIG_IKCONFIG_PROC
```

Example 4 (unknown):
```unknown
Documentation/
```

---

## Table of contents

**URL:** https://wiki.archlinux.org/title/Table_of_contents

---

## Help:Reading

**URL:** https://wiki.archlinux.org/title/Reload

**Contents:**
- Organization
- Formatting
- Root, regular user or another user
- Append, add, create, edit
  - Make executable
- Source
- Installation of packages
  - Official packages
  - Arch User Repository
- Control of systemd units

Because the vast majority of the ArchWiki contains indications that may need clarification for users new to Arch Linux (or GNU/Linux in general), this rundown of basic procedures was written both to avoid confusion in the assimilation of the articles and to deter repetition in the content itself.

Most articles on the ArchWiki do not attempt to provide a holistic introduction to a single topic; they are instead written in adherence to the "Don't Repeat Yourself" principle, under the assumption that the user will seek out and read any supporting material that they do not yet understand. Where possible, such supporting material is indicated in the article via special formatting, see #Formatting.

Because of this organization, it may be necessary to examine several related sources in order to fully understand an ArchWiki article. In particular, users who are new to Arch (or GNU/Linux in general) should expect to end up reading a great number of articles even when solving simple problems. It is especially important to study the supporting material before seeking additional help from other users.

Some lines are written like so:

Others have a different prefix:

The numeral or hash sign (#) indicates that the command needs to be run as root, whereas the dollar sign ($) shows that the command should be run as a regular user.

When the commands need to run as a specific user, they will be prefixed by the username in square brackets, for example:

This means you should use a privilege elevation tool, e.g. with sudo:

A notable exception to watch out for:

In this example, the context surrounding the numeral sign communicates that this is not to be run as a command; it should be edited into a file instead. So in this case, the numeral sign denotes a comment. A comment can be explanatory text that will not be interpreted by the associated program. Bash scripts denotation for comments happens to coincide with the root PS1.

After further examination, "give away" signs include the uppercase character following the # sign. Usually, Unix commands are not written this way and most of the time they are short abbreviations instead of full-blown English words (e.g., Copy becomes cp).

Regardless, most articles make this easy to discern by notifying the reader:

Append to ~/path/to/file:

When prompted to append to, add to, create, or edit one or more files, it is implied that you should use one of the following methods.

To create or modify multiline files, it is suggested to use a text editor. For example, using the nano command to edit the file /etc/bash.bashrc is:

To create or overwrite a file from a string, it may be simpler to use output redirection. The following example creates or overwrites the contents of the file /etc/hostname with the text myhostname.

Output redirection can also be used to append a string to a file. The following example appends the text [custom-repo] to the file /etc/pacman.conf.

When prompted to create directories, use the mkdir command:

After creating a file, if it is meant to be run as a script (whether manually or called by another program), it needs to be set as executable, for example with:

See chmod. Some applications such as file managers may provide graphical interfaces to do the same.

Some applications, notably command-line shells, use scripts for their configuration: after modifying them, they must be sourced in order for the changes to be applied. In the case of bash, for example, this is done by running (you can also replace source with .):

When the wiki suggests modifying such a configuration script, it will not explicitly remind you to source the file, and only in some cases will it point to this section with a reminder link.

When an article invites you to install some packages in the conventional way, it will not indicate the detailed instructions to do so; instead, it will simply mention the names of the packages to be installed.

The subsections below give an overview of the generic installation procedures depending on the package type.

For packages from the official repositories, you will read something like:

This means that you have to run:

The pacman article contains detailed explanations to deal with package management in Arch Linux proficiently.

For packages from the Arch User Repository (AUR), you will read something like:

This means that in general you have to follow the foobarAUR link, download the PKGBUILD archive, extract it, verify the content and finally run, in the same folder:

The Arch User Repository article contains all the detailed explanations and best practices to deal with AUR packages.

When an article invites to start, enable, etc., some systemd unit (e.g. a service), it will not indicate the detailed instructions to do so, but instead you will read something like:

This means that you have to run:

A notable command that does not follow this exact pattern is systemctl daemon-reload which will be called without arguments.

The systemd#Using units section contains structured list of available actions (like start, enable, enable and start, etc.) with their corresponding systemctl commands.

It is important to remember that there are two different kinds of configurations on a GNU/Linux system. System-wide configuration affects all users. Since system-wide settings are generally located in the /etc directory, root privileges are required in order to alter them. For example, to apply a Bash setting that affects all users, /etc/bash.bashrc should be modified.

User-specific configuration affects only a single user. Dotfiles are used for user-specific configuration. For example, the file ~/.bashrc is the user-specific configuration file. The idea is that each user can define their own settings, such as aliases, functions and other interactive features like the prompt, without affecting other users' preferences.

Bash and other Bourne-compatible shells, such as Zsh, also source files depending on whether the shell is a login shell or an interactive shell. See Bash#Configuration files and Zsh#Startup/Shutdown files for details.

Some code blocks may contain so-called pseudo-variables, which, as the name says, are not actual variables used in the code. Instead they are generic placeholders and have to be manually replaced with system-specific configuration items before the code may be run or parsed. Common shells such as bash and zsh provide tab-completion to auto-complete parameters for common commands such as systemctl.

In the articles that comply with Help:Style/Formatting and punctuation, pseudo-variables are formatted in italics. For example:

In this case interface_name is used as a pseudo-variable placeholder in a systemd template unit. All systemd template units, identifiable by the @ sign, require a system-specific configuration item as argument. See systemd#Using units.

In this case the pseudo-variables are used to describe the parameters that must be substituted for them. Details on how to gather them are elaborated on in the section Securely wipe disk#Calculate blocks to wipe manually, which features the command.

This article or section needs expansion.

In case of file examples, pasting pseudo-variables in real configuration files might break the programs that use them.

In most cases, ellipses (...) are not part of the actual file content or code output, and instead represent omitted or optional text that is not relevant for the discussed subject.

For example HOOKS=(... encrypt ... filesystems ...) or:

Be aware though that, in a few instances, ellipses may be a meaningful part of the code syntax: attentive users should be able to recognize these cases by the context.

**Examples:**

Example 1 (unknown):
```unknown
# mkinitcpio -p linux
```

Example 2 (unknown):
```unknown
$ makepkg -s
```

Example 3 (unknown):
```unknown
sudo command
```

Example 4 (unknown):
```unknown
[postgres]$ initdb -D /var/lib/postgres/data
```

---

## Reset lost root password

**URL:** https://wiki.archlinux.org/title/Reset_lost_root_password

**Contents:**
- Using sudo
- Using the debug shell
- Using bash as init
- Using a LiveCD
  - Change root
- See also

This guide will show you how to reset a forgotten root password. Several methods are listed to help you accomplish this.

If you have installed sudo and have configured permissions for either the wheel group or a user whose password you recall, you can change the root password by running sudo passwd root.

With a LiveCD a couple methods are available: change root and use the passwd command, or erase the password field entry directly editing the password file. Any Linux capable LiveCD can be used, albeit to change root it must match your installed architecture type. Here we only describe how to reset your password with chroot, since manual editing the password file is significantly more risky.

**Examples:**

Example 1 (unknown):
```unknown
sudo passwd root
```

Example 2 (unknown):
```unknown
systemd.debug_shell
```

Example 3 (unknown):
```unknown
debug-shell.service
```

Example 4 (unknown):
```unknown
Ctrl+Alt+F9
```

---

## Help:Reading

**URL:** https://wiki.archlinux.org/title/Enable/start

**Contents:**
- Organization
- Formatting
- Root, regular user or another user
- Append, add, create, edit
  - Make executable
- Source
- Installation of packages
  - Official packages
  - Arch User Repository
- Control of systemd units

Because the vast majority of the ArchWiki contains indications that may need clarification for users new to Arch Linux (or GNU/Linux in general), this rundown of basic procedures was written both to avoid confusion in the assimilation of the articles and to deter repetition in the content itself.

Most articles on the ArchWiki do not attempt to provide a holistic introduction to a single topic; they are instead written in adherence to the "Don't Repeat Yourself" principle, under the assumption that the user will seek out and read any supporting material that they do not yet understand. Where possible, such supporting material is indicated in the article via special formatting, see #Formatting.

Because of this organization, it may be necessary to examine several related sources in order to fully understand an ArchWiki article. In particular, users who are new to Arch (or GNU/Linux in general) should expect to end up reading a great number of articles even when solving simple problems. It is especially important to study the supporting material before seeking additional help from other users.

Some lines are written like so:

Others have a different prefix:

The numeral or hash sign (#) indicates that the command needs to be run as root, whereas the dollar sign ($) shows that the command should be run as a regular user.

When the commands need to run as a specific user, they will be prefixed by the username in square brackets, for example:

This means you should use a privilege elevation tool, e.g. with sudo:

A notable exception to watch out for:

In this example, the context surrounding the numeral sign communicates that this is not to be run as a command; it should be edited into a file instead. So in this case, the numeral sign denotes a comment. A comment can be explanatory text that will not be interpreted by the associated program. Bash scripts denotation for comments happens to coincide with the root PS1.

After further examination, "give away" signs include the uppercase character following the # sign. Usually, Unix commands are not written this way and most of the time they are short abbreviations instead of full-blown English words (e.g., Copy becomes cp).

Regardless, most articles make this easy to discern by notifying the reader:

Append to ~/path/to/file:

When prompted to append to, add to, create, or edit one or more files, it is implied that you should use one of the following methods.

To create or modify multiline files, it is suggested to use a text editor. For example, using the nano command to edit the file /etc/bash.bashrc is:

To create or overwrite a file from a string, it may be simpler to use output redirection. The following example creates or overwrites the contents of the file /etc/hostname with the text myhostname.

Output redirection can also be used to append a string to a file. The following example appends the text [custom-repo] to the file /etc/pacman.conf.

When prompted to create directories, use the mkdir command:

After creating a file, if it is meant to be run as a script (whether manually or called by another program), it needs to be set as executable, for example with:

See chmod. Some applications such as file managers may provide graphical interfaces to do the same.

Some applications, notably command-line shells, use scripts for their configuration: after modifying them, they must be sourced in order for the changes to be applied. In the case of bash, for example, this is done by running (you can also replace source with .):

When the wiki suggests modifying such a configuration script, it will not explicitly remind you to source the file, and only in some cases will it point to this section with a reminder link.

When an article invites you to install some packages in the conventional way, it will not indicate the detailed instructions to do so; instead, it will simply mention the names of the packages to be installed.

The subsections below give an overview of the generic installation procedures depending on the package type.

For packages from the official repositories, you will read something like:

This means that you have to run:

The pacman article contains detailed explanations to deal with package management in Arch Linux proficiently.

For packages from the Arch User Repository (AUR), you will read something like:

This means that in general you have to follow the foobarAUR link, download the PKGBUILD archive, extract it, verify the content and finally run, in the same folder:

The Arch User Repository article contains all the detailed explanations and best practices to deal with AUR packages.

When an article invites to start, enable, etc., some systemd unit (e.g. a service), it will not indicate the detailed instructions to do so, but instead you will read something like:

This means that you have to run:

A notable command that does not follow this exact pattern is systemctl daemon-reload which will be called without arguments.

The systemd#Using units section contains structured list of available actions (like start, enable, enable and start, etc.) with their corresponding systemctl commands.

It is important to remember that there are two different kinds of configurations on a GNU/Linux system. System-wide configuration affects all users. Since system-wide settings are generally located in the /etc directory, root privileges are required in order to alter them. For example, to apply a Bash setting that affects all users, /etc/bash.bashrc should be modified.

User-specific configuration affects only a single user. Dotfiles are used for user-specific configuration. For example, the file ~/.bashrc is the user-specific configuration file. The idea is that each user can define their own settings, such as aliases, functions and other interactive features like the prompt, without affecting other users' preferences.

Bash and other Bourne-compatible shells, such as Zsh, also source files depending on whether the shell is a login shell or an interactive shell. See Bash#Configuration files and Zsh#Startup/Shutdown files for details.

Some code blocks may contain so-called pseudo-variables, which, as the name says, are not actual variables used in the code. Instead they are generic placeholders and have to be manually replaced with system-specific configuration items before the code may be run or parsed. Common shells such as bash and zsh provide tab-completion to auto-complete parameters for common commands such as systemctl.

In the articles that comply with Help:Style/Formatting and punctuation, pseudo-variables are formatted in italics. For example:

In this case interface_name is used as a pseudo-variable placeholder in a systemd template unit. All systemd template units, identifiable by the @ sign, require a system-specific configuration item as argument. See systemd#Using units.

In this case the pseudo-variables are used to describe the parameters that must be substituted for them. Details on how to gather them are elaborated on in the section Securely wipe disk#Calculate blocks to wipe manually, which features the command.

This article or section needs expansion.

In case of file examples, pasting pseudo-variables in real configuration files might break the programs that use them.

In most cases, ellipses (...) are not part of the actual file content or code output, and instead represent omitted or optional text that is not relevant for the discussed subject.

For example HOOKS=(... encrypt ... filesystems ...) or:

Be aware though that, in a few instances, ellipses may be a meaningful part of the code syntax: attentive users should be able to recognize these cases by the context.

**Examples:**

Example 1 (unknown):
```unknown
# mkinitcpio -p linux
```

Example 2 (unknown):
```unknown
$ makepkg -s
```

Example 3 (unknown):
```unknown
sudo command
```

Example 4 (unknown):
```unknown
[postgres]$ initdb -D /var/lib/postgres/data
```

---

## Kernel parameters

**URL:** https://wiki.archlinux.org/title/Kernel_line

**Contents:**
- Boot loader configuration
  - Clover
  - GRUB
  - GRUB Legacy
  - LILO
  - Limine
  - rEFInd
  - Syslinux
  - systemd-boot
- dracut

There are three ways to pass options to the kernel and thus control its behaviour:

Between the three methods, the configurable options differ in availability, their name and the method in which they are specified. This page only explains the second method (kernel command line parameters) and shows a list of the most used kernel parameters in Arch Linux.

Most parameters are associated with subsystems and work only if the kernel is configured with those subsystems built in. They also depend on the presence of the hardware they are associated with.

Kernel command line parameters either have the format parameter, or parameter=value, or module.parameter=value.

Kernel parameters can be set either temporarily by editing the boot entry in the boot loader boot selection menu, or permanently by modifying the boot loader configuration file.

The following examples add the quiet and splash parameters to the Clover, GRUB, GRUB Legacy, LILO, Limine, rEFInd, Syslinux and systemd-boot boot loaders.

dracut is capable of embedding the kernel parameters in the initramfs, thus allowing to omit them from the boot loader configuration. See dracut#Kernel command line options. Note that this only works for parameters understood by dracut, like root= and rd.*. They do not become real kernel parameters.

See EFI boot stub#Using UEFI directly.

Even without access to your boot loader it is possible to change your kernel parameters to enable debugging (if you have root access). This can be accomplished by overwriting /proc/cmdline which stores the kernel parameters. However /proc/cmdline is not writable even as root, so this hack is accomplished by using a bind mount to mask the path.

First create a file containing the desired kernel parameters:

Then use a bind mount to overwrite the parameters:

You can cat /proc/cmdline to confirm that your change was successful.

This list is not comprehensive. In addition to the kernel itself, other programs can also read parameters from /proc/cmdline and change their behavior.

**Examples:**

Example 1 (unknown):
```unknown
/etc/modprobe.d/
```

Example 2 (unknown):
```unknown
parameter=value
```

Example 3 (unknown):
```unknown
module.parameter=value
```

Example 4 (unknown):
```unknown
cat /proc/cmdline
```

---

## Benchmarking

**URL:** https://wiki.archlinux.org/title/Benchmarking

**Contents:**
- Standalone tools
  - UnixBench
  - interbench
  - fio
  - ttcp
  - iperf
  - time
  - hdparm
  - gnome-disks
  - KDiskMark

Benchmarking is the act of measuring performance and comparing the results to another system's results or a widely accepted standard through a unified procedure. This unified method of evaluating system performance can help answer questions such as:

Many tools can be used to determine system performance, the following provides a list of tools available.

Install unixbenchAUR, to run the benchmark run ubench.

interbench is an application designed to benchmark interactivity in Linux. It is designed to measure the effect of changes in Linux kernel design or system configuration changes such as CPU, I/O scheduler and filesystem changes and options.

interbench is available in the AUR: interbenchAUR.

fio (Flexible I/O Tester) is a utility that can simulate various workloads such as several threads issuing reads using asynchronous I/O. Fio spawns a number of threads or processes doing a particular type of I/O action as specified by the user. Docs

ttcp (Test TCP) measures point-to-point bandwidth over any network connection. The program must be provided on both nodes between which bandwidth is to be determined.

Various flavors of ttcp can be found in the AUR:

iperf is an easy to use point-to-point bandwidth testing tool that can use either TCP or UDP. It has nicely formatted output and a parallel test mode.

iperf can be installed, or a different version of iperf is available with iperf3.

The time(1) command provides timing statistics about the command run by displaying the time that passed between invocation and termination. time contains the time command and some shells provide time as a builtin command.

Storage media can be benchmarked with hdparm (hdparm). Using hdparm with the -Tt switch, one can time sequential reads. This method is independent of partition alignment!

There is a graphical benchmark called gnome-disks contained in the gnome-disk-utility package that will give min/max/ave reads along with average access time and a nice graphical display. This method is independent of partition alignment!

Users will need to navigate through the GUI to the benchmark button ("More actions..." > "Benchmark Volume..."). Example

kdiskmark is an HDD and SSD benchmark tool with a very friendly graphical user interface. KDiskMark with its presets and powerful GUI calls Flexible I/O Tester and handles the output to provide an easy to view and interpret comprehensive benchmark result.

Will plot a detailed graphic with the boot sequence: kernel time, userspace time, time taken by each service. Example

The dd utility can be used to measure both reads and writes. This method is dependent on partition alignment! In other words, if you failed to properly align your partitions, this fact will be seen here since you are writing and reading to a mounted filesystem.

First, enter a directory on the SSD with at least 1.1 GB of free space (and one that gives your user wrx permissions) and write a test file to measure write speeds and to give the device something to read:

Next, clear the buffer-cache to accurately measure read speeds directly from the device:

Now that the last file is in the buffer, repeat the command to see the speed of the buffer-cache:

Finally, delete the temp file

Dcfldd does not print the average speed in MB/s like good old dd does but with time you can work around that.

Time the run clearing the disk:

Calculate MB/s by dividing the output of the dcfldd command by the time in seconds. For this example: 75776Mb / (16.4 min * 60) = 77.0 MB/s.

7z benchmark command can be used to measure the CPU speed in MIPS and also to check RAM for errors. Just install p7zip and run the command below. More detailed information can be found at [1].

peakperf-gitAUR is a microbenchmark that achieves peak performance on x86_64 CPUs. Some issues may reduce the performance provided by your CPU, like CPU cooling. With peakperf you can check if your CPU provides the full power it is capable of doing.

You can calculate the performance (measured in GFLOP/s) you should get using your CPU (see [2]) and compare it with the performance that peakperf gives you. If both values are the same (or very similar), your CPU behaves as it should.

cryptsetup benchmark can be used to measure the speed of various cryptographic algorithms (ciphers).

libkcapiAUR allows user-space to access the Linux kernel crypto API. The package provides several binaries, in particular kcapi-speed to benchmark hash, symmetric ciphers, AEAD ciphers, hash-based and cipher-based rng.

To list available ciphers:

Usage example of benchmarking AES using AES-NI in ECB mode 128bit keys tested for 10 seconds and using 1024 blocks to process

Output for both decription and encryption:

bonnie++ is a C++ rewrite of the original Bonnie benchmarking suite is aimed at performing several tests of hard drive and filesystem performance.

IOzone is useful for performing a broad filesystem analysis of a vendors computer platform.

This program is available in the AUR: iozoneAUR.

The following can approximate the output of several tests of CrystalDiskMark, a popular Windows benchmarking utility:

Results will be in Kb/s so divide by 1024 to get them in MB/s.

See also BBS Article: iozone to evaluate I/O schedulers... results NOT what you would expect!.

hardinfo2AUR can gather information about your system's hardware and operating system, perform benchmarks, and generate printable reports either in HTML or in plain text formats. HardInfo performs CPU and FPU benchmarks and has a very clean GTK-based interface.

The Phoronix Test Suite is the most comprehensive testing and benchmarking platform available that provides an extensible framework for which new tests can be easily added. The software is designed to effectively carry out both qualitative and quantitative benchmarks in a clean, reproducible, and easy-to-use manner.

The Phoronix Test Suite is based upon the extensive testing and internal tools developed by Phoronix.com since 2004 along with support from leading tier-one computer hardware and software vendors. This software is open-source and licensed under the GNU GPLv3.

Originally developed for automated Linux testing, support to the Phoronix Test Suite has since been added for OpenSolaris, Apple macOS, Microsoft Windows, and BSD operating systems. The Phoronix Test Suite consists of a lightweight processing core (pts-core) with each benchmark consisting of an XML-based profile and related resource scripts. The process from the benchmark installation, to the actual benchmarking, to the parsing of important hardware and software components is heavily automated and completely repeatable, asking users only for confirmation of actions.

The Phoronix Test Suite interfaces with OpenBenchmarking.org as a collaborative web platform for the centralized storage of test results, sharing of test profiles and results, advanced analytical features, and other functionality. Phoromatic is an enterprise component to orchestrate test execution across multiple systems with remote management capabilities.

This suite can be installed with the package phoronix-test-suiteAUR. There is also a developmental version available with phoronix-test-suite-gitAUR.

S, an I/O Benchmark Suite, is a small collection of scripts to measure storage I/O performance.

Download or clone the project, install its dependencies and run it as root (privileges needed to change disk scheduler).

s-tui is an aesthetically pleasing and useful curses-style interface that shows graphs of CPU frequency, utilization, temperature, power consumption and has a built in stress tester.

sysbench is an all-round multi-threaded benchmark tool. Written in C and Perl, it can be used in CLI directly to benchmark filesystem, DRAM, CPU, thread-based scheduler and POSIX mutex performance. Or it can be used as Lua script interpreter to benchmark any arbitrarily complex workload. It provides a collection of scripts for database benchmarks.

geekbenchAUR is a proprietary cross-platform suite of benchmarks that measures processor, GPU and memory performance by simulating a variety of real-world workloads, and provides scores that can be saved on Geekbench's website and compared (within the same version) against other systems, both your own and others'.

Geekbench is not useful as an independent objective test of performance, since the scores it give do not relate to any one individual task or how well a PC will perform at one or any of them; however it can be useful as a rough indication of a computer's performance relative to others, or the same computer on different operating systems/distros or under different conditions. It is commonly used in CPU benchmark comparison charts for this reason.

Performance characteristics can be measured quantitatively using iozoneAUR. Sustained read and write values can, but often do not, correlate to real-world use cases of I/O heavy operations, such as unpacking and writing a number of files on a system update. A relevant metric to consider in these cases is the random write speed for small files.

The example invocation tests a 10M file using a 4k record size:

Basemark GPU is an evaluation tool to analyze and measure graphics API (OpenGL 4.5, OpenGL ES 3.1, Vulkan and Microsoft DirectX 12) performance across mobile and desktop platforms. Basemark GPU targets both Desktop and Mobile platforms by providing both High Quality and Medium Quality modes. The High-Quality mode addresses cutting-edge Desktop workloads while the Medium Quality mode addresses equivalent Mobile workloads.

If you are using AMD GPU and have several vulkan implementations installed simultaneously, in the Test page you will see them as separate GPUs in Graphics Device dropdown list.

Basemark GPU is available in basemarkAUR package.

Blender-benchmark will gather information about the system, such as operating system, RAM, graphics cards, CPU model, as well as information about the performance of the system during the execution of the benchmark. After that, the user will be able to share the result online on the Blender Open Data platform, or to save the data locally.

Blender-benchmark is available in the blender-benchmarkAUR package.

furmarkAUR displays a very graphically-intensive image and the frame rate that results. While it can be used as a benchmark for GPU performance, its main purpose is instead as a stress test, intended to maximise GPU usage - this makes it useful for assessing system stability under full load and also for assessing the thermal performance of a graphics card.

GFXBench is a high-end graphics benchmark that measures mobile and desktop performance with next-gen graphics features across all platforms. As a true cross-API benchmark, GFXBench supports all the industry-standard and vendor-specific APIs including OpenGL, OpenGL ES, Vulkan, Metal, DirectX/Direct3D and DX12.

Vulkan API tests are currently under development and are only available for their corporate partners.

GFXBench is available in gfxbenchAUR package.

glmark2 is an OpenGL 2.0 and ES 2.0 benchmark.

glmark2 is available in glmark2 package.

glxgears is a popular OpenGL test that renders a very simple OpenGL performance and outputs the frame rate. Though glxgears can be useful as a test of direct rendering capabilities of the graphics driver, it is an outdated tool that is not representative of the current state of GNU/Linux graphics and overall OpenGL possibilities. glxgears only tests a small segment of the OpenGL capabilities that might be used in a game. Performance increases noted in glxgears will not necessarily be realized in any given game. See here for more information.

glxgears can be installed via the mesa-utils and lib32-mesa-utils (for multilib) packages.

GpuTest is a cross-platform (Windows, Linux and Max OS X) GPU stress test and OpenGL benchmark. GpuTest comes with several GPU tests including some popular ones from Windows'world (FurMark or TessMark).

GpuTest is available in gputestAUR package.

intel-gpu-tools gives you some top-like info for the integrated GPU. This can be quite useful in diagnosing GPU acceleration issues.

To use it, install the intel-gpu-tools package.

Unigine corp. has produced several modern OpenGL benchmarks based on their graphics engine with features such as:

Unigine benchmarks have found recent usage by those looking to overclock their systems. Heaven especially has been used for initial stability testing of overclocks.

These benchmarks can be found in AUR:

vkmark is an extensible Vulkan benchmarking suite with targeted, configurable scenes.

vkmark is available in vkmark package.

xmrig is a high performance, open source, cross platform RandomX, KawPow, CryptoNight and GhostRider unified CPU/GPU miner and RandomX benchmark.

Radeon GPU users running the open-source xf86-video-amdgpu driver, both opencl-mesa and rocm-opencl-runtime are required. To stress-test the GPU:

**Examples:**

Example 1 (unknown):
```unknown
# fio --filename=/mnt/test.fio --size=8GB --direct=1 --rw=randrw --bs=4k --ioengine=libaio --iodepth=256 --runtime=120 --numjobs=4 --time_based --group_reporting --name=iops-test-job --eta-newline=1
```

Example 2 (unknown):
```unknown
$ time tar -zxvf archive.tar.gz
```

Example 3 (unknown):
```unknown
# hdparm -Tt /dev/sdX
```

Example 4 (unknown):
```unknown
/dev/sdX:
Timing cached reads:   x MB in  y seconds = z MB/sec
Timing buffered disk reads:  x MB in  y seconds = z MB/sec
```

---

## Slock

**URL:** https://wiki.archlinux.org/title/Slock

**Contents:**
- Installation
- Configuration
- Usage
- Tips and tricks
  - Lock on suspend
  - Block VT switching and prevent killing X
- See also

Slock, or the "Simple X display locker", is a display locker for X that aims to be minimal, fast, and lightweight.[1].

Install the slock package.

Configuration is done by editing the config.h header file and then recompiling the package. After configuration you should create a package.

Simply run slock to lock the screen. You can also provide an argument to be run after the screen has been locked:

To unlock the screen, just type your password.

Create the following service which locks the screen.

Enable the slock@user.service systemd unit for it to take effect for the username user.

slock recommends blocking VT switching so that the screen lock cannot be bypassed. For the same reason, slock recommends preventing users from killing the X server. See Xorg#Block TTY access and Xorg#Prevent a user from killing X.

**Examples:**

Example 1 (unknown):
```unknown
$ slock cmd [arg ...]
```

Example 2 (unknown):
```unknown
/etc/systemd/system/slock@.service
```

Example 3 (unknown):
```unknown
[Unit]
Description=Lock X session using slock for user %i
Before=sleep.target

[Service]
User=%i
Environment=DISPLAY=:0
ExecStart=/usr/bin/slock

[Install]
WantedBy=sleep.target
```

Example 4 (unknown):
```unknown
slock@user.service
```

---

## X resources

**URL:** https://wiki.archlinux.org/title/Xrdb

**Contents:**
- Installation
- Configuration
  - Basic syntax
  - Wildcard matching
  - Comments
  - Include files
  - Default settings
  - Samples
- Usage
  - Load resource file

X resources file is a user-level configuration dotfile, typically located at ~/.Xresources. It can be used to set configuration parameters for X client applications. Among other things, it can be used to:

Install the xorg-xrdb package for X server resource database utility and xorg-docs for X.org documentations.

X(7)  RESOURCES and XrmGetDatabase(3)  FILE SYNTAX provide detailed information on X resources mechanism and file syntax.

~/.Xresources is a conventional file name, xrdb does not claim it. You can use any other file names, like ~/.config/X11/Xresources and ~/.config/X11/Xresources.d/application-name (also see #Samples and #Include files).

The syntax of an X resources file is a sequence of resource lines as follows:

application_name and Class substrings will never contain a dot (.), the resourceName substring may contain a dot. For example, Dialog.bodyFont is a XScreenSaver internal resource that is specified to set the body font and fallback font:

Question mark (?) and asterisk (*) can be used as wildcards, making it easy to write a single rule that can be applied to many different applications or elements. ? is used to match any single component name, while * is used to represent any number of intervening components including none.

Using the previous example, if you want to apply the same font to all programs (not just XScreenSaver) that contain the class name Dialog which contains the resource name headingFont, you could write:

If you want to apply this same rule to all programs that contain the resource headingFont, regardless of its class, you could write:

See XrmGetResource(3)  MATCHING RULES for more information.

Lines starting with an exclamation mark (!) are ignored, for example:

The two-character sequence \newline (backslash followed by newline), which allows a value to be broken across multiple lines, is also recognized inside the comments. In the following sample all four lines are commented out, despite only one exclamation mark is used:

To spread resource configuration across multiple files (e.g. to use its own file for each application), use C preprocessor #include directive:

If files that are referenced with #include are not reachable from the applied configuration file directory, you need to pass a directory to search for:

To see the default settings for your installed X11 applications, look in /usr/share/X11/app-defaults/.

Detailed information on program-specific resources is usually provided in the man page for the program. xterm(1)  RESOURCES is a good example, as it contains a list of X resources and their default values.

To see the currently loaded resources:

Resources are stored in the X server, so have to only be read once. They are also accessible to remote X11 clients (such as those forwarded over SSH).

Load a resource file (such as the conventional .Xresources), replacing any current settings:

Load a resource file, and merge with the current settings:

If you are using a copy of the default xinitrc as your .xinitrc it already merges ~/.Xresources.

If you are using a custom one, add:

If you want to get the value of a resource (for example if you want to use it in a bash script) you can use xgetresAUR:

Display managers such as GDM may use the --nocpp argument for xrdb.

It is not rare for xrdb -query to output nothing. Try following #Load resource file and #xinitrc from above. And note some of the files mentioned there could be empty.

**Examples:**

Example 1 (unknown):
```unknown
~/.Xresources
```

Example 2 (unknown):
```unknown
~/.Xresources
```

Example 3 (unknown):
```unknown
~/.config/X11/Xresources
```

Example 4 (unknown):
```unknown
~/.config/X11/Xresources.d/application-name
```

---

## Fail2ban

**URL:** https://wiki.archlinux.org/title/Fail2ban

**Contents:**
- Installation
- Usage
  - fail2ban-client
- Configuration
  - Enabling jails
  - Receive an alert e-mail
  - Firewall and services
- Tips and tricks
  - Custom SSH jail
  - systemd backend: journald filtering

Fail2ban scans log files (e.g. /var/log/httpd/error_log) and bans IPs that show the malicious signs like too many authentication attempts, scanning for vulnerabilities, etc. Generally Fail2ban is then used to update firewall rules to reject the IP addresses for a specified amount of time, although any other arbitrary action (e.g. sending an email) could also be configured.

Install one of the following packages:

Configure Fail2ban and enable/start fail2ban.service.

The fail2ban-client allows monitoring jails (reload, restart, status, etc.), to view all available commands:

To view all enabled jails:

To check the status of a jail, e.g. for sshd:

For a compact version for all jails, including banned IPs:

This article or section needs expansion.

Due to the possibility of Pacnew and Pacsave files being created for /etc/fail2ban/jail.conf during an upgrade, jail.conf(5)  CONFIGURATION FILES FORMAT recommends that users create a /etc/fail2ban/jail.local file to "ease upgrades".

For example, to change the default ban time to 1 day:

Or create separate name.local files under the /etc/fail2ban/jail.d directory, e.g. /etc/fail2ban/jail.d/sshd.local.

Reload fail2ban.service to apply the configuration changes.

By default all jails are disabled. Append enabled = true to the jail you want to use, e.g. to enable the OpenSSH jail:

See #Custom SSH jail.

If you want to receive an e-mail when someone has been banned, you have to configure an SMTP client (e.g. msmtp) and change default action, as given below.

By default, Fail2ban uses iptables. However, configuration of most firewalls and services is straightforward. For example, to use nftables:

See /etc/fail2ban/action.d/ for other examples, e.g. ufw.conf.

Edit /etc/fail2ban/jail.d/sshd.local, add this section and update the list of trusted IP addresses in ignoreip:

When using the systemd backend to improve performance, configure a filter with journalmatch. For example, to parse only kernel-level log messages:

See also systemd.journal-fields(7).

Currently, Fail2ban must be run as root. Therefore, you may wish to consider hardening the process with systemd.

Create a drop-in file for fail2ban.service:

The CapabilityBoundingSet parameters CAP_DAC_READ_SEARCH will allow Fail2ban full read access to every directory and file. CAP_NET_ADMIN and CAP_NET_RAW allow Fail2ban to operate on any firewall that has command-line shell interface. See capabilities(7) for more info.

By using ProtectSystem=strict the filesystem hierarchy will only be read-only, ReadWritePaths allows Fail2ban to have write access on required paths.

Finally, do a daemon-reload to apply the changes of the unit and restart fail2ban.service.

**Examples:**

Example 1 (unknown):
```unknown
/var/log/httpd/error_log
```

Example 2 (unknown):
```unknown
fail2ban.service
```

Example 3 (unknown):
```unknown
$ fail2ban-client
```

Example 4 (unknown):
```unknown
# fail2ban-client status
```

---

## Ghostmirror

**URL:** https://wiki.archlinux.org/title/Ghostmirror

**Contents:**
- Installation
- Usage
  - Best Mirror
  - Sort Mirror
  - Direct replace mirrorlist
- Automation
  - Prepare
  - Activate
  - systemd
- Analyzer

This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.

GhostMirror It can generate a mirrorlist from the most recent Mirrors, compare them with the local mirror, and indicate whether they are outdated, identical, or more up-to-date than the current local mirror in use. It can perform speed tests using ping and download methods. It can analyze a non-working mirror to identify potential issues, a feature particularly useful for mirror maintainers. It can also function as a systemd service, automatically determining when mirrors need to be rechecked. The main difference with Reflector is that Reflector uses the mirror list status to check the state of each mirror, particularly the lastsync, and uses ping as a measure of speed. Ghostmirror does not consider lastsync reliable, so it downloads the mirror databases and compares them, showing the actual state of a mirror. For speed, in addition to ping, Ghostmirror downloads packages to detect the actual speed of the mirror; the result is therefore more accurate.

Install the ghostmirrorAUR package.

To see all of the available options, run the following command:

If your mirrorlist already contains a good number of mirrors that you consider valid, you can skip this step. From my experience, a good number is more than 20. Feel free to have anywhere from 1 to 100+ mirrors. One of the first things you'll definitely want to do is search for reliable mirrors to include in your list. To achieve this goal, you'll need several options at your disposal. -Po show a progress and show a colorful table. -c select country, to find good mirrors, it is recommended to search within your own country and neighboring ones. The more mirrors you add, the slower the search might become, but this operation doesn't need to be done periodicallyit's just an initial sift through the countless mirrors available. for example Italy,Germany,France -l the path where you want to save the new mirrorlist. -L max numbers of output mirror in list. -S sort mode, we need remove error mirror and for this add state as first sortmode, after you can add outofdate in this mode display first the mirror sync, can also add morerecent to ensure you never go out of sync, and at the end, you can add ping to try to prioritize the closest ones.

The estimated field won't provide an optimal value without a speed test, but for finding a stable mirror, it's not necessary. If the operation is particularly slow, you can try increasing the number of simultaneous downloads with -d

Once you have a good mirrorlist, you can perform periodic checks to reorganize it and continue using the best mirrors based on your usage patterns. Let's keep the options -Pol of previous command. -mu set path mirrorlist and use only uncommented mirror. -s apply test for mirror speed. We need to change the sorting method. While we were previously looking for the closest mirrors, now we will focus on finding the most stable and fastest ones.

now you can copy to apply modify to pacman

Instead of running the command as a user, you can execute it as root to work directly on the /etc/pacman.d/mirrorlist

The process I previously referred to as "Sort Mirror" can be automated using systemd timers, specifically, use the linger functionality for automation. You wont need to worry about anything, as linger will be automatically activated along with its configuration filesyoull just need to run the appropriate command.

you need manual make dir for a new location of mirrorlist, need location where user can edit this without root privilege

inform pacman where you have stored mirrorlist. edit file /etc/pacman.conf, search and replace this line and change <username> with your username.

create a mirrorlist in ~/.config/ghostmirror in the same mode "Best Mirror" or simple copy from /etc/pacman.d/mirrorlist The command we will run will with -D enable linger if it's not already enabled, create a new mirrorlist, start the timer, and use the command for subsequent automatic mirror checks.

By default, it uses the first element of the estimated time to determine when the service should run again. You can choose to add a time in the format -t hh:mm:ss, default 00:00:00. If you don't want to use the estimated time but a fixed period to refresh the mirrors, you can use a fixed date with -f. Before using these parameters, check if the values are correct with

can check running timer

force refresh mirrorlist

To analyze mirrors, simply add the -i option to check if the mirror redirects to another server, verify if the URLs are correct, the errors reported by the mirror, and optionally the package names that have not been synchronized. example for exexute investigation only in specific mirrorlist

**Examples:**

Example 1 (unknown):
```unknown
$ ghostmirror --help
```

Example 2 (unknown):
```unknown
$ ghostmirror -Po -c Italy,Germany,France -l ./mirrorlist.new -L 30 -S state,outofdate,morerecent,ping
```

Example 3 (unknown):
```unknown
$ ghostmirror -Po -mu ./mirrorlist.new -l ./mirrorlist.new -s light -S state,outofdate,morerecent,estimated,speed
```

Example 4 (unknown):
```unknown
# cp /etc/pacman.d/mirrorlist /etc/pacman.d/mirrorlist.bak
# cp ./mirrorlist.new /etc/pacman.d/mirrorlist
```

---

## sxhkd

**URL:** https://wiki.archlinux.org/title/Sxhkd

**Contents:**
- Installation
- Configuration
  - Example
    - Bind a command to a single press of a key
- Usage
- See also

sxhkd is a simple X hotkey daemon, by the developer of bspwm, that reacts to input events by executing commands.

Install sxhkd or sxhkd-gitAUR.

sxhkd defaults to $XDG_CONFIG_HOME/sxhkd/sxhkdrc for its configuration file. An alternate configuration file can be specified with the -c option.

Each line of the configuration file is interpreted as so:

Where MODIFIER is one of the following names: super, hyper, meta, alt, control, ctrl, shift, mode_switch, lock, mod1, mod2, mod3, mod4, mod5. If @ is added at the beginning of the keysym, the command will be run on key release events, otherwise on key press events. The KEYSYM names are those you will get from xev.

Mouse hotkeys can be defined by using one of the following special keysym names: button1, button2, button3, ..., button24. The hotkey can contain a sequence of the form STRING_1, ...,STRING_N}, in which case, the command must also contain a sequence with N elements: the pairing of the two sequences generates N hotkeys. If the command includes curly braces ({, }) eg. awk '{print $1}', escape them with backslash \ eg. awk '\{print $1\}'. In addition, the sequences can contain ranges of the form A-Z where A and Z are alphanumeric characters.

What is actually executed is SHELL -c COMMAND, which means you can use environment variables in COMMAND. SHELL will be the content of the first defined environment variable in the following list: SXHKD_SHELL, SHELL. If sxhkd receives a SIGUSR1 signal, it will reload its configuration file.

Some users may wish to bind a command to a single keypress, in the way that the Super key opens the Start menu on Windows. In sxhkd, this is accomplished by binding to a chord chain composed of the key press and key release event of a single keysym, as in the following:

This complicated pattern is necessary because without the explicit chord chain (i.e. a single @Super_L, sxhkd would trigger the binding on any release of the keyeven when the key is used in another chord. Furthermore, keysyms must be used instead of modifiers (Super_L as opposed to super) because sxhkd does not support the use of the @ symbol (denoting key release) for modifiers.

After configuring it, you may wish to set up sxhkd to autostart; see the corresponding article for your desktop environment or window manager for details.

If your desktop environment supports the Desktop Application Autostart Specification you can start sxhkd by creating an sxhkd.desktop file in the appropiate directory:

**Examples:**

Example 1 (unknown):
```unknown
$XDG_CONFIG_HOME/sxhkd/sxhkdrc
```

Example 2 (unknown):
```unknown
[MODIFIER + ]*[@]KEYSYM
    COMMAND
```

Example 3 (unknown):
```unknown
mode_switch
```

Example 4 (unknown):
```unknown
awk '{print $1}'
```

---

## su

**URL:** https://wiki.archlinux.org/title/Su

**Contents:**
- Installation
- Usage
- Tips and tricks
  - Login shell
  - su and wheel
  - Nologin users

The su core utility (substitute user) is used to assume the identity of another user on the system, root by default.

See PAM for ways to configure su's behavior.

su is part of the util-linux package.

To assume the login of another user, pass the username that you want to become to su, as in:

By default, when running as a regular user, you will be prompted for the password of the user you are attempting to become. When running su as root, no password is required.

If no username is passed, su assumes the root user, and the password for which you are prompted will be that of root.

For more information, see su(1).

The default behavior of su is to remain within the current directory and to maintain the environmental variables of the original user (rather than switch to those of the new user).

Note the following important contrasting considerations:

Thus, it is advisable that administrative users, as well as any other users that are authorized to use su (and it is suggested that there be very few, if any) acquire the habit of always running the su command with the -l/--login option. It has two effects:

Thus, administrators should generally use su as follows:

An identical result is produced by adding the username root:

Likewise, the same can be done for any other user (e.g. for a user named archie):

To log into a passwordless user, first log in as root and then log into the passwordless user account from the root shell:

BSD su allows only members of the wheel user group to assume root's identity by default. This is not the default behavior of GNU su, but this behavior can be mimicked using PAM. Uncomment the appropriate line in /etc/pam.d/su and /etc/pam.d/su-l:

You cannot run commands as an other user by simply using su user -c command if they are not allowed to login (i.e. they have /bin/false or /usr/bin/nologin set as their shell).

You can work around this by specifying the shell to use:

**Examples:**

Example 1 (unknown):
```unknown
$ su username
```

Example 2 (unknown):
```unknown
$ su -l root
```

Example 3 (unknown):
```unknown
# su -l archie
```

Example 4 (unknown):
```unknown
$ su -l
# runuser -l archie
```

---

## Help:Reading

**URL:** https://wiki.archlinux.org/title/Executable

**Contents:**
- Organization
- Formatting
- Root, regular user or another user
- Append, add, create, edit
  - Make executable
- Source
- Installation of packages
  - Official packages
  - Arch User Repository
- Control of systemd units

Because the vast majority of the ArchWiki contains indications that may need clarification for users new to Arch Linux (or GNU/Linux in general), this rundown of basic procedures was written both to avoid confusion in the assimilation of the articles and to deter repetition in the content itself.

Most articles on the ArchWiki do not attempt to provide a holistic introduction to a single topic; they are instead written in adherence to the "Don't Repeat Yourself" principle, under the assumption that the user will seek out and read any supporting material that they do not yet understand. Where possible, such supporting material is indicated in the article via special formatting, see #Formatting.

Because of this organization, it may be necessary to examine several related sources in order to fully understand an ArchWiki article. In particular, users who are new to Arch (or GNU/Linux in general) should expect to end up reading a great number of articles even when solving simple problems. It is especially important to study the supporting material before seeking additional help from other users.

Some lines are written like so:

Others have a different prefix:

The numeral or hash sign (#) indicates that the command needs to be run as root, whereas the dollar sign ($) shows that the command should be run as a regular user.

When the commands need to run as a specific user, they will be prefixed by the username in square brackets, for example:

This means you should use a privilege elevation tool, e.g. with sudo:

A notable exception to watch out for:

In this example, the context surrounding the numeral sign communicates that this is not to be run as a command; it should be edited into a file instead. So in this case, the numeral sign denotes a comment. A comment can be explanatory text that will not be interpreted by the associated program. Bash scripts denotation for comments happens to coincide with the root PS1.

After further examination, "give away" signs include the uppercase character following the # sign. Usually, Unix commands are not written this way and most of the time they are short abbreviations instead of full-blown English words (e.g., Copy becomes cp).

Regardless, most articles make this easy to discern by notifying the reader:

Append to ~/path/to/file:

When prompted to append to, add to, create, or edit one or more files, it is implied that you should use one of the following methods.

To create or modify multiline files, it is suggested to use a text editor. For example, using the nano command to edit the file /etc/bash.bashrc is:

To create or overwrite a file from a string, it may be simpler to use output redirection. The following example creates or overwrites the contents of the file /etc/hostname with the text myhostname.

Output redirection can also be used to append a string to a file. The following example appends the text [custom-repo] to the file /etc/pacman.conf.

When prompted to create directories, use the mkdir command:

After creating a file, if it is meant to be run as a script (whether manually or called by another program), it needs to be set as executable, for example with:

See chmod. Some applications such as file managers may provide graphical interfaces to do the same.

Some applications, notably command-line shells, use scripts for their configuration: after modifying them, they must be sourced in order for the changes to be applied. In the case of bash, for example, this is done by running (you can also replace source with .):

When the wiki suggests modifying such a configuration script, it will not explicitly remind you to source the file, and only in some cases will it point to this section with a reminder link.

When an article invites you to install some packages in the conventional way, it will not indicate the detailed instructions to do so; instead, it will simply mention the names of the packages to be installed.

The subsections below give an overview of the generic installation procedures depending on the package type.

For packages from the official repositories, you will read something like:

This means that you have to run:

The pacman article contains detailed explanations to deal with package management in Arch Linux proficiently.

For packages from the Arch User Repository (AUR), you will read something like:

This means that in general you have to follow the foobarAUR link, download the PKGBUILD archive, extract it, verify the content and finally run, in the same folder:

The Arch User Repository article contains all the detailed explanations and best practices to deal with AUR packages.

When an article invites to start, enable, etc., some systemd unit (e.g. a service), it will not indicate the detailed instructions to do so, but instead you will read something like:

This means that you have to run:

A notable command that does not follow this exact pattern is systemctl daemon-reload which will be called without arguments.

The systemd#Using units section contains structured list of available actions (like start, enable, enable and start, etc.) with their corresponding systemctl commands.

It is important to remember that there are two different kinds of configurations on a GNU/Linux system. System-wide configuration affects all users. Since system-wide settings are generally located in the /etc directory, root privileges are required in order to alter them. For example, to apply a Bash setting that affects all users, /etc/bash.bashrc should be modified.

User-specific configuration affects only a single user. Dotfiles are used for user-specific configuration. For example, the file ~/.bashrc is the user-specific configuration file. The idea is that each user can define their own settings, such as aliases, functions and other interactive features like the prompt, without affecting other users' preferences.

Bash and other Bourne-compatible shells, such as Zsh, also source files depending on whether the shell is a login shell or an interactive shell. See Bash#Configuration files and Zsh#Startup/Shutdown files for details.

Some code blocks may contain so-called pseudo-variables, which, as the name says, are not actual variables used in the code. Instead they are generic placeholders and have to be manually replaced with system-specific configuration items before the code may be run or parsed. Common shells such as bash and zsh provide tab-completion to auto-complete parameters for common commands such as systemctl.

In the articles that comply with Help:Style/Formatting and punctuation, pseudo-variables are formatted in italics. For example:

In this case interface_name is used as a pseudo-variable placeholder in a systemd template unit. All systemd template units, identifiable by the @ sign, require a system-specific configuration item as argument. See systemd#Using units.

In this case the pseudo-variables are used to describe the parameters that must be substituted for them. Details on how to gather them are elaborated on in the section Securely wipe disk#Calculate blocks to wipe manually, which features the command.

This article or section needs expansion.

In case of file examples, pasting pseudo-variables in real configuration files might break the programs that use them.

In most cases, ellipses (...) are not part of the actual file content or code output, and instead represent omitted or optional text that is not relevant for the discussed subject.

For example HOOKS=(... encrypt ... filesystems ...) or:

Be aware though that, in a few instances, ellipses may be a meaningful part of the code syntax: attentive users should be able to recognize these cases by the context.

**Examples:**

Example 1 (unknown):
```unknown
# mkinitcpio -p linux
```

Example 2 (unknown):
```unknown
$ makepkg -s
```

Example 3 (unknown):
```unknown
sudo command
```

Example 4 (unknown):
```unknown
[postgres]$ initdb -D /var/lib/postgres/data
```

---

## Paperkey

**URL:** https://wiki.archlinux.org/title/Paperkey

**Contents:**
- Installation
- Usage
  - Backup
  - Restore secret key
    - Error: unable to parse OpenPGP packets (is this armored data?)
- Tips and tricks
  - Print secret key directly
  - Encode the secret key as QR Code
  - Restore the secret key from QR code

Paperkey is a command line tool to export OpenPGP keys on paper. It reduces the size of the exported key, by removing the public key parts from the private key. Paperkey also includes CRC-24 checksums in the key to allow the user to check whether their private key has been restored correctly.

Install the paperkey package.

To create a backup of your GnuPG key, pipe the private key to paperkey:

To restore the secret key you need to have a file with the paperkey data and the public key. Then run the following command to import the private key to ~/.gnupg:

Alternatively, restore the private key to a file:

If you receive this error while restoring your key, you need to dearmor your public key first:

If no --output argument is given, paperkey will print its output to stdout. It is possible to print the key directly without intermediate file, which might have security implications. To do so, install CUPS, and pipe to lpr:

By default, paperkey will output the secret key as human readable text. While this format guarantees the ability to read and restore the printed information, it is not very convenient. The --output-type raw option tells paperkey to output the raw secret key data instead. This enables the use of other encodings, including machine-readable ones such as the QR code.

The qrencode program can be used for this:

It is possible to increase the error correction level to maximum with the --level H option. This provides a lost data restoration rate of about 30% at the cost of reduced capacity. Should the secret key not fit in the QR code, the lower Q and M error correction levels are also available and give restoration rates of about 25% and 15% respectively. The default error correction level is L which allows restoration of about 7% of lost data.

With zbar it is possible to restore the key using a camera:

The same options can also be applied to zbarimg:

If you are using a scanned image, you might have to blur it by

**Examples:**

Example 1 (unknown):
```unknown
$ gpg --export-secret-key key-id | paperkey --output secret-key-paper.asc
```

Example 2 (unknown):
```unknown
$ paperkey --pubring public-key.gpg --secrets secret-key-paper.asc | gpg --import
```

Example 3 (unknown):
```unknown
$ paperkey --pubring public-key.gpg --secrets secret-key-paper.asc --output secret-key.gpg
```

Example 4 (unknown):
```unknown
$ gpg --dearmor public-key.asc
```

---

## Bug reporting guidelines

**URL:** https://wiki.archlinux.org/title/Reporting_bug_guidelines

**Contents:**
- Before reporting
  - Search for duplicates
  - Upstream or Arch?
  - Bug or feature?
    - Reasons for not being a bug
    - Reasons for not being a feature
  - Gather useful information
- Opening a bug
  - Creating an account
  - Where to open the bug

Opening (and closing) bug reports on the Arch Linux bug tracker in GitLab is one of many possible ways to help the community. However, poorly-formed bug reports can be counter-productive. When bugs are incorrectly reported, developers waste time investigating and closing invalid reports. This document will guide anyone wanting to help the community by efficiently reporting and hunting bugs.

See also: How to Report Bugs Effectively by Simon Tatham

Preparing a detailed and well-formed bug report is not difficult, but requires an effort on behalf of the reporter. The work done before reporting a bug is arguably the most useful part of the job. Unfortunately, few people take the time to do this work properly.

The following steps will guide you in preparing your bug report or feature request.

If you encounter a problem or desire a new feature, there is a high probability that someone else already had this problem or idea. If so, an appropriate bug report may already exist. In this case, please do not create a duplicate report; see #Following up on bugs instead.

Search thoroughly for existing information, including:

Arch Linux is a GNU/Linux distribution. Arch developers and Package Maintainers are responsible for compiling, packaging, and distributing software from a wide range of sources. Upstream refers to these sources  the original authors or maintainers of software that is distributed in Arch Linux. For example, the popular Firefox web browser is developed by the Mozilla Project.

If Arch is not responsible for a bug, the problem will not be solved by reporting the bug to Arch developers. Responsibility for a bug is said to lie upstream when it is not caused through the distribution's porting and integration efforts.

By reporting bugs upstream, you will not only help Archers and Arch developers, but you will also help other people in the free software community as the solution will trickle down to all distributions.

Once you have reported a bug upstream or have found other relevant information from upstream, it might be helpful to post this in the Arch bug tracker, so both Arch developers and users are made aware of it.

So what is Arch Linux responsible for?

If a bug/feature is not under Arch's responsibility, report it upstream. See also #Reasons for not being a bug.

Here is a list of useful information that should be mentioned in your bug report:

If you have to paste a lot of text, like the output of dmesg, or an Xorg log, is it preferred to save it as a file on your computer and attach it to your bug report. Attaching a file rather than using a pastebin to present relevant information is preferable in general due to the fact that pastebined content may suffer by expired links or any other potential problems. Attaching a file guarantees the provided information will always be available.

When you are sure it is a bug or a feature and you gathered all useful information, then you are ready to open a bug report or feature request.

You have to create an account on Arch's GitLab, which manages its login via Arch Linux SSO.

Once you have determined your feature or bug is related to Arch and not an upstream issue, you will need to file your problem in the correct project. This is most easily done via Add new bug on the respective packages page on archlinux.org.

You can alternatively also get to the correct page using pkgctl from devtools:

Problems with packages in the AUR are not reported in the bug tracker. The AUR allows you to add comments to a package, which you should use to report bugs to the package maintainer.

Please write a concise and descriptive title for your bug/feature request.

Here is a list of recommendations:

This article or section is out of date.

Choosing a critical severity will not help to solve the bug faster. It will only make truly critical problems less visible and probably make the developer assigned to your bug a bit less open to fixing it.

Here is a general usage of severities:

This is maybe one of the most difficult parts of bug reporting. You have to choose from the section #Gather useful information which information you will add to your bug report. This will depend on which your problem is. If you do not know what the relevant pieces of information are, do not be shy: it is better to give more information than needed than not enough.

A good tutorial on reporting bugs can be found at https://www.chiark.greenend.org.uk/~sgtatham/bugs.html.

However, developers or bug hunters will ask you for more information if needed. Fortunately after a few bug reports you will know what information should be given.

Short information can be inlined in your bug report, whereas long information (logs, screenshots...) should be attached.

Do not think the work is done once you have reported a bug!

Developers or other users will probably ask you for more details and give you some potential fixes to try. Without continued feedback, bugs cannot be closed and the end result is both angry users and developers.

You can vote for your favourite bugs via reactions on the issue. The number of reactions indicates to the developers how many people are impacted by the bug without creating too much noise. However, this is not a very effective way of getting the bug solved. Much more important would be posting any additional information you know about the bug if you were not the original reporter.

Watching a bug is important: you will receive an email when new comments are added or the bug status has changed. This can be done via the "..." menu in the upper right corner by toggling the Notification switch there. If you opened a issue or commented on it you will automatically be subscribed to changes.

People will take the time to look at your bug report and will try to help you. You need to continue to help them resolve your bug. Not answering to their questions will keep your bug unresolved and likely hamper enthusiasm to fix it.

Please take the time to give people more information if requested and try the solutions proposed.

Developers or bug hunters will close your bug if you do not answer questions after a few weeks or a month.

Sometimes, a bug is only present in certain version(s) of a given package, and the bug is fixed in a new version of the package. If this is the case, say so in the bug report comments, and request that the bug be closed.

Sometimes people report a bug but do not notify when they have solved it on their own, leaving people searching for a solution that has already been found. Please close the bug if you found a solution, and give the solution in the bug report comments.

During its life, a bug may go through several states:

The Bug Wranglers are responsible for dispatching bugs and setting their status together with the help of the respective maintainers of the package the bug was opened against.

**Examples:**

Example 1 (unknown):
```unknown
pacman -Qi package_name
```

Example 2 (unknown):
```unknown
/var/log/messages
```

Example 3 (unknown):
```unknown
~/.local/share/xorg/Xorg.0.log
```

Example 4 (unknown):
```unknown
/var/log/Xorg.0.log
```

---

## Optical disc drive

**URL:** https://wiki.archlinux.org/title/Optical_disc_drive

**Contents:**
- Burning
  - Install burning utilities
  - Making an ISO image from existing files
    - Basic options
    - graft-points
  - Mounting an ISO image
  - Converting img/ccd to an ISO image
  - Learning the name of your optical drive
  - Reading the volume label of a CD or DVD
  - Creating an ISO image from a CD, DVD, or BD

This article or section needs expansion.

The burning process of optical disc drives consists of creating or obtaining an image and writing it to an optical medium. The image may in principle be any data file. If you want to mount the resulting medium, then it is usually an ISO 9660 file system image file. Audio and multi-media CDs are often burned from a .bin file, under control of a .toc file or a .cue file which tell the desired track layout.

This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.

If you want to use programs with graphical user interface, then follow #Burning CD/DVD/BD with a GUI.

The programs listed here are command line oriented. They are the back ends which are used by most free GUI programs for CD, DVD, and BD. GUI users might get to them when it comes to troubleshooting or to scripting of burn activities.

You need at least one program for creation of file system images and one program that is able to burn data onto your desired media type.

Available programs for ISO 9660 image creation are:

The traditional choice is mkisofs, because it is the older one.

Available programs for burning to media are:

The traditional choices are cdrecord for CD and growisofs for DVD and Blu-ray Disc, because cdrecord was first to offer CD writing without description file and growisofs was first to offer writing to DVD and BD without artificial restrictions by the burn program. For writing TOC/CUE/BIN files to CD, install cdrdao.

The free GUI programs for CD, DVD, and BD burning depend on at least one of the above packages.

xorrisofs supports the mkisofs options which are shown in this document.

cdrskin supports the shown cdrecord options; xorrecord also supports those which do not deal with audio CD.

The simplest way to create an ISO image is to first copy the needed files to one directory, for example: ./for_iso.

Then generate the image file with mkisofs:

Each of those options are explained in the following sections.

It is also possible to let mkisofs to collect files and directories from various paths

-graft-points enables the recognition of pathspecs which consist of a target address in the ISO file system (e.g. /photos) and a source address (e.g. /home/user/photos). Both are separated by a "=" character.

So this example puts the directories /home/user/photos, /home/user/mail and /home/user/holidays/photos, respectively in the ISO image as /photos, /mail and /photos/holidays.

Programs mkisofs and xorrisofs accept the same options. For secure backups, consider using xorrisofs with option --for_backup, which records eventual ACLs and stores an MD5 checksum for each data file.

See the mkisofs(8) and xorrisofs(1) man pages for more info about their options.

You can mount an ISO image if you want to browse its files. To mount the ISO image, we can use:

Do not forget to unmount the image when your inspection of the image is done:

See also Mounting images as user for mounting without root privileges.

To convert an img/ccd image, you can use ccd2iso:

For the remainder of this section the name of your recording device is assumed to be /dev/sr0.

which should report Vendor_info and Identification fields of the drive.

If no drive is found, check whether any /dev/sr* exist and whether they offer read/write permission (wr-) to you or your group. If no /dev/sr* exists then try loading module sr_mod manually.

If you want to get the name/label of the media, use dd:

In order to only copy actual data from the disc and not the empty blocks filling it up, first retrieve its block/sector count and size (2048 most of the time):

Then use dd to copy the data using the obtained values:

If the original medium was bootable, then the copy will be a bootable image. You may use it as a pseudo CD for a virtual machine or burn it onto an optical medium which should then become bootable. [1]

This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.

dvdisasterAUR or dvdisaster-unofficialAUR is a tool, that adds error correction data to optical media. This data can help recover content from scratched or damaged discs.

1. Insert the CD, DVD, or Blu-ray Disc into your optical drive. 2. Make sure the disc is not mounted. You can unmount it using:

3. Run dvdisaster from the command line or find it in your application menu. 4. In the dvdisaster interface, choose Create error correction data. 5. Select the disc type (CD/DVD/BD) from the drop-down menu. 6. Click the Load Disc button to scan the contents of your optical media. 7. dvdisaster will analyze the disc and display its structure. 8. Choose a location where the error recovery (ECC) file will be saved. 9. Set the error correction level. Higher levels provide better recovery at the cost of larger ECC file sizes. 2. Click Generate to begin creating the error correction data. 10. The process may take several minutes depending on the size of your disc. 11. Once the ECC file is created, dvdisaster will prompt you to verify the file. 12. Save both the original disc image (ISO) and the ECC file for future use. 13. It is recommended to store your ISO and ECC files on multiple devices or cloud storage for maximum safety.

Tips for Best Results

This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.

If your optical disc becomes scratched or otherwise damaged, dvdisaster can help recover lost data using an error correction (ECC) file. 1. Insert the damaged CD, DVD, or Blu-ray Disc into your optical drive. 2. Unmount the disc to avoid conflicts:

3. Start dvdisaster 4. In the dvdisaster interface, choose Scan and Repair mode. 5. Load the damaged disc by selecting the drive from the dropdown menu or using the Load Disc button. 5. Click Load ECC to select the corresponding error correction file (usually `.ecc`) created when you first burned the disc. 6. Ensure the ECC file matches the exact disc structure. If you do not have a matching ECC file, this method will not work. 7. Click Scan to begin reading the disc and identifying damaged sectors. 8. dvdisaster will display a visual representation of the discs status, showing good, unreadable, and corrected sectors. 9. Once scanning is complete, click Repair to rebuild missing or corrupt data using the ECC file. 10. The recovered data will be written to a new ISO image file. 11. Choose a destination to save the rebuilt ISO image. Example: `/home/user/recovered_disc.iso` 12. Click **"Save"** to complete the process. 13. Mount the ISO to verify that files have been successfully recovered:

14. Check the contents at `/mnt` to ensure data integrity.

Tips for Best Results

Video: https://www.youtube.com/watch?v=zolvCcxcN4g

Used CD-RW media need to be erased before you can write over the previously recorded data. This is done by

There are two options for blanking: blank=fast and blank=all. Full blanking lasts as long as a full write run. It overwrites the payload data on the CD. Nevertheless this should not be considered as securely making those data unreadable. For that purpose, several full write runs with random data are advised.

Alternative commands are:

To erase the DVD-RW use the dvd+rw-format utility from dvd+rw-tools:

Alternative commands are:

Such fastly blanked DVD-RW are not suitable for multi-session and cannot take input streams of unpredicted length. For that purpose one has to use one of:

The other media types are either write-once (CD-R, DVD-R, DVD+R, BD-R) or are overwritable without the need for erasing (DVD-RAM, DVD+RW, BD-RE).

Formatted DVD-RW media can be overwritten without previous erasure. So consider to apply once in their life time

Unlike DVD-RAM, DVD+RW, and BD-RE, formatted DVD-RW cannot be used as (slow) hard disk directly, but rather need the mediation of driver pktcdvd. See pktsetup(8).

BD-RE need formatting before first use. This is done automatically by the burn programs when they detect the unformatted state. Nevertheless the size of the payload area can be influenced by expert versions of the format commands shown above for DVD-RW.

BD-R can be used unformatted or formatted. Unformatted they are written with full nominal speed and offer maximum storage capacity. Formatted they get checkread during write operations and bad blocks get replaced by blocks from the Spare Area. This reduces write speed to a half or less of nominal speed. The default sized Spare Area reduces the storage capacity by 768 MiB.

growisofs formats BD-R by default. The others do not. growisofs can be kept from formatting. cdrskin and xorriso can write with full nominal speed on formatted BD-RE or BD-R:

To burn a readily prepared ISO image file isoimage.iso onto an optical medium, run for CD:

and for CD, DVD or BD:

You can verify the integrity of the burnt medium to make sure it contains no errors. Always eject the medium and reinsert it before verifying; it will guarantee that any kernel cache will not be used to read the data.

First calculate the MD5 checksum of the original ISO image:

Next calculate the MD5 checksum of the ISO file system on the medium. Although some media types deliver exactly the same amount of data as have been submitted to the burn program, many others append trailing garbage when being read. So you should restrict reading to the size of the ISO image file.

Both runs should yield the same MD5 checksum (here: e5643e18e05f5646046bb2e4236986d8). If they do not, you will probably also get an I/O error message from the dd run. dmesg might then tell about SCSI errors and block numbers, if you are interested.

It is not necessary to store an emerging ISO file system on hard disk before writing it to optical media. Only very old CD drives in very old computers could suffer failed burns due to an empty drive buffer.

If you omit option -o from mkisofs then it writes the ISO image to standard output. This can be piped into the standard input of burn programs.

Option -waiti is not really needed here. It prevents cdrecord from writing to the medium before mkisofs starts its output. This would allow mkisofs to read the medium without disturbing an already started burn run. See next section about multi-session.

On DVD and BD, you may let growisofs operate mkisofs for you and burn its output on-the-fly:

ISO 9660 multi-session means that a medium with readable file system is still writable at its first unused block address, and that a new ISO directory tree gets written to this unused part. The new tree is accompanied by the content blocks of newly added or overwritten data files. The blocks of data files, which shall stay as in the old ISO tree, will not be written again.

Linux and many other operating systems will mount the directory tree in the last session on the medium. This youngest tree will normally show the files of the older sessions, too.

CD-R and CD-RW stay writable (aka "appendable") if cdrecord option -multi was used

Then the medium can be inquired for the parameters of the next session

By help of these parameters and of the readable medium in the drive you can produce the add-on ISO session

Finally append the session to the medium and keep it appendable again

Programs cdrskin and xorrecord do this too with DVD-R, DVD+R, BD-R and unformatted DVD-RW. Program cdrecord does multi-session with at least DVD-R and DVD-RW. They all do with CD-R and CD-RW, of course.

Most re-usable media types do not record a session history that would be recognizable for a mounting kernel. But with ISO 9660 it is possible to achieve the multi-session effect even on those media.

growisofs and xorriso can do this and hide most of the complexity.

By default, growisofs uses mkisofs as a backend for creating ISO images forwards most of its program arguments to . See above examples of mkisofs. It bans option -o and deprecates option -C. By default it uses the mkisofs. You may specify to use one of the others compatible backend program by setting environment variable MKISOFS:

The wish to begin with a new ISO file system on the optical medium is expressed by option -Z

The wish to append more files as new session to an existing ISO file system is expressed by option -M

For details see the growisofs(1) manual and the manuals of mkisofs and xorrisofs.

xorriso learns the wish to begin with a new ISO file system from the blank state of the medium. So it is appropriate to blank it if it contains data. The command -blank as_needed applies to all kinds of re-usable media and even to ISO images in data files on hard disk. It does not cause error if applied to a blank write-once medium.

On non-blank writable media xorriso appends the newly given disc files if command -dev is used rather than -outdev. Of course, no command -blank should be given here

For details see the xorriso(1) man page and especially its examples.

BD-RE and formatted BD-R media are normally written with enabled Defect Management. This feature reads the written blocks while they are still stored in the drive buffer. In case of poor read quality the blocks get written again or redirected to the Spare Area where the data get stored in replacement blocks.

This checkreading reduces write speed to at most half of the nominal speed of drive and BD medium. Sometimes it is even worse. Heavy use of the Spare Area causes long delays during read operations. So Defect Management is not always desirable.

cdrecord does not format BD-R. It has no means to prevent Defect Management on BD-RE media, though.

growisofs formats BD-R by default. The Defect Management can be prevented by option -use-the-force-luke=spare:none. It has no means to prevent Defect Management on BD-RE media, though.

cdrskin, xorriso and xorrecord do not format BD-R by default. They do with cdrskin blank=format_if_needed, resp. xorriso -format as_needed, resp. xorrecord blank=format_overwrite. These three programs can disable Defect Management with BD-RE and already formatted BD-R by cdrskin stream_recording=on, resp. xorriso -stream_recording on, resp. xorrecord stream_recording=on.

Create your audio tracks and store them as uncompressed, 16-bit, 44100-Hz, stereo WAV files.

If the files are 24 bit encoded (for example, an online music service only provides 24 bit WAV files), they need to be converted to 16 bit in order to play in RedBook-compliant CD players. To check, ensure sox is installed, cd to the directory with your WAV files, and run:

If the file have the wrong encoding, convert the files using sox:

To convert MP3 to WAV, ensure lame is installed, cd to the directory with your MP3 files, and run:

In case you get an error when trying to burn WAV files converted with LAME, try decoding with mpg123:

To convert AAC to WAV ensure faad2 is installed and run:

To fix the bitrate of an already existing WAV file (or many other formats), try using sox:

Name the audio files in a manner that will cause them to be listed in the desired track order when listed alphabetically, such as 01.wav, 02.wav, 03.wav, etc.

With cdrtools, use the following command to simulate burning the WAV files as an audio CD:

If everything worked, you can remove the -dummy flag to actually burn the CD.

Alternatively, with cdrdao, create a "Table of content" file with the following command:

This will make it so that no gaps exits between tracks. Optionally, if you would like to insert a X-second gap between certain tracks, you can edit the toc file and insert the following line between the TRACK AUDIO and FILE lines for that track:

Then, we burn the CD:

The speed can be adjusted, lower speed producing a higher quality result. This is because the Audio-CD format has less advanced error correction than the data storage format.

To test the new audio CD, use MPlayer:

To burn a BIN/CUE image run:

ISO images only store a single data track. If you want to create an image of a mixed-mode disc (data track with multiple audio tracks) then you need to make a TOC/BIN pair:

Some software only likes CUE/BIN pair, you can make a CUE sheet with toc2cue (part of cdrdao):

If you are experiencing problems, you may ask for advice at mailing list cdwrite@other.debian.org, or try to write to the one of support mail addresses if some are listed near the end of the program's man page.

Tell the command lines you tried, the medium type (e.g. CD-R, DVD+RW, ...), and the symptoms of failure (program messages, disappointed user expectation, ...). You will possibly get asked to obtain the newest release or development version of the affected program and to make test runs. But the answer might as well be, that your drive dislikes the particular medium.

There are several applications available to burn CDs in a graphical environment.

See also Wikipedia:Comparison of disc authoring software.

Playback of audio CDs requires the libcdio package. To enable KDE Applications like Dolphin to read audio CDs install audiocd-kio.

If you wish to play encrypted DVDs, you must install the libdvd* packages:

Additionally, you must install player software. Popular DVD players are MPlayer, xine and VLC. See the video players list, the specific instructions for MPlayer, and the specific instructions for VLC.

See Blu-ray#Playback.

Ripping is the process of copying audio or video content to a hard disk, typically from removable media or media streams.

See also Wikipedia:Comparison of DVD ripper software.

Often, the process of ripping a DVD can be broken down into two subtasks:

Some utilities perform both tasks, whilst others focus on one aspect or the other.

If you try to burn it may stop at the first step called Normalization.

As a workaround you can disable the normalization plugin using the Edit > Plugins menu

If you get an error like

it may be because there is no device node /dev/dvd on your system. Udev no longer creates /dev/dvd and instead uses /dev/sr0. To fix this, edit the VLC configuration file (~/.config/vlc/vlcrc):

If playing DVD videos causes the system to be very loud, it may be because the disc is spinning faster than it needs to. To temporarily change the speed of the drive, run:

Any speed that is supported by the drive can be used, or 0 for the maximum speed.

Setting CD-ROM and DVD-ROM drive speed

If optical drive is constantly checking for a new disk causing it to make unnecessary noise, consider turning SATA "Hot Plug" on for your optical drive in BIOS.

If playback does not work and you have a new computer (new DVD-Drive) the reason might be that the region code is not set. You can read and set the region code with the regionsetAUR package.

Make sure the region of your DVD reader is set correctly; otherwise, you will get loads of inexplicable CSS-related errors. Use the regionsetAUR package to do so.

If ripping still does not work with the correct region set, refer to the libdvdcss developer documentation for enabling log messages and setting other relevant options.

If you use a GUI program and experience problems which the program's log blames on some backend program, then try to reproduce the problem by the logged backend program arguments. Whether you succeed with reproducing or not, you may report the logged lines and your own findings to the places mentioned in #Burn backend problems section.

Here are some typical messages about the drive disliking the medium. This can only be solved by using a different drive or a different medium. A different program will hardly help.

Brasero with backend growisofs:

Brasero with backend libburn:

Using growisofs from dvd+rw-tools for burning 50GB BD-R DL discs might result in a fatal error and damaged media, such as:

This happened at the 25GB boundary when starting to write the second layer. Using cdrecord from cdrtools works with no problems. Tested with a 'HL-DT-ST BD-RE WH16NS40' LG burner, and Verbatim BD-R DL 6x discs (#96911). FS#47797

If after ejecting a cd, either by using the eject command, or pushing the drive button, the drive disc tray autocloses before being able to remove the disc, try the following command:

If that solves the problem, make the change permanent:

If the above does not work and as a last resort measure, you can unload the disc module from the kernel via:

the disc drive should now behave as expected but will not mount disc anymore. After putting a disc into the drive, reactivate the module via:

the disc should now mount.

See General troubleshooting#Cannot use some peripherals after kernel upgrade.

**Examples:**

Example 1 (unknown):
```unknown
$ mkisofs -V "ARCHIVE_2013_07_27" -J -r -o isoimage.iso ./for_iso
```

Example 2 (unknown):
```unknown
-joliet-long
```

Example 3 (unknown):
```unknown
$ mkisofs -V "BACKUP_2013_07_27" -J -r -o backup_2013_07_27.iso \
  -graft-points \
  /photos=/home/user/photos \
  /mail=/home/user/mail \
  /photos/holidays=/home/user/holidays/photos
```

Example 4 (unknown):
```unknown
-graft-points
```

---

## Ceph

**URL:** https://wiki.archlinux.org/title/Ceph

**Contents:**
- Terminology
- Installation
  - Packages
  - NTP Client
- Bootstrapping a storage cluster
  - Starting a monitor
- See also

Ceph is a storage platform with a focus on being distributed, resilient, and having good performance and high reliability. Ceph can also be used as a block storage solution for virtual machines or through the use of FUSE, a conventional filesystem. Ceph is extremely configurable, with administrators being able to control virtually all aspects of the system. A command line interface is used to monitor and control the cluster. The platform also contains authentication & authorization features, and various gateways to make it compatible with systems such as OpenStack Swift and Amazon S3.

From Wikipedia: Ceph (software):

This article or section is out of date.

The official documentation states "the manual procedure is primarily for exemplary purposes for those developing deployment scripts with Chef, Juju, Puppet, etc.".

Install it with the package cephAUR.

Install cephAUR on all nodes that will be in the cluster.

Install and run a time synchronisation client on the node. See Time synchronization for details.

Before a storage cluster can operate, the monitors for that cluster must be bootstrapped with several identifiers and keyrings.

The upstream Ceph documentation is well-written and kept updated with the latest releases.

To boostrap a storage cluster, follow the steps documented in the official manual deployment guide.

Since your system most likely uses systemd, you can enable a monitor as a systemd unit.

As an example, for a monitor named node1 start and enable ceph-mon@node1.service.

**Examples:**

Example 1 (unknown):
```unknown
ceph-mon@node1.service
```

---

## umask

**URL:** https://wiki.archlinux.org/title/Umask

**Contents:**
- Meaning of the mode mask
- Display the current mask value
- Set the mask value
  - Set umask value for KDE / Plasma
  - Site-specific
- See also

The umask utility is used to control the file-creation mode mask, which determines the initial value of file permission bits for newly created files. The behaviour of this utility is standardized by POSIX and described in the POSIX Programmer's Manual. Because umask affects the current shell execution environment, it is usually implemented as built-in command of a shell.

The mode mask contains the permission bits that should not be set on a newly created file, hence it is the logical complement of the permission bits set on a newly created file. If some bit in the mask is set to 1, the corresponding permission for the newly created file will be disabled. Hence the mask acts as a filter to strip away permission bits and helps with setting default access to files.

The resulting value for permission bits to be set on a newly created file is calculated using bitwise material nonimplication (also known as abjunction), which can be expressed in logical notation:

That is, the resulting permissions R are the result of bitwise conjunction of default permissions D and the bitwise negation of file-creation mode mask M.

For example, let us assume that the file-creation mode mask is 027. Here the bitwise representation of each digit represents:

With the information provided by the table below this means that for a newly created file, for example owned by User1 user and Group1 group, User1 has all the possible permissions (octal value 7) for the newly created file, other users of the Group1 group do not have write permissions (octal value 5), and any other user does not have any permissions (octal value 0) to the newly created file. So with the 027 mask taken for this example, files will be created with 750 permissions.

To display the current mask, simply invoke umask without specifying any arguments. The default output style depends on implementation, but it is usually octal:

When the -S option, standardized by POSIX, is used, the mask will be displayed using symbolic notation. However, the symbolic notation value will always be the logical complement of the octal value, i.e. the permission bits to be set on the newly created file:

This article or section is out of date.

You can set the umask value through the umask command. The string specifying the mode mask follows the same syntactic rules as the mode argument of chmod (see the POSIX Programmer's Manual for details).

System-wide umask value can be set in /etc/profile (e.g. /etc/profile.d/umask.sh) or in the default shell configuration files (e.g. /etc/bash.bashrc). Most Linux distributions, including Arch, set a umask default value of 022 at /etc/login.defs. One can also set umask with pam_umask.so but it may be overridden by /etc/profile or similar.

If you need to set a different value, you can either directly edit such file, thus affecting all users, or call umask from your shell's user configuration file, e.g. ~/.bashrc to only change your umask, however these changes will only take effect after the next login. To change your umask during your current session only, simply run umask and type your desired value. For example, running umask 077 will give you read and write permissions for new files, and read, write and execute permissions for new folders.

As mentioned by pam_umask(8)  DESCRIPTION, umask=value can also be used in the /etc/passwd section of the Users and groups#User database. See the discussion about setting UMASK in GECOS field

Setting the umask value via /etc/profile does no longer work for KDE / Plasma sessions because these are started as systemd user units.

The umask value can be set via pam_umask.so or a systemd drop-in file:

Using pam_umask.so allows to set the system-wide umask value for both, text console and graphical KDE sessions in one single place. Any changes in /etc/profile or systemd configuration can be omitted. Therefore, pam_umask.so needs to be enabled in a configuration file that is included by both, /etc/pam.d/login and /etc/pam.d/systemd-user.

Add the following line to /etc/pam.d/system-login:

Edit /etc/login.defs and update the UMASK value. For example:

**Examples:**

Example 1 (unknown):
```unknown
R: (D & (~M))
```

Example 2 (unknown):
```unknown
u=rwx,g=rx,o=
```

Example 3 (unknown):
```unknown
/etc/profile
```

Example 4 (unknown):
```unknown
/etc/profile
```

---

## Thunderbird

**URL:** https://wiki.archlinux.org/title/Thunderbird

**Contents:**
- Installation
- Securing
- Extensions
  - OpenPGP: signing and encryption
- Tips and tricks
  - Config editor
  - Set the default browser
  - Plain text mode and font uniformity
  - Migrate profile to another system
  - Export and import

Thunderbird is an open source email, news, and chat client previously developed by the Mozilla Foundation.

Install the thunderbird package, with a language pack if required.

Other versions include:

A version overview, both past and future, can be read on MozillaWiki:Releases.

From Thunderbird 78.2.1 onwards, this functionality is integrated into Thunderbird. This was previously provided by the Enigmail add-on, which is not compatible with Thunderbird 78+. To migrate keys from Enigmail to Thunderbird, as well as learn what is currently supported, see Thunderbird OpenPGP FAQ. Before migration, make sure that a strong passphrase is used for the master password. Otherwise the private key will not be properly protected.

Thunderbird can be extensively configured by clicking the Thunderbird Menu > Settings > General and looking for the Config Editor button at the bottom of the page. Alternatively, if the menu bar is toggled the Config Editor button can be found by clicking Edit > Settings > General

Thunderbird uses the default browser as defined by the XDG MIME Applications. This is commonly modified by desktop environments (for example GNOME's Control Center: Details > Default Applications > Web).

This can be overridden with network.protocol-handler.warn-external in the #Config editor

If the following is all set to false (default), set them to true and Thunderbird will ask you which application to use when you click on a link (remember to also check "Remember my choice for .. links").

Plain Text mode lets you view all your emails without HTML rendering and is available in View > Message Body As. This defaults to the Monospace font but the size is still inherited from original system fontconfig settings. The following example will overwrite this with Ubuntu Mono of 10 pixels (available in: ttf-ubuntu-font-family).

Remember to run fc-cache -fv to update system font cache. See Font configuration for more information.

Before you start with Importing or Exporting tasks, backup your complete ~/.thunderbird profile:

With migration you just copy your current Thunderbird profile to another PC or a new Thunderbird installation:

Before you start with Importing or Exporting tasks, backup your complete ~/.thunderbird profile:

If your accounts are broken or you want to join two different Thunderbird installations, you can install ImportExportTools NG add-on for both Thunderbird installations and following this just export and import all your data to the new installation.

Thunderbird (up to at least 31.4.0-1) sorts mail by date with the oldest on top without any threading. While this can be changed per folder, it is easier to set a sane default instead as described here.

Set these preferences in the #Config editor:

The default message store format is mbox. To enable the use of Maildir, see MozillaWiki:Thunderbird/Maildir. You basically have to set the following preference in the #Config editor:

Some limitations up to at least 31.4.0-1: only the "tmp" and "cur" directories are supported. The "new" directory is completely ignored. The read state of mails are stored in a separate ".msf" file, so initially all local mail using Maildir will be marked as unread even when located in the "cur" directory. It is also possible to change this setting in the regular user interface now: Go to Menu > Settings > General > Indexing > Message Store Type for new accounts and choose File per message (maildir).

Install hunspell and a hunspell language dictionary and restart Thunderbird.

See the Firefox article for how to set the default spell checking language.

Make sure that mail.biff.use_system_alert in the #Config editor is set to "true" (default). This option means that extensions (such as Gnome Integration) are not needed for these newer versions of Thunderbird.

You might also need to install a notification server.

Thunderbird can be configured to play sounds when new emails are received and when calendar reminders are dispatched. This requires libcanberra.

Thunderbird should conform to GTK#Themes as defined on your system. However, two tweaks are desirable for full consistency. These are most beneficial for dark themes.

Further customization can be attained by creating and editing a userchrome.css. See Firefox/Tweaks#General user interface CSS settings and Mozillazine's userchrome page.

Thunderbird unfortunately lacks an easy way to disable single-key bindings, such that pressing e.g. "a" on the keyboard will not archive a message. The tbkeys-lite extension provides a means of editing and deleting such bindings and is available for Thunderbird 68.0 onwards.

Starting with v78.1, Thunderbird now ships with integrated OpenPGP support previously provided by Add-Ons like Enigmail. It will offer you to migrate your existing Enigmail keys into Thunderbird upon the first start after the update. If you do not want to store your private keys inside Thunderbird, it is possible to use Thunderbird with an external GnuPG installation in order to keep your keys safe or use a Smartcard.

To view any OpenPGP keys stored inside Thunderbird:

To enable external GnuPG support in Thunderbird:

Refer to MozillaWiki:Thunderbird:OpenPGP:Smartcards for further instructions and specialized configurations.

Starting with version 128, Thunderbird defaults to Wayland instead of XWayland and does not require any configuration. Older versions of Thunderbird support opting into Wayland mode via an environment variable.

To route the connection to the server through the Tor network the proxy settings must be set accordingly.

An LDAP clash (Bugzilla#292127) arises on systems configured to use it to fetch user information. A possible workaround consists of renaming the conflicting bundled LDAP library.

If you want to reinstall a previously deleted account with the same account data, you might get a popup with "Incoming server already exists". See bug Bugzilla#1121151 for details. Unfortunately, if you get this error you can now only clean reinstall Thunderbird:

If Thunderbird is configured to show an alert when a new message arrives, or at launch, the lack of a notification daemon may freeze the interface (white screen) for many seconds. You can solve this issue by disabling alerts or installing a notification server.

Thunderbird should use the LC_TIME environment variable for localization, but it might not do so in all contexts. Some problems can be mitigated by setting Menu > Preferences > Preferences > Advanced > Date and Time Formatting to Regional settings locale, a setting which was introduced in Thunderbird 56.

With version 60, Gecko started using the CLDR project for localization, including datetime formatting, which uses different settings than most other software based purely on LC_TIME. There is a bug report for this issue that includes workarounds with varying effects. To achieve ISO-8601-formatted dates in Thunderbird and a week beginning on Monday, use LC_TIME=lt_LT thunderbird.

Starting in Thunderbird version 91, one can set a number of preferences to make Thunderbird compliant with ISO-8601. Most programs can be set to ISO-8601 by setting your region locale to en_DK, but by default Thunderbird ignores regional locale preferences. See [3] for details.

Sometimes Thunderbird fails to log in to G Suite with Authentication failure while connecting to server imap.gmail.com error. It can be fixed with setting general.useragent.compatMode.firefox setting to true in #Config editor and then passing authentication stage again.

Apparently by default, SMTP authentication is disabled for Outlook 365 accounts. Use the Microsoft 365 admin center to enable it. Refer to: Enable SMTP AUTH for specific mailboxes.

Version 102.7.0 of Thunderbird included changes to the OAUTH2 implementation that affected access to Outlook 365 accounts (see [4] and [5]). Affected users should directly upgrade to 102.7.1 or newer.

If, after being redirected to your institution's login page, inserting credentials and pressing the login button, you are being redirected to the same login page, try to:

**Examples:**

Example 1 (unknown):
```unknown
mail.smtpserver.default.hello_argument
```

Example 2 (unknown):
```unknown
mailnews.headers.useMinimalUserAgent
```

Example 3 (unknown):
```unknown
general.useragent.override
```

Example 4 (unknown):
```unknown
javascript.enabled
```

---

## dwm

**URL:** https://wiki.archlinux.org/title/Dwm

**Contents:**
- Installation
  - Configuration
- Starting
- Usage
- Tips and tricks
  - Statusbar configuration
    - Conky statusbar
  - Restart dwm
  - Bind the right Alt key to Mod4
  - Use both Alt keys as Meta in DWM

dwm is a dynamic window manager for Xorg. It manages windows in tiled, stacked, and full-screen layouts, as well as many others with the help of optional patches. Layouts can be applied dynamically, optimizing the environment for the application in use and the task being performed. dwm is extremely lightweight and fast, written in C and with a stated design goal of remaining under 2000 source lines of code. It provides multihead support for xrandr and Xinerama.

dwm can be installed with the dwmAUR package. Make any required configuration changes before building and installing, see makepkg.

dwm is configured at compile-time by editing some of its source files, specifically config.h. For detailed information on these settings, see the included, well-commented config.def.h as well as the customisation section on the dwm website.

The official website has a number of patches that can add extra functionality to dwm. These patches primarily make changes to the dwm.c file but also make changes to the config.h file where appropriate. For information on applying patches, see the Patching packages article.

Select Dwm from the menu in a display manager of choice. Alternatively, to start dwm with startx append exec dwm to ~/.xinitrc and prepend other programs to execute them as well, for example:

See the dwm tutorial for information on basic dwm usage.

For more examples of status bars, see [1].

dwm reads the name of the root window and redirects it to the statusbar. The root window is the window within which all other windows are drawn and arranged by the window manager. Like any other window, the root window has a title/name, but it is usually undefined because the root window always runs in the background.

The information that you want dwm to show in the statusbar should be defined with xsetroot -name "" command in ~/.xinitrc or ~/.xprofile (if you are using a display manager). For example:

Dynamically updated information should be put in a loop which is forked to background - see the example below:

In this case the date is shown in RFC:3339 format and PCManFM is launched at startup.

Conky can be printed to the statusbar with xsetroot -name:

If you do not want to spawn too many PIDs by 'xsetroot' command, you can compile this C program:

Save this code to file dwm-setstatus.c, compile:

move 'dwm-setstatus' within your $PATH (/usr/local/bin, for example)

To do this, conky needs to be told to output text to the console only. The following is a sample conkyrc for a dual core CPU, displaying several usage statistics:

For icons and color options, see dzen.

To restart dwm without logging out or closing applications, change or add a startup script so that it loads dwm in a while loop, for example:

dwm can now be restarted without destroying other X windows by pressing the usual Mod-Shift-Q combination.

It is a good idea to place the above startup script into a separate file, ~/bin/startdwm for instance, and execute it through ~/.xinitrc. Consider running the script with exec to avoid security implications with remaining logged in after the X server is terminated; see Xinit#Autostart X at login for more information. From this point on, when you wish to end the X session, simply execute pkill dwm, or bind it to a convenient keybind. Alternatively, you could setup your dwm session script so that it relaunches dwm only if the binary changes. This could be useful in the case where you change a setting or update the dwm code base.

When using Mod4 (the Super/Windows Key) as the MODKEY, it may be equally convenient to have the right Alt key (Alt_R) act as Mod4. This will allow you to perform otherwise awkward keystrokes one-handed, such as zooming with Alt_R+Enter.

First, find out which keycode is assigned to Alt_R:

Then simply add the following to the startup script (e.g. ~/.xinitrc), changing the keycode 113 if necessary to the result gathered by the previous xmodmap command:

Reassign Alt_R to Super_L:

Make sure X keeps it out of the "mod1" group:

After doing so, any functions that are triggered by the Super_L key press will also be triggered by an Alt_R key press.

Use xmodmap to assign Alt_L as a secondary meta key in DWM (provided already using Mod1Mask (Alt_R))

By default, dwm's bar adds 2px around the size of the font. To change this, modify the following line:

To disable focus follows mouse behaviour, comment out the following line in definition of struct handler:

Note that this change can cause some difficulties; the first click on an inactive window will only bring the focus to it. To interact with window contents (buttons, fields etc), you need to click again. Also, if you have several monitors, you may notice that the keyboard focus does not switch to another monitor activated by clicking.

For some windows, such as preferences dialogs, it does not make sense for these windows to be tiled - they should be free-floating instead. For example, to make Firefox's preferences dialog float, add the following to your rules array in config.h:

To get the properties of other Windows, the program xprop can be used.

Further information can be found on the dwm site.

Tilda works best when added to all tags, and configured to be floating. To do so, add the following to your rules array in config.h:

Launch tilda with -C option:

Now you can configure Tilda, the following options are provided as a recommendation:

It is important you enable the pulldown-animation, otherwise Tilda will keep jumping down each time you unhide it, must be a dwm issue.

Install the scrot package. Next create two scripts:

for making screenshots and

for making screenshots with a selection box. Give them executable permissions. In config.h add the following:

This maps taking screenshots to the print key and taking screenshots with a selection box to the shift + print keys.

Add to the top of config.h,

to use multimedia keys. Now we can map common tasks to these keys.

Install the pipewire package. Now in config.h we may add commands for mute and volume increase/decrease.

Install the brightnessctl package. Now in config.h we may add commands for dimming and brightening the screen.

A patch is available. It runs ~/.dwm/autostart_blocking.sh and ~/.dwm/autostart.sh & before entering the handler loop. One or both of these files can be omitted.

See Java#Gray window, applications not resizing with WM, menus immediately closing.

If there are empty gaps of desktop space outside terminal windows, it is likely due to the terminal's font size. Either adjust the size until finding the ideal scale that closes the gap, or toggle resizehints to 0 in config.h.

This will cause dwm to ignore resize requests from all client windows, not just terminals. The downside to this workaround is that some terminals may suffer redraw anomalies, such as ghost lines and premature line wraps, among others.

Alternatively, if you use the st terminal emulator, you can apply the anysize patch and recompile st.

dwm status bar does not have letter-shaping support. However, you can use tools like| fribidi for applying letter-shaping using Unicode Arabic Presentation Form-B. E.g instead of:

**Examples:**

Example 1 (unknown):
```unknown
config.def.h
```

Example 2 (unknown):
```unknown
redshift -O3500; xset r rate 300 50; exec dwm
```

Example 3 (unknown):
```unknown
xsetroot -name ""
```

Example 4 (unknown):
```unknown
~/.xprofile
```

---

## zram

**URL:** https://wiki.archlinux.org/title/Zram

**Contents:**
- Usage as swap
  - Manually
  - Using a udev rule
  - Using zram-generator
  - Using zramswap
- Tips and tricks
  - Checking zram statistics
  - Multiple zram devices
  - Optimizing swap on zram
  - Enabling a backing device for a zram block

zram, formerly called compcache, is a Linux kernel module for creating a compressed block device in RAM, i.e. a RAM disk with on-the-fly disk compression. The block device created with zram can then be used for swap or as a general-purpose RAM disk. The two most common uses for zram are for the storage of temporary files (/tmp) and as a swap device. Initially, zram had only the latter function, hence the original name "compcache" ("compressed cache").

Initially the created zram block device does not reserve or use any RAM. Only as files need or want to be swapped out, they will be compressed and moved into the zram block device. The zram block device will then dynamically grow or shrink as required.

Even when assuming that zstd only achieves a conservative 1:2 compression ratio (real world data shows a common ratio of 1:3), zram will offer the advantage of being able to store more content in RAM than without memory compression.

A simple size to start with is half of the total system memory.

To set up one zstd compressed zram device with half the system memory capacity and a higher-than-normal priority (only for the current session):

To disable it again, either reboot or run:

A detailed explanation of all steps, options and potential problems is provided in the official documentation of the zram module.

For a permanent solution, use a method from one of the following sections.

The example below describes how to set up swap on zram automatically at boot with a single udev rule and an fstab entry. No additional packages are needed to make this work.

Explicitly load the module at boot:

Create the following udev rule adjusting the disksize attribute as necessary:

Add /dev/zram to your fstab with a higher than default priority and the x-systemd.makefs option:

zram-generator provides systemd-zram-setup@zramN.service units to automatically initialize zram devices without users needing to enable/start the template or its instances. See zram-generator(8).

To use it, install zram-generator, and create /etc/systemd/zram-generator.conf with the following:

zram-size is the size (in MiB) of the zram device, you can use ram to represent the total memory.

compression-algorithm specifies the algorithm used to compress in zram device. cat /sys/block/zram0/comp_algorithm gives the available compression algorithm (as well as the current one included in brackets). See zram-generator.conf(5).

Then run daemon-reload, start your configured systemd-zram-setup@zramN.service instance (N matching the numerical instance-ID, in the example it is systemd-zram-setup@zram0.service).

You can check the swap status of your configured /dev/zramN device(s) by reading the unit status of your systemd-zram-setup@zramN.service instance(s), by using zramctl(8), or by using swapon(8).

zramswapAUR provides an automated script for setting up a swap with a higher priority and a default size of 20% of the RAM size of your system. To do this automatically on every boot, enable zramswap.service.

Use zramctl(8). Example:

By default, loading the zram module creates a single /dev/zram0 device.

If you need more than one /dev/zram device, specify the amount using the num_devices kernel module parameter or add them as needed afterwards.

Since zram behaves differently than disk swap, we can configure the system's swap to take full potential of the zram advantages:

Explanation of the configuration:

These values are what Pop!_OS uses. That Pop!_OS GitHub pull request also links to some testing done by users on r/Fedora, which determined that vm.page-cluster = 0 is ideal. They also found a high swappiness value to be ideal, which matches what is suggested by the kernel docs:

On a system with a hard drive, random I/O against the in-memory device would be orders of magnitude faster than I/O against the filesystem, so swappiness should be ~200. Even on a system with a fast SSD, a high swappiness value may be ideal.

zram can be configured to push incompressible pages to a specified block device:

To add a backing device manually:

To add a backing device to your zram block device using zram-generator, update /etc/systemd/zram-generator.conf with the following under your [zramX] device you want the backing device added to:

Incompressible pages can then be pushed to the block device by executing:

zram can also be used as a generic RAM-backed block device, e.g. a /dev/ram with less physical memory usage, but slightly lower performance. However there are some caveats:

The obvious way around this is to stack a loop device on-top the zram, using losetup, specifying the desired block size using the -b option and the -P option to process partition tables and automatic creation of the partition loop devices.

Copy the disk image to the new /dev/zramx, then create a loop device. If the disk image has a partition table, the block size of the loop device must match the block size used by the partition table, which is typically 512 or 4096 bytes.

**Examples:**

Example 1 (unknown):
```unknown
# modprobe zram
# zramctl /dev/zram0 --algorithm zstd --size "$(($(grep -Po 'MemTotal:\s*\K\d+' /proc/meminfo)/2))KiB"
# mkswap -U clear /dev/zram0
# swapon --discard --priority 100 /dev/zram0
```

Example 2 (unknown):
```unknown
# swapoff /dev/zram0
# modprobe -r zram
```

Example 3 (unknown):
```unknown
/etc/modules-load.d/zram.conf
```

Example 4 (unknown):
```unknown
/etc/udev/rules.d/99-zram.rules
```

---

## MPRIS

**URL:** https://wiki.archlinux.org/title/MPRIS

**Contents:**
- Supported clients
- Control utilities
  - Playerctl
  - mpris-player-control
  - D-Bus
- Bluetooth

MPRIS (Media Player Remote Interfacing Specification) is a standard D-Bus interface which aims to provide a common programmatic API for controlling media players.

It provides a mechanism for discovery, querying and basic playback control of compliant media players, as well as a track list interface which is used to add context to the active media item.

This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.

This article or section needs expansion.

The playerctl utility provides a command line tool to send commands to MPRIS clients. The most common commands are play-pause, next, previous and stop:

playerctl will send the command to the first player it finds. To select a player manually, use the --player option, e.g. --player=vlc. For better automation playerctl comes with a daemon that keeps track of media player activity and directs commands to the one with most recent activity. You can spawn it into the background with:

In order to start playerctld when you log in, you may create the following systemd/User service:

You should then do a daemon-reload before enabling the service with the --user flag.

Additionally, playerctld has the ability to change an "active" player, which can be useful when you have multiple simultaneous media streams:

To switch to the next player, use:

To switch to the previous player, use:

The mpris_player_control is a shell script which integrates dbus-send and pactl to control MPRIS clients. It supports the Play, Pause, PlayPause and Stop actions and sink volume control (mute/unmute/up/down) for Spotify.

Run mpris_player_control -h to show basic script usage.

An alternative to the above is to manually use D-Bus, which should be available by default as it is a dependency of systemd.

For example, the following commands can be used to control Spotify with the supported Methods:

Similarly using busctl(1):

Media control from Bluetooth headsets and similar devices may be forwarded to MPRIS.

Install the bluez-utils package and run mpris-proxy. In order to start up mpris-proxy in the background and/or when your system starts, start/enable mpris-proxy.service with the --user flag.

**Examples:**

Example 1 (unknown):
```unknown
XF86AudioPlay
```

Example 2 (unknown):
```unknown
XF86AudioPause
```

Example 3 (unknown):
```unknown
XF86AudioStop
```

Example 4 (unknown):
```unknown
XF86AudioPrev
```

---

## ASUS Linux

**URL:** https://wiki.archlinux.org/title/ASUS_Linux

**Contents:**
- Software
  - asusctl
  - supergfxctl
  - rog-control-center
  - Custom kernel
- See also

ASUS Linux is a suite of tools designed to improve the performance and functionality of ASUS laptops when running Linux. It comes in 2 main parts, asusctl and supergfxctl, the former interacts with the asus-wmi and the asus-armoury kernel modules to control BIOS level features and the latter is used to control the dedicated GPU in dual GPU systems.

The project is maintained by Luke Jones and is hosted on GitLab.

ASUS Linux provides many packages, please see subsections below.

There is a custom repository which contains prebuilt binaries available: Unofficial user repositories#g14, provided by Luke Jones.

This repository is the officially recommended way of installing the asus-linux utilities by the asus-linux developers, it was created and is being maintained by them.

See: https://asus-linux.org/guides/arch-guide/

asusctlAUR is a CLI utility for ASUS ROG & TUF laptop, to name some of the important features it gives users control over:

For usage instructions see asusctl.

supergfxctlAUR is a CLI utility for managing GPU switching functionality on ASUS hybrid laptops, particularly dedicated GPU MUX control.

For usage instructions see supergfxctl.

rog-control-centerAUR is a GUI frontend for asusctl and supergfxctl.

The ASUS Linux project maintains a set of kernel patches specific to ASUS mobile devices and packages them into a kernel. Typically using this kernel is not required however in some edge cases (usually for very recent laptops) all your laptop features might not function without it.

Install linux-g14AUR and linux-g14-headersAUR.

**Examples:**

Example 1 (unknown):
```unknown
asus-armoury
```

---

## Gaming

**URL:** https://wiki.archlinux.org/title/Running_program_in_separate_X_display

**Contents:**
- Game technicality
- Common game dependencies
  - Mandatory (for high coverage)
  - Optional (but still common)
  - Rare (less common)
- Machine requirements
  - Drivers
  - Dependency for the machine & substitutes
- Game environments
- Game compatibility

Linux has long been considered an "unofficial" gaming platform; the support and target audience provided to it is not a primary priority for most gaming organizations. Changes to this situation have accelerated, starting from 2021 onward, as big players like Valve, the CodeWeavers group and the community have made tremendous improvements to the ecosystem, allowing Linux to truly become a viable platform for gaming. Further, more and more indie development teams strive to use cross-platform rendering engines in order to have their game able to compile and run on Linux.

When it comes to gaming, the majority of user's thoughts are often directed towards popular AAA games which are usually written exclusively for the Microsoft Windows platform. This is understandable, however, it is not the only and sole availability. Please refer to #Game environments and #Getting games further down the page where you can find software to run games from other platforms.

If you however are fixated on getting games written for Microsoft Windows to work on Linux, then a different mindset, tools and approach is required; understanding internals and providing functional substitution. Please read #Game technicality below.

There are ultimately three complications that arise from attempting to play AAA games made for Windows on Linux. They are:

Graphics SDK forward graphical calls to the underlying graphics driver which then proceeds to talk to the GPU hardware.

A huge amount of games use DirectX as their main driving SDK. Linux, natively supports only OpenGL and Vulkan. Linux by itself does not support DirectX or any of the aforementioned technologies (Visual C++, MFC, .NET).

Instead, several opensource equivalents have been written which attempt to provide identical functionality, ultimately achieving the same result from a graphics point of view. These equivalents have their "own" written substitutes which attempt to "re-invent" what the original SDK calls would possibly achieve from a black box point of view. Popular ones include:

For example, a call to load, transform and shade vertices on DirectX may be re-written from scratch in a new .dll/.so owned by Wine, providing their own "hypothetical" belief on what the function may be doing underneath, and forward it instead to an OpenGL alternative, effectively trying to achieve similar results. Since these calls are direct equivalents and treated "as if" DirectX was running, performance is not impacted. (with the exception of the starting overhead to interop with these)

These tools are often brought in the distribution together on the system at the same time. A prefix (Wine's terminology for a directory mimicking a Windows sandbox) is created and configured. Dependencies are installed inside the prefix (the "sandbox" still needs the game's redistributables), often with winetricks, followed by an attempt to run the game "as if" it was executed from Windows.

This, nowadays, fortunately works for most games (aside from anti-cheat protected ones, which require a kernel driver that Wine/Proton does not yet have). If a game does not work, it is usually as a result of incompatible packages, missing dependencies or unimplemented functionality by Wine/Proton.

Lutris is a piece of software that provides runners and sandboxes that handle dependencies for you when you install games, if the above process is found tedious and/or complicated.

In order to gain a more in-depth understanding of what you will intend to do if you decide to go the Wine/Proton route, it is worthwhile to cover the common dependencies that games require in order to execute. Architecture also needs to be considered in mind, whether x86 or x64, preferably both.

A prefix would need to have the following populated into it in order to run most Windows games.

This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.

It is not enough to just populate a prefix with the dependencies the game will need. The kernel itself has to have the substitution it will provide to the calls the game will make. As already mentioned, drivers and alternatives are available.

Wine/Proton are not the only approaches to play games. Different environments exist to play games in Linux, and have just as many (or more) games than on Windows:

Having the vm.max_map_count set to a low value can affect the stability and performance of some games. It can therefore be desirable to increase the size permanently by creating the following sysctl config file.

Arch Linux uses the value 1048576 by default [1], which Fedora considers a safe value [2]. This default value is likely to be sufficient for current games, as increasing the value was mostly important when Arch used the kernel default of 65530 [3]. A value of 2147483642 (MAX_INT - 5) is the default in SteamOS.

Apply the changes without reboot by running:

Just because games are available for Linux does not mean that they are native; they might be pre-packaged with Wine or DOSBox.

For list of games packaged for Arch in official repositories or the AUR see List of games.

For Wine wrappers, see Wine#Third-party applications.

Game launchers are important tools for managing and running games on various platforms. Below is a comparison of different game launchers available for Linux, particularly focusing on their features and compatibility.

Certain games or game types may need special configuration to run or to run as expected. For the most part, games will work right out of the box in Arch Linux with possibly better performance than on other distributions due to compile time optimizations. However, some special setups may require a bit of configuration or scripting to make games run as smoothly as desired.

Running a multi-screen setup may lead to problems with full-screen games. In such a case, running a second X server is one possible solution. For NVIDIA users, a solution may be found in NVIDIA#Gaming using TwinView.

Many games grab the keyboard, noticeably preventing you from switching windows (also known as alt-tabbing).

Some SDL games (e.g. Guacamelee) let you disable grabbing by pressing Ctrl-g.

In some cases like those mentioned above, it may be necessary or desired to run a second X server. Running a second X server has multiple advantages such as better performance, the ability to "tab" out of your game by using Ctrl+Alt+F7/Ctrl+Alt+F8, no crashing your primary X session (which may have open work on) in case a game conflicts with the graphics driver. The new X server will be akin a remote access login for the ALSA, so your user need to be part of the audio group to be able to hear any sound.

To start a second X server (using the free first person shooter game Xonotic as an example) you can simply do:

This can further be spiced up by using a separate X configuration file:

A good reason to provide an alternative xorg.conf here may be that your primary configuration makes use of NVIDIA's Twinview which would render your 3D games like Xonotic in the middle of your multiscreen setup, spanned across all screens. This is undesirable, thus starting a second X with an alternative configuration where the second screen is disabled is advised. Please note, that the X configuration file location is relative to the /etc/X11 directory.

A game starting script making use of Openbox for your home directory or /usr/local/bin may look like this:

After making it executable you would be able to do:

For games that require exceptional amount of mouse skill (like shooters), adjusting the mouse polling rate can help improve accuracy on high-end mice. The improvement can be by a few milliseconds when increasing from typical polling rates like 250hz-500hz to 1000hz, but increasing past 1000hz will get at most slightly less than a millisecond of improvement. In general, latency is influenced by the quality of the mouse's microcontroller, firmware, and sensor, and the mice with the lowest latency currently available don't necessarily improve past 1000hz.

For games using OpenAL, if you use headphones you may get much better positional audio using OpenAL's HRTF filters. To enable, create:

Alternatively, install openal-hrtfAUR from the AUR, and edit the options in /etc/openal/alsoftrc.conf.

For Source games, the ingame setting `dsp_slow_cpu` must be set to `1` to enable HRTF, otherwise the game will enable its own processing instead. You will also either need to set up Steam to use native runtime, or link its copy of openal.so to your own local copy. For completeness, also use the following options:

If you are using PulseAudio, you may wish to tweak some default settings to make sure it is running optimally.

PulseAudio is built to be run with realtime priority, being an audio daemon. However, because of security risks of it locking up the system, it is scheduled as a regular thread by default. To adjust this, first make sure you are in the audio group. Then, uncomment and edit the following lines in /etc/pulse/daemon.conf:

and restart PulseAudio.

PulseAudio on Arch uses speex-float-1 by default to remix channels, which is considered a 'medium-low' quality remixing. If your system can handle the extra load, you may benefit from setting it to one of the following instead:

Matching the buffers can reduce stuttering and increase performance marginally. See here for more details.

Cloud gaming has gained a lot of popularity in the last few years, because of low client-side hardware requirements. The only important thing is stable internet connection (over the Ethernet cable or 5 GHz Wi-Fi recommended) with a minimum speed of 510 Mbit/s (depending on the video quality and framerate).

See Gamepad#Gamepad over network for using a gamepad over a network with services that do not normally support this.

See also main article: Improving performance. For Wine programs, see Wine#Performance. For a good gaming experience low latency, a consistent response time (no jitter) and enough throughput (frames per second) are needed. If you have multiple sources with a little jitter they are likely to overlap sometimes and produce noticeable stutter. Therefore most of the time it is preferable to decrease the throughput a little to gain more response time consistency. On the other hand if you get low average FPS or if the gaming application itself produces spikes in CPU demand it is better to increase raw throughput.

User space programs and especially games do many calls to clock_gettime(2) to get the current time for calculating game physics, fps and so on. The time usage can be seen by running

and looking at the overhead of read_hpet (or acpi_pm_read).

If you are not dependent on a very precise timer you can switch from hpet (high precision event timer) or acpi_pm (ACPI Power Management Timer) to the faster TSC (time stamp counter) timer. Add the kernel parameters

to make TSC available and enable it. After that reboot and confirm the clocksource by running

You can see all currently available timers by running

and change between them by echoing one into current_clocksource. On a Zen 3 system benchmarking with [5] shows a ~50 times higher throughput of tsc compared to hpet or acpi_pm.

You can install the realtime kernel to get some advantages by default (see realtime kernel article) but it costs CPU throughput and can delay interrupt handling. Additionally the realtime kernel is not compatible with nvidia-open-dkms and does not change the scheduler for SCHED_NORMAL (also named SCHED_OTHER) processes, which is the default process scheduling type. The following kernel parameter changes improve the response time consistency for the realtime kernel as well as other kernels such as the default linux kernel:

Proactive compaction for (Transparent) Hugepage allocation reduces the average but not necessarily the maximum allocation stalls. Disable proactive compaction because it introduces jitter according to kernel documentation (inner workings):

Reduce the watermark boost factor to defragment only one pageblock (2MB on 64-bit x86) in case of memory fragmentation. After a memory fragmentation event this helps to better keep the application data in the last level processor cache.

If you have enough free RAM increase the number of minimum free Kilobytes to avoid stalls on memory allocations: [7][8]. Do not set this below 1024 KB or above 5% of your systems memory. Reserving 1GB:

If you have enough free RAM increase the watermark scale factor to further reduce the likelihood of allocation stalls (explanations [9][10]). Setting watermark distances to 5% of RAM:

Avoid swapping (locking pages that introduces latency and uses disk IO) unless the system has no more free memory:

Make sure zswap is enabled. If there arises the need for swapping pages, then zswap helps reduce swapping related stutter. If you do not have a swap partition, you can use zram instead of zswap.

Enable Multi-Gen Least Recently Used (MGLRU) but reduce the likelihood of lock contention at a minor performance cost [11]:

Disable zone reclaim (locking and moving memory pages that introduces latency spikes):

Disable Transparent Hugepages (THP) at a performance cost. Even if defragmentation is disabled, THPs might introduce latency spikes [12][13][14]. Enable only when the application specifically requests it by using madvise and advise:

Note that if your game uses TCMalloc (e.g., Dota 2 and CS:GO) then it is not recommended to disable THP as it comes with a large performance cost [15].

Reduce the maximum page lock acquisition latency while retaining adequate throughput [16][17][18]:

Tweak the scheduler settings. The following scheduler settings are in conflict with cfs-zen-tweaksAUR so for each setting choose only one provider. By default the linux kernel scheduler is optimized for throughput and not latency. The following hand-made settings change that and are tested with different games to be a noticeable improvement. They might not be optimal for your use case; consider modifying them as necessary [19][20]:

Further, it is recommended to test different schedulers as described in Improving_performance#CPU scheduler. For example, the scx_cosmos scheduler from scx-scheds shows promise for improving response time consistency, as it optimizes task-to-CPU locality (enhancing cache hit consistency between runs), reduces locking contention, and is suitable for general usage scenarios, prioritizing interactive tasks under load.

Usually, the advice for permanently setting kernel parameters is to configure create a sysctl configuration file or change your boot loader options. However, since our change span both procfs (/proc, containing sysctl) and sysfs (/sys), the most convenient way is to use systemd-tmpfiles:

After that reboot and see if the values applied correctly.

A device experiences buffer bloat when a large buffer queues requests, causing new requests to be blocked for a long time. This can occur with both sending and receiving data.

Buffer bloat is solved by having a smart queuing algorithm that gives fair and timely share towards the device. Smart queuing algorithms produce quicker and more consistent response times, but increase CPU overhead. For interactive workloads reduced buffer bloat is desirable.

For storage devices, the BFQ scheduler is the most effective at minimizing buffer bloat and improving interactivity Improving performance#The scheduling algorithms.

You can use the default fq_codel or the improved cake smart queue management algorithms. They can be set via sysctl. Your home internet router can cause buffer bloat, which might be mitigated by reconfiguring it, installing a new router OS, or replacing the router (more info).

Here are some easy ways to test for network buffer bloat.

Change the PCI Express Latencies similar to CachyOS [21]. Reduce the maximum cycles a PCI-E Client can occupy the bus, except for sound cards [22]. Note that these settings are in conflict with Professional audio#Optimizing system configuration.

To enable these settings consistently run them as super user on system boot.

Whether or not disabling SMT improves your gaming experience depends on your hardware and gaming applications.

Advantages of disabling SMT:

If you disable SMT, do so in the BIOS/UEFI to improve single-core performance. (Example of SMT sharing CPU internal resources: [23])

Set the environment variable

for your games, to avoid needing to load program code at run time (see ld.so(8)), leading to a delay the first time a function is called. Do not set this for startplasma-x11 or other programs that link in libraries that do not actually exist on the system anymore and are never called by the program. If this is the case, the program fails on startup trying to link a nonexistent shared object, making this issue easily identifiable. Most games should start fine with this setting enabled. Since packages inside the official repository are built with RELRO [24] by default, only packages from other sources should be affected in terms of first time latency, bugs in linking might still be triggered.

GameMode is daemon and library combo that allows games to request a set of optimisations be temporarily applied to the host OS. This can improve game performance.

Gamescope is a microcompositor from Valve that is used on the Steam Deck. Its goal is to provide an isolated compositor that is tailored towards gaming and supports many gaming-centric features.

See AMDGPU#ACO compiler

Direct Rendering Infrastructure (DRI) Configuration Files apply for all DRI drivers including Mesa and Nouveau. You can change the DRI configuration systemwide in /etc/drirc or per user in $HOME/.drirc. If they do not exist, you have to create them first. Both files use the same syntax; documentation for these options can be found at https://dri.freedesktop.org/wiki/ConfigurationOptions/. To reduce input latency by disabling synchronization to vblank, add the following:

Most games can benefit if given the correct scheduling policies for the kernel to prioritize the task. These policies should ideally be set per-thread by the application itself.

For programs which do not implement scheduling policies on their own, application known as schedtool, and its associated daemon schedtooldAUR can handle many of these tasks automatically.

To edit what programs relieve what policies, simply edit /etc/schedtoold.conf and add the program followed by the schedtool arguments desired.

SCHED_ISO (only implemented in BFS/MuQSSPDS schedulers found in -pf and -ck kernels)  will not only allow the process to use a maximum of 80 percent of the CPU, but will attempt to reduce latency and stuttering wherever possible. Most if not all games will benefit from this:

SCHED_FIFO provides an alternative, that can even work better. You should test to see if your applications run more smoothly with SCHED_FIFO, in which case by all means use it instead. Be warned though, as SCHED_FIFO runs the risk of starving the system! Use this in cases where -I is used below:

Secondly, the nice level sets which tasks are processed first, in ascending order. A nice level of -4 is recommended for most multimedia tasks, including games:

There is some confusion in development as to whether the driver should be multithreading, or the program. Allowing both the driver and program to simultaneously multithread can result in significant performance reductions, such as framerate loss and increased risk of crashes. Examples of this include a number of modern games, and any Wine program which is running with GLSL enabled. To select a single core and allow only the driver to handle this process, simply use the -a 0x# flag, where # is the core number, e.g.:

Some CPUs are hyperthreaded and have only 2 or 4 cores but show up as 4 or 8, and are best accounted for:

which use virtual cores 0101, or 1 and 3.

For most games which require high framerates and low latency, usage of all of these flags seems to work best. Affinity should be checked per-program, however, as most native games can understand the correct usage. For a general case:

As a general rule, any other process which the game requires to operate should be reniced to a level above that of the game itself. Strangely, Wine has a problem known as reverse scheduling, it can often have benefits when the more important processes are set to a higher nice level. Wineserver also seems unconditionally to benefit from SCHED_FIFO, since it rarely consumes the whole CPU and needs higher prioritization when possible.

You might want to set your mouse acceleration to control your mouse more accurately.

If your mouse has more than 3 buttons, you might want to see Mouse buttons.

If you are using a gaming mouse (especially Logitech and Steelseries) you may want to configure your mouse polling rate, DPI, LEDs etc. using piper. See this page for a full list of supported devices by piper. Alternatively solaar for logitech devices.

You can change and manipulate various RGBs with openrgb, for a list of currently supported devices see [25]

**Examples:**

Example 1 (unknown):
```unknown
vm.max_map_count
```

Example 2 (unknown):
```unknown
/etc/sysctl.d/80-gamecompatibility.conf
```

Example 3 (unknown):
```unknown
vm.max_map_count = 2147483642
```

Example 4 (unknown):
```unknown
/usr/lib/sysctl.d/10-arch.conf
```

---

## Mosh

**URL:** https://wiki.archlinux.org/title/Mosh

**Contents:**
- Installation
- Usage

Mosh is an alternative interactive SSH terminal. It has support for roaming and local echo. It also aims to improve responsiveness on intermittent, and high latency connections. To achieve the aim, Mosh uses UDP on the transport layer with AES-128 OCB mode encryption for the session. It employs the OpenSSH dependency for the initial authentication of the session. Hence, Mosh can be used as an add-on for an existing OpenSSH configuration.

Install the mosh package.

The server and client can use different versions of Mosh, but some features are not available in older versions. For example, truecolor support requires that both the server and client use Mosh 1.4.0 or newer.

Mosh sessions by default use the first available UDP port in the 60001-60999 range, so it should be accessible in the server.

To send ssh options for connecting:

You can make options permanent by using the usual OpenSSH Client Configuration.

**Examples:**

Example 1 (unknown):
```unknown
$ mosh user@server-address
```

Example 2 (unknown):
```unknown
$ mosh --ssh="ssh -p 2222" user@server-address
```

Example 3 (unknown):
```unknown
--predict=experimental
```

---

## Blackbox

**URL:** https://wiki.archlinux.org/title/Blackbox

**Contents:**
- Installation
- Starting
- Utilities
- See also

Blackbox is a lightweight stacking window manager. It was last updated November 3, 2005 with no plans for future releases, however its design has inspired many forks and spiritual successors.

An updated version of the original CVS source plus collected patches is maintained at BlackBox GitHub and is used by the Arch package.

Install the blackbox package.

Run blackbox with xinit.

There are several additional packages to enhance the functionality of Blackbox (though they may also work well with other minimalist window managers):

---

## Synchronization and backup programs

**URL:** https://wiki.archlinux.org/title/Synchronization_and_backup_programs

**Contents:**
- Important considerations
- Data synchronization
  - Legend
  - Table
- Incremental backups
  - Single machine
    - Chunk-based increments
    - File-based increments
  - Network oriented
- Version control systems

This page lists and compares applications that synchronize data between two or more locations, and those that build on top of such functionality to make incremental copies of important data for backup purposes. Because of their relationship, the two groups share several traits that justify describing them in the same article.

In order to choose the best program for one's own needs, the following aspects should be considered:

These applications simply keep directories synchronized between multiple locations/machines, in a "mirror" fashion. Nonetheless, most of them still allow storing and reverting to old revisions of modified or deleted files.

This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.

This article or section needs expansion.

Applications that can do incremental backups remember and take into account what data has been backed up during the last run (so-called "diffs") and eliminate the need to have duplicates of unchanged data. Restoring the data to a certain point in time would require locating the last full backup and all the incremental backups from then to the moment when it is supposed to be restored. This sort of backup is useful for those who do it very often.

These applications are aimed at backing up data from the machine they are installed on, although the backup destination can be located on an external machine or storage media.

This article or section needs expansion.

If a file is modified, these applications store only its changed parts at the next snapshot. Compared to #File-based increments applications, these are more space-efficient, especially when large files receive small modifications; on the other hand, the archived snapshots have to be opened with the backup application that created them, since the files have to be reconstructed from the stored binary diffs.

This article or section needs expansion.

If a file is modified, these applications store its new version entirely at the next snapshot. Compared to #Chunk-based increments applications, these are less space-efficient, especially when large files receive small modifications; on the other hand, often the archived snapshots can be opened without the need to have the backup application installed.

This article or section needs expansion.

These applications have been designed to centralize the backup of several machines connected to a network, through a server-client model. In general they are more complicated to deploy, compared to #Single machine solutions.

While version control systems are mostly used for source code, they can track any files in a directory.

See List of applications/Utilities#Version control systems and dotfiles.

**Examples:**

Example 1 (unknown):
```unknown
--link-dest
```

---

## Realtime process management

**URL:** https://wiki.archlinux.org/title/Realtime_process_management

**Contents:**
- Configuration
  - Configuring PAM
  - Configuring systemd services
- Usage
  - Using chrt
- Hard and soft realtime
- Tips and tricks
  - PAM-enabled login
  - Checking Limits
- See also

This article provides information on prioritizing process threads in real time, as opposed to at startup only. It shows how you can control CPU, memory, and other resource utilization of individual processes, or all processes run by a particular group.

While many recent processors are powerful enough to play a dozen video or audio streams simultaneously, it is still possible that another thread hijacks the processor for half a second to complete another task. This results in short interrupts in audio or video streams. It is also possible that video/audio streams get out of sync. While this is annoying for a casual music listener; for a content producer, composer or video editor this issue is much more serious as it interrupts their workflow.

The solution is to run time-sensitive processes in realtime. In linux, this means changing the process to a realtime scheduler, like SCHED_RR or SCHED_FIFO. See sched(7) for descriptions of these schedulers.

On Arch Linux, system, group and user-wide configuration can be achieved using PAM and systemd.

The realtime package group provides additional tools to modify the realtime scheduling policies of IRQs and processes.

The /etc/security/limits.conf file provides configuration for the pam_limits PAM module, which sets limits on system resources (see limits.conf(5)).

There are two types of resource limits that pam_limits provides: hard limits and soft limits. Hard limits are set by root and enforced by the kernel, while soft limits may be configured by the user within the range allowed by the hard limits.

Installing the package realtime-privileges and adding the user to the realtime group, provides reasonable default values (e.g. relevant for Professional audio).

Processes spawned by systemd system services need to specifically set up equivalents to limits.conf. Further information can be found in the sections systemd.exec(5)  CREDENTIALS and systemd.exec(5)  PROCESS PROPERTIES in systemd.exec(5).

To run in realtime, a process running in the realtime group must set LIMIT_RTPRIO to any value above 0, then change to a realtime scheduler, like SCHED_FIFO or SCHED_RR. Here is an example:

Again, LIMIT_RTPRIO must be above 0 for the process to be able to change its scheduler. This is why realtime-privileges raises the rtprio limit to 98 for all processes in the realtime group. See sched(7) for more.

To do this automatically, or in case you are running somebody else's software, you can use chrt. Here is an example of starting ls -al in realtime under the SCHED_FIFO scheduler:

This shouldn't normally be required. Applications that need to run in realtime, like jack2, normally set their schedulers all on their own.

Realtime is a synonym for a process which has the capability to run in time without being interrupted by any other process. However, cycles can occasionally be dropped despite this. Low power supply or a process with higher priority could be a potential cause. To solve this problem, there is a scaling of realtime quality. This article deals with soft realtime. Hard realtime is usually not so much desired as it is needed. An example could be made for car's ABS (anti-lock braking system). This can not be "rendered" and there is no second chance.

PAM is installed and configured on default Arch Linux installations. Nearly all display managers are pam-enabled, too. You can check which modules use pam_limits.so with the following command:

On a default installation, only two files use the module directly:

/etc/pam.d/system-auth is included by other pam-aware applications. You can further search /etc/pam.d:

In the default configuration this shows which applications are using system-auth and thus pam_limits.so. For example

This covers both graphical and console login.

In Bash, use ulimit (see bash(1)  ulimit) to check limits, for example the rtprio limit can be checked as follows:

This is the value configured by realtime-privileges.

**Examples:**

Example 1 (unknown):
```unknown
/etc/security/limits.conf
```

Example 2 (unknown):
```unknown
/etc/security/limits.d
```

Example 3 (unknown):
```unknown
limits.conf
```

Example 4 (unknown):
```unknown
LIMIT_RTPRIO
```

---

## XDG MIME Applications

**URL:** https://wiki.archlinux.org/title/XDG_MIME_Applications

**Contents:**
- Shared MIME database
  - New MIME types
- mimeapps.list
  - Format
- Utilities
  - lsdesktopf
  - selectdefaultapplication
- Troubleshooting
  - Missing desktop entry
  - Missing association

The XDG MIME Applications specification builds upon the shared MIME database and desktop entries to provide default applications.

The XDG Shared MIME-info Database specification facilitates a shared MIME database across desktop environments and allows applications to easily register new MIME types system-wide.

The database is built from the XML files installed by packages in /usr/share/mime/packages/ using the tools from shared-mime-info.

The files in /usr/share/mime/ should not be directly edited, however it is possible to maintain a separate database on a per-user basis in the ~/.local/share/mime/ tree.

"URI scheme handling [..] are handled through applications handling the x-scheme-handler/foo MIME type, where foo is the URI scheme in question."[1]

This article or section needs expansion.

This example defines a new MIME type application/x-foobar and assigns it to any file with a name ending in .foo. Simply create the following file:

And then update the MIME database:

Of course this will not have any effect if no desktop entries are associated with the MIME type. You may need to create new desktop entries or modify mimeapps.list.

The XDG standard is the most common for configuring desktop environments. Default applications for each MIME type are stored in mimeapps.list files, which can be stored in several locations. They are searched in the following order, with earlier associations taking precedence over later ones:

Additionally, it is possible to define desktop environment-specific default applications in a file named desktop-mimeapps.list where desktop is the name of the desktop environment (from the XDG_CURRENT_DESKTOP environment variable). For example, /etc/xdg/xfce-mimeapps.list defines system-wide default application overrides for Xfce. These desktop-specific overrides take precedence over the corresponding non-desktop-specific file. For example, /etc/xdg/xfce-mimeapps.list takes precedence over /etc/xdg/mimeapps.list but is still overridden by ~/.config/mimeapps.list.

To discover all the files that are scanned it is possible to enable debug mode by setting the environment variable XDG_UTILS_DEBUG_LEVEL=2: e.g. the xdg-mime query default <type> command will print each configuration file it is searching for MIME information.

Consider the following example:

Each section assigns one or more desktop entries to MIME types.

Each section is optional and can be omitted if unneeded.

While it is possible to configure default applications and MIME types by directly editing mimeapps.list and the shared MIME database, there are many tools that can simplify the process. These tools are also important because applications may delegate opening of files to these tools rather than trying to implement the MIME type standard themselves.

If you use a desktop environment you should first check if it provides its own utility. That should be preferred over these alternatives.

The official xdg-utils contain tools for managing MIME types and default applications according to the XDG standard (xdg-mime). Most importantly it provides xdg-open which many applications use to open a file with its default application.

lsdesktopfAUR provides several methods of searching the MIME database and desktop MIME entries.

For example, to see all MIME extensions in the system's .desktop files that have MIME type video you can use lsdesktopf --gm -gx video or to search in the XML database files use lsdesktopf --gdx -gx video. To get a quick overview of how many and which .desktop files can be associated with a certain MIME type, use lsdesktopf --gen-mimeapps. To see all file name extensions in XML database files, use lsdesktopf --gdx -gfx.

selectdefaultapplication-gitAUR is GUI application that lists up all applications supporting various mimetypes and lets you quickly set it as default for all or some of the mimetypes it supports (by modifying mimeapps.list).

It shows the "readable" name and file extensions as well, so you do not need to remember the name of the mimetypes.

If a file is not being opened by your desired default application, there are several possible causes. You may need to check each case.

A desktop entry is required in order to associate an application with a MIME type. Ensure that such an entry exists and can be used to (manually) open files in the application.

If the application's desktop entry does not specify the MIME type under its MimeType key, it will not be considered when an application is needed to open that type. Edit mimeapps.list to add an association between the .desktop file and the MIME type.

If the desktop entry is associated with the MIME type, it may simply not be set as the default. Edit mimeapps.list to set the default association.

Applications are free to ignore or only partially implement the XDG standard. Check for usage of deprecated files such as ~/.local/share/applications/mimeapps.list and ~/.local/share/applications/defaults.list. If you are attempting to open the file from another application (e.g. a web browser or file manager) check if that application has its own method of selecting default applications.

This article or section needs expansion.

Desktop environments and file managers supporting the specifications launch programs according to definition in the .desktop files. See Desktop entries#Application entry.

Usually, configuration of the packaged .desktop files is not required, but it may not be bug-free. Even if an application containing necessary MIME type description in the .desktop file MimeType variable that is used for association, it can fail to start correctly, not start at all or start without opening a file.

This may happen, for example, if the Exec variable is missing internal options needed for how to open a file, or how the application is shown in the menu. The Exec variable usually begins with %; for its currently supported options, see exec-variables.

The following table lists the main variable entries of .desktop files that affect how an application starts, if it has a MIME type associated with it.

This article or section is a candidate for moving to KDE#Troubleshooting.

Install archlinux-xdg-menu and run XDG_MENU_PREFIX=arch- kbuildsycoca6 or add export XDG_MENU_PREFIX=plasma- to .xinitrc.

**Examples:**

Example 1 (unknown):
```unknown
/usr/share/mime/packages/
```

Example 2 (unknown):
```unknown
/usr/share/mime/packages/
```

Example 3 (unknown):
```unknown
/usr/share/mime/
```

Example 4 (unknown):
```unknown
~/.local/share/mime/
```

---

## pass

**URL:** https://wiki.archlinux.org/title/Pass

**Contents:**
- Installation
- Basic usage
- Data organization
- Migrating to pass
- Extensions
- Advanced usage
- Multiple pass contexts (e.g. teaming)
- Git integration
  - Git helper usage
    - git configuration

From the official website:

pass is a simple password manager for the command line. pass is a shell script that makes use of existing tools like GnuPG, tree and Git.

Install the pass package.

An optional Qt GUI is available via the qtpass package.

To initialize the password store:

To create a new password, first provide a descriptive hierarchical name. In this example, this is archlinux.org/wiki/username:

To get a view of the password store do the following. Note the example output which shows the hierarchy we just created:

To generate a new random password for the above example, do the following, where n is the desired password length as a number:

To retrieve a password, enter the gpg passphrase at the following prompt, again using the example name from above:

Users of Xorg with xclip installed can retrieve the password directly onto the clipboard temporarily (e.g., to paste into web forms). In a Wayland session, should use wl-clipboard instead. To do so, do the following (again with the same example hierarchical name from above):

pass comes with a dmenu wrapper to enable easy searching/copying. To use it, install the optional dependency dmenu and run:

Then selecting an entry will copy its password to the clipboard. See dmenu(1) for customization options such as case-insensitivity. You may want to set this to a systemwide keybinding in order to easily access passwords from any application.

By default, the credential file created with pass insert will only contain your password. However, it may not be enough since several applications ask for detail data like username, url, etc. You can edit an existing file the way you want with command pass edit password_name. Below is the preferred organizational scheme provided by pass-project page. When using the option -c or --clip with this scheme, only the password will be copied.

There are multiple scripts listed on the pass-project page to import passwords from other programs

Since version 1.7, pass supports extensions developed by the community. These extensions extend the features of pass with the support of new commands.

Environment variables can be used to alter where pass looks to do store and git operations via:

For more information on how this can be used to support multiple pass repositories see this link. The following pw() example alias sends the second line of the named database to the clipboard before sending the first line five seconds thereafter and finally an OTP code five seconds after that. Assuming that a password occupies the first line and a username the second line and an OTP URI exists anywhere in the named database, the net effect is passing username > password > otp code for consecutive primary pasting into available (e.g. browser) entry fields:

One can use aliases to set up different pass contexts, which helps when collaborating with different teams. We have gotten this working in bash as follows:

Add aliases to your ~/.bashrc:

Add these for bash-completion to your ~/.bash_completion and make sure bash-completion is installed:

Or for zsh (source: /usr/share/zsh/site-functions/_pass)

Now you can initialize into ~/.pass/red and ~/.pass/blue and have two pass contexts with the passred and passblue aliases. You can generalize this further into as many contexts as you like.

You can use pass as a credentials helper for git. Install the pass-git-helperAUR. Details are described in the github README file.

Configure pass-git-helper as a git credentials helper by calling:

Create the file ~/.config/pass-git-helper/git-pass-mapping.ini. It is used to map git remote hosts to your pass database. The format is something like this:

You can use wildcards in the host part, as shown in the example.

As usual with pass, the helper assumes that the password is contained in the first line of the passwordstore entry. Additionally, if a second line is present, this line is interpreted as the username.

For this to work, you have to use pass insert --multiline to create a multi line password store entry.

You are able to setup a password management system by setting up a central Git server for pass. This allows you to synchronize your central password repository through multiple client environments.

On the server run git init --bare ~/.password-store to create a bare repository you can push to.

See SSH keys#Copying the public key to the remote server

This section assumes you have configured GnuPG and have a key pair to encrypt passwords. On your local client ensure you have a local password store on the client, then enable management of local changes through Git, add your remote Git repository, and push your local pass history.

Create a local password store:

Enable management of local changes through Git:

Add the remote git repository as 'origin':

Push your local pass history:

Now you can use the standard Git commands, prefixed by pass. For example: pass git push, or pass git pull. pass will automatically create commits when you use it to modify your password store.

The following error can occur when attempting to insert a new entry:

This occurs if the trust level of the GnuPG key is set to anything other than "ultimate". Edit the key used for pass to set its trust level to "ultimate":

The following error can occur when your GPG key expires (e.g., after a year) and you try to add a new password:

To fix this, either extend the current GPG key's expiration date or switch to a new one (i.e., key rotation).

To switch to a new key and re-encrypt the store:

**Examples:**

Example 1 (unknown):
```unknown
$ pass init gpg-id_or_email
```

Example 2 (unknown):
```unknown
$ pass insert archlinux.org/wiki/username
```

Example 3 (unknown):
```unknown
Password Store
 archlinux.org
     wiki
         username
```

Example 4 (unknown):
```unknown
$ pass generate archlinux.org/wiki/username n
```

---

## Mail server

**URL:** https://wiki.archlinux.org/title/Mail_server

**Contents:**
- Software
- Ports
- MX record
- Authentication
  - Sender Policy Framework
  - Sender Rewriting Scheme
  - DKIM
  - ARC
- Testing
  - Dedicated tools

A mail server consists of multiple components. A mail transfer agent (MTA) receives and sends emails via SMTP. Received and accepted emails are then passed to a mail delivery agent (MDA), which stores the mail in a mailbox (usually in mbox or Maildir format). If you want users to be able to remotely access their mail using email clients (MUA), you need to run a POP3 and/or IMAP server.

This article or section needs expansion.

Below is a table containing all mail servers with the features they support.

Hosting a mail server requires a domain name with an MX record pointing to the domain name of your mail transfer agent. The domain name used as the value of the MX record must map to at least one address record (A, AAAA) and must not have a CNAME record to conform with RFC 2181, otherwise you may not get mail from some mail servers. Configuring DNS records is usually done from the configuration interface of your domain name registrar.

This article or section is a candidate for merging with Mail server#Software.

There are various email authentication techniques.

To allow other mail exchangers to validate mails apparently sent from your domain, you need to set a DNS TXT record as explained in the Wikipedia article (there is also an online wizard). To validate incoming mail using SPF you need to configure your mail transfer agent to use a SPF implementation. There are several SPF implementations available: libspf2, perl-mail-spf and perl-mail-spf-query.

The following websites let you validate your SPF record:

The Sender Rewriting Scheme (SRS) is a secure scheme to allow forwardable bounces for server-side forwarded emails without breaking the Sender Policy Framework.

For Postfix, see Postfix#Sender Rewriting Scheme.

DomainKeys Identified Mail (DKIM) is a domain-level email authentication method designed to detect email spoofing.

Available DKIM implementations are OpenDKIM and perl-mail-dkim.

ARC is an experimental Standard, the Authenticated Received Chain (ARC) email authentication system. It allows an intermediate mail server like a mailing list or forwarding service to sign an email's original authentication results. ARC is implemented by Google, Microsoft, ProtonMail, Fastmail, and others.

Available ARC implementations are rspamd (as a module of rspamd), and OpenARC (standalone).

There are several options to help you test DNS records, deliver ability, and encryption support.

There are several handy web sites that can help you testing.

Most mail servers can be configured to strip users' IP addresses and user agents from outgoing mail.

See ClamAV for email antivirus scanning.

See SpamAssassin for filtering of spam emails.

See Roundcube and Squirrelmail for setting up of a webmail.

**Examples:**

Example 1 (unknown):
```unknown
v=spf1 -all
```

---

## Gaming

**URL:** https://wiki.archlinux.org/title/Gaming

**Contents:**
- Game technicality
- Common game dependencies
  - Mandatory (for high coverage)
  - Optional (but still common)
  - Rare (less common)
- Machine requirements
  - Drivers
  - Dependency for the machine & substitutes
- Game environments
- Game compatibility

Linux has long been considered an "unofficial" gaming platform; the support and target audience provided to it is not a primary priority for most gaming organizations. Changes to this situation have accelerated, starting from 2021 onward, as big players like Valve, the CodeWeavers group and the community have made tremendous improvements to the ecosystem, allowing Linux to truly become a viable platform for gaming. Further, more and more indie development teams strive to use cross-platform rendering engines in order to have their game able to compile and run on Linux.

When it comes to gaming, the majority of user's thoughts are often directed towards popular AAA games which are usually written exclusively for the Microsoft Windows platform. This is understandable, however, it is not the only and sole availability. Please refer to #Game environments and #Getting games further down the page where you can find software to run games from other platforms.

If you however are fixated on getting games written for Microsoft Windows to work on Linux, then a different mindset, tools and approach is required; understanding internals and providing functional substitution. Please read #Game technicality below.

There are ultimately three complications that arise from attempting to play AAA games made for Windows on Linux. They are:

Graphics SDK forward graphical calls to the underlying graphics driver which then proceeds to talk to the GPU hardware.

A huge amount of games use DirectX as their main driving SDK. Linux, natively supports only OpenGL and Vulkan. Linux by itself does not support DirectX or any of the aforementioned technologies (Visual C++, MFC, .NET).

Instead, several opensource equivalents have been written which attempt to provide identical functionality, ultimately achieving the same result from a graphics point of view. These equivalents have their "own" written substitutes which attempt to "re-invent" what the original SDK calls would possibly achieve from a black box point of view. Popular ones include:

For example, a call to load, transform and shade vertices on DirectX may be re-written from scratch in a new .dll/.so owned by Wine, providing their own "hypothetical" belief on what the function may be doing underneath, and forward it instead to an OpenGL alternative, effectively trying to achieve similar results. Since these calls are direct equivalents and treated "as if" DirectX was running, performance is not impacted. (with the exception of the starting overhead to interop with these)

These tools are often brought in the distribution together on the system at the same time. A prefix (Wine's terminology for a directory mimicking a Windows sandbox) is created and configured. Dependencies are installed inside the prefix (the "sandbox" still needs the game's redistributables), often with winetricks, followed by an attempt to run the game "as if" it was executed from Windows.

This, nowadays, fortunately works for most games (aside from anti-cheat protected ones, which require a kernel driver that Wine/Proton does not yet have). If a game does not work, it is usually as a result of incompatible packages, missing dependencies or unimplemented functionality by Wine/Proton.

Lutris is a piece of software that provides runners and sandboxes that handle dependencies for you when you install games, if the above process is found tedious and/or complicated.

In order to gain a more in-depth understanding of what you will intend to do if you decide to go the Wine/Proton route, it is worthwhile to cover the common dependencies that games require in order to execute. Architecture also needs to be considered in mind, whether x86 or x64, preferably both.

A prefix would need to have the following populated into it in order to run most Windows games.

This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.

It is not enough to just populate a prefix with the dependencies the game will need. The kernel itself has to have the substitution it will provide to the calls the game will make. As already mentioned, drivers and alternatives are available.

Wine/Proton are not the only approaches to play games. Different environments exist to play games in Linux, and have just as many (or more) games than on Windows:

Having the vm.max_map_count set to a low value can affect the stability and performance of some games. It can therefore be desirable to increase the size permanently by creating the following sysctl config file.

Arch Linux uses the value 1048576 by default [1], which Fedora considers a safe value [2]. This default value is likely to be sufficient for current games, as increasing the value was mostly important when Arch used the kernel default of 65530 [3]. A value of 2147483642 (MAX_INT - 5) is the default in SteamOS.

Apply the changes without reboot by running:

Just because games are available for Linux does not mean that they are native; they might be pre-packaged with Wine or DOSBox.

For list of games packaged for Arch in official repositories or the AUR see List of games.

For Wine wrappers, see Wine#Third-party applications.

Game launchers are important tools for managing and running games on various platforms. Below is a comparison of different game launchers available for Linux, particularly focusing on their features and compatibility.

Certain games or game types may need special configuration to run or to run as expected. For the most part, games will work right out of the box in Arch Linux with possibly better performance than on other distributions due to compile time optimizations. However, some special setups may require a bit of configuration or scripting to make games run as smoothly as desired.

Running a multi-screen setup may lead to problems with full-screen games. In such a case, running a second X server is one possible solution. For NVIDIA users, a solution may be found in NVIDIA#Gaming using TwinView.

Many games grab the keyboard, noticeably preventing you from switching windows (also known as alt-tabbing).

Some SDL games (e.g. Guacamelee) let you disable grabbing by pressing Ctrl-g.

In some cases like those mentioned above, it may be necessary or desired to run a second X server. Running a second X server has multiple advantages such as better performance, the ability to "tab" out of your game by using Ctrl+Alt+F7/Ctrl+Alt+F8, no crashing your primary X session (which may have open work on) in case a game conflicts with the graphics driver. The new X server will be akin a remote access login for the ALSA, so your user need to be part of the audio group to be able to hear any sound.

To start a second X server (using the free first person shooter game Xonotic as an example) you can simply do:

This can further be spiced up by using a separate X configuration file:

A good reason to provide an alternative xorg.conf here may be that your primary configuration makes use of NVIDIA's Twinview which would render your 3D games like Xonotic in the middle of your multiscreen setup, spanned across all screens. This is undesirable, thus starting a second X with an alternative configuration where the second screen is disabled is advised. Please note, that the X configuration file location is relative to the /etc/X11 directory.

A game starting script making use of Openbox for your home directory or /usr/local/bin may look like this:

After making it executable you would be able to do:

For games that require exceptional amount of mouse skill (like shooters), adjusting the mouse polling rate can help improve accuracy on high-end mice. The improvement can be by a few milliseconds when increasing from typical polling rates like 250hz-500hz to 1000hz, but increasing past 1000hz will get at most slightly less than a millisecond of improvement. In general, latency is influenced by the quality of the mouse's microcontroller, firmware, and sensor, and the mice with the lowest latency currently available don't necessarily improve past 1000hz.

For games using OpenAL, if you use headphones you may get much better positional audio using OpenAL's HRTF filters. To enable, create:

Alternatively, install openal-hrtfAUR from the AUR, and edit the options in /etc/openal/alsoftrc.conf.

For Source games, the ingame setting `dsp_slow_cpu` must be set to `1` to enable HRTF, otherwise the game will enable its own processing instead. You will also either need to set up Steam to use native runtime, or link its copy of openal.so to your own local copy. For completeness, also use the following options:

If you are using PulseAudio, you may wish to tweak some default settings to make sure it is running optimally.

PulseAudio is built to be run with realtime priority, being an audio daemon. However, because of security risks of it locking up the system, it is scheduled as a regular thread by default. To adjust this, first make sure you are in the audio group. Then, uncomment and edit the following lines in /etc/pulse/daemon.conf:

and restart PulseAudio.

PulseAudio on Arch uses speex-float-1 by default to remix channels, which is considered a 'medium-low' quality remixing. If your system can handle the extra load, you may benefit from setting it to one of the following instead:

Matching the buffers can reduce stuttering and increase performance marginally. See here for more details.

Cloud gaming has gained a lot of popularity in the last few years, because of low client-side hardware requirements. The only important thing is stable internet connection (over the Ethernet cable or 5 GHz Wi-Fi recommended) with a minimum speed of 510 Mbit/s (depending on the video quality and framerate).

See Gamepad#Gamepad over network for using a gamepad over a network with services that do not normally support this.

See also main article: Improving performance. For Wine programs, see Wine#Performance. For a good gaming experience low latency, a consistent response time (no jitter) and enough throughput (frames per second) are needed. If you have multiple sources with a little jitter they are likely to overlap sometimes and produce noticeable stutter. Therefore most of the time it is preferable to decrease the throughput a little to gain more response time consistency. On the other hand if you get low average FPS or if the gaming application itself produces spikes in CPU demand it is better to increase raw throughput.

User space programs and especially games do many calls to clock_gettime(2) to get the current time for calculating game physics, fps and so on. The time usage can be seen by running

and looking at the overhead of read_hpet (or acpi_pm_read).

If you are not dependent on a very precise timer you can switch from hpet (high precision event timer) or acpi_pm (ACPI Power Management Timer) to the faster TSC (time stamp counter) timer. Add the kernel parameters

to make TSC available and enable it. After that reboot and confirm the clocksource by running

You can see all currently available timers by running

and change between them by echoing one into current_clocksource. On a Zen 3 system benchmarking with [5] shows a ~50 times higher throughput of tsc compared to hpet or acpi_pm.

You can install the realtime kernel to get some advantages by default (see realtime kernel article) but it costs CPU throughput and can delay interrupt handling. Additionally the realtime kernel is not compatible with nvidia-open-dkms and does not change the scheduler for SCHED_NORMAL (also named SCHED_OTHER) processes, which is the default process scheduling type. The following kernel parameter changes improve the response time consistency for the realtime kernel as well as other kernels such as the default linux kernel:

Proactive compaction for (Transparent) Hugepage allocation reduces the average but not necessarily the maximum allocation stalls. Disable proactive compaction because it introduces jitter according to kernel documentation (inner workings):

Reduce the watermark boost factor to defragment only one pageblock (2MB on 64-bit x86) in case of memory fragmentation. After a memory fragmentation event this helps to better keep the application data in the last level processor cache.

If you have enough free RAM increase the number of minimum free Kilobytes to avoid stalls on memory allocations: [7][8]. Do not set this below 1024 KB or above 5% of your systems memory. Reserving 1GB:

If you have enough free RAM increase the watermark scale factor to further reduce the likelihood of allocation stalls (explanations [9][10]). Setting watermark distances to 5% of RAM:

Avoid swapping (locking pages that introduces latency and uses disk IO) unless the system has no more free memory:

Make sure zswap is enabled. If there arises the need for swapping pages, then zswap helps reduce swapping related stutter. If you do not have a swap partition, you can use zram instead of zswap.

Enable Multi-Gen Least Recently Used (MGLRU) but reduce the likelihood of lock contention at a minor performance cost [11]:

Disable zone reclaim (locking and moving memory pages that introduces latency spikes):

Disable Transparent Hugepages (THP) at a performance cost. Even if defragmentation is disabled, THPs might introduce latency spikes [12][13][14]. Enable only when the application specifically requests it by using madvise and advise:

Note that if your game uses TCMalloc (e.g., Dota 2 and CS:GO) then it is not recommended to disable THP as it comes with a large performance cost [15].

Reduce the maximum page lock acquisition latency while retaining adequate throughput [16][17][18]:

Tweak the scheduler settings. The following scheduler settings are in conflict with cfs-zen-tweaksAUR so for each setting choose only one provider. By default the linux kernel scheduler is optimized for throughput and not latency. The following hand-made settings change that and are tested with different games to be a noticeable improvement. They might not be optimal for your use case; consider modifying them as necessary [19][20]:

Further, it is recommended to test different schedulers as described in Improving_performance#CPU scheduler. For example, the scx_cosmos scheduler from scx-scheds shows promise for improving response time consistency, as it optimizes task-to-CPU locality (enhancing cache hit consistency between runs), reduces locking contention, and is suitable for general usage scenarios, prioritizing interactive tasks under load.

Usually, the advice for permanently setting kernel parameters is to configure create a sysctl configuration file or change your boot loader options. However, since our change span both procfs (/proc, containing sysctl) and sysfs (/sys), the most convenient way is to use systemd-tmpfiles:

After that reboot and see if the values applied correctly.

A device experiences buffer bloat when a large buffer queues requests, causing new requests to be blocked for a long time. This can occur with both sending and receiving data.

Buffer bloat is solved by having a smart queuing algorithm that gives fair and timely share towards the device. Smart queuing algorithms produce quicker and more consistent response times, but increase CPU overhead. For interactive workloads reduced buffer bloat is desirable.

For storage devices, the BFQ scheduler is the most effective at minimizing buffer bloat and improving interactivity Improving performance#The scheduling algorithms.

You can use the default fq_codel or the improved cake smart queue management algorithms. They can be set via sysctl. Your home internet router can cause buffer bloat, which might be mitigated by reconfiguring it, installing a new router OS, or replacing the router (more info).

Here are some easy ways to test for network buffer bloat.

Change the PCI Express Latencies similar to CachyOS [21]. Reduce the maximum cycles a PCI-E Client can occupy the bus, except for sound cards [22]. Note that these settings are in conflict with Professional audio#Optimizing system configuration.

To enable these settings consistently run them as super user on system boot.

Whether or not disabling SMT improves your gaming experience depends on your hardware and gaming applications.

Advantages of disabling SMT:

If you disable SMT, do so in the BIOS/UEFI to improve single-core performance. (Example of SMT sharing CPU internal resources: [23])

Set the environment variable

for your games, to avoid needing to load program code at run time (see ld.so(8)), leading to a delay the first time a function is called. Do not set this for startplasma-x11 or other programs that link in libraries that do not actually exist on the system anymore and are never called by the program. If this is the case, the program fails on startup trying to link a nonexistent shared object, making this issue easily identifiable. Most games should start fine with this setting enabled. Since packages inside the official repository are built with RELRO [24] by default, only packages from other sources should be affected in terms of first time latency, bugs in linking might still be triggered.

GameMode is daemon and library combo that allows games to request a set of optimisations be temporarily applied to the host OS. This can improve game performance.

Gamescope is a microcompositor from Valve that is used on the Steam Deck. Its goal is to provide an isolated compositor that is tailored towards gaming and supports many gaming-centric features.

See AMDGPU#ACO compiler

Direct Rendering Infrastructure (DRI) Configuration Files apply for all DRI drivers including Mesa and Nouveau. You can change the DRI configuration systemwide in /etc/drirc or per user in $HOME/.drirc. If they do not exist, you have to create them first. Both files use the same syntax; documentation for these options can be found at https://dri.freedesktop.org/wiki/ConfigurationOptions/. To reduce input latency by disabling synchronization to vblank, add the following:

Most games can benefit if given the correct scheduling policies for the kernel to prioritize the task. These policies should ideally be set per-thread by the application itself.

For programs which do not implement scheduling policies on their own, application known as schedtool, and its associated daemon schedtooldAUR can handle many of these tasks automatically.

To edit what programs relieve what policies, simply edit /etc/schedtoold.conf and add the program followed by the schedtool arguments desired.

SCHED_ISO (only implemented in BFS/MuQSSPDS schedulers found in -pf and -ck kernels)  will not only allow the process to use a maximum of 80 percent of the CPU, but will attempt to reduce latency and stuttering wherever possible. Most if not all games will benefit from this:

SCHED_FIFO provides an alternative, that can even work better. You should test to see if your applications run more smoothly with SCHED_FIFO, in which case by all means use it instead. Be warned though, as SCHED_FIFO runs the risk of starving the system! Use this in cases where -I is used below:

Secondly, the nice level sets which tasks are processed first, in ascending order. A nice level of -4 is recommended for most multimedia tasks, including games:

There is some confusion in development as to whether the driver should be multithreading, or the program. Allowing both the driver and program to simultaneously multithread can result in significant performance reductions, such as framerate loss and increased risk of crashes. Examples of this include a number of modern games, and any Wine program which is running with GLSL enabled. To select a single core and allow only the driver to handle this process, simply use the -a 0x# flag, where # is the core number, e.g.:

Some CPUs are hyperthreaded and have only 2 or 4 cores but show up as 4 or 8, and are best accounted for:

which use virtual cores 0101, or 1 and 3.

For most games which require high framerates and low latency, usage of all of these flags seems to work best. Affinity should be checked per-program, however, as most native games can understand the correct usage. For a general case:

As a general rule, any other process which the game requires to operate should be reniced to a level above that of the game itself. Strangely, Wine has a problem known as reverse scheduling, it can often have benefits when the more important processes are set to a higher nice level. Wineserver also seems unconditionally to benefit from SCHED_FIFO, since it rarely consumes the whole CPU and needs higher prioritization when possible.

You might want to set your mouse acceleration to control your mouse more accurately.

If your mouse has more than 3 buttons, you might want to see Mouse buttons.

If you are using a gaming mouse (especially Logitech and Steelseries) you may want to configure your mouse polling rate, DPI, LEDs etc. using piper. See this page for a full list of supported devices by piper. Alternatively solaar for logitech devices.

You can change and manipulate various RGBs with openrgb, for a list of currently supported devices see [25]

**Examples:**

Example 1 (unknown):
```unknown
vm.max_map_count
```

Example 2 (unknown):
```unknown
/etc/sysctl.d/80-gamecompatibility.conf
```

Example 3 (unknown):
```unknown
vm.max_map_count = 2147483642
```

Example 4 (unknown):
```unknown
/usr/lib/sysctl.d/10-arch.conf
```

---

## Help:Reading

**URL:** https://wiki.archlinux.org/title/Enable

**Contents:**
- Organization
- Formatting
- Root, regular user or another user
- Append, add, create, edit
  - Make executable
- Source
- Installation of packages
  - Official packages
  - Arch User Repository
- Control of systemd units

Because the vast majority of the ArchWiki contains indications that may need clarification for users new to Arch Linux (or GNU/Linux in general), this rundown of basic procedures was written both to avoid confusion in the assimilation of the articles and to deter repetition in the content itself.

Most articles on the ArchWiki do not attempt to provide a holistic introduction to a single topic; they are instead written in adherence to the "Don't Repeat Yourself" principle, under the assumption that the user will seek out and read any supporting material that they do not yet understand. Where possible, such supporting material is indicated in the article via special formatting, see #Formatting.

Because of this organization, it may be necessary to examine several related sources in order to fully understand an ArchWiki article. In particular, users who are new to Arch (or GNU/Linux in general) should expect to end up reading a great number of articles even when solving simple problems. It is especially important to study the supporting material before seeking additional help from other users.

Some lines are written like so:

Others have a different prefix:

The numeral or hash sign (#) indicates that the command needs to be run as root, whereas the dollar sign ($) shows that the command should be run as a regular user.

When the commands need to run as a specific user, they will be prefixed by the username in square brackets, for example:

This means you should use a privilege elevation tool, e.g. with sudo:

A notable exception to watch out for:

In this example, the context surrounding the numeral sign communicates that this is not to be run as a command; it should be edited into a file instead. So in this case, the numeral sign denotes a comment. A comment can be explanatory text that will not be interpreted by the associated program. Bash scripts denotation for comments happens to coincide with the root PS1.

After further examination, "give away" signs include the uppercase character following the # sign. Usually, Unix commands are not written this way and most of the time they are short abbreviations instead of full-blown English words (e.g., Copy becomes cp).

Regardless, most articles make this easy to discern by notifying the reader:

Append to ~/path/to/file:

When prompted to append to, add to, create, or edit one or more files, it is implied that you should use one of the following methods.

To create or modify multiline files, it is suggested to use a text editor. For example, using the nano command to edit the file /etc/bash.bashrc is:

To create or overwrite a file from a string, it may be simpler to use output redirection. The following example creates or overwrites the contents of the file /etc/hostname with the text myhostname.

Output redirection can also be used to append a string to a file. The following example appends the text [custom-repo] to the file /etc/pacman.conf.

When prompted to create directories, use the mkdir command:

After creating a file, if it is meant to be run as a script (whether manually or called by another program), it needs to be set as executable, for example with:

See chmod. Some applications such as file managers may provide graphical interfaces to do the same.

Some applications, notably command-line shells, use scripts for their configuration: after modifying them, they must be sourced in order for the changes to be applied. In the case of bash, for example, this is done by running (you can also replace source with .):

When the wiki suggests modifying such a configuration script, it will not explicitly remind you to source the file, and only in some cases will it point to this section with a reminder link.

When an article invites you to install some packages in the conventional way, it will not indicate the detailed instructions to do so; instead, it will simply mention the names of the packages to be installed.

The subsections below give an overview of the generic installation procedures depending on the package type.

For packages from the official repositories, you will read something like:

This means that you have to run:

The pacman article contains detailed explanations to deal with package management in Arch Linux proficiently.

For packages from the Arch User Repository (AUR), you will read something like:

This means that in general you have to follow the foobarAUR link, download the PKGBUILD archive, extract it, verify the content and finally run, in the same folder:

The Arch User Repository article contains all the detailed explanations and best practices to deal with AUR packages.

When an article invites to start, enable, etc., some systemd unit (e.g. a service), it will not indicate the detailed instructions to do so, but instead you will read something like:

This means that you have to run:

A notable command that does not follow this exact pattern is systemctl daemon-reload which will be called without arguments.

The systemd#Using units section contains structured list of available actions (like start, enable, enable and start, etc.) with their corresponding systemctl commands.

It is important to remember that there are two different kinds of configurations on a GNU/Linux system. System-wide configuration affects all users. Since system-wide settings are generally located in the /etc directory, root privileges are required in order to alter them. For example, to apply a Bash setting that affects all users, /etc/bash.bashrc should be modified.

User-specific configuration affects only a single user. Dotfiles are used for user-specific configuration. For example, the file ~/.bashrc is the user-specific configuration file. The idea is that each user can define their own settings, such as aliases, functions and other interactive features like the prompt, without affecting other users' preferences.

Bash and other Bourne-compatible shells, such as Zsh, also source files depending on whether the shell is a login shell or an interactive shell. See Bash#Configuration files and Zsh#Startup/Shutdown files for details.

Some code blocks may contain so-called pseudo-variables, which, as the name says, are not actual variables used in the code. Instead they are generic placeholders and have to be manually replaced with system-specific configuration items before the code may be run or parsed. Common shells such as bash and zsh provide tab-completion to auto-complete parameters for common commands such as systemctl.

In the articles that comply with Help:Style/Formatting and punctuation, pseudo-variables are formatted in italics. For example:

In this case interface_name is used as a pseudo-variable placeholder in a systemd template unit. All systemd template units, identifiable by the @ sign, require a system-specific configuration item as argument. See systemd#Using units.

In this case the pseudo-variables are used to describe the parameters that must be substituted for them. Details on how to gather them are elaborated on in the section Securely wipe disk#Calculate blocks to wipe manually, which features the command.

This article or section needs expansion.

In case of file examples, pasting pseudo-variables in real configuration files might break the programs that use them.

In most cases, ellipses (...) are not part of the actual file content or code output, and instead represent omitted or optional text that is not relevant for the discussed subject.

For example HOOKS=(... encrypt ... filesystems ...) or:

Be aware though that, in a few instances, ellipses may be a meaningful part of the code syntax: attentive users should be able to recognize these cases by the context.

**Examples:**

Example 1 (unknown):
```unknown
# mkinitcpio -p linux
```

Example 2 (unknown):
```unknown
$ makepkg -s
```

Example 3 (unknown):
```unknown
sudo command
```

Example 4 (unknown):
```unknown
[postgres]$ initdb -D /var/lib/postgres/data
```

---

## Kernel parameters

**URL:** https://wiki.archlinux.org/title/Kernel_parameter

**Contents:**
- Boot loader configuration
  - Clover
  - GRUB
  - GRUB Legacy
  - LILO
  - Limine
  - rEFInd
  - Syslinux
  - systemd-boot
- dracut

There are three ways to pass options to the kernel and thus control its behaviour:

Between the three methods, the configurable options differ in availability, their name and the method in which they are specified. This page only explains the second method (kernel command line parameters) and shows a list of the most used kernel parameters in Arch Linux.

Most parameters are associated with subsystems and work only if the kernel is configured with those subsystems built in. They also depend on the presence of the hardware they are associated with.

Kernel command line parameters either have the format parameter, or parameter=value, or module.parameter=value.

Kernel parameters can be set either temporarily by editing the boot entry in the boot loader boot selection menu, or permanently by modifying the boot loader configuration file.

The following examples add the quiet and splash parameters to the Clover, GRUB, GRUB Legacy, LILO, Limine, rEFInd, Syslinux and systemd-boot boot loaders.

dracut is capable of embedding the kernel parameters in the initramfs, thus allowing to omit them from the boot loader configuration. See dracut#Kernel command line options. Note that this only works for parameters understood by dracut, like root= and rd.*. They do not become real kernel parameters.

See EFI boot stub#Using UEFI directly.

Even without access to your boot loader it is possible to change your kernel parameters to enable debugging (if you have root access). This can be accomplished by overwriting /proc/cmdline which stores the kernel parameters. However /proc/cmdline is not writable even as root, so this hack is accomplished by using a bind mount to mask the path.

First create a file containing the desired kernel parameters:

Then use a bind mount to overwrite the parameters:

You can cat /proc/cmdline to confirm that your change was successful.

This list is not comprehensive. In addition to the kernel itself, other programs can also read parameters from /proc/cmdline and change their behavior.

**Examples:**

Example 1 (unknown):
```unknown
/etc/modprobe.d/
```

Example 2 (unknown):
```unknown
parameter=value
```

Example 3 (unknown):
```unknown
module.parameter=value
```

Example 4 (unknown):
```unknown
cat /proc/cmdline
```

---

## Advanced Format

**URL:** https://wiki.archlinux.org/title/Advanced_Format

**Contents:**
- Changing sector size
  - Advanced Format hard disk drives
  - NVMe solid state drives
  - Using manufacturer specific programs
    - Intel
    - Seagate
- Partition alignment
- dm-crypt
- File systems
- Known issues

This article or section is a candidate for moving to Storage layout and alignment.

The minimum physical storage unit of a hard disk drive (HDD) is a sector. The solid state drive (SSD) equivalent is a page.[1] Storage device firmware abstract away their physical sectors into logical sectors that software can operate on. The size of this sector refers to the size of the smallest addressable unit on the disk.

The different "layers", namely the device, stacked block devices, and file systems, should utilize the same sector sizes. If they do not, the mapping process from the firmware's translation layer, although usually transparent, will result in overhead that can be avoided.

You can check the reported logical and physical sector sizes of storage devices with lsblk:

Alternatively, sysfs entries can be queried:

Sector sizes can also be seen in fdisk, smartctl and hdparm output.

These values again reflect what the device reports but may not be accurate for SSDs or NVMe drives, which often abstract or omit true physical characteristics.

For more detailed information on NVMe drives, such as supported LBA formats and atomic write unit granularity (if available), the following NVMe CLI commands may help:

However, many drives will show only a single LBA format (e.g., lbads:9, meaning 512-byte logical blocks), and an awupf value of 0, indicating no atomic write unit information is provided.

Some NVMe drives and "enterprise" SATA hard disk drives support changing their reported sector size using standard NVMe (Format NVM from NVM Command Set Specification 1.0 or later) or ATA (SET SECTOR CONFIGURATION EXT from ATA Command Set - 4 or later) commands, respectively. For hard disk drives this changes the logical sector size in order to match the physical sector size for optimal performance. While for NVMe solid state drives, both the logical and physical sector size values get changed.

SATA solid state drives typically do not support changing their sector size. Exception are certain Intel SATA SSDs that can change the reported physical sector size, but not the logical sector size.[2] Follow #Intel to change their reported physical sector size.

Changing the sector size of a drive is a complex process that requires low-level formatting. As an alternative, you can manually specify the desired sector size when creating file systems on the drive to get optimal performance. See #dm-crypt and #File systems.

To determine if the sector size of an Advanced Format hard disk drive can be changed, use the hdparm utility:

Advanced Format drives whose Sector Configuration Log lists multiple logical sector sizes will show a list of them:

Hard disk drives which do not support multiple changeable logical sector sizes will simply report the current sector sizes. E.g., an Advanced Format 512e drive:

For optimal performance on these types of drives, ensure the #dm-crypt sector size or #File systems block size is at least 4096 bytes and aligns to it.

An Advanced Format 4Kn drive:

4Kn drives already have the optimal configuration out of the box and do not need special considerations when partitioning/formatting. They can be simply used as is.

If your SATA HDD supports multiple logical sector sizes and the optional ATA command SET SECTOR CONFIGURATION EXT (usually only available in so-called "enterprise" class HDDs), you can use hdparm to change between the supported logical sector sizes. To set it to 4096 bytes, i.e. 4Kn, run:

Afterwards, hdparm should report the logical sector size as 4096 bytes:

Most solid state drives (SSDs) report their logical block address size as 512 bytes, even though they use larger blocks physically - typically 4 KiB, 8 KiB, or sometimes larger.

To check the formatted logical block address size (FLBAS) of an NVMe drive, use the nvme-cli utility in addition with Identify Namespace command:

smartctl can also display the supported logical block address sizes, but it does not provide user friendly descriptions. E.g.:

To change the logical block address size, use nvme format and specify the preferred value with the --lbaf parameter:

This should take just a few seconds to proceed.

If the above generic utilities do not allow changing the sector size, it may still be possible to change it using an utility from the drive's manufacturer.

For Intel use the Intel Memory and Storage (MAS) Tool (intel-mas-cli-toolAUR) with the -set PhysicalSectorSize=4096 option. Notice that only reported physical sector size will be changed, logical sector size will remain the same.

Seagate provides openseachestAUR. The utilities can be used on non-Seagate drives too since they use standard ATA and NVMe commands.

Scan all drives to find the correct one, and print info from the one you found:

Should print out information about the drive. Make sure to check the serial number.

Check the logical block sizes supported by the drive:

If 4096 is listed, you can change the logical sector size to it as follows:

This will take a couple of minutes, after which your drive now uses a 4 KiB native sector size.

Aligning partitions correctly avoids excessive read-modify-write cycles. A typical practice for personal computers is to have each partition's start and size aligned to 1 MiB (1 048 576 bytes) marks. This covers all common page and block size scenarios, as it is divisible by all commonly used sizes1 MiB, 512 KiB, 128 KiB, 4 KiB, and 512 B.

checkpartitionsalignment.sh is a bash script that checks for alignment using Parted and awk.

This article or section needs expansion.

As of Cryptsetup 2.4.0, luksFormat automatically detects the optimal encryption sector size for LUKS2 format [6].

However, for this to work, the device needs to report the correct default sector size, see #Changing sector size.

After using cryptsetup luksFormat, you can check the sector size used by the LUKS2 volume with

If the default sector size is incorrect, you can force create a LUKS2 container with a 4 KiB sector size and otherwise default options with:

The command will abort on an error if the requested size does not match your device:

If you encrypted your device with the wrong sector size, the device can be re-encrypted by running:

This article or section needs expansion.

On 4Kn disks (4096 byte physical sector size and 4096 byte logical sector size) all mkfs utilities will use a block size of 4096 bytes. On 512e (4096 byte physical sector size, 512 byte logical sector size) and 512n (512 byte physical sector size and 512 byte logical sector size) disks, each mkfs utility behaves differently.

If the storage device does not report the correct sector size, you can explicitly format the partitions according to the physical sector size.

In particular shingled magnetic recording (SMR) drives that are firmware-managed are severely and negatively impacted if using a logical sector size of 512 bytes if their physical sector size is of 4096 bytes. Those drives have different performance writing zones and remapping reallocation occurs while being idle, but during heavy active writes (e.g., RAID resilvering, backups, writing many small files, rsync, etc.), a different file system sector size could drop write speed to single digit megabytes/second, as the higher performance write areas get depleted, and the sector translation layer gets overworked on the shingled areas.

Here are some examples to set the 4096-byte sector size explicitly:

Syslinux does not support FAT file systems with a sector size other than 512 bytes.

systemd-homed does not work on 4Kn drives and LUKS with a 4096 byte sector size. See systemd issues 30393 and 30394.

This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.

Some NVMe drives do not support the nvme format command, which is required to change the sector size (LBA format). This is still compliant with the NVMe specification. The issue may also present itself as INVALID_OPCODE(2001) or INVALID_FORMAT: The LBA Format specified is not supported. This may be due to various conditions(0x10a). If this is the case, review the links under References below.

To check whether your drive supports the Format NVM command:

Look at the oacs field (Optional Admin Command Support), which is a bitmask. Format NVM support corresponds to bit 1 (value 0x02). If bit 1 is not set, your controller does not support Format NVM and you will not be able to change the LBA format.

Example (Format NVM not supported):

Convert 0x1d to binary: 0001 1101  Bit 1 is not set (second bit from the right is 0)  Format NVM is not supported

In some cases, formatting still fails even when Format NVM is supported. This is commonly seen on drives using Pyrite but notably not OPAL security, especially on Intel platforms using S0ix (Modern Standby). In other cases, oacs may report Format NVM as unsupported, yet formatting works after a suspend/resume cycle.

Known on Hynix PC601, Lenovo and Dell laptops, Samsung and WD SN750 drives.

**Examples:**

Example 1 (unknown):
```unknown
$ lsblk -td
```

Example 2 (unknown):
```unknown
NAME    ALIGNMENT MIN-IO OPT-IO PHY-SEC LOG-SEC ROTA SCHED       RQ-SIZE  RA WSAME
sda             0   4096      0    4096    4096    1 mq-deadline      64 128    0B
nvme1n1         0   4096      0    4096    4096    0 none           1023 128    0B
nvme0n1         0   4096      0    4096    4096    0 none           1023 128    0B
```

Example 3 (unknown):
```unknown
$ cat /sys/class/block/drive/queue/physical_block_size
$ cat /sys/class/block/drive/queue/logical_block_size
```

Example 4 (unknown):
```unknown
# nvme id-ns /dev/nvme0n1 | grep -i lbaf
# nvme id-ctrl /dev/nvme0n1 | grep -i awupf
```

---

## User:Scry3r

**URL:** https://wiki.archlinux.org/title/User:Scry3r

Klimenko Maxim Sergievich - IT administrator from Ukraine, like listen alternative and underground music, always try to understand our history and start maintain, little for now, group of packages.

---

## Kernel mode setting

**URL:** https://wiki.archlinux.org/title/Kernel_mode_setting

**Contents:**
- Background
- Configuration
  - Late KMS start
  - Early KMS start
    - mkinitcpio
    - Booster
    - Dracut
- Troubleshooting
  - My fonts are too tiny
- Forcing modes and EDID

This article or section needs expansion.

Kernel Mode Setting (KMS) is a method for setting display resolution and depth in the kernel space rather than user space.

The Linux kernel's implementation of KMS enables native resolution in the framebuffer and allows for instant console (tty) switching. KMS also enables newer technologies (such as DRI2) which will help reduce artifacts and increase 3D performance, even kernel space power-saving.

Previously, setting up the video card was the job of the X server. Because of this, it was not easily possible to have fancy graphics in virtual consoles. Also, each time a switch from X to a virtual console was made (Ctrl+Alt+F2), the server had to give control over the video card to the kernel, which was slow and caused flickering. The same "painful" process happened when the control was given back to the X server (Alt+F7 when X runs in VT7).

With Kernel Mode Setting (KMS), the kernel is now able to set the mode of the video card. This makes fancy graphics during bootup, virtual console and X fast switching possible, among other things.

At first, note that for any method you use, you should always disable:

This article or section is out of date.

This article or section needs expansion.

Intel, Nouveau, ATI and AMDGPU drivers already enable KMS automatically for all chipsets, so you do not need to do anything.

The proprietary NVIDIA driver supports KMS (since 364.12), which has to be manually enabled.

KMS is typically initialized after the initramfs stage. However, it is possible to enable KMS already during the initramfs stage. Add the required module for the video driver to the initramfs configuration file:

Initramfs configuration instructions are slightly different depending on the initramfs generator you use.

For in-tree modules, make sure kms is included in the HOOKS array in /etc/mkinitcpio.conf (this is the default since mkinitcpio v33).

For out-of-tree modules, place the module names in the MODULES array. For example, to enable early KMS for the NVIDIA graphics driver:

If you are using the #Forcing modes and EDID method, you should embed the custom file into initramfs as well:

Then regenerate the initramfs.

If you use Booster, you can load required modules with this config change:

If you are using the #Forcing modes and EDID method, you should embed the custom file into your booster images as well:

Then regenerate the booster images.

Dracut enables early loading (at the initramfs stage, via modprobe) through its --force_drivers command or force_drivers+="" config entry line. For example:

See Linux console#Fonts for how to change your console font to a large font. The Terminus font (terminus-font) is available in many sizes, such as ter-132b which is larger.

Alternatively, disabling modesetting might switch to lower resolution and make fonts appear larger.

If your native resolution is not automatically configured or no display at all is detected, then your monitor might send none or just a skewed EDID file. The kernel will try to catch this case and will set one of the most typical resolutions.

In case you have the EDID file for your monitor, you merely need to explicitly enforce it (see below). However, most often one does not have direct access to a sane file and it is necessary to either extract an existing one and fix it or to generate a new one.

Generating new EDID binaries for various resolutions and configurations is possible during kernel compilation by following the upstream documentation (also see here for a short guide). Other solutions are outlined in details in this article. Extracting an existing one is in most cases easier, e.g. if your monitor works fine under Windows, you might have luck extracting the EDID from the corresponding driver, or if a similar monitor works which has the same settings, you may use get-edid(1) from the read-edid package. You can also try looking in /sys/class/drm/*/edid.

After having prepared your EDID, place it in a directory, e.g. called edid under /usr/lib/firmware/ and copy your binary into it.

To load it at boot, specify the following in the kernel command line:

In order to apply it only to a specific connector, use:

If you want to set multiple edid files, use:

If you are doing early KMS, you must include the custom EDID file in the initramfs, otherwise you will run into problems.

The value of the drm.edid_firmware parameter may also be altered after boot by writing to /sys/module/drm/parameters/edid_firmware:

This will only take effect for newly plugged in displays, already plugged-in screens will continue to use their existing EDID settings. For external displays, replugging them is sufficient to see the effect however.

To load an EDID after boot, you can use debugfs instead of a kernel command line parameter if the kernel is not in lockdown mode. This is very useful if you swap the monitors on a connector or just for testing. Once you have an EDID file as above, run:

If your monitor supports hotplugging, you can also trigger a hotplug to make the monitor use the new EDID you just loaded (e.g. into edid_override), so you don't have to physically replug the monitor nor reboot:

From the nouveau wiki:

You can override the modes of several outputs using video= several times, for instance, to force DVI to 1024x768 at 85 Hz and TV-out off:

To get the name and current status of connectors, you can use the following shell oneliner:

You may want to disable KMS for various reasons. To disable KMS, add nomodeset as a kernel parameter. See Kernel parameters for more info.

Along with the nomodeset kernel parameter, for an Intel graphics card, you need to add i915.modeset=0, and for an Nvidia graphics card, you need to add nouveau.modeset=0. For Nvidia Optimus dual-graphics system, you need to add all the three kernel parameters (i.e. "nomodeset i915.modeset=0 nouveau.modeset=0").

**Examples:**

Example 1 (unknown):
```unknown
Ctrl+Alt+F2
```

Example 2 (unknown):
```unknown
nvidia nvidia_modeset nvidia_uvm nvidia_drm
```

Example 3 (unknown):
```unknown
<video><model type='type'>
```

Example 4 (unknown):
```unknown
/etc/mkinitcpio.conf
```

---

## Archiving and compression

**URL:** https://wiki.archlinux.org/title/Archiving_and_compression

**Contents:**
- Archiving only
- Compression tools
  - Compression only
  - Archiving and compression
  - Feature charts
    - Decompress
- Usage comparison
  - Archiving only usage
  - Compression only usage
  - Archiving and compression usage

The traditional Unix archiving and compression tools are separated according to the Unix philosophy:

These tools are often used in sequence by firstly creating an archive file and then compressing it.

Of course there are also tools that do both, which tend to additionally offer encryption, error detection and recovery.

See also #Archiving only usage.

These compression programs implement their own file format.

See also #Archiving and compression usage.

Some of the tools above are capable of handling multiple formats, allowing for fewer installed packages.

See also Bash/Functions#Extract.

To extract an archive, its file format needs to be determined. If the file is properly named you can deduce its format from the file extension.

Otherwise you can use the file tool, see file(1).

Some file systems support on-the-fly compression of file data:

The open-sourced VDO project was integrated into the Linux kernel project, which provides a deduplication and compression device mapper layer in the interest of increasing storage efficiency. A userspace tools for managing VDO volumes is available in the AUR: vdoAUR

See Character encoding#Troubleshooting.

**Examples:**

Example 1 (unknown):
```unknown
--use-compress-program=lz4
```

Example 2 (unknown):
```unknown
tar cfv archive.tar file1 file2
```

Example 3 (unknown):
```unknown
tar xfv archive.tar
```

Example 4 (unknown):
```unknown
tar -tvf archive.tar
```

---

## SDDM

**URL:** https://wiki.archlinux.org/title/SDDM

**Contents:**
- Installation
- Configuration
  - Autologin
  - Passwordless login
  - Unlock KDE Wallet automatically on login
  - Theme settings
    - Current theme
    - Editing themes
    - Customizing a theme
    - Testing (previewing) a theme

The Simple Desktop Display Manager (SDDM) is a display manager. It is the recommended display manager for the LXQt desktop environment.

From Wikipedia:Simple Desktop Display Manager:

Install the sddm package. Optionally install sddm-kcm for the KConfig Module and qt5-declarative for enabling Qt5 theme support.

Follow Display manager#Loading the display manager to start SDDM at boot.

The default configuration file for SDDM can be found at /usr/lib/sddm/sddm.conf.d/default.conf. For any changes, create configuration file(s) in /etc/sddm.conf.d/. See sddm.conf(5) for all options.

The sddm-kcm package (included in the plasma group) provides a GUI to configure SDDM in Plasma's system settings. There is also a Qt-based sddm-conf-gitAUR available.

Everything should work out of the box, since Arch Linux uses systemd and SDDM defaults to using systemd-logind for session management.

SDDM supports automatic login through its configuration file, for example:

This configuration causes a KDE Plasma session to be started for user john when the system is booted. Available session types can be found in /usr/share/xsessions/ for X and in /usr/share/wayland-sessions/ for Wayland.

To autologin into KDE Plasma while simultaneously locking the session (e.g. to allow autostarted apps to warm up), see KDE#Lock screen.

It is possible to configure SDDM to allow logging into some accounts without a password. This differs from automatic login in that the user still has to choose which account to log into, and it differs from simply setting the account password to the empty string in that it only allows interactive users to log in (and not, for example, remote users logged in via SSH).

SDDM goes through PAM so you must configure the SDDM configuration of PAM:

In order to also allow unlocking the KDE Plasma lock screen without a password, also add the same line at the top of /etc/pam.d/kde:

You must then also be part of the nopasswdlogin group to be able to login interactively without entering your password:

See KDE Wallet#Unlock KDE Wallet automatically on login.

Theme settings can be changed in the [Theme] section. If you use Plasma's system settings, themes may show previews.

Set to breeze for the default Plasma theme.

Some themes are available in the AUR, for example archlinux-themes-sddmAUR.

Set the current theme through the Current value, e.g. Current=archlinux-simplyblack.

The default SDDM theme directory is /usr/share/sddm/themes/. You can add your custom made themes to that directory under a separate subdirectory. Note that SDDM requires these subdirectory names to be the same as the theme names. Study the files installed to modify or create your own theme.

To override settings in the theme.conf configuration file, create a custom theme.conf.user file in the same directory. For example, to change the theme's background:

You can preview an SDDM theme if needed. This is especially helpful if you are not sure how the theme would look if selected or just edited a theme and want to see how it would look without logging out. You can run something like this:

This should open a new window for every monitor you have connected and show a preview of the theme.

To set the mouse cursor theme, set CursorTheme to your preferred cursor theme.

Valid Plasma mouse cursor theme names are breeze_cursors, Breeze_Snow and breeze-dark. In case SDDM defaults to the Adwaita theme on KDE setups, one should set the Inherits variable in /usr/share/icons/default/index.theme manually.

This article or section is out of date.

SDDM reads the user icon (a.k.a. "avatar") as a PNG image from either ~/.face.icon for each user, or the common location for all users specified by FacesDir in an SDDM configuration file. The configuration setting can be placed in either /etc/sddm.conf directly, or, better, a file under /etc/sddm.conf.d/ such as /etc/sddm.conf.d/avatar.conf.

To use the FacesDir location option, place a PNG image for each user named username.face.icon at the location specified by the FacesDir key in the configuration file. The default location for FacesDir is /usr/share/sddm/faces/. You can change the default FacesDir location to suit your needs. Here is an example:

The other option is to put a PNG image named .face.icon at the root of your home directory. In this case, no changes to any SDDM configuration file is required. However, you need to make sure that sddm user can read the PNG image file(s) for the user icon(s).

To set proper permissions run:

You can check permissions with:

See SDDM README: No User Icon.

If you want to enforce numlock to be enabled, set Numlock=on in the [General] section.

If SDDM is running under Wayland, the numlock setting currently does not work. You may need to change KWin settings to enable it, see this issue.

See Xrandr#Configuration.

Sometimes it is useful to set up the correct monitor's PPI settings on a "Display Manager" level.[1] To do so you need to add to ServerArguments the parameter -dpi your_dpi at the end of the string. For example:

Create the following file:

When using Wayland, the HiDPI scaling depends on the greeter used.[2] For instance, when using a Qt-based greeter such as Breeze, add the following configuration:

Install qt6-virtualkeyboard (or another virtual keyboard).

Create the following file:

SDDM now displays a button in lower-left corner of login screen to open the virtual keyboard.

But that configuration does not include login, only unlocking of existing sessions.

Because KWallet requires passwordbased auth on login (see below), you might still want to skip the setup for fprint with SDDM.

SDDM works with a fingerprint reader when using fprint. After installing fprint and adding fingerprint signatures, add the following to the top of /etc/pam.d/sddm:

In order to use either a password or a fingerprint, you can instead add the following to the top of the file:

Note that KWallet cannot be unlocked using a fingerprint reader (see KDE Wallet#Unlock KDE Wallet automatically on login), but the first line ensures that a password login will automatically unlock KWallet.

If you now press enter in the empty password field, the fingerprint reader should start working.

Traditionally, the X11 display server has been run with root privileges by default. This rootful mode allows X11 to have unrestricted access to system resources, which was necessary for its operation in environments where direct hardware access and management were common. However, with the increasing emphasis on security in modern computing environments, efforts have been made to transition to rootless modes, which is also why Wayland by default runs in rootless mode.

Starting unprivileged X11 (and Wayland) sessions has been supported since sddm 0.20.0.[3]

To enable rootless mode: create a new configuration file under /etc/sddm.conf.d/, name it something meaningful, and add the following to it, replacing x11-user with wayland if necessary.

To confirm whether you are running in rootless mode check which user owns the compositor process (e.g. kwin_wayland)

Adding the following lines to your configuration file sets the Wayland compositor to KWin and enables the wlr_layer_shell Wayland protocol extension. This requires layer-shell-qt and layer-shell-qt5AUR for SDDM themes using Qt6 and Qt5, respectively.

To enable virtual keyboard support (e.g. using qt6-virtualkeyboard or maliit-keyboard), append the --inputmethod flag with the appropriate keyboard to the kwin_wayland command as shown below. Do not set the option InputMethod in the section General, since this will cause the virtual keyboard to no longer be shown.

Changes to your display configuration made in a Plasma Wayland session (e.g. monitor layout, resolution, etc) will not persist to SDDM. To make them persist open Plasma's System Settings and navigate to Colors & Themes > Login Screen (SDDM) and click "Apply Plasma Settings...". You will need to have permission to perform this action.

The same can be achieved manually with the following:

To enable correct display & monitor handling in SDDM (scaling, monitor resolution, hz,...), you can copy or modify the appropriate configuration file from your home directory to the one of SDDM:

To also enable correct input handling in SDDM (tap-to-click, touchscreen mapping,...), you can copy the appropriate configuration file from your home directory to the one of SDDM:

Greeter crashes if there is no available disk space. Check your disk space with df -h.

If disk space is not the issue, it may be due to a bug. Switch to another TTY, and then try loginctl unlock-session session_id or to restart SDDM.

Try removing ~/.Xauthority and logging in again without rebooting. Rebooting without logging in creates the file again and the problem will persist.

SDDM follows the systemd convention of starting the first graphical session on tty1.

Note that the config file still has the option MinimumVT but is ignored since SDDM version 0.20: sddm.conf(5)  MinimumVT=.

By default, SDDM is configured to displays only users with a UID in the range of 1000 to 60513. If the UIDs of the desired users are outside this range then you will have to modify the range.

For example, for a UID of 501, set MinimumUid and hide those with shells used by system users:

For users with a higher UIDs, set MaximumUid to the appropriate value.

User avatars are not shown on the greeter if the number of users exceeds DisableAvatarsThreshold parameter or if avatars are not enabled at all as controlled by EnableAvatars parameter. To circumvent this add the following lines to your sddm configuration:

SDDM loads the keyboard layout specified in /etc/X11/xorg.conf.d/00-keyboard.conf. You can generate this configuration file by localectl set-x11-keymap command. See Keyboard configuration in Xorg for more information.

An alternative way of setting the keyboard layout that will only set it in SDDM and not subsequent sessions is to invoke a setxkbmap command in the startup script of SDDM, located at /usr/share/sddm/scripts/Xsetup. See Xorg/Keyboard configuration#Using setxkbmap for examples.

SDDM may also incorrectly display the layout as US but will immediately change to the correct layout after you start typing your password [4]. This seems to not be a bug in SDDM but in X server.[5]

Issue may be caused by HiDPI usage for monitors with corrupted EDID [6]. If you have enabled HiDPI, try to disable it.

If even the above fails, you can try setting your screen size in a Xorg configuration file:

SDDM by default tries to display avatars of users by accessing ~/.face.icon file. If your home directory is an autofs, for example if you use dm-crypt, this will make it wait for 60 seconds, until autofs reports that the directory cannot be mounted.

You can disable avatars by creating the following configuration file:

SDDM uses a random fresh UUID for the auth file as described in details at [7]. So in order to find that file one may use a script:

This may be needed if one needs to start x11vnc when there is no user logged in. For example:

It happens that the X monitor layout is not setup correctly on multiscreen setup leading to overlapping greeters. To solve this add the following lines to order your sddm greeter layout from left to right:

It can happen that the SDDM login session appears on a different display than your primary display if multiple displays are connected. This problem can be annoying if the secondary display is rotated and the primary display is not. A simple fix to this problem is to use xrandr to configure the displays before the login session using Xsetup script. E.g. here xrandr reports that there are two connected displays where the secondary display (DP-2) is left of the primary display (DP-4).

The following Xsetup recreates the above setup for the login window:

One may encounter a complete black screen or with only cursor/display device logo on it after the logout of any user. This happens because sddm.service starts faster than the NVIDIA drivers. Consider using early KMS.

Use a different theme than the default.

If you set up SDDM with kwin_wayland compositor, one may encounter a screen out of sync when booting. If returning back to X11 works well, and you're using mesa-amber drivers, chances are Wayland will work well by just replacing your current mesa-amber drivers with mesa. You can find more details about this issue at KDE Bug 483804.

Some SDDM themes do not specify QtVersion in metadata.desktop and SDDM starts with incompatible greeter (qt5 instead of qt6).

If you have set custom theme but on reboot there is default theme with error Library import requires a version, you need to add QtVersion=6 to /usr/share/sddm/themes/ThemeName/metadata.desktop.

You could see an SDDM with a 12hr (am/pm) clock where you would expect a 24h clock. This is more likely for people who use a non-native language (like french people using US English) but still expecting units to be in their local format.

This behavior is not determined by SDDM settings, but in /etc/locale.conf. Make sure you have an LC_TIME set with the locale of choice. See Locale for details.

**Examples:**

Example 1 (unknown):
```unknown
/usr/lib/sddm/sddm.conf.d/default.conf
```

Example 2 (unknown):
```unknown
/etc/sddm.conf.d/
```

Example 3 (unknown):
```unknown
systemd-logind
```

Example 4 (unknown):
```unknown
/etc/sddm.conf.d/autologin.conf
```

---

## Frequently asked questions

**URL:** https://wiki.archlinux.org/title/Frequently_asked_questions

**Contents:**
- General
  - What is Arch Linux?
  - Why would I not want to use Arch?
  - Why would I want to use Arch?
  - What architectures does Arch support?
  - Does Arch follow the Linux Foundation's Filesystem Hierarchy Standard (FHS)?
  - I am a complete GNU/Linux beginner. Should I use Arch?
  - Is Arch designed to be used as a server? A desktop? A workstation?
  - I really like Arch, except the development team needs to implement feature X
  - When will the new release be made available?

See the Arch Linux article.

You may not want to use Arch, if:

Because Arch is the best.

Arch only supports the x86_64 (sometimes called amd64) architecture. Support for i686 was dropped in November 2017 [1].

There are derivative distributions for the i686 architecture [2] and ARM CPUs [3], each with their own community channels. They are not supported by Arch Linux.

If you wish for Arch to support other architectures, you can help out with existing porting efforts or start your own. See Getting involved#Help porting Arch Linux to other architectures.

Arch Linux follows the file system hierarchy for operating systems using the systemd service manager. See file-hierarchy(7) for an explanation of each directory along with their designations. In particular, /bin, /sbin, and /usr/sbin are symbolic links to /usr/bin, and /lib and /lib64 are symbolic links to /usr/lib.

If you are a beginner and want to use Arch, you must be willing to invest time into learning a new system, and accept that Arch is designed as a 'do-it-yourself' distribution; it is the user who assembles the system.

Before asking for help, do your own independent research by searching the Web, the forum and the superb documentation provided by the Arch Wiki. There is a reason these resources were made available to you in the first place. Many thousands of volunteered hours have been spent compiling this excellent information.

See also Arch terminology#RTFM and the Installation guide.

Arch is not designed for any particular type of use. Rather, it is designed for a particular type of user. Arch targets competent users who enjoy its 'do-it-yourself' nature, and who further exploit it to shape the system to fit their unique needs. Therefore, in the hands of its target user base, Arch can be used for virtually any purpose. Many use Arch on both their desktops and workstations. And of course, archlinux.org, aur.archlinux.org and almost all of Arch's infrastructure runs on Arch.

Get involved, contribute your code/solution to the community. If it is well-regarded by the community and development team, perhaps it will be merged. The Arch community thrives on contribution and sharing of code and tools.

Arch Linux releases are simply a live environment for installation or rescue, which include the base meta package and a few other packages. The releases are issued usually in the first half of every month.

It is the user who is ultimately responsible for the stability of their own rolling release system. The user decides when to upgrade, and merges necessary changes when required. If the user reaches out to the community, help is often provided in a timely manner. The difference between Arch and other distributions in this regard is that Arch is truly a 'do-it-yourself' distribution; complaints of breakage are misguided and unproductive, since upstream changes are not the responsibility of Arch devs.

See the System maintenance article for tips on how to make an Arch Linux system as stable as possible.

Arch gets plenty of press as it is. The goal of Arch Linux is not to be large; rather, organic, sustainable growth occurs naturally amongst the target user base.

Possibly so. Feel free to volunteer your time! Visit the forums, IRC channels, and mailing lists, and see what needs to be done. See also Getting involved for details.

A guided installer with a text-based user interface is available. See archinstall for details.

See General recommendations.

Since many are available to you, use the one that best fits your needs. Have a look at the Desktop environment and Window manager articles.

See Arch compared to other distributions.

See also System maintenance.

Is your network configured correctly? Have a look at the Network configuration article. For advanced setups, you may also want to look at traffic shaping.

One of the most commonly used kernels, linux, tends to be newer than the kernel of other, more stable, Linux distributions. Because of that, you may rarely experience a kernel regression or driver bug, especially if using Wi-Fi. Do note that the vast majority of those bugs are not Arch Linux-specific as Arch Linux only applies the most basic of patches. This needs to be taken upstream. See #I have found an error with package X. What should I do?.

Essentially, unused RAM is wasted RAM.

Many new users notice how the Linux kernel handles memory differently than they are used to. Since accessing data from RAM is much faster than from a storage drive, the kernel caches recently accessed data in memory. The cached data is only cleared when the system begins to run out of available memory and new data needs to be loaded.

We could distinguish the difference from free command:

It is important to note the difference between "free" and "available" memory. In the above example, a system with 377 GiB of total RAM appears to be using more than half of it, with only 146 GiB as free memory. However, 196 GiB of it is "buff/cache". There is still 337 GiB available for starting new applications, without swapping. See free(1) for details. The result of all this? Performance!

See this wonderful article if your curiosity has been piqued. There is also a website dedicated to clearing this confusion: https://www.linuxatemyram.com/.

The answer to this question depends on your system. There are some fine utilities that may help you find the answer.

Have you mistyped your password or cancelled a sudo command three times in fifteen minutes? If so, you have triggered a prevention mechanism against brute-force attacks: see Security#Lock out user after three failed login attempts for more details.

You may want to voluntarily "phone home" by contributing to the pkgstats project that collects anonymous data of package popularity to help Arch developers prioritize their efforts.

See the pacman, pacman/Tips and tricks and Official repositories pages for more answers.

First, you need to figure out if this error is something the Arch team can fix. Often it is not (e.g. Firefox crashes may be the fault of the Mozilla team); this is called an upstream error, see Bug reporting guidelines#Upstream or Arch?. If it is an Arch problem, there is a series of steps you can take:

This has been discussed on the Arch mailing list. Some proposed a .pac file extension, but there is no plan to change the package extension. As Tobias Kieslich, one of the Arch developers, put it, "A package is a [compressed] tarball! And it can be opened, investigated and manipulated by any tar-capable application. Moreover, the mime-type is automatically detected correctly by most applications."

Pacman is a front-end to libalpm(3)the "Arch Linux Package Management" librarywhich allows alternative front-ends, like a GUI front-end, to be written.

If you think an idea has merit, you may choose to discuss it on pacman-dev. Also check https://gitlab.archlinux.org/pacman/pacman/-/issues for existing feature requests.

However, the best way to get a feature added to pacman or Arch Linux is to implement it yourself. The patch or code may or may not be officially accepted, but perhaps others will appreciate, test and contribute to your effort.

If you are using a desktop environment like KDE or GNOME, the program should automatically show up in your menu if it comes with a desktop entry. If you are trying to run the program from a terminal and do not know the binary name, use:

Several distributions, such as Debian, have different versions of shared libraries packaged as different packages: libfoo1, libfoo2, libfoo3 and so on. In this way it is possible to have applications compiled against different versions of libfoo installed on the same system.

In case of a distribution like Arch, only the latest packaged versions are officially supported. By dropping support for outdated software, package maintainers are able to spend more time ensuring that the newest versions work as expected. As soon as a new version of a shared library becomes available from upstream, it is added to the repositories and affected packages are rebuilt to use the new version.

This scenario should not happen at all. Assuming an application called foobaz is in one of the official repositories and builds successfully against a new version of a shared library called libbaz, it will be updated along with libbaz. If, however, it does not build successfully, foobaz package will have a versioned dependency (e.g. libbaz 1.5), and will be removed by pacman during libbaz upgrade, due to a conflict.

If foobaz is a package that you built yourself and installed from AUR, you should rebuild foobaz against the new version of libbaz. If the build fails, report the bug to the foobaz developers.

No, it is not possible. Major kernel updates (e.g. linux 3.5.0-1 to linux 3.6.0-1) are always accompanied by rebuilds of all supported kernel driver packages. On the other hand, if you have an unsupported driver package (e.g. from the AUR) installed on your system, then a kernel update might break things for you if you do not rebuild it for the new kernel. Users are responsible for updating any unsupported driver packages that they have installed.

Follow the System maintenance#Upgrading the system section.

pacman mirrors are not synced immediately. It may take over 24 hours before an update is available to you. The only options are be patient or use another mirror. MirrorStatus can help you identify an up-to-date mirror.

Package updates will be released when they are ready. The specific amount of time can be as short as a few hours after upstream releases a minor bugfix update to as long as several weeks after a large package group's major update. The amount of time from an upstream's new version to Arch releasing a new package depends on the specific packages and the availability of the package maintainers. Additionally, some packages spend some time in the testing repository, so this can prolong the time before a package is updated. Package maintainers attempt to work quickly to bring stable updates to the repositories. If you find a package in the official repositories that is out of date, go to that package's page at the package website and flag it.

If you are lucky, it might work, for a time. Regardless, it is not a proper solution, because:

Instead, e.g. use/write a compat (compatibility) package, which provides the required library version.

If your processor is x86_64 compatible, you will have the lm (long mode) flag in /proc/cpuinfo. For example,

Under Windows, using the freeware CPU-Z helps determine whether your CPU is 64-bit compatible. CPUs with AMD's instruction set "AMD64" or Intel's solution "EM64T" should be compatible with the x86_64 releases and binary packages.

It is faster under most circumstances and as an added bonus also inherently more secure due to the nature of Address space layout randomization (ASLR) in combination with Position-independent code (PIC) and the NX Bit which is not available in the stock i686 kernel due to disabled Physical Address Extension (PAE). If your computer has more than 4 GiB of RAM, only a 64-bit OS will be able to fully utilize it.

Programmers also increasingly tend to care less about 32-bit ("legacy") as "new" x86 CPUs typically support the 64-bit extensions.

There are many more reasons we could list here to tell you to avoid 32-bit, but between the kernel, userspace and individual programs it is simply not viable to list every last thing that 64-bit does much better these days.

**Examples:**

Example 1 (unknown):
```unknown
total        used        free      shared  buff/cache   available
Mem:           377Gi        40Gi       146Gi       1.1Gi       196Gi       337Gi
Swap:          377Gi       1.1Gi       376Gi
```

Example 2 (unknown):
```unknown
$ pacman -Qlq package_name | grep /usr/bin/
```

Example 3 (unknown):
```unknown
/proc/cpuinfo
```

Example 4 (unknown):
```unknown
$ grep -w lm /proc/cpuinfo
```

---

## XDG Autostart

**URL:** https://wiki.archlinux.org/title/XDG_Autostart

**Contents:**
- Prerequisites
- Directories

The XDG Autostart specification defines a method for autostarting ordinary desktop entries on desktop environment startup and removable medium mounting, by placing them in specific #Directories.

You need to use either a desktop environment that supports it, or a dedicated implementation, like:

The Autostart directories in order of preference are:

System-wide desktop entries can be overridden by user-specific entries with the same filename.

For more details, read the specification.

**Examples:**

Example 1 (unknown):
```unknown
$XDG_CONFIG_HOME/autostart
```

Example 2 (unknown):
```unknown
~/.config/autostart
```

Example 3 (unknown):
```unknown
$XDG_CONFIG_DIRS/autostart
```

Example 4 (unknown):
```unknown
/etc/xdg/autostart
```

---

## twm

**URL:** https://wiki.archlinux.org/title/Twm

**Contents:**
- Installation
- Starting
- Configuration
  - .twmrc examples
- Tips and tricks
  - Patched version
- Troubleshooting
  - Oversized window titles and menus
- See also

twm is a window manager for Xorg. It is a small program, being built against Xlib rather than using a widget library, and as such, it is very light on system resources. Though simple, it is highly configurable; fonts, colours, border widths, title bar buttons, etc. can all be set by the user.

twm was written by Tom LaStrange, a developer who was frustrated by the limitations of uwm (Ultrix Window Manager), the only window manager around when X11 was first released. [1]

twm supplanted uwm as the default window manager supplied with X11 from the X11R4 release in 1989. [2]

twm has stood for Tom's Window Manager, Tab Window Manager and more recently Timeless Window Manager.

twm is installed through the xorg-twm package.

You can also start twm with a display manager. The twm.desktop file does not exist so we have to create it at /usr/share/xsessions/. In the newly created /usr/share/xsessions/twm.desktop file, copy and paste:

By default, twm looks very dated and unintuitive. By creating the file ~/.twmrc, you can customize twm to make it more friendly.

twm(1) gives full details of the commands which can be used in your ~/.twmrc file.

Many ~/.twmrc files have been posted online. Some examples include:

A Google search for "twmrc" can also be used to find new ideas.

There is a patched version, not in the repositories, with updated features such as transparency. A description and build script is available on the xorg mailing list. It can be tried out by installing xcompmgr, running the build script, putting the resulting twm and dot.twmrc files in a convenient directory, and editing the ~/.xinitrc file so that the last two lines are

You might find that titlebars and menu entries in TWM are extremely large - twice the size that one might typically expect. This is a locale issue with TWM that occurs when a UTF-8 locale is used. Setting the locale to C fixes the issue. See [3].

**Examples:**

Example 1 (unknown):
```unknown
twm.desktop
```

Example 2 (unknown):
```unknown
/usr/share/xsessions/
```

Example 3 (unknown):
```unknown
/usr/share/xsessions/twm.desktop
```

Example 4 (unknown):
```unknown
/usr/share/xsessions/twm.desktop
```

---

## Wake-on-LAN

**URL:** https://wiki.archlinux.org/title/Wake-on-LAN

**Contents:**
- Hardware settings
- Software configuration
  - Enable WoL on an Ethernet network adapter
  - Enable WoL on a WiFi network adapter
  - Make it persistent
    - systemd.link
    - systemd service
    - udev
    - cron
    - netctl

Wake-on-LAN (WoL) is a feature to switch on a computer via the network.

Wake-On-LAN only works if you fulfill the following preconditions:

You have to prepare the following in your BIOS/UEFI:

Depending on the hardware, the network driver may have WoL switched off by default.

To query this status or to change the settings, install ethtool, determine the name of the network interface, and query it using the command:

The Wake-on values define what activity triggers wake up: d (disabled), p (PHY activity), u (unicast activity), m (multicast activity), b (broadcast activity), a (ARP activity), and g (magic packet activity). The value g is required for WoL to work, if not, the following command enables the WoL feature in the driver:

This command might not last beyond the next reboot and in this case must be repeated via some mechanism. Common solutions are listed in the following subsections.

Depending on the hardware, the network driver may have WoL switched off by default.

To query this status or to change the settings, install iw, and query it using the command (interface will probably be phy0):

To enable wowlan you need to pick several features like magic-packet and disconnect to make WoL work:

This command might not last beyond the next reboot and in this case must be repeated via some mechanism. Common solutions are listed in the following subsections.

Link-level configuration is possible through systemd.link files. The actual setup is performed by the net_setup_link udev builtin. Add the WakeOnLan option to the network link file:

Also see systemd.link(5) for more information.

This is an equivalent of previous systemd.link option, but uses a standalone systemd service.

Alternatively install the wol-systemdAUR package, then activate this new service by starting wol@interface.service.

udev is capable of running any command as soon as a device is visible. The following rule will turn on WOL on all network interfaces whose name matches en*. The file name is important and must start with a number between 81 and 99 so that it runs after 80-net-setup-link.rules, which renames interfaces with predictable names. Otherwise, NAME would be undefined and the rule would not run.

The $name placeholder will be replaced by the value of the NAME variable for the matched device.

A command can be run each time the computer is (re)booted using "@reboot" in a crontab. First, make sure cron is enabled, and then edit a crontab for the root user that contains the following line:

If using netctl, one can make this setting persistent by adding the following the netctl profile:

NetworkManager provides Wake-on-LAN ethernet support. One way to enable Wake-on-LAN by magic packet is through nmcli.

First, search for the name of the wired connection:

By following, one can view current status of Wake-on-LAN settings:

Enable Wake-on-LAN by magic packet on that connection:

Then reboot, possibly two times. To disable Wake-on-LAN, substitute magic with ignore.

The Wake-on-LAN settings can also be changed from the GUI using nm-connection-editor.

You can disable Wake-on-LAN for all connections permanently by adding a dedicated configuration file :

When using TLP for suspend/hibernate, the WOL_DISABLE setting should be set to N in /etc/tlp.conf to allow resuming the computer with WoL.

To trigger WoL on a target machine, its MAC address must be known. To obtain it, execute the following command from the machine:

Here the MAC address is 48:05:ca:09:0e:6a.

In its simplest form, Wake-on-LAN broadcasts the magic packet as an ethernet frame, containing the MAC address within the current network subnet, below the IP protocol layer. The knowledge of an IP address for the target computer is not necessary, as it operates on layer 2 (Data Link).

If used to wake up a computer over the internet or in a different subnet, it typically relies on the router to relay the packet and broadcast it. In this scenario, the external IP address of the router must be known. Keep in mind that most routers by default will not relay subnet directed broadcasts as a safety precaution and need to be explicitly told to do so.

Applications that are able to send magic packets for Wake-on-LAN:

If you are connected directly to another computer through a network cable, or the traffic within a LAN is not firewalled, then using Wake-on-LAN should be straightforward since there is no need to worry about port redirects.

In the simplest case the default broadcast address 255.255.255.255 is used:

To broadcast the magic packet only to a specific subnet or host, use the -i switch:

When the source and target computers are separated by a NAT router, different solution can be envisaged:

Otherwise Wake-on-LAN can be achieved via port forwarding. The router needs to be configured using one of these two options:

In any case, run the following command from the source computer to trigger wake-up:

In order to make sure the WoL packets reach the target computer, one can listen to the UDP port, usually port 9, for magic packets. The magic packet frame expected contains 6 bytes of FF followed by 16 repetitions of the target computer's MAC (6 bytes each) for a total of 102 bytes.

This can be performed by installing openbsd-netcat on the target computer and using the following command:

Then wait for the incoming traffic to appear in the nc terminal.

Install ngrep on the target computer and type the following command:

Here is a script that illustrates the use of wol with different machines:

Setting auto negotiation to yes may help if WOL is configured through nmcli and network adapter is still powered off on shutdown.

It is known that some motherboards are affected by a bug that can cause immediate or random wake-up after a shutdown whenever the BIOS WoL feature is enabled (as discussed in this thread for example).

The following actions in the BIOS preferences can solve this issue with some motherboards:

The issue can also be solved by adding the following kernel boot parameter: xhci_hcd.quirks=270336 This activates the following quirks:

Some laptops have a battery draining problem after shutdown [1]. This might be caused by enabled WOL. To solve this problem, disable it by using ethtool as mentioned above.

Users with a Realtek 8168 8169 8101 8111(C) based NICs (cards / and on-board) may notice a problem where the NIC seems to be disabled on boot and has no link light. See Network configuration/Ethernet#Realtek no link / WOL problem.

If the link light on the network switch is enabled when the computer is turned off but Wake on LAN is still not working, booting the system using the r8168AUR kernel module at least once and then switching back to the r8169 kernel module included with the kernel has been reported to fix it.

For the r8168 module you might need to set the s5wol=1 kernel module parameter to enable the wake on LAN functionality.

Users with a Realtek 8125 NIC have reported being unable to use the Wake on LAN feature with the r8169 kernel module. Installing r8125-dkmsAUR enables the functionality.

Make sure the correct kernel driver is in use with lspci -k. If not, blacklist the r8169 module.

Additionally, it might be necessary to enable WOL support and disable power saving features of the card.

For some newer Atheros-based NICs (such as Atheros AR8161 and Killer E2500), WOL support has been disabled in the mainline alx module due to a bug causing unintentional wake-up (see this patch discussion). A patch can be applied (or installed as a DKMS module using the alx-wol-dkmsAUR package) which both restores WOL support and fixes the underlying bug, as outlined in this thread.

See also the pre-patched sources in [2].

**Examples:**

Example 1 (unknown):
```unknown
# ethtool interface | grep Wake-on
```

Example 2 (unknown):
```unknown
Supports Wake-on: pumbag
Wake-on: d
```

Example 3 (unknown):
```unknown
# ethtool -s interface wol g
```

Example 4 (unknown):
```unknown
# iw interface wowlan show
```

---

## Readline

**URL:** https://wiki.archlinux.org/title/Readline

**Contents:**
- Installation
- Editing mode
  - Mode indicator in prompt
  - Different cursor shapes for each mode
- Fast word movement
- History
- Faster completion
- Colorized completion
- Macros
- Disabling control echo

Readline is a library by the GNU Project, used by Bash and other CLI programs to edit and interact with the command line. See readline(3) for more information.

The readline package is most likely already installed as a dependency of Bash.

By default Readline uses Emacs style shortcuts for interacting with command line. However, vi style editing interface is also supported by adding the following to ~/.inputrc:

Alternatively, to set it only for Bash by adding the following line to ~/.bashrc:

Vi-style editing has two modes: command and insert. You can display which one is currently active by adding the following option:

This will print a string in your prompt ((cmd)/(ins) by default) that can be customized with the vi-ins-mode-string and vi-cmd-mode-string variables.

You can set a different cursor shape for each mode by using "\1 .. \2" escapes:

This will set a block shaped cursor when in command mode and a pipe cursor when in insert mode. Note that you must have the mode indicator enabled for this to work (see #Mode indicator in prompt).

The Virtual Console uses different escape codes, so you should check first which term is being used:

See software cursor for VGA for further details.

Xterm supports moving between words with Ctrl+Left and Ctrl+Right by default. To achieve this effect with other terminal emulators, find the correct terminal codes[dead link 2025-08-16domain name not resolved], and bind them to backward-word and forward-word in ~/.inputrc.

For example, for urxvt:

Another example for macOS style (Alt+Left and Alt+Right) word moving:

The history configuration is split into:

Usually, pressing the up arrow key will cause the last command to be shown regardless of the command that has been typed so far. However, users may find it more practical to list only past commands that match the current input.

For example, if the user has typed the following commands:

In this situation, when typing ls and pressing the up arrow key, current input will be replaced with man mount, the last performed command. If the history search has been enabled, only past commands beginning with ls (the current input) will be shown, in this case ls /usr/src/linux-2.6.15-ARCH/kernel/power/Kconfig.

You can enable the history search mode by adding the following lines to /etc/inputrc or ~/.inputrc:

If you are using vi mode, you can add the following lines to the readline configuration file (from BBS#54972):

If you chose to add these lines to ~/.inputrc, it is recommended that you also add the following line at the beginning of this file to avoid strange things like BBS#112537:

Alternatively, one can use reverse-search-history (incremental search) by pressing Ctrl+r, which does not search based on previous input but instead jumps backwards in the history buffer as commands are typed in a search term. Pressing Ctrl+r again during this mode will display the previous line in the buffer that matches the current search term, while pressing Ctrl+g (abort) will cancel the search and restore the current input line. So in order to search through all previous mount commands, press Ctrl+r, type 'mount' and keep pressing Ctrl+r until the desired line is found.

The forward equivalent to this mode is called forward-search-history and is bound to Ctrl+s by default. Beware that most terminals override Ctrl+s to suspend execution until Ctrl+q is entered. (This is called XON/XOFF flow control). For activating forward-search-history, either disable flow control by issuing:

or use a different key in inputrc. For example, to use Alt+s which is not bound by default:

When performing tab completion, a single tab attempts to partially complete the current word. If no partial completions are possible, a double tab shows all possible completions.

The double tab can be changed to a single tab by setting:

Or you can set it such that a single tab will perform both steps: partially complete the word and show all possible completions if it is still ambiguous:

You can enable coloring of completion of filenames with the colored-stats option. You can also color the identical prefix of completion-lists with colored-completion-prefix. For example:

Readline also supports binding keys to keyboard macros, for example:

To keep it permanently, add the part within single quotes to readline's configuration file:

Now type a line and press Alt+w. Readline will act as though Ctrl+e (end-of-line) had been pressed, appended with ' # macro'.

Use any of the existing keybindings within a readline macro, which can be quite useful to automate frequently used idioms. For example, this one makes Ctrl+Alt+l append | less to the line and run it (Ctrl+m is equivalent to Enter):

The next one prefixes the line with yes | when pressing Ctrl+Alt+y, confirming any yes/no question the command might ask:

This example wraps the line in su -c '' and runs it, if Alt+s is pressed:

This example prefixes the line with sudo , if Alt+s is pressed. It lets you review the result and will not input the Enter key.

As a last example, quickly send a command in the background with Ctrl+Alt+b, discarding all of its output:

Readline causes the terminal to echo ^C after Ctrl+c is pressed. For users who wish to disable this, simply add the following to ~/.inputrc:

By default, bracketed paste mode is on. It can be set manually in ~/.inputrc:

This ensures that pasting into Readline inserts the clipboard as single string of characters, instead of inserting characters as if they were entered from the keyboard. This is a safety measure to prevent Readline from automatically modifying and running pasted commands.

**Examples:**

Example 1 (unknown):
```unknown
set editing-mode vi
```

Example 2 (unknown):
```unknown
set show-mode-in-prompt on
```

Example 3 (unknown):
```unknown
vi-ins-mode-string
```

Example 4 (unknown):
```unknown
vi-cmd-mode-string
```

---

## nftables

**URL:** https://wiki.archlinux.org/title/Nftables

**Contents:**
- Installation
  - Front-ends
- Usage
  - Simple firewall
- Configuration
  - Tables
    - Create table
    - List tables
    - List chains and rules in a table
    - Delete table

nftables is a netfilter project that aims to replace the existing {ip,ip6,arp,eb}tables framework. It provides a new packet filtering framework, a new user-space utility (nft), and a compatibility layer for {ip,ip6}tables. It uses the existing hooks, connection tracking system, user-space queueing component, and logging subsystem of netfilter.

It consists of three main components: a kernel implementation, the libnl netlink communication and the nftables user-space front-end. The kernel provides a netlink configuration interface, as well as run-time rule-set evaluation, libnl contains the low-level functions for communicating with the kernel, and the nftables front-end is what the user interacts with via nft.

You can also visit the official nftables wiki page for more information.

Install the userspace utilities package nftables.

Alternatively, installing iptables-nft, which includes nftables as a dependency, will automatically uninstall iptables (an indirect dependency of the base meta package) and prevent conflicts between iptables and nftables when used together. See #Using iptables-nft for details.

nftables makes no distinction between temporary rules made in the command line and permanent ones loaded from or saved to a file.

All rules have to be created or loaded using nft command line utility.

Refer to #Configuration section on how to use.

Current ruleset can be printed with:

Remove all ruleset leaving the system with no firewall:

Read ruleset from /etc/nftables.conf by restarting nftables.service.

nftables comes with a simple and secure firewall configuration stored in the /etc/nftables.conf file.

The nftables.service will load rules from that file when started or enabled.

nftables user-space utility nft performs most of the rule-set evaluation before handing rule-sets to the kernel. Rules are stored in chains, which in turn are stored in tables. The following sections indicate how to create and modify these constructs.

To read input from a file use the -f/--file option:

Note that any rules already loaded are not automatically flushed.

See nft(8) for a complete list of all commands.

This article or section needs expansion.

Tables hold #Chains. Unlike tables in iptables, there are no built-in tables in nftables. The number of tables and their names is up to the user. However, each table only has one address family and only applies to packets of this family. Tables can have one of five families specified:

ip (i.e. IPv4) is the default family and will be used if family is not specified.

To create one rule that applies to both IPv4 and IPv6, use inet. inet allows for the unification of the ip and ip6 families to make defining rules for both easier.

See nft(8)  ADDRESS FAMILIES for a complete description of address families.

In all of the following, family_type is optional, and if not specified is set to ip.

The following adds a new table:

To list all chains and rules of a specified table:

For example, to list all the rules of the my_table table of the inet family:

This will destroy all chains in the table.

To flush/clear all rules from a table:

The purpose of chains is to hold #Rules. Unlike chains in iptables, there are no built-in chains in nftables. This means that if no chain uses any types or hooks in the netfilter framework, packets that would flow through those chains will not be touched by nftables, unlike iptables.

Chains have two types. A base chain is an entry point for packets from the networking stack, where a hook value is specified. A regular chain may be used as a jump target for better organization.

See the traffic flow diagram showing the ordering between individual hooks. Within a given hook, netfilter performs operations in order of increasing numerical priority.

In all of the following family_type is optional, and if not specified is set to ip.

To add a base chain it is mandatory to specify type, hook and priority values:

chain_type can be filter, route, or nat.

For IPv4/IPv6/Inet address families hook_type can be prerouting, input, forward, output, or postrouting. See nft(8)  CHAINS for a list of supported family_type, chain_type and hook_type combinations.

priority_value takes either a priority name or an integer value. See nft(8)  CHAINS for a list of standard priority names and values. Chains with lower numbers are processed first and can be negative. [5]

Optionally base chains can have a policy (drop or the default accept), to define what happens to packets not explicitly accepted or refused in contained rules.

For example, to add a base chain that filters input packets:

Replace add with create in any of the above to add a new chain but return an error if the chain already exists.

The following adds a regular chain named chain_name to the table named table_name:

For example, to add a regular chain named my_tcp_chain to the my_table table of the inet address family:

The following lists all chains without rules (see #List rules) of a family_type:

For example, the following lists the chains of ipv6:

if you omit the family_type all chains are printed.

To edit a chain, simply call it by its name and define the rules you want to change.

For example, to change the my_input chain policy of the default table from accept to drop

The chain must not contain any rules or be a jump target.

To flush rules from a chain:

Rules are either constructed from expressions or statements and are contained within chains.

To add a rule to a chain:

The rule is appended at handle_value, which is optional. If not specified, the rule is appended to the end of the chain.

The --handle switch, which can be added to any valid list command, must be used to determine a rule handle. This switch tells nft to list the handles in its output. The --numeric argument is useful for viewing some numeric output, like unresolved IP addresses.

To prepend the rule to the position:

If handle_value is not specified, the rule is prepended to the chain.

Typically a statement includes some expression to be matched and then a verdict statement. Verdict statements include accept, drop, queue, continue, return, jump chain_name, and goto chain_name. Other statements than verdict statements are possible. See nft(8) for more information.

There are various expressions available in nftables and, for the most part, coincide with their iptables counterparts. The most noticeable difference is that there are no generic or implicit matches. A generic match was one that was always available, such as --protocol or --source. Implicit matches were protocol-specific, such as --sport when a packet was determined to be TCP.

The following is an incomplete list of the matches available:

The following is an incomplete list of match arguments (for a more complete list, see nft(8)):

The following lists all rules of a chain:

For example, the following lists the rules of the chain named my_output in the inet table named my_table:

Individual rules can only be deleted by their handles. Obtaining the handles was shown at #Add rule. Assuming

All the chains in a table can be flushed with the nft flush table command. Individual chains can be flushed using either the nft flush chain or nft delete rule commands.

The first command flushes all of the chains in the ip table_name table. The second flushes the chain_name chain in the family_type table_name table. The third deletes all of the rules in chain_name chain in the family_type table_name table.

Sets are named or anonymous, and consist of one or more elements, separated by commas, enclosed by curly braces. Anonymous sets are embedded in rules and cannot be updated, you must delete and re-add the rule. E.g., you cannot just remove "http" from the dports set in the following:

Named sets can be updated, and can be typed and flagged. sshguard uses named sets for the IP addresses of blocked hosts.

To add or delete elements from the set, use:

Note the type ipv4_addr can include a CIDR netmask (the /32 here is not necessary, but is included for completeness' sake). Note also, the set defined here by TABLE ip sshguard { SET attackers } is addressed as ip sshguard attackers.

Flush the current ruleset:

Dump the current ruleset:

Now you can edit /tmp/nftables and apply your changes with:

When using jumps in configuration file, it is necessary to define the target chain first. Otherwise one could end up with Error: Could not process rule: No such file or directory.

If your box has more than one network interface, and you would like to use different rules for different interfaces, you may want to use a "dispatching" filter chain, and then interface-specific filter chains. For example, let us assume your box acts as a home router, you want to run a web server accessible over the LAN (interface enp3s0), but not from the public internet (interface enp2s0), you may want to consider a structure like this:

Alternatively you could choose only one iifname statement, such as for the single upstream interface, and put the default rules for all other interfaces in one place, instead of dispatching for each interface.

nftables has a special keyword masquerade "where the source address is automagically set to the address of the output interface" (source). This is particularly useful for situations in which the IP address of the interface is unpredictable or unstable, such as the upstream interface of routers connecting to many ISPs. Without it, the Network Address Translation rules would have to be updated every time the IP address of the interface changed.

Example for a machine with two interfaces: LAN connected to enp3s0, and public internet connected to enp2s0:

Since the table type is inet both IPv4 and IPv6 packets will be masqueraded. If you want only ipv4 packets to be masqueraded (since extra adress space of IPv6 makes NAT not required) meta nfproto ipv4 expression can be used infront of oifname "enp2s0" or the table type can be changed to ip.

This example will masquerade traffic exiting through a WAN interface called eth0 and forward ports 22 and 80 to 10.0.0.2. You will need to set net.ipv4.ip_forward to 1 via sysctl.

Use this snippet to count HTTPS connections:

To print the counters, run nft list set inet filter https.

Use this snippet to drop all HTTPS connections for 1 minute from a source IP (or /64 IPv6 range) that exceeds the limit of 10/second.

To print the blackholed IPs, run nft list set inet dev blackhole_ipvX.

The output of nft list ruleset command is a valid input file for it as well. Current rule set can be saved to file and later loaded back in.

See Simple stateful firewall for more information.

Flush the current ruleset:

Add the input, forward, and output base chains. The policy for input and forward will be to drop. The policy for output will be to accept.

Add two regular chains that will be associated with tcp and udp:

Related and established traffic will be accepted:

All loopback interface traffic will be accepted:

Drop any invalid traffic:

Accept ICMP and IGMP:

New udp traffic will jump to the UDP chain:

New tcp traffic will jump to the TCP chain:

At this point you should decide what ports you want to open to incoming connections, which are handled by the TCP and UDP chains. For example to open connections for a web server add:

To accept HTTPS connections for a webserver on port 443:

To accept SSH traffic on port 22:

To accept incoming DNS requests:

Be sure to make your changes permanent when satisfied.

Sshguard is program that can detect brute-force attacks and modify firewalls based on IP addresses it temporarily blacklists. See Sshguard#nftables on how to set up nftables to be used with it.

You can log packets using the log action. The most simple rule to log all incoming traffic is:

See nftables wiki for details.

Listen to all events, report in native nft format.

meta nftrace set 1 ruleset packet tracing on/off. Use monitor trace command to watch traces.

In another shell "include" the file inside the interactive shell:

Example, adjust to your needs:

This file adds a temporary table (flags owner) so that it gets automatically removed, if the calling (process) interactive nft is closed. The Base Chain needs to be adjusted for your use case. You can create multiple chains and multiple rules with "meta nftrace set 1" "ip protocol icmp" is used just as an example and is not necessary. There are many ways to achieve a similar effect, the advantage is that by closing the interactive shell the previous state is automatically restored, and if an error is inside the file nothing gets executed.

See nftables wiki and a python tool automating the process and coloring.

The factual accuracy of this article or section is disputed.

The older iptables language remains quite dominant in Linux documentation, and quite a few things still depend on iptables to run (such as Docker's networking). Although it's also perfectly workable to use legacy iptables and nftables at the same time, using iptables-nft's translation is preferred because:

There are two ways to use the old iptables language with nftables:

systemd-networkd's connections can use the NFTSet= option to populate predefined named sets with host IP addresses, network prefixes and interface indexes. This allows to avoid hardcoding them in /etc/nftables.conf. The NFTSet= option is supported in [Address], [DHCPv4], [DHCPv6] and [IPv6AcceptRA] sections. See systemd.network(5)  [ADDRESS] SECTION OPTIONS.

For example, to process connections from a local network (where IP addresses are assigned via DHCP or SLAAC) in a separate my_input_lan chain:

Using nftables can interfere with Docker networking (and probably other container runtimes as well). You can find various workarounds on the internet which either involve patching iptables rules and ensuring a defined service start order or disabling dockers iptables management completely which makes using docker very restrictive (think port forwarding or docker-compose).

A reliable method is letting docker run in a separate network namespace where it can do whatever it wants. It is probably best to not use iptables-nft to prevent docker from mixing nftables and iptables rules.

Use the following docker service drop-in file:

Adjust the 10.0.0.* IP addresses if they are not appropriate for your setup.

Enable IP forwarding and set-up NAT for docker0 with the following postrouting rule:

Then, ensure that kernel IP forwarding is enabled.

Now you can setup a firewall and port forwarding for the docker0 interface using nftables without any interference.

**Examples:**

Example 1 (unknown):
```unknown
iptables-legacy
```

Example 2 (unknown):
```unknown
iptables-nft
```

Example 3 (unknown):
```unknown
# nft list ruleset
```

Example 4 (unknown):
```unknown
# nft flush ruleset
```

---

## Universal 2nd Factor

**URL:** https://wiki.archlinux.org/title/Universal_2nd_Factor

**Contents:**
- Authentication for websites
  - Firefox
  - Chromium/Chrome
- Authentication for user sessions
  - Installing the PAM module
  - Adding a key
  - Passwordless sudo
  - GDM login
  - SDDM/KDE
  - Other authentication methods

Universal 2nd Factor (U2F) is an open standard that strengthens and simplifies two-factor authentication (2FA) using specialized USB or NFC devices based on similar security technology found in smart cards.

While initially developed by Google and Yubico, with contribution from NXP Semiconductors, the standard is now hosted by the FIDO Alliance.

For all articles on U2F and U2F-devices see: Category:Universal 2nd Factor.

WebAuthn is a more recent standard.

U2F is supported by major sites like Google, Facebook, Twitter, or GitHub. Check out 2fa.directory or dongleauth.com to find other websites and links to setup documentation. For all browsers which support it, likely the only action required is to install libfido2. Yubico offers a demo page for testing.

Firefox/Tweaks#Fido U2F authentication

Chromium/Tips and tricks#U2F authentication

Yubico, the company creating the YubiKey, develops an U2F PAM module. It can be used to act as a second factor during login or replace the need for a password entirely.

The module is part of the package pam-u2f.

Keys need to be added with the tool pamu2fcfg:

After entering your PIN, click the button of your U2F key to confirm the key.

If you own more than one key, append the next ones with

as the first line. Be sure to replace the hostname as mentioned above. Then create a new terminal and type sudo ls. Your key's LED should flash and after clicking it the command is executed. The option cue is set to provide indication of what to do, i.e. Please touch the device.

In order to make the token the only method of sudo (ie. no password fallback) you will need to comment out the other auth methods present. This is usually just the default system-auth include.

You should also change sufficient to required in the above pam_u2f.so line.

after the existing auth lines. Please note the use of the nouserok option which allows the rule to fail if the user did not configure a key. This way setups with multiple users where only some of them use a U2F key are supported.

Some multi-function security keys (ex. Trezor Model T) which can do U2F / PAM may not advertise the feature on system boot which is intentionally out of the CTAP 2.0 specification[2]. With multiple U2F keys present, this can result in two-minute long delays when used with GDM as pam-u2f does sequential lookups and will wait for the (Trezor) device to timeout before offering presence / touch with the secondary U2F key [3]. You could try adding the nodetect option alongside debug and finish any device specific login (ex. screen PIN) before GDM loads.

SDDM does not appear to support pam_u2f with initial user login. Autologin can be used instead, and then edit /etc/pam.d/kde to just control screen locking.

If you are using a U2F key with biometric authentication (e.g. Yubikey Bio) and want 1FA, use /etc/pam.d/kde-fingerprint, commenting out the pam_fprintd.so line and placing your changes in its place. This avoids an unnecessary "unlock" button that gets displayed after authentication, rather than simply unlocking immediately[4].

Enable the PAM module for other services like explained above. For example, to secure the screensaver of Cinnamon, edit /etc/pam.d/cinnamon-screensaver.

For Polkit, copy the default configuration at /usr/lib/pam.d/polkit-1 to /etc/pam.d/polkit-1 and make your changes there.

If you managed to lock yourself out of the system, boot into recovery mode or from a USB pen drive. Then revert the changes in the PAM configuration and reboot.

In case the pam-u2f module silently fails, add debug keyword to the auth line in a file in /etc/pam.d/.

OpenSSH 8.2 supports FIDO/U2F hardware tokens natively, see SSH keys#FIDO/U2F.

This article or section is a candidate for moving to systemd-cryptenroll.

Since version 248, systemd can be use to unlock a LUKS partition using a FIDO2 key.

First, you will need to setup your /etc/crypttab file (see below), or customize your initramfs if you wish to unlock your root partition. The full procedure is similar to the use of a TPM chip for unlocking. See systemd-cryptenroll#Trusted Platform Module.

To register the key, you will need to use the systemd-cryptenroll utility. First, run the following command to list your detected keys:

Then you can register the key in a LUKS slot, specifying auto value (or path to the FIDO2 device such as /dev/hidrawX if you have multiple):

For a non-root data partition the crypttab would look like this:

This should also work if your encrypted partition is a logical volume managed under LVM:

**Examples:**

Example 1 (unknown):
```unknown
authfile=/path/to/u2f_keys
```

Example 2 (unknown):
```unknown
$ mkdir ~/.config/Yubico
$ pamu2fcfg -i pam://hostname > ~/.config/Yubico/u2f_keys
```

Example 3 (unknown):
```unknown
$ pamu2fcfg -i pam://hostname -n >> ~/.config/Yubico/u2f_keys
```

Example 4 (unknown):
```unknown
/etc/pam.d/sudo
```

---

## MinIO

**URL:** https://wiki.archlinux.org/title/MinIO

**Contents:**
- Installation
- Configuration
  - Credentials
  - Single server with 1 disk
  - Single server with 4 disks

MinIO offers high-performance, S3 compatible object storage.

Install the minio package. The minio-client is optional.

The configuration file is located in /etc/minio/minio.conf.

Uncomment MINIO_ROOT_USER and MINIO_ROOT_PASSWORD and set them to a value of your choice.

The default configuration will start a single server with a single disk. MinIO objects will be stored in /srv/minio/data. Change the value of MINIO_VOLUMES to update the path.

MINIO_OPTS might contains extra parameters that will be given to the MinIO server when started.

Start MinIO by starting minio.service. Check that MinIO has been started by checking the service's unit status; example output is as follows:

Edit etc/minio/minio.conf:

Start minio.service. Verify that MinIO is running by checking its unit status; example output is as follows:

**Examples:**

Example 1 (unknown):
```unknown
/etc/minio/minio.conf
```

Example 2 (unknown):
```unknown
MINIO_ROOT_USER
```

Example 3 (unknown):
```unknown
MINIO_ROOT_PASSWORD
```

Example 4 (unknown):
```unknown
MINIO_ROOT_USER
```

---

## AIDE

**URL:** https://wiki.archlinux.org/title/AIDE

**Contents:**
- Installation
- Configuration
- Usage
  - Cron
  - Security
- See also

Advanced Intrusion Detection Environment (AIDE) is a host-based intrusion detection system (HIDS) for checking the integrity of files. It does this by creating a baseline database of files on an initial run, and then checks this database against the system on subsequent runs. File properties that can be checked against include inode, permissions, modification time, file contents, etc.

AIDE only does file integrity checks. It does not check for rootkits or parse logfiles for suspicious activity, like some other HIDS (such as OSSEC) do. For these features, you can use an additional HIDS (see [1] for a possibly biased comparison), or use standalone rootkit scanners (rkhunter, chkrootkit) and log monitoring solutions (logwatch, logcheck).

You can install the aideAUR package, or you can instead install aide-selinuxAUR if you want to use it in a system with SELinux and Audit framework enabled.

The default configuration file at /etc/aide.conf has defaults based on Fedora and is heavily commented. If you want to change the rules, see aide.conf(5) and the AIDE Manual for further documentation.

To check your configuration, use aide -D.

To initialize the database, use aide -i or aideinit. Depending on your configuration and system, this command can take a while to complete.

You can check the system against the baseline database using aide -C, or update the baseline db using aide -u.

For more info, see aide(1).

This article or section is out of date.

AIDE can be run manually if desired, but you may want to run it automatically instead. How you set this up will depend on your cron daemon and MUA (if email notification is desired).

If cron is set up to automatically mail all job output, it can be as simple as

For examples of more complicated cron scripts see [2] or [3].

Since the database is stored on the root filesystem, attackers can easily modify it to cover their tracks if they compromise your system. You may want to copy the database to offline, read-only media and perform checks against this copy periodically.

**Examples:**

Example 1 (unknown):
```unknown
/etc/aide.conf
```

Example 2 (unknown):
```unknown
#!/bin/bash -e
```

Example 3 (unknown):
```unknown
# these should be the same as what's defined in /etc/aide.conf
database=/var/lib/aide/aide.db.gz
database_out=/var/lib/aide/aide.db.new.gz

if [! -f "$database" ]; then
        echo "$database not found" >&2
        exit 1
fi

aide -u || true

mv $database $database.back
mv $database_out $database
```

---

## Sugar

**URL:** https://wiki.archlinux.org/title/Sugar

**Contents:**
- Installation
  - From Activity Library
- Starting Sugar
  - Graphically
  - Manually
- See also

Initially developed for the OLPC initiative, Sugar is a desktop environment geared towards children and education.

Sugar has a special Taxonomy to name the parts of its system. The graphical interface itself constitute the Glucose group. This is the core system can reasonably expect to be present when installing Sugar. But to really use the environment, you need activities (some sort of applications). Base activities are part of Fructose. Then, Sucrose is constituted by both Glucose and Fructose and represents what should be distributed as a basic sugar desktop environment. Extra activities are part of Honey. Note that Ribose (the underlaying operating system) is here replaced by Arch.

The Sugar Activity Library provides many Activity Bundles packaged as zip files with the .xo extension. These bundles can be downloaded and installed to the user's directory from Sugar, but the installation does not ensure that the dependencies are satisfied. Therefore it is not the recommended way to install activities, because they likely fail to start due missing dependencies. A commonly used dependency is webkit2gtk for web activities.

In order to check why the activity fails to start, look at the log file located at ~/.sugar/default/logs/app_id-1.log.

Sugar can be started either graphically, using a display manager, or manually from the console.

Select the session Sugar from the display manager's session menu.

If sugar-runner is installed, Sugar can be launched with the sugar-runner command.

Alternative method is to add exec sugar to the ~/.xinitrc file. After that, Sugar can be launched with the startx command (see xinitrc for additional details). After setting up the ~/.xinitrc file, it can also be arranged to start X at login.

**Examples:**

Example 1 (unknown):
```unknown
~/.sugar/default/logs/app_id-1.log
```

Example 2 (unknown):
```unknown
sugar-runner
```

---

## Undervolting CPU

**URL:** https://wiki.archlinux.org/title/Undervolting_CPU

**Contents:**
- Background
  - The voltages
  - How far can you go?
  - When undervolting is necessary
- intel-undervolt
  - Installation
  - Configuration and usage
- amdctl
  - Installation

Undervolting is a process where voltage to CPU is reduced in order to reduce its energy consumption and heat without affecting performance. Note that most desktop motherboards allow tweaking CPU voltage settings in BIOS as well.

Modern CPUs implement frequency scaling, where the operating frequency is dynamically adjusted to save power when idle and increase performance when under use. As frequency increases, the voltage required to switch the gate quickly enough also increases, so CPUs also scale their voltage alongside the frequency. The default frequency-voltage curve is relatively conservative, and depending on the silicon lottery (how much your CPU's logic gates outperform the curve's expected parameters), a lower voltage can be used without lowering the frequency.

Because power dissipation is related to the square of voltage, undervolting can cause significant power savings and heat reduction. A lower voltage is also usually associated with a longer lifespan of the device, as most semiconductor deterioration mechanisms (BTI, electromigration) are at least partly temperature and voltage-dependent.

Risks of undervolting include incorrect calculation results (possibly complete failure to boot) due to using voltages that are not sufficient for a frequency. Be prepared to reboot your system or reset your BIOS through the JBAT jumper to undo your undervolt.

To test the stability and correctness of an undervolt, stress testing using an application with built-in numerical checks (mprime, linpack) is strongly recommended. Just booting isn't sufficient as calculation errors can result in unexpected, potentially dangerous, program behavior (as demonstrated by "undervolt attacks"). As CPUs age, the gates may also become more sluggish and require higher voltage to stabilize, so be prepared to revise your undervolting settings.

On Intel CPUs you can separately control the core, cache, System Agent (also known as NB/SOC/Uncore) and the integrated GPU voltages. If there is integrated analog I/O you can control that part's voltage too. The core and the cache are self-explanatory. The System Agent includes the memory controller.

On Zen-based AMD CPUs you can separately control core, NB/SOC/Uncore, VDDG_CCD, VDDG_IOD, VDDP_CLDO, and PLL/1P8. The first two are similar to their Intel counterparts. VDDG_CCD is for signaling from cores to the IO die. VDDG_IOD is for signaling from the IO die to the cores. VDDP_CLDO is for the memory controller. The PLL/1P8 voltage is the voltage of the base clock signal.

When you lower one voltage, make sure to run the corresponding test in mprime. For example, if you touch the memory controller or SOC-related voltages, run the Large test.

Decreasing Intel CPU and CPU Cache by 100 to 200 mV is usually (outwardly) stable. Going above 200 mV may result in a crash, or may not have any effect at all.

A number of AM5 (Ryzen 7xxx, 9xxx) motherboards are known to apply an overly high SOC/Uncore voltage when EXPO is enabled. This has lead to a number of CPU failures with discolored areas or even bulges corresponding to the IO die. A BIOS update should be applied to fix the problem. If no update is available and the SOC voltage remains above 1.3 V, manual undervolting should be performed in BIOS.

There is a similar issue in 13th and 14th generation Intel CPUs where the core voltage is too high, causing accelerated aging. A microcode update should fix this issue.

Intel-undervolt is a tool based on this article for undervolting Haswell and newer Intel CPUs using MSR and MCHBAR registers. In addition, it also allows to change power and temperature limits. It is not compatible with Tiger Lake and above, but is compatible with intel_pstate.

The tool can be installed as intel-undervolt.

The following command prints in use voltage settings:

Now edit the configuration file /etc/intel-undervolt.conf. Example configuration with undervolted CPU Cache by -100mV:

Once you saved configuration file - test it:

It will print Success if settings were applied. You can double check in use configuration using the following command:

Once you find stable configuration, you can also enable intel-undervolt.service to make changes persistent.

This article or section is out of date.

amdctl is a tool for undervolting K10 and newer AMD CPUs.

The tool can be installed as amdctl-gitAUR.

**Examples:**

Example 1 (unknown):
```unknown
intel_pstate
```

Example 2 (unknown):
```unknown
# intel-undervolt read
```

Example 3 (unknown):
```unknown
/etc/intel-undervolt.conf
```

Example 4 (unknown):
```unknown
/etc/intel-undervolt.conf
```

---

## wpa_supplicant

**URL:** https://wiki.archlinux.org/title/Wpa_supplicant

**Contents:**
- Installation
- Overview
- Connecting with wpa_cli
- Connecting with wpa_passphrase
- Advanced usage
  - Configuration
  - Connection
    - Manual
    - At boot (systemd)
      - 802.1x/radius

wpa_supplicant is a cross-platform supplicant with support for WPA, WPA2 and WPA3 (IEEE 802.11i). It is suitable for desktops, laptops and embedded systems. It is the IEEE 802.1X/WPA component that is used in the client stations. It implements key negotiation with a WPA authenticator and it controls the roaming and IEEE 802.11 authentication/association of the wireless driver.

Install the wpa_supplicant package, which includes the main program wpa_supplicant, the passphrase tool wpa_passphrase, and the text front-end wpa_cli.

Optionally, also install the official wpa_supplicant_guiAUR which provides wpa_gui, a graphical front-end for wpa_supplicant, or wpa-cuteAUR which is a fork from an earlier version of wpa_gui with a couple of fixes and improvements.

The first step to connect to an encrypted wireless network is having wpa_supplicant obtain authentication from a WPA authenticator. In order to do this, wpa_supplicant must be configured so that it will be able to submit the correct credentials to the authenticator.

Once you are authenticated you need to assign an IP address, see Network configuration#IP addresses.

This connection method allows scanning for available networks, making use of wpa_cli, a command line tool which can be used to configure wpa_supplicant. See wpa_cli(8) for details.

In order to use wpa_cli, a control interface must be specified for wpa_supplicant, and it must be given the rights to update the configuration. Do this by creating a minimal configuration file:

Now start wpa_supplicant with:

This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.

It can be terminated with:

This will present an interactive prompt (>), which has tab completion and descriptions of completed commands. There is also a help command.

Use the scan and scan_results commands to see the available networks:

To associate with MYSSID, add the network, set the credentials and enable it:

If the SSID does not have password authentication, you must explicitly configure the network as keyless by replacing the command set_network 0 psk "passphrase" with set_network 0 key_mgmt NONE.

Finally save this network in the configuration file and quit wpa_cli:

Once association is complete, you must obtain an IP address. See Network configuration#Network management for details.

This connection method allows quickly connecting to a network whose SSID is already known, making use of wpa_passphrase, a command line tool which generates the minimal configuration needed by wpa_supplicant. For example:

This means that wpa_supplicant can be associated with wpa_passphrase and started with:

Finally, you should obtain an IP address, see Network configuration#IP addresses.

For networks of varying complexity, possibly employing extensive use of EAP, it will be useful to maintain a customised configuration file. For an overview of the configuration with examples, refer to wpa_supplicant.conf(5); for details on all the supported configuration parameters, refer to the example file /usr/share/doc/wpa_supplicant/wpa_supplicant.conf.[1]

As explained in #Connecting with wpa_passphrase, a basic configuration file can be generated with:

This will only create a network section. A configuration file with also the ability of #Connecting with wpa_cli and some other common options may look like:

If security is not a concern, the passphrase can also be defined in clear text in the network section by enclosing it in quotes:

If the network does not have a passphrase, e.g. a public Wi-Fi:

To connect to a WPA-Enterprise network, see #802.1x/radius.

Further network blocks may be added manually, or using wpa_cli as illustrated in #Connecting with wpa_cli. In order to use wpa_cli, a control interface must be set with the ctrl_interface option. Setting ctrl_interface_group=wheel allows users belonging to such group to execute wpa_cli. This setting can be used to enable users without root access (or equivalent via sudo etc) to connect to wireless networks. Also add update_config=1 so that changes made with wpa_cli to example.conf can be saved. Note that any user that is a member of the ctrl_interface_group group will be able to make changes to the file if this is turned on.

fast_reauth=1 and ap_scan=1 are the wpa_supplicant options active globally at the time of writing. Whether you need them, or other global options too for that matter, depends on the type of network to connect to. If you need other global options, simply copy them over to the file from /usr/share/doc/wpa_supplicant/wpa_supplicant.conf.

Alternatively, wpa_cli set can be used to see options' status or set new ones. Multiple network blocks may be appended to this configuration: the supplicant will handle association to and roaming between all of them. The strongest signal defined with a network block usually is connected to by default, one may define priority= to influence behaviour. For example to auto-connect to any unsecured network as a fallback with the lowest priority:

Once you have finished the configuration file, you can optionally use it as a system-wide or per-interface default configuration by naming it according to the paths listed in #At boot (systemd). This also applies if you use additional network manager tools, which may rely on the paths (for example Dhcpcd#10-wpa_supplicant).

First start wpa_supplicant command, whose most commonly used arguments are:

See wpa_supplicant(8) for the full argument list. For example:

followed by a method to obtain an ip address manually as indicated in the #Overview, for example:

The wpa_supplicant package provides multiple systemd service files:

To enable wireless at boot, enable an instance of one of the above services on a particular wireless interface. For example, enable the wpa_supplicant@interface systemd unit.

Now choose and enable an instance of a service to obtain an ip address for the particular interface as indicated in the #Overview. For example, enable the dhcpcd@interface systemd unit.

To connect a wired adapter using 802.1x/radius you will need to specify some configurations and enable the necessary service for the adapter. This is useful for headless servers using systemd-networkd.

You may need to specify the wired driver with the -D wired command line option (see #Manual) if the default driver does not support your adapter.

Replace adapter with the wired adapter you wish to connect, and adapt the settings to match your 802.1x/radius requirements.

Since this file is storing a plaintext password, chown it to root:root and chmod it to 600.

To use the hash instead of the plaintext password, you can use the hash keyword:

To hash your password:

After invoking the command above, provide your plain password and then press Ctrl+d.

Before running the wpa_supplicant-wired@adapter.service service, make sure to set the device down:

wpa_cli can run in daemon mode and execute a specified script based on events from wpa_supplicant. Two events are supported: CONNECTED and DISCONNECTED. Some environment variables are available to the script, see wpa_cli(8) for details.

The following example will use notify-send to notify the user about the events:

Remember to make the script executable, then use the -a flag to pass the script path to wpa_cli:

When connected to a wireless network with multiple access points, wpa_supplicant is typically responsible for roaming between access points. Choosing a new access point requires wpa_supplicant to perform a scan of available networks, which causes a brief interruption in connectivity to the current access point while the wireless radio scans other frequencies. After a scan, if wpa_supplicant detects a closer access point (BSSID) in the current network (SSID), in terms of signal strength (RSSI), it will re-associate to the closer access point.

The default configuration of wpa_supplicant has relatively timid roaming: it will rescan only when the association to the current access point is lost. This means that, if a client moves far away from its current access point, but not far enough to completely lose signal, the client will keep using the weak signal instead of roaming to a closer access point.

To make wpa_supplicant more aggressive about roaming, set the bgscan parameter in the configuration file, such as:

The above example will cause wpa_supplicant to scan every 30 seconds when the signal is weak (below -70), and every 3600 seconds otherwise. bgscan can be specified either in specific network blocks or globally for all networks.

In order to determine why you are unable to connect to an access point you can run wpa_supplicant with the -d flag for debug messages, wait a couple seconds then look for lines that list SSIDs and the reason they were not connected to. For example:

In this case we are trying to connect to an access point with the SSID home. The reason the connection fails is skip RSN IE - no mgmt frame protection enabled but AP requires it, so we need to add ieee80211w=2 to our configuration file.

On some (especially old) hardware, wpa_supplicant may fail with the following error:

This indicates that the standard nl80211 driver does not support the given hardware. The deprecated wext driver might still support the device:

If the command works to connect, and the user wishes to use systemd to manage the wireless connection, it is necessary to edit the wpa_supplicant@.service unit provided by the package and modify the ExecStart line accordingly:

When you use wireless to connect to network shares you might have the problem that the shutdown takes a very long time. That is because systemd runs against a 3 minute timeout. The reason is that WPA supplicant is shut down too early, i.e. before systemd tries to unmount the share(s). A bug report suggests a work-around by editing the wpa_supplicant@.service as follows:

wpa_supplicant may not work properly if directly passed via stdin particularly long or complex passphrases which include special characters. This may lead to errors such as failed 4-way WPA handshake, PSK may be wrong when launching wpa_supplicant.

In order to solve this try using here strings wpa_passphrase <MYSSID> <<< "<passphrase>" or passing a file to the -c flag instead:

In some instances it was found that storing the passphrase cleartext in the psk key of the wpa_supplicant.conf network block gave positive results (see [2]). However, this approach is rather insecure. Using wpa_cli to create this file instead of manually writing it gives the best results most of the time and therefore is the recommended way to proceed.

If the institution the user studies or works at did not upgrade their network tunnel's encryption to at least TLS 1.2 yet and still uses TLS 1.0 or 1.1 for network traffic encryption in their Eduroam Wi-Fi infrastructure, OpenSSL 3.x throws an "unsupported protocol" error and the client machine's Wi-Fi backend (either wpa_supplicant or iwd) refuses to establish a connection any further. Fortunately, an easy workaround exists for OpenSSL's TLS 1.0 and 1.1 deprecation without making the client computer's whole Wi-Fi connection stack globally vulnerable to attacks, although it only works with NetworkManager.

Consult NetworkManager#WPA Enterprise connections fail to authenticate with OpenSSL "unsupported protocol" error for the solution.

Connman users can visit the ConnMan#Connecting to eduroam (802.1X) article for their own version of the fix.

Make sure to define the following within the network block of the configuration to enable connections to pure WPA3 access points:

Additionally, Intel Wi-Fi 6 cards may need sae_pwe=1 in the main (non network) section of the config file.

Mixed WPA2-PSK/WPA3-SAE access points will require an alternative setting for key_mgmt as shown below:

You can check for hardware support of MFP/PMF (Management Frame Protection / Protected Management Frames) on the interface client by running:

Most Wi-Fi devices support this standard introduced in 2009, except some limited (aka non x86_64 related) or old hardware.

/etc/wpa_supplicant/wpa_supplicant.conf uses normalized access points names in UTF-8. If in wpa_cli scan_result you see something like KEBAP\xc3\x87I HAL\xc4\xb0L, we want to convert escaped hex sequences into their actual characters:

will result into KEBAPI HALL to be inserted into the configuration file.

See the echo(1) man page for details about the -e option.

With the default configuration, wpa_supplicant will sometimes excessively log information to the journal, such as the following every second:

The factual accuracy of this article or section is disputed.

It is impossible to eliminate these log entries by setting wpa_supplicant's own log level to WARNING or higher. The simplest way to do so is by instead adding a systemd configuration file:

Then, daemon-reload systemd and restart wpa_supplicant.service.

**Examples:**

Example 1 (unknown):
```unknown
/etc/wpa_supplicant/wpa_supplicant.conf
```

Example 2 (unknown):
```unknown
ctrl_interface=/run/wpa_supplicant
update_config=1
```

Example 3 (unknown):
```unknown
update_config
```

Example 4 (unknown):
```unknown
# wpa_supplicant -B -i interface -c /etc/wpa_supplicant/wpa_supplicant.conf
```

---

## Category:Web

**URL:** https://wiki.archlinux.org/title/HTTPS

Web browsers request web pages from web servers using the Hypertext Transfer Protocol (HTTP).

The secure version of HTTP is HTTPS, which uses TLS.

The markup language of the web is the Hypertext Markup Language (HTML), which is rendered by web browsers and can be written directly in a regular text editor, converted from Markdown or generated with a WYSIWYG editor.

---

## Device file

**URL:** https://wiki.archlinux.org/title/Lsblk

**Contents:**
- Block devices
  - Block device names
    - SCSI
    - NVMe
    - MMC
    - SCSI optical disc drive
    - virtio-blk
    - Partition
  - Utilities
    - lsblk

This article or section needs expansion.

On Linux they are in the /dev directory, according to the Filesystem Hierarchy Standard.

On Arch Linux the device nodes are managed by udev.

A block device is a special file that provides buffered access to a hardware device. For a detailed description and comparison of virtual file system devices, see Wikipedia:Device file#Block devices.

The beginning of the device name specifies the kernel's used driver subsystem to operate the block device.

Storage devices, like hard disks, SSDs and flash drives, that support the SCSI command (SCSI, SAS, UASP), ATA (PATA, SATA) or USB mass storage connection are handled by the kernel's SCSI driver subsystem. They all share the same naming scheme.

The name of these devices starts with sd. It is then followed by a lower-case letter starting from a for the first discovered device (sda), b for the second discovered device (sdb), and so on.

The name of storage devices, like SSDs, that are attached via NVM Express (NVMe) starts with nvme. It is then followed by a number starting from 0 for the device controller, nvme0 for the first discovered NVMe controller, nvme1 for the second, and so on. Next is the letter "n" and a number starting from 1 expressing the namespace on a controller, i.e. nvme0n1 for first discovered namespace on first discovered controller, nvme0n2 for second discovered namespace on first discovered controller, and so on.

This article or section needs expansion.

SD cards, MMC cards and eMMC storage devices are handled by the kernel's mmc driver and name of those devices start with mmcblk. It is then followed by a number starting from 0 for the device, i.e. mmcblk0 for first discovered device, mmcblk1 for second discovered device and so on.

The name of optical disc drives (ODDs), that are attached using one of the interfaces supported by the SCSI driver subsystem, start with sr. The name is then followed by a number starting from 0 for the device, ie. sr0 for the first discovered device, sr1 for the second discovered device, and so on.

Udev also provides /dev/cdrom that is a symbolic link to /dev/sr0. The name will always be cdrom regardless of the drive's supported disc types or the inserted media.

The name of drives attached to a virtio block device (virtio-blk) interface start with vd. It is then followed by a lower-case letter starting from a for the first discovered device (vda), b for the second discovered device (vdb), and so on.

Partition device names are a combination of the drive's device name and the partition number assigned to them in the partition table, i.e. /dev/drivepartition. For drives whose device name ends with a number, the drive name and partition number is separated with the letter "p", i.e. /dev/driveppartition.

The util-linux package provides the lsblk(8) utility which lists block devices, for example:

In the example above, only one device is available (sda), and that device has three partitions (sda1 to sda3), each with a different file system.

You can use the -o/--output option to enable a specific list of output columns:

The above is based on the options provided by the -f/--fs argument with removal of UUID and addition of partition label and disk size, which are useful when identifying multiple disks. See lsblk --help for a full list of supported columns.

This article or section needs expansion.

wipefs can list or erase file system, RAID or partition-table signatures (magic strings) from the specified device to make the signatures invisible for libblkid(3). It does not erase the file systems themselves nor any other data from the device.

See wipefs(8) for more information.

For example, to erase all signatures from the device /dev/sdb and create a signature backup ~/wipefs-sdb-offset.bak file for each signature:

Device nodes that do not have a physical device.

**Examples:**

Example 1 (unknown):
```unknown
/dev/nvme0n1
```

Example 2 (unknown):
```unknown
/dev/nvme2n5
```

Example 3 (unknown):
```unknown
/dev/mmcblkXboot{0,1}
```

Example 4 (unknown):
```unknown
/dev/mmcblkXrpmb
```

---

## Multihead

**URL:** https://wiki.archlinux.org/title/Multihead

**Contents:**
- Historical background
- RandR
  - Configuration using xrandr
    - VGA1 left of HDMI1 at their preferred resolutions
    - VGA1 right of HDMI1 at fixed resolutions
    - Combine screens into virtual display
  - Configuration using xorg.conf
    - Example: dualhead configuration using relative coordinates
    - Example: dualhead configuration using relative coordinates with custom resolutions
    - Example: dualhead configuration using absolute coordinates

Multi-head, multi-screen, multi-display or multi-monitor represent setups with multiple display devices attached to a single computer. This article provides a general description for several multi-head setup methods, and provides some configuration examples.

X Window System (X, X11) is the underlying graphical user interface (GUI) for most Unix/Linux computers that provide one. It was developed in 1984 at MIT. After about 35 years of development, tweaks, new features and ideas, it is generally acknowledged to be a bit of a beast. During the time of early development, the common configuration was a single running X that provided individual views to Xterminals on a time-sharing system. Today, X typically provides a single screen on a desktop or laptop.

In short, there are many ways to configure GUIs using X. When using modern versions, sometimes you can even get away with limited or no configuration. In the last few years, the boast is that X is self-configuring. Certainly, a good rule of thumb is that less configuration is better - that is, only configure what the defaults got wrong.

RandR (Rotate and Resize) is an X Window System extension, which allows clients to dynamically change (e.g. resize, rotate, reflect) screens. In most cases, it can fully replace the old Xinerama setup. See an explanation why RandR is better than Xinerama.

RandR can be configured for the current session via the xrandr tool, arandr or persistently via an xorg.conf file.

You may arrange your screens either relatively to each other (using the --right-of, --left-of, --above, --below options), or by absolute coordinates (using the --pos option; note that in this case you usually need to know resolutions of your monitors). See xrandr(1) for details. Some frequently used settings are described below.

--right-of places the previous screen (HDMI1) to the right of the specified screen (VGA1).

--left-of places the previous screen (HDMI1) to the left of the specified screen (VGA1).

Since randr version 1.5, it has been possible to combine monitors into one virtual display. This is an updated version of what was possible with Xinerama and works with open source drivers and does not require an Xorg restart. Some desktop environments do not support this feature yet. Openbox has been tested and works with this feature.

Get monitor list by doing xrandr --listmonitors

Create virtual display xrandr --setmonitor SomeName auto DisplayPort-4,DisplayPort-3,HDMI-A-0. auto determines the size of the virtual display, setting this to auto will automatically create the correct size of the display array. Monitor order in this command does not matter and the monitors need to be rearranged correctly after or before this command is executed.

For a more detailed explanation see this page.

This is similar to using xrandr, separate Monitor section is needed for each screen. As an Identifier, the same value as reported by xrandr -q is used (i.e. Identifier "VGA1" is used instead of --output VGA1).

The ID for each monitor can be found by running the $ xrandr -q command and should be defined as Monitor-<ID> inside the Device section.

See Xrandr#Adding undetected resolutions.

There are no negative coordinates, the setup's leftmost and highest possibly targeted point is at 0,0

The following options allow you to automatically detect when a new display is connected and then change the layout based on that. This can be useful for laptop users who frequently work in multiple different environments that require different setups.

This article or section needs expansion.

This is the original way of configuring multiple monitors with X, and it has been around for decades. Each physical monitor is assigned as an X screen, and while you can move the mouse between them, they are more or less independent.

Normally the X display has a single identifier such as :0, set in the DISPLAY environment variable; but in this configuration, each screen has a different $DISPLAY value. The first screen is :0.0, the second is :0.1 and so on.

With this configuration, it is not possible to move windows between screens, apart from a few special programs like GIMP and Emacs, which have multi-screen support. For most programs, you must change the DISPLAY environment variable when launching to have the program appear on another screen:

Alternatively, if you have a terminal on each screen, launching programs will inherit the DISPLAY value and appear on the same screen they were launched on. But, moving an application between screens involves closing it and reopening it again on the other screen.

Working this way does have certain advantages. For example, windows popping up on one screen will not steal the focus away from you if you are working on another screen - each screen is quite independent.

TwinView is NVIDIA's extension which makes two monitors attached to a video card appear as a single screen. TwinView provides Xinerama extensions so that applications are aware there are two monitors connected, and thus it is incompatible with Xinerama. However, if you only have two monitors and they are both connected to the same NVIDIA card, there is little difference between TwinView and Xinerama (although in this situation TwinView may offer slightly better performance.)

If you wish to attach more than two monitors or monitors attached to other video cards, you will need to use Xinerama instead of TwinView. Likewise, as of April 2012, both monitors must be in the same orientation - you cannot have one in landscape and the other in portrait mode.

In the past, TwinView was the only way to get OpenGL acceleration with NVIDIA cards while being able to drag windows between screens. However modern versions of the NVIDIA closed-source driver are able to provide OpenGL acceleration even when using Xinerama.

See NVIDIA#TwinView for an example configuration.

Xinerama is the old way of doing genuine multihead X. Xinerama combines all monitors into a single screen (:0) making it possible to drag windows between screens.

Xinerama is configured via custom X configuration files. There is also a GUI tool named WideGuy to make toggling Xinerama easier. Note that to use WideGuy you still need an Xorg configuration with a ServerLayout section.

Here are some X configuration examples:

This is a ServerLayout section which controls where each monitor sits relative to the others.

Each Screen in the above section is defined in a separate file, such as this one:

You will need to create a Device section for each monitor, i.e. a dual head video card will have two Device sections. The following example shows how to configure two video cards each providing two outputs, for a total of four monitors.

**Examples:**

Example 1 (unknown):
```unknown
$ xrandr --output VGA1 --auto --output HDMI1 --auto --right-of VGA1
```

Example 2 (unknown):
```unknown
$ xrandr --output VGA1 --mode 1024x768 --pos 1920x0 --output HDMI1 --mode 1920x1080 --pos 0x0
```

Example 3 (unknown):
```unknown
$ xrandr --output VGA1 --mode 1024x768 --output HDMI1 --mode 1920x1080 --left-of VGA1
```

Example 4 (unknown):
```unknown
xrandr --listmonitors
```

---

## MariaDB

**URL:** https://wiki.archlinux.org/title/MariaDB

**Contents:**
- Installation
- Configuration
  - Add user
  - Configuration files
  - Enable auto-completion
  - Using UTF8MB4
  - Using a tmpfs for tmpdir
  - Time zone tables
- Security
  - Improve initial security

MariaDB is a reliable, high performance and full-featured database server which aims to be an 'always Free, backward compatible, drop-in' replacement of MySQL. Since 2013 MariaDB is Arch Linux's default implementation of MySQL.[1]

MariaDB is the default implementation of MySQL in Arch Linux, provided with the mariadb and mariadb-lts packages.

Install mariadb or mariadb-lts, and run the following command before starting the mariadb.service:

Now mariadb.service can be started and/or enabled.

To simplify administration, you might want to install a front-end.

By default both root user and the user running the server can administer the database.

To administer the server, run mariadb as the user running the server:

Creating a new user takes two steps: create the user; grant privileges. In the below example, the user monty with some_pass as password is being created, then granted full permissions to the database mydb:

MariaDB configuration options are read from the following files in the given order (according to mysqld --help --verbose | head -10 output):

Create a configuration file in /etc/my.cnf.d/ with a .cnf extension to ensure that upgrades preserve your configuration.

Depending on the scope of the changes you want to make (system-wide, user-only...), use the corresponding file. See this entry of the Knowledge Base for more information.

The MariaDB client completion feature is disabled by default. To enable it system-wide edit /etc/my.cnf.d/client.cnf, and add auto-rehash under client-mariadb. Note that this must not be placed under mysqld. Completion will be enabled next time you run the MariaDB client.

Append the following values to the main configuration file located at /etc/my.cnf.d/my.cnf:

Restart mariadb.service to apply the changes. Changing the character set does not change existing table formats, only newly created tables, and the protocol interaction that fetches data.

See #Maintenance to optimize and check the database health.

The directory used by MariaDB for storing temporary files is named tmpdir. For example, it is used to perform disk based large sorts, as well as for internal and explicit temporary tables.

Create the directory with appropriate permissions:

Add the following tmpfs mount to your /etc/fstab file:

Add to your /etc/my.cnf.d/server.cnf file under the mysqld group:

Stop mariadb.service, mount /var/lib/mysqltmp/ and start mariadb.service.

Although time zone tables are created during the installation, they are not automatically populated. They need to be populated if you are planning on using CONVERT_TZ() in SQL queries.

To populate the time zone tables with all the time zones:

Optionally, you may populate the table with specific time zone files:

The mariadb-secure-installation command will interactively guide you through a number of recommended security measures, such as removing anonymous accounts and removing the test database:

By default, MariaDB will listen on the 0.0.0.0 address, which includes all network interfaces. In order to restrict MariaDB to listen only to the loopback address, add the following line in /etc/my.cnf.d/server.cnf:

This will bind to both 127.0.0.1 and ::1, and enable MariaDB to receive connections both in IPv4 and IPv6.

By default, MariaDB is accessible via both Unix sockets and the network. If MariaDB is only needed for the localhost, you can improve security by not listening on TCP port 3306, and only listening on Unix sockets instead. To do this, add the following line in /etc/my.cnf.d/server.cnf:

You will still be able to log in locally as before, but only using Unix sockets.

To allow remote access to the MariaDB server, ensure that MariaDB has networking enabled and is listening on the appropriate interface.

Grant any MariaDB user remote access (example for root):

Check current users with remote access privileged:

Now grant remote access for your user (here root):

You can change the '%' wildcard to a specific host if you like. The password can be different from user's main password.

For security reasons, the systemd service file contains ProtectHome=true, which prevents MariaDB from accessing files under the /home, /root and /run/user hierarchies. The datadir has to be in an accessible location and owned by the mysql user and group.

You can modify this behavior by creating a supplementary service file as described here.

Upon a major version release of mariadb (for example mariadb-10.3.10-1 to mariadb-10.9.4-1), it is wise to upgrade the system database to make new server features available:

To upgrade from 10.3.x to 10.9.x:

If the (new) daemon is not starting, see #Unable to run mariadb-upgrade because MariaDB cannot start.

mariadb-clients ships with mariadb-check which can be used to check, repair, and optimize tables within databases from the shell. See mariadb-check(1) for more. Several command tasks are shown:

To check all tables in all databases:

To analyze all tables in all databases:

To repair all tables in all databases:

To optimize all tables in all databases:

There are various tools and strategies[dead link 2025-08-15HTTP 404] to back up your databases.

If you are using the default InnoDB storage engine, a suggested[dead link 2025-08-15HTTP 404] way of backing up all your bases online while provisioning for point-in-time recovery (also known as "roll-forward", when you need to restore an old backup and replay the changes that happened since that backup) is to execute the following command:

This will prompt for MariaDB's root user's password, which was defined during database #Configuration.

Specifying the password on the command line is strongly discouraged, as it exposes it to discovery by other users through the use of ps aux or other techniques. Instead, the aforementioned command will prompt for the specified user's password, concealing it away.

As SQL tables can get pretty large, it is recommended to pipe the output of the aforementioned command in a compression utility like xz(1):

Decompressing the backup thus created and reloading it in the server is achieved by doing:

This will recreate and repopulate all the databases previously backed up (see this or this).

If you want to setup non-interactive backup script for use in cron jobs or systemd timers, see option files and this illustration for mariadb-dump.

Basically you should add the following section to the relevant configuration file:

Mentioning a user here is optional, but doing so will free you from having to mention it on the command line. If you want to set this for all tools, including mariadb-client, use the [client] group.

The database can be dumped to a file for easy backup. The following shell script will do this for you, creating a db_backup.xz file in the same directory as the script, containing your database dump:

See also the official mariadb-dump page in the MariaDB[dead link 2025-08-15HTTP 404] manuals.

A python-based software package named Holland Backup allows to automate all of the backup work. It supports direct mysqldump, LVM snapshots to tar files (mysqllvm), LVM snapshots with mysqldump (mysqldump-lvm), and xtrabackup methods to extract the data. The Holland framework supports a multitude of options and is highly configurable to address almost any backup situation.

The main hollandAUR and holland-commonAUR packages provide the core framework; one of the sub-packages (holland-mysqldumpAUR, holland-mysqllvmAUR and/or holland-xtrabackupAUR must be installed for full operation. Example configurations for each method are in the /usr/share/doc/holland/examples/ directory and can be copied to /etc/holland/backupsets/, as well as using the holland mk-config command to generate a base configuration for a named provider.

Try run MariaDB in a standalone:

Check and auto repair all tables in all databases, see more:

Forcefully optimize all tables, automatically fixing table errors that may come up.

If using MySQL databases on ZFS, the error InnoDB: Operating system error number 22 in a file operation may occur.

A workaround is to disable aio_writes in /etc/my.cnf.d/my.cnf:

This may happen if you are using a long (>80) password. mariadb CLI cannot handle that many characters in readline mode. So, if you are planning to use the recommended password input mode:

Consider changing the password to smaller one.

This article or section is out of date.

By default, mariadbd creates binary log files at /var/lib/mysql/mysql-bin.XXXXXX with the numbers ascending. These logs are useful for replication master server or data recovery, but these binary logs can easily eat up large amounts of disk space. If you do not plan to use replication or data recovery features, you may disable binary logging by commenting out these lines in /etc/my.cnf.d/my.cnf then restart:

Or, if you want to keep these logs but keep their size in check and old logs deleted, you can set these limits then restart:

Alternatively, there exists a MariaDB command to manually purge logs older than a specific one. For example, you may see a file named mysql-bin.000023 and want to delete every log older than it. As long as the log-bin=mysql-bin setting is in effect, you would run:

To use MariaDB with OpenRC you need to add the following lines to the [mariadb] section in the MySQL configuration file, located at /etc/my.cnf.d/my.cnf.

You should now be able to start MariaDB using:

Increase the number of file descriptors by creating a systemd drop-in, e.g.:

Before MariaDB 10.5, redo log was unnecessarily split into multiple files.[4][dead link 2025-08-15HTTP 404]

Do NOT ever remove the old binary logs /var/lib/mysql/ib_logfile* out of the way.

To resolve this, install MariaDB 10.4. Start it and let it undergo a clean shutdown. After that happens you can upgrade to 10.5 again. Same applies if another version of MariaDB was specified.

Symptom: When running mariadb-upgrade or mariadb-check, it return one or more error like these:

Where "xxx" usually is the system table inside the mysql database.

**Examples:**

Example 1 (unknown):
```unknown
/var/lib/mysql
```

Example 2 (unknown):
```unknown
/usr/lib/tmpfiles.d/mariadb.conf
```

Example 3 (unknown):
```unknown
mariadb.service
```

Example 4 (unknown):
```unknown
# mariadb-install-db --user=mysql --basedir=/usr --datadir=/var/lib/mysql
```

---

## dm-crypt

**URL:** https://wiki.archlinux.org/title/Dm-crypt

**Contents:**
- Usage
- Example scenarios
- See also

dm-crypt is the Linux kernel's device mapper crypto target. From Wikipedia:dm-crypt, it is:

---

## Syncthing

**URL:** https://wiki.archlinux.org/title/Syncthing

**Contents:**
- Installation
- Running Syncthing
  - Starting Syncthing
  - Autostarting Syncthing
    - System service
    - User service: on login
    - User service: on boot
    - User service: on mount
  - Syncthing-GTK
  - Web-GUI

Syncthing is an open-source file synchronization client/server application written in Go, which implements its own - equally free - Block Exchange Protocol. All transit communications between syncthing nodes are encrypted using TLS and all nodes are uniquely identified with cryptographic certificates.

Install the syncthing package.

Syncthing provides a #Web-GUI for control and monitoring. GUI wrappers like #Syncthing-GTK and #Syncthing Tray (provided in separate packages) also exist.

Run the syncthing binary manually from a terminal. The multiple optional parameters are described in syncthing(1).

Syncthing can either be installed as a systemd system-wide service or as a systemd user service to run automatically at startup.

Running Syncthing as a system service ensures that it is running at startup even if the user has no active session, it is intended to be used on a server. Enable and start the syncthing@myusername.service where myusername is the actual name of the Syncthing user.

To set the umask in the service for all users, consider using a systemd drop-in file:

Then, restart any relevant service units.

This will set up all new downloaded files for synchronized directories to be readable and writeable only by the relevant user. Fixing existing files to have more restrictive permissions can be done with a chmod og-rwx -R <relevant directory here>. Depending on your usecase, you may want to use slightly more relaxed permissions than "only readable/writable by the specific users with syncthing running" - alter the umask values accordingly.

To configure the Syncthing umask for a single user, simply add the username after the @ symbol in the path to the drop-in file to select the appropriate instantiation of the templated systemd unit. This will override the umask only when starting the syncthing service for that single user.

Running Syncthing as a systemd user service ensures that Syncthing only starts after the user has logged into the system (e.g., via the graphical login screen, or ssh). This method is intended to be used on a (multiuser) computer. To run the user service, start/enable the user unit syncthing.service (i.e. with the --user flag).

The systemd-user service can be started at boot time (i.e. without logging in) using systemd/User#Automatic start-up of systemd user instances.

The Syncthing systemd user service can be started after a specific (optionally encrypted) device has been mounted, and stopped when the device has been unmounted. To create a user service dependent on a mount point, after the device has been mounted, find the systemd mount name by running systemctl list-units -t mount. Then create a new service similar to the one below:

syncthing-gtkAUR provides a GTK graphical user interface, desktop notifications and integration with the file managers Nautilus, Nemo and Caja. Syncthing can be launched by Syncthing-GTK: use the interface settings to run syncthing-gtk at startup, and to state whether to launch the syncthing daemon.

Syncthing provides a web interface accessible by default on http://localhost:8384.

syncthingtray-qt6AUR complements the Web-GUI by providing a Qt-based system tray icon and desktop notifications. There exists a desktop environment neutral version and a Plasmoid for Plasma. It also provides integration with systemd and the Dolphin file manager.

For further remarks, see the pinned comments on the AUR. When you are unsure about configuration it is also advisable to read upstream's README.

The packages also comes with the syncthingctl utility which allows to interact with Syncthing from the command line.

After installation, Syncthing already has a proper start-up configuration although no default sync folder is created. New servers and/or folders can be added by visiting the web interface. For detailed instructions on how to setup a simple network, read Syncthing's getting started.

To add a folder to share, click Add Folder on the left of web interface. On the left is the list of repositories, which are folders you can choose to share with other nodes. On the right is the list of nodes you have added.

To add another node, click Add Node underneath the list of nodes. You will be prompted for their Node ID (which can be found on the other machine by clicking Edit > Show ID) as well as a short name and the address. If you specify "dynamic" for the address, the syncthing announce server will be used to automatically exchange addresses between nodes. If you want to know more about Node IDs, including the cryptographic implications, you can read the appropriate Syncthing documentation page.

After saving the configuration, the syncthing server restarts automatically. Next, you can either change the configuration of the default node (click its name and then Edit), or create a new one to share data with. Simply tick the node you wish to share the data with, and they will have permission to access it.

In the typical case several machines share a LAN (Local Area Network) behind a NAT (Network Address Translation) router, it is advised for a versatile configuration to:

inotify (inode notify) is a Linux kernel subsystem that acts to extend filesystems to notice changes to the filesystem, and report those changes to applications. Syncthing supports inotify and the functionality can be enabled in the configuration menu for individual folders.

One can participate in the Syncthing infrastructure by running a global discovery server or a relay server.

Syncthing has the ability to connect two devices via a relay when it is not possible to establish a direct connection between them. Relayed connections are end-to-end encrypted in the usual manner, so the relay has no insight into the connection other than the knowledge of the IP addresses and device IDs.

Anyone can run a relay server and it will automatically join the Syncthing relay pool and be available to all Syncthing's users. To run your own relay, install syncthing-relaysrv and Start/Enable syncthing-relaysrv.service. Rate limiting and other options can be configured via the command line. These options can be set in the ExecStart directive of the service drop-in file as follows:

Global discovery is used by Syncthing to find peers on the internet. Any device announces itself at startup to the discovery server which stores the device ID, IP address, port and current time. Then on request, for a given device ID, it returns the information stored in JSON format, for instance.

As an example, the request https://discovery.syncthing.net/?device=ITZRNXE-YNROGBZ-HXTH5P7-VK5NYE5-QHRQGE2-7JQ6VNJ-KZUEDIU-5PPR5AM returns {"seen":"2020-02-29T14:56:08.34589801Z","addresses":["quic://212.121.228.172:22000","tcp://212.121.228.172:22000"]} .

A list of public of global discovery server is provided. In addition, anyone can run a discovery server, to run your own, install the syncthing-discosrv package.

The default unit file provided by the package stores data at /var/lib/syncthing-discosrv and appears to work fine if you don't need to customize any flags; see list.

An example replacement unit file follows, which stores data at /var/discosrv instead of /var/lib/syncthing-discosrv. The user/group syncthing-discosrv needs permissions to be able to read the certificate files. You need to edit the systemd unit file to correctly point to the certificates and to undertake any other configuration change you may want.

To point the client to your discovery server, change the Global Discovery Servers variable under Settings to https://yourserver:8443/ (default port) or whatever port you have reconfigured to. The variable takes a comma-separated list of discovery servers. It is possible to include multiple ones, including the default one.

If you are using self-signed certificates, the client refuses to connect unless you append the discovery server ID to its domain. The ID is printed to stdout upon launching the discovery server. Amend the Global Discovery Servers entry to add the ID: https://yourserver.com:8443/?id=AAAAAAA-BBBBBBB-CCCCCCC-DDDDDDD-EEEEEEE-FFFFFFF-GGGGGGG-HHHHHHH.

Syncthing can be quite noisy even while it is not doing anything. The service ExecStart can be overridden to choose a different log level than INFO. Valid levels are DEBUG, INFO, WARN, and ERROR.

It is possible to have Syncthing connect both locally and globally within a VirtualBox virtual machine (VM) while keeping its network adapter in the standard NAT mode (as opposed to bridged networking attached to the host computer's adapter).

To enable this mode, Syncthing should listen to a port in the VM different from the listening port already used by the host. For example, if the default 22000 port is used by the host, one could use 22001 in the VM. The listening port in the VM can be changed through Syncthing's Sync Protocol Listen Addresses to tcp://:22001 in the GUI Settings.

The 22001/TCP port of the host must be forwarded to the guest in this configuration. This can be done with the following command:

In this setup, relaying should not be necessary: local devices can connect to the VM on port 22001 while global devices are accessible as long as they have themselves an open port.

Syncthing can be run through a proxy to enable use behind a corporate firewall or tunneling via SSH. According to the using proxies documentation it is necessary to set the all_proxy environment variable, and it must indicate a socks5 proxy type.

You must then do a daemon-reload and restart the syncthing@myusername.service.

This file can be edited using systemd on the syncthing@myusername.service according to the systemd#Editing provided units section.

SyncthingFUSE is a FUSE driver which provides access to a syncthing share without actually syncing it to local storage. When you open a file, the contents are served from a local cache, if possible. If the contents are not in the cache, SyncthingFUSE asks peers for the contents and adds them to the cache. The local cache will not grow larger than a fixed size, though. If no peers are currently available for the file, opening it will fail.

Occasionally, Syncthing may be impacted by database issues. A common symptom of this is when "Out of Sync Items" is reported by the client but never resolved, even after disconnecting devices and restarting Syncthing. To force a rescan of files and resync of the database the next time Syncthing is started, use the following command:

If Syncthing complains that there is a read-only file system, although the user (e.g. root) has write permissions, check the template unit's definition:

Within the [Service] part, there is a Hardening part and below that, a ProtectSystem directive which is set to full by default. See systemd.exec(5)  SANDBOXING for more information on this directive.

Create a drop-in file to override the value to something that suits your needs. If you are trying to sync a sub-folder of /etc, ProtectSystem=true should do the trick.

See Debugging Syncthing.

**Examples:**

Example 1 (unknown):
```unknown
syncthing@myusername.service
```

Example 2 (unknown):
```unknown
syncthing@username.service
```

Example 3 (unknown):
```unknown
Syncthing-Fork
```

Example 4 (unknown):
```unknown
/etc/systemd/system/syncthing@.service.d/override.conf
```

---

## Character encoding

**URL:** https://wiki.archlinux.org/title/Character_encoding

**Contents:**
- UTF-8
  - Terminal
  - Unicode character insertion
- URL encoding
- Troubleshooting
  - Incorrect archive encoding
  - Incorrect file name encoding
  - Incorrect file encoding
    - Vim
  - Incorrect MP3 ID3 tag encoding

Character encoding is the process of interpreting bytes to readable characters. UTF-8 is the dominant encoding since 2009 and is promoted as a de-facto standard [1].

Most terminals use UTF-8 by default. However, some (e.g: gnome-terminal, rxvt-unicode) need to be launched from a UTF-8 locale. To do that, set the codeset part of your locale to .UTF-8 and reload your session. See Locale#Setting the locale for instructions.

See List of applications/Utilities#Text input.

URIs accept US-ASCII characters only and use percent-encoding to encode non-ASCII characters. This can result in very long and human-unreadable URIs.

In Firefox, it is possible to copy decoded URLs by enabling the browser.urlbar.decodeURLsOnCopy flag in about:config, or by inserting a space to the start of the URL, then selecting it (with the space) and copying it. However, this trick does not work on Chromium, and there is no equivalent flag. Alternatively, select starting at the end of the URL until right after the https:// part, then copy.

For command line usage, you can use python to translate encoded URLs from stdin.

Encoding problems are usually due to two programs communicating with different encodings, with one side typically not using UTF-8, resulting in mojibake.

Zip archives sometimes use encodings other than Unicode for the filenames. This issue is most commonly seen with archives created in legacy versions of Windows (XP, Vista, and 7), where File Explorer will prioritize a character set that more closely matches the system locale even if it uses a non-Latin writing system.

unzip is built with patches to enable character set conversion. bsdunzip of libarchive has same feature. However, it won't do so automatically and will still default to the assumption that filenames are UTF8-encoded. To properly extract an archive with non-UTF8 filenames, use unzip -O followed by the target encoding, e.g. CP936 is a common Simplified Chinese charset in old versions of Windows:

Other common charsets include GBK, also for Simplified Chinese:

And CP932 (not Shift-JIS), for Japanese:

If unsure about which charset you need, you can list an archive's files without extracting them by adding the -l flag, which allows you to verify that the filenames are printed correctly:

Alternatively, unzip-natspecAUR detects encoding for file names automatically.

unar -e of unarchiver may be used for some non zip archives encoded without UTF-8.

Use convmv for encoding-conversion mv:

By default, convmv shows what would be done without actual moving. After figuring out the original encoding using -f (e.g: for Chinese GBK), add the --notest option to proceed with the move operation.

By default, convmv skips file name conversion if it is already UTF8-encoded. Use the --nosmart option to force the conversion.

Use convmv --list to find the supported encodings.

Use the iconv command to convert the format. For example:

-f specifies the original encoding and -t specifies the output encoding. Use iconv -l to query all supported encodings and -o to specify the output file.

If the locale is UTF-8, opening other char-encoded files may be garbled. You can add a fallback adding to vimrc a line similar to:

Alternatively, you can explicitly set it by :set fileencoding=ansi. Vim will do the conversion via iconv automatically. See :h charset-conversion.

To modify the MP3 file tag, convert using python-mutagen or mp3unicode:

If file modification is undesired, you can tweak the behavior of media players. For players that use GStreamer as the backend, such as Rhythmbox and totem, set the environment variable:

quodlibet supports tag editing and setting ID3v2 encoding. Go to File > Preferences > Advanced, click I know what I'm doing! and enter a space-separated list of encodings in the ID3 encodings field. You can also edit the configuration file manually:

Generally, the mounted character set is different from the locales, which can be set by modifyinig fstab. If the locale is utf8, modify the line to:

If the locale is GBK, it should be:

When using Arch as a Samba server, adding the following line to /etc/samba/smb.conf can solve the garbled problem of Windows clients:

If you use UTF8 locale, the downloaded file name from a non-Unicode-encoded server might be garbled. For lftp, make the following settings under .lftp/rc:

For gftp, you can do the following settings in .gftp/gftprc:

However, the downloaded file name is still garbled and needs to be patched and compiled. The patch address is: https://www.teatime.com.tw/%7Etommy/linux/gftp_remote_charsets.patch

**Examples:**

Example 1 (unknown):
```unknown
xterm*utf8: 2
```

Example 2 (unknown):
```unknown
browser.urlbar.decodeURLsOnCopy
```

Example 3 (unknown):
```unknown
about:config
```

Example 4 (python):
```python
$ python3 -c "import sys; from urllib.parse import unquote; print(unquote(sys.stdin.read().strip()))"
```

---

## Kernel

**URL:** https://wiki.archlinux.org/title/Kernel

**Contents:**
- Officially supported kernels
- Compilation
  - kernel.org kernels
  - Unofficial kernels
- Troubleshooting
  - Kernel panics
    - Examine panic message
      - QR code on a blue screen
      - Console way
      - Example scenario: bad module

According to Wikipedia:

Arch Linux is based on the Linux kernel. There are various alternative Linux kernels available for Arch Linux in addition to the latest stable kernel. This article lists some of the options available in the repositories with a brief description of each. There is also a description of patches that can be applied to the system's kernel. The article ends with an overview of custom kernel compilation with links to various methods.

Kernel packages are installed under the /usr/lib/modules/ path and subsequently used to copy the vmlinuz executable image to /boot/. [1] When installing a different kernel or switching between multiple kernels, you must configure your boot loader to reflect the changes. For downgrading the kernel to an older version, see Downgrading packages#Downgrading the kernel.

Community support on forum and bug reporting is available for officially supported kernels.

Following methods can be used to compile your own kernel:

Some of the listed packages may also be available as binary packages via Unofficial user repositories.

Many of these unofficial kernels contain features that need to be enabled manually. Try reading the documentation in the patches themselves (many already include changes to the Documentation/ directory in the kernel source) or searching up the name of the patchset on the web.

A kernel panic occurs when the Linux kernel enters an unrecoverable failure state. The state typically originates from buggy hardware drivers resulting in the machine being deadlocked, non-responsive, and requiring a reboot. Just prior to deadlock, a diagnostic message is generated, consisting of: the machine state when the failure occurred, a call trace leading to the kernel function that recognized the failure, and a listing of currently loaded modules. Thankfully, kernel panics do not happen very often using mainline versions of the kernel--such as those supplied by the official repositories--but when they do happen, you need to know how to deal with them.

If a kernel panic occurs very early in the boot process, you may see a message on the console containing Kernel panic - not syncing:, but once systemd is running, kernel messages will typically be captured and written to the system log. However, when a panic occurs, the diagnostic message output by the kernel is almost never written to the log file on disk because the machine deadlocks before system-journald gets the chance.

Since linux 6.10 (for drm_panic), the kernel will display a panic as a QR code (by default) in a blue screen. The stack trace is visible at the URL given by the QR code. For Arch Linux, it is a link to https://panic.archlinux.org. The URL contains various information and the stack trace compressed by gzip and encoded in the URL fragment which is not transferred to the server (it is processed on the client side).

An example panic with a link and screenshot can be seen in a forum post.

You can revert to the old behavior by passing the parameter panic_screen=kmsg to the drm kernel module (or drm.panic_screen=kmsg as kernel parameter) to display the stack trace in a console.

The "old" style way of viewing the crash on the console as it happens is still available (without resorting to setting up a kdump crashkernel). Boot with the following kernel parameters and attempting to reproduce the panic on tty1:

It is possible to make a best guess as to what subsystem or module is causing the panic using the information in the diagnostic message. In this scenario, we have a panic on some imaginary machine during boot. Pay attention to the lines highlighted in bold:

We can surmise then, that the panic occurred during the initialization routine of module firewire_core as it was loaded. (We might assume then, that the machine's firewire hardware is incompatible with this version of the firewire driver module due to a programmer error, and will have to wait for a new release.) In the meantime, the easiest way to get the machine running again is to prevent the module from being loaded. We can do this in one of two ways:

This article or section is out of date.

The factual accuracy of this article or section is disputed.

You will need a root shell to make changes to the system so the panic no longer occurs. If the panic occurs on boot, there are several strategies to obtain a root shell before the machine deadlocks:

See General troubleshooting#Debugging regressions.

Try linux-mainlineAUR to check if the issue is already fixed upstream. The pinned comment also mentions a repository which contains already built kernels, so it may not be necessary to build it manually, which can take some time.

It may also be worth considering trying the LTS kernel (linux-lts) to debug issues which did not appear recently. Older versions of the LTS kernel can be found in the Arch Linux Archive.

If the issue still persists, bisect the linux-gitAUR kernel and report the bug in accordance to the kernel process for reporting regressions. Depending on the Bugtracker (B:) entry in the MAINTAINERS file this then entails opening an issue via the subsystems mailing lists, Kernel Bugzilla, or in other issue trackers like the DRM Gitlab. It is important to try the "vanilla" version without any patches to make sure it is not related to them. If a patch causes the issue, report it to the author of the patch.

You can shorten kernel build times by building only the modules required by the local system using modprobed-db, or by make localmodconfig. Of course you can completely drop irrelevant drivers, for example sound drivers to debug a network problem.

**Examples:**

Example 1 (unknown):
```unknown
/usr/lib/modules/
```

Example 2 (unknown):
```unknown
/proc/config.gz
```

Example 3 (unknown):
```unknown
CONFIG_IKCONFIG_PROC
```

Example 4 (unknown):
```unknown
Documentation/
```

---

## Trusted Platform Module

**URL:** https://wiki.archlinux.org/title/Trusted_Platform_Module

**Contents:**
- Checking TPM support
- Usage
  - LUKS encryption
  - SSH
  - systemd-creds
  - GnuPG
  - OpenSSL
  - Other good examples of TPM 2.0 usage
- Accessing PCR registers
- PCR policies

This article or section needs expansion.

Trusted Platform Module (TPM) is an international standard for a secure cryptoprocessor, which is a dedicated microprocessor designed to secure hardware by integrating cryptographic keys into devices.

In practice a TPM can be used for various different security applications such as secure boot, key storage and random number generation.

TPM is naturally supported only on devices that have TPM hardware support. If your hardware has TPM support but it is not showing up, it might need to be enabled in the BIOS settings.

Most modern computers support TPM 2.0, as it has been required for Windows 10 certification since 2016. To check support on your system, use any of the following methods:

TPM 2.0 allows direct access via /dev/tpm0 (one client at a time), kernel-managed access via /dev/tpmrm0, or managed access through the tpm2-abrmd resource manager daemon. According to a systemd project member, using tpm2-abrmd is no longer recommended. There are two choices of userspace tools, tpm2-tools by Intel and ibm-tssAUR by IBM.

TPM 2.0 requires UEFI boot; BIOS or Legacy boot systems can only use TPM 1.2.

Some TPM chips can be switched between 2.0 and 1.2 through a firmware upgrade (which can be done only a limited number of times).

Many informative resources to learn how to configure and make use of TPM 2.0 services in daily applications are available from the tpm2-software community.

It is possible to encrypt volumes using keys securely stored in the TPM. This approach ensures that your drives remain locked unless the TPM is present and specific conditions are met, such as the integrity of the firmware or Secure Boot state (see #Accessing PCR registers).

This mechanism can be used to automatically decrypt the root volume during the boot process, similarly to how BitLocker works on Windows or FileVault on macOS. While this provides strong protection if the drive is removed from the computer with the TPM, data protection will only rely on basic measures like user passwords and system settings if the entire PC is stolen. To mitigate this, you can:

systemd-cryptenroll and Clevis allow locking LUKS volumes with a key stored in the TPM. Additionally, systemd-cryptenroll enables tying the encryption to signed policies instead of static PCR values (See systemd-cryptenroll(1)).

For TPM sealed SSH keys, there are two options:

systemd-creds uses TPM to securely store and retrieve credentials used by systemd units.

This article or section needs expansion.

GnuPG, since version 2.3, supports moving compatible keys into the TPM. See Using a TPM with GnuPG 2.3 for the instructions.

There are OpenSSL providers and OpenSSL engines that implement storing keys in the TPM.

This article or section needs expansion.

Platform Configuration Registers (PCR) allow binding of the encryption of secrets to specific software versions and system state via hashes, so that the enrolled key is only accessible (may be "unsealed") if specific trusted software and/or configuration is used.

PCRs are intended to be used for platform hardware and software integrity verification between boots (e.g. protection against Evil Maid attack).

The TCG PC Client Specific Platform Firmware Profile Specification defines the registers in use, and The Linux TPM PCR Registry assigns Linux system components using them.

On Windows, BitLocker uses PCR8-11 (Legacy) or PCR11-14 (UEFI) for its own purposes. Documentation from tianocore[4].

tpm2-totp facilitates this check with a human observer and dedicated trusted device.

The current PCR values can be listed with systemd-analyze(1):

Or, alternatively with tpm2_pcrread(1) from tpm2-tools:

With TPM2 it is possible to bind secrets, like the LUKS root decryption key, to a signed policy rather than raw PCR values. These policies add flexibility by allowing PCR values to vary, provided there is a valid PCR signature for these values which matches the public key enrolled with the secret. For instance, rather than simply tying the encryption key to the PCR7 value (which only checks that Secure Boot hasn't been disabled or tampered with) you can bind it to a PCR policy that verifies the booting of a known Unified kernel image (UKI), with all its components measured during the boot process. See systemd-cryptenroll(1)  TPM2_PCRs_and_policies.

The example below enables decrypting a root partition during boot, (i.e. in the initramfs stage), without the encryption key being accessible after the system has fully booted up. See ukify(1) for examples with multiple trust policies for different phases of the boot.

First generate the necessary signing keys:

Create /etc/kernel/uki.conf or copy the template from /usr/lib/kernel/uki.conf. Although PCR policies can be used without Secure Boot, it makes sense to also sign your UKI, so update the following as needed with your keys:

To generate the UKI, use a tool which reads /etc/kernel/uki.conf by default, such as kernel-install or mkinitcpio.

Finally, enroll the TPM2 policy in your LUKS volumes.

Check the UKI with # ukify inspect /path/to/uki to make sure it contains the .pcrsig section.

Also check that the LUKs volume registered the public key:

If you followed the instruction described above for automatically unlocking luks2 devices with enrolled keys in a TPM2 hardware module, but still receive a prompt to input a password during the initramfs boot stage. You may need to early load the kernel module (you can obtain its name with systemd-cryptenroll --tpm2-device=list) that is responsible for handling your specific TPM2 module.

Starting from Linux Kernel 6.10 CONFIG_TCG_TPM2_HMAC was enabled by default and now TPM must support AES-128-CFB for session encryption. TPMs that do not support this mode, such as older Intel PTT, will fail to initialize the driver, resulting in A TPM error (714) occurred attempting to create NULL primary. Replacing hardware is the only complete solution. systemd-cryptenroll also requires AES-128-CFB support, so disabling kernel option isn't enough and it will fails producing this error: TPM device not usable as it does not support the required functionality (AES-128-CFB missing?).

**Examples:**

Example 1 (unknown):
```unknown
journalctl -k --grep=tpm
```

Example 2 (unknown):
```unknown
/sys/class/tpm/tpm0/device/description
```

Example 3 (unknown):
```unknown
/sys/class/tpm/tpm0/device/firmware_node/description
```

Example 4 (unknown):
```unknown
/sys/class/tpm/tpm0/tpm_version_major
```

---

## Enlightenment

**URL:** https://wiki.archlinux.org/title/Entrance

**Contents:**
- Enlightenment
  - Installation
  - Starting Enlightenment
    - Entrance
    - Manually
  - Configuration
    - Network
    - Polkit agent
    - GNOME Keyring integration
    - System tray

This comprises both the Enlightenment window manager and Enlightenment Foundation Libraries (EFL), which provide additional desktop environment features such as a toolkit, object canvas, and abstracted objects. It has been under development since 2005, but in February 2011 the core EFLs saw their first stable 1.0 release.

Install the enlightenment package.

You might also want to install some EFL-based applications that integrates well with Enlightenment:

The following are EFL-based applications, most in an early stage of development and not yet released:

Simply choose Enlightenment session from your favourite display manager or configure xinitrc to start it from the console.

Enlightenment has a new display manager called Entrance, which is provided by the entrance-gitAUR package. Entrance is quite sophisticated and its configuration is controlled by /etc/entrance/entrance.conf. It can be used by enabling entrance.service.

If you prefer to start Enlightenment manually, enter startx /usr/bin/enlightenment_start in the console. See xinitrc for details.

To try the Wayland compositor, enter enlightenment_start instead.

Enlightenment has a sophisticated configuration system that can be accessed from the Main menu's Settings submenu.

Enlightenment's preferred network manager is ConnMan which can be installed from the connman package. Follow the instructions on ConnMan to do the configuration.

For extended configuration, you may also install Econnman (available in AUR as econnmanAUR or econnman-gitAUR) and its associated dependencies. This is not required for general functionality though.

Adding the ConnMan Gadget to the Shelf

You can also use networkmanager to manage your network connections - see NetworkManager for more information.

Note however that the applet will need Appindicator support to show in Enlightenment's system tray. See NetworkManager#Appindicator. As an alternative to using the applet, NetworkManager includes both CLI and TUI interfaces for network configuration - see NetworkManager#Usage.

Enlightenment version DR 0.24.0 ships with a built-in polkit agent, and no extra polkit package is required to authenticate for privileged actions. Earlier versions of Enlightenment do not ship with a graphical polkit authentication agent. If you want to access privileged actions (e.g. mount a filesystem on a system device), you have to install one and autostart it. For that you should go to Settings Panel > Apps > Startup Applications > System and activate it.

It is possible to use gnome-keyring in Enlightenment. However, at the time of writing, you need a small hack to make it work in full. First, you must tell Enlightenment to autostart gnome-keyring. For that you should go to Settings Panel > Apps > Startup Applications > System and activate Certificate and Key Storage, GPG Password Agent, SSH Key Agent and "Secret Storage Service". After this, you should set the following:

This "hack" is used to override the automatic setting of the variable by "enlightenment-start" from "ssh-agent" to gnome-keyring.

More information on this topic in the GNOME Keyring article.

Enlightenment has support for a system tray but it is disabled by default. To enable the system tray, open the Enlightenment main menu, navigate to the Settings submenu and click on the Modules option. Scroll down until you see the Systray option. Highlight that option and click the Load button. Now that the module has been loaded, it can be added to the shelf. Right click on the shelf you wish to add the Systray to, hightlight the Shelf submenu and click on the Contents option. Scroll down until you see Systray. Highlight that option and click the Add button.

Enlightenment provides a notification server through its Notification extension.

More themes to customize the look of Enlightenment are available from:

You can install the themes (coming in .edj format) using the theme configuration dialog or by moving them to ~/.e/e/themes.

To alter the GTK theme, go to Settings > All > Look > Application Theme.

Many Modules provide Gadgets that can be added to your desktop or on a shelf. Some Modules (such as CPUFreq) only provide a single Gadget while others (such as Composite) provide additional features without any gadgets. Note that certain gadgets such as Systray can only be added to a shelf while others such as Moon can only be loaded on the desktop.

Beyond the modules described here, more "extra" modules are available from e-modules-extra-gitAUR.

The Scale Windows module, which requires compositing to be enabled, adds several features. The scale windows effect shrinks all open windows and brings them all into view. This is known in "Mission Control" in macOS. The scale pager effect zooms out and shows all desktops as a wall, like the compiz expo plugin. Both can be added to the desktop as a gadget or bound to a key binding, mouse binding or screen edge binding.

Some people like to change the standard window selection key binding ALT + Tab to use Scale Windows to select windows. To change this setting, you navigate to Menu > Settings > Settings Panel > Input > Keys. From here, you can set any key binding you would like.

To replace the window selection key binding functionality with Scale Windows, scroll through the left panel until you find the ALT section and then find and select ALT + Tab. Then, scroll through the right panel looking for the "Scale Windows" section and choose either Select Next or Select Next (All) depending on whether you would like to see windows from only the current desktop or from all desktops and click Apply to save the binding.

Available from upstream git.

If you find some unexpected behavior, there are a few things you can do:

If you are sure you found a bug open an issue for the relevant component at https://git.enlightenment.org/.

When the configuration needs to be reset and the settings windows can no longer be approached, configuration for the compositor can be reset using the hardcoded keybinding Ctrl + Alt + Shift + Home.

If fonts are too small and your screen is unreadable, be sure the right font packages are installed. ttf-dejavu and ttf-bitstream-vera are valid candidates.

You also should consider just increasing the scaling size under the Scaling. You can set scaling under Settings > Settings Panel > Look > Scaling.

You may find that Enlightenment routinely dims the backlight to 0% on logout and will only restore it to 100% when you log into another Enlightenment session. Enlightenment assumes that whatever comes after it will set the backlight to whatever it prefers, if anything as this is what Enlightenment does at start. This is especially problematic when using another desktop environment alongside Enlightenment that cannot control backlight as the backlight will not automatically be restored to its normal level when using that desktop environment. To fix this issue, open the Enlightenment Settings Panel and, under the Look tab, click on the Composite option. Tick the Don't fade backlight box and click OK.

You may find that the cursor theme for the desktop is different to the one used in applications such as Firefox. This is because desktop applications are using X cursor themes whilst Enlightenment has its own set of cursor themes. For consistency, you can set Enlightenment to always use the X cursor theme. To do this, open the Enlightenment Settings Panel and click on the Input tab. Click on the Mouse option. Change the theme from Enlightenment to X and click OK. You should now find that the same cursor theme is used everywhere. If the X cursor theme itself is not always consistent, see Cursor themes#The default cursor theme.

You can just select wallpapers in the wallpaper settings dialog and import any image with the provided settings dialog, or you can put desired wallpapers into ~/.e/e/backgrounds/

LMB anywhere on the desktop will give access to the settings, select /Desktop/Backgrounds/

Any new image copied in the ~/.e/e/backgrounds/ folder will get the list of available backgrounds auto-updated. You can drop animated gifs and even mp4 and other video files in here and use them as wallpapers if you want. Select desired wallpaper from drop-down menu. Inside the appropriate tabs in the global settings, you can adjust things like tiling of the background image, filling screen and such.

Enlightenment, Development Release 16 was first released in 2000, and reached version 1.0 in 2009. Originally, the DR16 stood for the 0.16 version of the Enlightenment project. You will find it as "Enlightenment16" now in the Arch repositories, it is still under development today, regularly updated by its maintainer Kim 'kwo' Woelders. With compositing, shadows and transparencies, E16 kept all of the speed that presided over its foundation by original author Carsten "Rasterman" Haitzler but with up to date refinement.

Install enlightenment16AUR.

See /usr/share/doc/e16/e16.html for in depth documentation.

Most configuration files for E16 reside in ~/.e16 and are text-based, editable at will. That includes the Menus too.

Shortcut keys can be either modified by hand, or with the e16keyedit software provided as source on the sourceforge page of the e16 project. Note that the keyboard shortcuts file is not created in ~/.e16 by default. You can copy the packaged version to your home directory if you wish to make changes:

Create an Init, a Start and a Stop folder in your ~/.e16 folder: any .sh script found there will either be executed at Startup (from Init folder), at each Restart (from Start folder), or at Shutdown (from Stop folder); provided you allowed it through the MMB / settings / session / <enable scripts> button and made them executable. Typical examples involves starting PulseAudio or your favorite network manager applet.

Shadows, Transparent effects et al can be found in MMB or RMB /Settings, under Composite .

**Examples:**

Example 1 (unknown):
```unknown
/etc/entrance/entrance.conf
```

Example 2 (unknown):
```unknown
entrance.service
```

Example 3 (unknown):
```unknown
startx /usr/bin/enlightenment_start
```

Example 4 (unknown):
```unknown
enlightenment_start
```

---

## Valkey

**URL:** https://wiki.archlinux.org/title/Redis

**Contents:**
- Installation
  - Client-side software
- Configuration
  - Listen on socket
- Troubleshooting
  - Warning about Transparent Huge Pages (THP)
  - Warning about TCP backlog
  - Warning about overcommit_memory is set to 0
- Tips and tricks
  - Enabling tab autocompletion

From Wikipedia:Valkey:

Install the valkey package.

Start/enable valkey.service.

The Valkey configuration file is well-documented and located at /etc/valkey/valkey.conf.

Using Valkey over a Unix socket may give a performance increase, compared to TCP/IP [1].

The following changes should be made in /etc/valkey/valkey.conf to enable use of the unix socket:

Finally restart the valkey.service.

To solve warning messages as "you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Valkey", you may want to permanently disable this feature:

To solve warning messages as "The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128", increase the current value:

To solve warning messages as "overcommit_memory is set to 0! Background save may fail under low memory condition":

You can leverage redis-cli completion script from zsh-completions. To map redis-cli completion script to valkey-cli, put the following line to .zshrc:

**Examples:**

Example 1 (unknown):
```unknown
valkey.service
```

Example 2 (unknown):
```unknown
/etc/valkey/valkey.conf
```

Example 3 (unknown):
```unknown
/etc/valkey/valkey.conf
```

Example 4 (unknown):
```unknown
unixsocket /run/valkey/valkey.sock
```

---

## Help:Reading

**URL:** https://wiki.archlinux.org/title/Enabled

**Contents:**
- Organization
- Formatting
- Root, regular user or another user
- Append, add, create, edit
  - Make executable
- Source
- Installation of packages
  - Official packages
  - Arch User Repository
- Control of systemd units

Because the vast majority of the ArchWiki contains indications that may need clarification for users new to Arch Linux (or GNU/Linux in general), this rundown of basic procedures was written both to avoid confusion in the assimilation of the articles and to deter repetition in the content itself.

Most articles on the ArchWiki do not attempt to provide a holistic introduction to a single topic; they are instead written in adherence to the "Don't Repeat Yourself" principle, under the assumption that the user will seek out and read any supporting material that they do not yet understand. Where possible, such supporting material is indicated in the article via special formatting, see #Formatting.

Because of this organization, it may be necessary to examine several related sources in order to fully understand an ArchWiki article. In particular, users who are new to Arch (or GNU/Linux in general) should expect to end up reading a great number of articles even when solving simple problems. It is especially important to study the supporting material before seeking additional help from other users.

Some lines are written like so:

Others have a different prefix:

The numeral or hash sign (#) indicates that the command needs to be run as root, whereas the dollar sign ($) shows that the command should be run as a regular user.

When the commands need to run as a specific user, they will be prefixed by the username in square brackets, for example:

This means you should use a privilege elevation tool, e.g. with sudo:

A notable exception to watch out for:

In this example, the context surrounding the numeral sign communicates that this is not to be run as a command; it should be edited into a file instead. So in this case, the numeral sign denotes a comment. A comment can be explanatory text that will not be interpreted by the associated program. Bash scripts denotation for comments happens to coincide with the root PS1.

After further examination, "give away" signs include the uppercase character following the # sign. Usually, Unix commands are not written this way and most of the time they are short abbreviations instead of full-blown English words (e.g., Copy becomes cp).

Regardless, most articles make this easy to discern by notifying the reader:

Append to ~/path/to/file:

When prompted to append to, add to, create, or edit one or more files, it is implied that you should use one of the following methods.

To create or modify multiline files, it is suggested to use a text editor. For example, using the nano command to edit the file /etc/bash.bashrc is:

To create or overwrite a file from a string, it may be simpler to use output redirection. The following example creates or overwrites the contents of the file /etc/hostname with the text myhostname.

Output redirection can also be used to append a string to a file. The following example appends the text [custom-repo] to the file /etc/pacman.conf.

When prompted to create directories, use the mkdir command:

After creating a file, if it is meant to be run as a script (whether manually or called by another program), it needs to be set as executable, for example with:

See chmod. Some applications such as file managers may provide graphical interfaces to do the same.

Some applications, notably command-line shells, use scripts for their configuration: after modifying them, they must be sourced in order for the changes to be applied. In the case of bash, for example, this is done by running (you can also replace source with .):

When the wiki suggests modifying such a configuration script, it will not explicitly remind you to source the file, and only in some cases will it point to this section with a reminder link.

When an article invites you to install some packages in the conventional way, it will not indicate the detailed instructions to do so; instead, it will simply mention the names of the packages to be installed.

The subsections below give an overview of the generic installation procedures depending on the package type.

For packages from the official repositories, you will read something like:

This means that you have to run:

The pacman article contains detailed explanations to deal with package management in Arch Linux proficiently.

For packages from the Arch User Repository (AUR), you will read something like:

This means that in general you have to follow the foobarAUR link, download the PKGBUILD archive, extract it, verify the content and finally run, in the same folder:

The Arch User Repository article contains all the detailed explanations and best practices to deal with AUR packages.

When an article invites to start, enable, etc., some systemd unit (e.g. a service), it will not indicate the detailed instructions to do so, but instead you will read something like:

This means that you have to run:

A notable command that does not follow this exact pattern is systemctl daemon-reload which will be called without arguments.

The systemd#Using units section contains structured list of available actions (like start, enable, enable and start, etc.) with their corresponding systemctl commands.

It is important to remember that there are two different kinds of configurations on a GNU/Linux system. System-wide configuration affects all users. Since system-wide settings are generally located in the /etc directory, root privileges are required in order to alter them. For example, to apply a Bash setting that affects all users, /etc/bash.bashrc should be modified.

User-specific configuration affects only a single user. Dotfiles are used for user-specific configuration. For example, the file ~/.bashrc is the user-specific configuration file. The idea is that each user can define their own settings, such as aliases, functions and other interactive features like the prompt, without affecting other users' preferences.

Bash and other Bourne-compatible shells, such as Zsh, also source files depending on whether the shell is a login shell or an interactive shell. See Bash#Configuration files and Zsh#Startup/Shutdown files for details.

Some code blocks may contain so-called pseudo-variables, which, as the name says, are not actual variables used in the code. Instead they are generic placeholders and have to be manually replaced with system-specific configuration items before the code may be run or parsed. Common shells such as bash and zsh provide tab-completion to auto-complete parameters for common commands such as systemctl.

In the articles that comply with Help:Style/Formatting and punctuation, pseudo-variables are formatted in italics. For example:

In this case interface_name is used as a pseudo-variable placeholder in a systemd template unit. All systemd template units, identifiable by the @ sign, require a system-specific configuration item as argument. See systemd#Using units.

In this case the pseudo-variables are used to describe the parameters that must be substituted for them. Details on how to gather them are elaborated on in the section Securely wipe disk#Calculate blocks to wipe manually, which features the command.

This article or section needs expansion.

In case of file examples, pasting pseudo-variables in real configuration files might break the programs that use them.

In most cases, ellipses (...) are not part of the actual file content or code output, and instead represent omitted or optional text that is not relevant for the discussed subject.

For example HOOKS=(... encrypt ... filesystems ...) or:

Be aware though that, in a few instances, ellipses may be a meaningful part of the code syntax: attentive users should be able to recognize these cases by the context.

**Examples:**

Example 1 (unknown):
```unknown
# mkinitcpio -p linux
```

Example 2 (unknown):
```unknown
$ makepkg -s
```

Example 3 (unknown):
```unknown
sudo command
```

Example 4 (unknown):
```unknown
[postgres]$ initdb -D /var/lib/postgres/data
```

---

## Input method

**URL:** https://wiki.archlinux.org/title/Input_method

**Contents:**
- Input method framework
- List of available input method editors
- Configuration
  - Fcitx5
  - Fcitx
  - IBus
  - Uim
  - Emacs
  - Scim
  - Xim

From Wikipedia:Input method:

In simpler words, an IME is an application that allows us to use Latin characters in order to type non-Latin characters.

Some IMEs do this through a process called romanization, which is the transliteration of non-Latin language sounds into the Latin equivalents that most closely resemble them. As an example, the Japanese written word for "sake" or "rice wine" is , also written as , and romanized as "sake". The IME's role is to act as a middleman between the keyboard and the input fields, so that when we type "sake" it will intercept the keyboard's input, replace "sake" with  or  (as chosen by users) and type the native characters for us instead of the keys we pressed.

There are also IMEs that do not make use of romanization. One of the most prominent ones, Cangjie, does so by decomposing Chinese characters into their radicals, matching these radicals to a second set of its own internal radicals, and finally matching these internal radicals to the Latin characters. As an example, the Chinese written word for "wine" is also , which consists of the radicals , , ,  and . Cangjie matches these radicals to the internal radicals , , ,  and , and then matches these to the Latin characters emcw; this means that when we type "emcw", Cangjie will intercept the keyboard's input, replace "emcw" with , and type that character on the screen.

Most IMEs work as part of an input method framework (commonly abbreviated as IMF), which is an application that allows the user to easily switch between different IMEs. In fact, this is the exact same application that many of us unknowingly use everyday to switch between the different Latin keyboard layouts (e.g. English, Spanish, German, etc).

The most common IMF is IBus (often used in GTK-based environments like GNOME), followed by Fcitx5 (mostly used in Qt-based environments like KDE), Scim, Fcitx, and Uim. Very uncommon ones include Gcin, Nimf and Hime. [1] Additionally, Emacs is a very popular text editor that has its own internal IMF.

See also Wikipedia:List of input methods for Unix platforms.

The following table shows the IMEs for various languages currently available in the Arch repositories and the AUR.

This article or section needs expansion.

In order for your desktop environment to properly register an installed input method framework as available and assign it to handle user input, a set of environment variables must be configured accordingly.

See Fcitx5#Integration for more information.

See Fcitx for more information.

See IBus for more information.

See Uim for more information.

The factual accuracy of this article or section is disputed.

According to this Fcitx wiki entry, "in some case, including emacs and java. Emacs has a historical bug, that under en_US.UTF-8 or similar locale, it will never use XIM (Though emacs is a gtk app, it use XIM). The only way to walkaround this is to use LC_CTYPE to fix this."

See Scim for more information.

**Examples:**

Example 1 (unknown):
```unknown
GTK_IM_MODULE=gtk-im-context-simple
```

Example 2 (unknown):
```unknown
QT_IM_MODULE=simple
```

Example 3 (unknown):
```unknown
GTK_IM_MODULE=fcitx
QT_IM_MODULE=fcitx
XMODIFIERS=@im=fcitx
```

Example 4 (unknown):
```unknown
GTK_IM_MODULE=ibus
QT_IM_MODULE=ibus
XMODIFIERS=@im=ibus
```

---

## Core utilities

**URL:** https://wiki.archlinux.org/title/Ls

**Contents:**
- Essentials
  - Preventing data loss
- Nonessentials
- Alternatives
  - cat alternatives
  - cd alternatives
  - date alternatives
  - cp alternatives
  - ls alternatives
  - find alternatives

Core utilities are the basic, fundamental tools of a GNU/Linux system. This article provides an incomplete overview of them, links their documentation and describes useful alternatives. The scope of this article includes, but is not limited to, the GNU Core Utilities. Most core utilities are traditional Unix tools and many were standardized by POSIX but have been developed further to provide more features.

Most command-line interfaces are documented in man pages, utilities by the GNU Project are documented primarily in Info manuals, some shells provide a help command for shell builtin commands. Additionally most utilities print their usage when run with the --help flag.

The following table lists some important utilities which Arch Linux users should be familiar with. See also intro(1).

rm, mv, cp and shell redirections happily delete or overwrite files without asking. rm, mv, and cp all support the -i flag to prompt the user before every removal / overwrite. Some users like to enable the -i flag by default using aliases. Relying upon these shell options can be dangerous, because you get used to them, resulting in potential data loss when you use another system or user that does not have them. The best way to prevent data loss is to create backups.

This table lists core utilities that often come in handy.

The moreutils package provides useful tools like sponge(1) that are missing from the GNU coreutils.

Alternative core utilities are provided by the following packages:

See also Bash#Auto "cd" when entering just a path and Zsh#Remembering recent directories.

This article or section is a candidate for moving to List of applications/Other.

Using rsync#As cp/mv alternative allows you to resume a failed transfer, to show the transfer status, to skip already existing files and to make sure of the destination files integrity using checksums.

For graphical file searchers, see List of applications/Utilities#File searching.

While diffutils does not provide a word-wise diff, several other programs do:

See also List of applications/Utilities#Comparison, diff, merge.

These tools aim to replace grep for code search. They do recursive search by default, skip binary files and respect .gitignore.

This article or section needs expansion.

See also: dd and ddrescue

This subsection lists dd implementations whose interface and default behaviour is mostly compliant with the POSIX specification of dd(1p).

The GNU implementation of dd found in coreutils also conforms to POSIX. This subsection lists its forks.

This subsection lists dd alternatives that do not conform to POSIX (in terms of the JCL-resembling command-line syntax and default behaviour).

This subsection lists forks of bufferAUR, a general-purpose I/O buffering utility similar to dd but has a dynamic-sized buffer. It supports blockwise I/O and can be used when dumping from/to an LTO-tape to avoid shoe shining.

See also List of applications/Utilities#Disk usage display.

Many common packages already install most popular POSIX utilities as dependencies, but the posix metapackage can be installed to ensure all of them being always present.

Beside mandatory utilities, there are also metapackages for some of the optional categories:

Some commands (arch, kill, etc.) are missing from coreutils or taken from other packages. To complete them for compatibility, install uutils-coreutils and do:

**Examples:**

Example 1 (unknown):
```unknown
--color-words
```

Example 2 (unknown):
```unknown
# ln -sf /usr/bin/uu-coreutils /usr/local/bin/arch
# echo -e "#compdef arch=uu-arch\n_uu-arch" > /usr/local/share/zsh/site-functions/_arch
# echo "complete -c arch -w uu-arch" > /usr/local/share/fish/vendor_completions.d/arch.fish
```

---

## Archiving and compression

**URL:** https://wiki.archlinux.org/title/Extract

**Contents:**
- Archiving only
- Compression tools
  - Compression only
  - Archiving and compression
  - Feature charts
    - Decompress
- Usage comparison
  - Archiving only usage
  - Compression only usage
  - Archiving and compression usage

The traditional Unix archiving and compression tools are separated according to the Unix philosophy:

These tools are often used in sequence by firstly creating an archive file and then compressing it.

Of course there are also tools that do both, which tend to additionally offer encryption, error detection and recovery.

See also #Archiving only usage.

These compression programs implement their own file format.

See also #Archiving and compression usage.

Some of the tools above are capable of handling multiple formats, allowing for fewer installed packages.

See also Bash/Functions#Extract.

To extract an archive, its file format needs to be determined. If the file is properly named you can deduce its format from the file extension.

Otherwise you can use the file tool, see file(1).

Some file systems support on-the-fly compression of file data:

The open-sourced VDO project was integrated into the Linux kernel project, which provides a deduplication and compression device mapper layer in the interest of increasing storage efficiency. A userspace tools for managing VDO volumes is available in the AUR: vdoAUR

See Character encoding#Troubleshooting.

**Examples:**

Example 1 (unknown):
```unknown
--use-compress-program=lz4
```

Example 2 (unknown):
```unknown
tar cfv archive.tar file1 file2
```

Example 3 (unknown):
```unknown
tar xfv archive.tar
```

Example 4 (unknown):
```unknown
tar -tvf archive.tar
```

---

## EXWM

**URL:** https://wiki.archlinux.org/title/EXWM

**Contents:**
- Installation
- Configuration
  - Multi-monitor
  - System tray
- Embedding within LXDE
  - lxsession-logout
- Troubleshooting
  - Screen tearing in Firefox
  - Confusing Buffer Names
- See also

EXWM is a window manager based on Emacs.

Make sure you have emacs installed. You will also need xorg-xinit.

Install EXWM from within Emacs: M-x package-install RET exwm RET.

Edit xinitrc and add:

In your emacs init file, add:

to use the default settings. If you want to use your own settings, use (exwm-enable) instead of (exwm-config-example) (and you do not need to (require 'exwm-config)).

It is also possible to start emacs in server mode and to start EXWM from commandline. See https://github.com/ch11ng/exwm/issues/284.

EXWM is a full X window manager, so Emacs manages X windows such as your browser, vlc, etc. You may use all the normal Emacs window commands to control window placement. In X windows (i.e. not "normal" Emacs buffers), some commands are caught by EXWM and not passed through to the program. These keys are store in exwm-input-prefix-keys. Alternatively, you can set global commands by customizing exwm-input-global-keys. If you would rather set exwm-input-global-keys in elisp rather than using the customization feature, be aware that you may have to restart EXWM (and set exwm-input-global-keys before enabling exwm). Alternatively, you could try using the cset macro from the "or emacs" blog[dead link 2025-04-05domain name not resolved], which should work for redefining exwm-input-global-keys without restarting EXWM. To use s-& as a keyboard shortcut to launch a program (e.g. firefox), you can do:

EXWM can handle multi-monitor through the (optional) exwm-randr package. You will need to install xrandr and enable exwm-randr in your emacs configuration file before calling (exwm-enable). You will need to adjust the values of "DP-1" and "DP-2" to the values your computer uses; call xrandr at the command line with no arguments to see available outputs.

EXWM supports a system tray, but it is not enabled by default. To enable it, put the following before (exwm-enable) in your dotemacs file:

You may need to adjust the height afterwards; this can be adjusted with the exwm-systemtray-height variable.

EXWM can be used in place of openbox, allowing you to still use LXDE session management tools.

Before doing this, make sure you have your init file for emacs already set up to run EXWM (see above)

lxsession uses the window manager defined in ~/.config/lxsession/LXDE/desktop.conf (Openbox by default). If this file does not exist, it searches in /etc/xdg/lxsession/LXDE/desktop.conf instead.

Replace openbox-lxde in either file with emacs:

You can create the following function within emacs to log out, shutdown, or reboot cleanly from within a LXDE session:

This stores your recentf history to disk, prompts you to save, discard, or diff changes within unsaved buffers, then launches the logout manager. You can bind this function to any key within emacs.

You may experience screen tearing in some programs, particularly Firefox. You can try:

You may see the buffer names being named '*EXWM*'. This makes it confusing while switching between buffers .EXWM allows the buffers to name themself . To allow buffers to name themself put the following in your dotemacs .

**Examples:**

Example 1 (unknown):
```unknown
M-x package-install RET exwm RET
```

Example 2 (unknown):
```unknown
(require 'exwm)
(require 'exwm-config)
(exwm-config-example)
```

Example 3 (unknown):
```unknown
(exwm-enable)
```

Example 4 (unknown):
```unknown
(exwm-config-example)
```

---

## rkhunter

**URL:** https://wiki.archlinux.org/title/Rkhunter

**Contents:**
- Installation
- Configuration
  - Initial setup
  - Important files
- Usage
  - Basic commands
- Troubleshooting
  - False positives
- See also
  - External documentation

rkhunter (Rootkit Hunter) is a security monitoring tool for POSIX compliant systems. It scans for rootkits, and other possible vulnerabilities. It does so by searching for the default directories (of rootkits), misconfigured permissions, hidden files, kernel modules containing suspicious strings, and comparing hashes of important files with known good ones.

It is written in Bash, to allow for portability, and can run on most UNIX-based systems.

Install the rkhunter package.

Prior to running rkhunter for the first time, update the file properties database:

The main configuration file is located at /etc/rkhunter.conf.

By default, a log of the last system check will be placed at /var/log/rkhunter.log.

See rkhunter(8) for full details.

To update the file properties database:

It is necessary to ensure that the rkhunter data files are kept up-to-date by running:

To run a system check:

To validate the configuration file(s):

Out of the box, Rootkit Hunter will throw up some false warnings during the file properties check. This occurs because a few of the core utilities have been replaced by scripts. These warnings can be muted through white-listing:

**Examples:**

Example 1 (unknown):
```unknown
# rkhunter --propupd
```

Example 2 (unknown):
```unknown
/etc/rkhunter.conf
```

Example 3 (unknown):
```unknown
/var/log/rkhunter.log
```

Example 4 (unknown):
```unknown
# rkhunter --propupd
```

---

## eCryptfs

**URL:** https://wiki.archlinux.org/title/ECryptfs

**Contents:**
- Basics
  - Deficiencies
- Setup & mounting
  - Ubuntu tools
    - Encrypting a data directory
    - Encrypting a home directory
    - Mounting
      - Manually
      - Auto-mounting
  - ecryptfs-simple

This article describes basic usage of eCryptfs. It guides you through the process of creating a private and secure encrypted directory within your home directory to store sensitive files and private data.

In implementation eCryptfs differs from dm-crypt, which provides a block device encryption layer, while eCryptfs is an actual file-system  a stacked cryptographic file system. For comparison of the two you can refer to Data-at-rest encryption#Block device vs stacked filesystem encryption. One distinguished feature is that the encryption is stacked on an existing filesystem; eCryptfs can be mounted onto any single existing directory and does not require a separate partition (or size pre-allocation).

As mentioned in the summary eCryptfs does not require special on-disk storage allocation effort, such as a separate partition or pre-allocated space. Instead, you can mount eCryptfs on top of any single directory to protect it. That includes, for example, a user's entire home directory or single dedicated directories within it. All cryptographic metadata is stored in the headers of files, so encrypted data can be easily moved, stored for backup and recovered. There are other advantages, but there are also drawbacks, for instance eCryptfs is not suitable for encrypting complete partitions which also means you cannot protect swap space with it (but you can, of course, combine it with Dm-crypt/Swap encryption). If you are just starting to set up disk encryption, swap encryption and other points to consider are covered in Data-at-rest encryption#Preparation.

To familiarize with eCryptfs a few points:

Before using eCryptfs, the following disadvantages should be checked for applicability.

Before starting setup, check the eCryptfs documentation. The software is distributed with a very comprehensive set of manual pages.

eCryptfs has been included in Linux since version 2.6.19. Start by loading the ecryptfs module:

To actually mount an eCryptfs filesystem, you need to use userspace tools provided by the ecryptfs-utils package. Unfortunately, due to the poor design of these tools, you must choose between three ways of setting up eCryptfs, each with different tradeoffs:

Most of the user-friendly convenience tools installed by the ecryptfs-utils package assume that a very specific eCryptfs setup is being used, namely the one that is officially used by Ubuntu (where it can be selected as an option during installation). Unfortunately, these choices are not just default options, but are actually hard-coded in the tools. If this setup does not suit your needs, then you cannot use the convenience tools and will have to follow the steps at #Manual setup instead.

The setup used by these tools is as follows:

For a full $HOME directory encryption see #Encrypting a home directory

Before the data directory encryption is setup, decide whether it should later be mounted manually or automatically with the user log-in.

To encrypt a single data directory as a user and mount it manually later, run:

and follow the instructions. The option --nopwcheck enables you to choose a passphrase different to the user login passphrase and the option --noautomount is self-explanatory. So, if you want to setup the encrypted directory automatically on log-in later, just leave out both options right away.

The script will automatically create the ~/.Private/ and ~/.ecryptfs/ directory structures as described in the box above. It will also ask for two passphrases:

The mount point ("upper directory") for the encrypted folder will be at ~/Private/ by default. However, you can manually change this right after the setup command has finished running, by doing:

To actually use your encrypted folder, you will have to mount it - see #Mounting below.

The wrapper script ecryptfs-migrate-home will set up an encrypted home directory for a user and take care of migrating any existing files they have in their not yet encrypted home directory.

To run it, the user must be logged out and own no processes. The best way to achieve this is to log the user out, log into a console as the root user, and check that ps -U username returns no output. You also need to ensure that you have rsync, lsof, and which installed. Once the prerequisites have been met, run:

and follow the instructions. After the wrapper script is complete, follow the instructions for auto-mounting - see #Auto-mounting below. To complete this process, it is imperative that the user logs in before the next reboot.

Once everything is working, the unencrypted backup of the user's home directory, which is saved to /home/username.random_characters, should be deleted.

Executing the wrapper

and entering the passphrase is all needed to mount the encrypted directory to the upper directory ~/Private/, described in #Ubuntu tools.

will unmount it again.

The tools include another script useful in accessing the encrypted .Private data or home directory. Executing ecryptfs-recover-private as root will search the system (or an optional specific path) for the directory, interactively query the passphrase for it, and mount the directory. It can, for example, be used from a live-CD or different system to access the encrypted data in case of a recovery. Note that if booting from an Arch Linux ISO you must first install the ecryptfs-utils. Further, it will only be able to mount .Private directories created with the Ubuntu tools.

The default way to auto-mount an encrypted directory is via PAM. See pam_ecryptfs(8) and - for more details - 'PAM MODULE' in:

For auto-mounting, it is required that the encrypted directory passphrase is identical to the user's log-in password.

Use these steps to set up auto-mounting:

1. Check if ~/.ecryptfs/auto-mount, ~/.ecryptfs/auto-umount and ~/.ecryptfs/wrapped-passphrase exist (these are automatically created by ecryptfs-setup-private).

2. Add ecryptfs to the pam-stack exactly as following to allow transparent unwrapping of the passphrase on login:

Open /etc/pam.d/system-auth and after the line containing auth required pam_unix.so (or auth [default=die] pam_faillock.so authfail if present) add:

Next, above the line containing password required pam_unix.so (or -password [success=1 default=ignore] pam_systemd_home.so if present) insert:

And finally, after the line session required pam_unix.so add:

3. Re-login and check output of mount which should now contain a mountpoint, e.g.:

for the user's encrypted directory. It should be perfectly readable at ~/Private/.

The latter should be automatically unmounted and made unavailable when the user logs off.

Use ecryptfs-simple if you just want to use eCryptfs to mount arbitrary directories the way you can with EncFS. ecryptfs-simple does not require root privileges or entries in /etc/fstab, nor is it limited to hard-coded directories such as ~/.Private/. The package is available to be installed as ecryptfs-simpleAUR and from Xyne's repos.

As the name implies, usage is simple:

Automatic mounting: prompts for options on the first mount of a directory then reloads them next time:

Unmounting by source directory:

Unmounting by mountpoint:

The following details instructions to set up eCryptfs encrypted directories manually. This involves two steps. First, the passphrase is processed and loaded into the kernel keyring. Second, the filesystem is actually mounted using the key from the keyring.

There are two ways to add the passphrase to the kernel keyring in the first step. The simpler option is ecryptfs-add-passphrase, which uses a single passphrase to encrypt the files. The disadvantage is that you cannot change the passphrase later. It works like this:

You can also pipe a passphrase into ecryptfs-add-passphrase -. Keep in mind that if you leave your passphrase in a file, it will usually defeat the purpose of using encryption.

As an alternative to a plain passphrase, you can use a "wrapped passphrase", where the files are encrypted using a randomly generated key, which is itself encrypted with your passphrase and stored in a file. In this case, you can change your passphrase by unwrapping the key file with your old passphrase and rewrapping it using your new passphrase.

In the following we prompt for the wrapping passphrase and do a generation similar to the source[dead link 2025-08-15HTTP 404] and then use ecryptfs-wrap-passphrase to wrap it with the given password to ~/.ecryptfs/wrapped-passphrase:

Do not use a passphrase with more than 64 characters as this will result in an error later when using ecryptfs-insert-wrapped-passphrase-into-keyring.

Next, we can enter our passphrase to load the key into the keyring:

In either case, when you successfully add the passphrase to the kernel keyring, you will get a "key signature" like 78c6f0645fe62da0 which you will need in the next step.

There are two different ways of manually mounting eCryptfs, described in the following sections. The first way, using mount.ecryptfs_private, can be run as a regular user and involves setting up some configuration files. This method does not allow you to change the encryption settings, such as key size. The second way is to use a raw mount command, which gives you complete control over all settings, but requires you to either run it as root, or add an entry to /etc/fstab which lets a user mount eCryptfs.

This method involves running mount.ecryptfs_private from the ecryptfs-utils package, after first loading your passphrase. This binary requires no root privileges to work by default.

First choose a name for your configuration files in ~/.ecryptfs/ and decide on the lower and upper directories. In this example we use secret for the configuration files, put in encrypted data in ~/.secret/, and mount the decrypted files at ~/secret/. Create the required directories:

Now specify the directories in ~/.ecryptfs/secret.conf, using full paths. Its format looks like the one in /etc/fstab without the mount options:

Write the key signature you got from ecryptfs-add-passphrase or ecryptfs-insert-wrapped-passphrase-into-keyring (see above) into ~/.ecryptfs/secret.sig:

If you also want to enable filename encryption, add a second passphrase to the keyring (or reuse the first passphrase) and append its key signature to ~/.ecryptfs/secret.sig:

Finally, mount ~/.secret/ on ~/secret/:

When you are done, unmount it:

By running the actual mount command manually, you get complete control over the encryption options. The disadvantage is that you need to either run mount as root, or add an entry to /etc/fstab for each eCryptfs directory so users can mount them.

First create your private directories. In this example, we use the same ones as the previous section:

Now, supposed you have created the wrapped keyphrase above, you need to insert the encryption key once to the root user's keyring:

so that the following mount command succeeds:

Once you have chosen the right mount options, you can add an entry to /etc/fstab so regular users can mount eCryptfs on these directories. Copy the mount options to a new /etc/fstab entry and add the options user and noauto. The full entry will look similar to (bold entries added):

The setup is now complete and the directory should be mountable by the user.

To mount the encrypted directory as the user, the passphrase must be unwrapped and made available in the user's keyring. Following above section example:

Now the directory can be mounted without the mount helper questions:

and files be placed into the secret directory. The above two steps are necessary every time to mount the directory manually.

To finalize, the preliminary passphrase to wrap the encryption passphrase may be changed:

The unmounting should also clear the keyring, to check the user's keyring or clear it manually:

Different methods can be employed to automount the previously defined user-mount in /etc/fstab on login. As a first general step, follow point (1) and (2) of #Auto-mounting.

Then, if you login via console, a simple way is to specify the user-interactive mount and umount in the user's shell configuration files, for example Bash#Configuration files.

The factual accuracy of this article or section is disputed.

Another method is to automount the eCryptfs directory on user login using pam_mount. To configure this method, add the following lines to /etc/security/pam_mount.conf.xml:

Please prefer writing manually these lines instead of simply copy/pasting them (especially the lclmount line). Otherwise, you might get some corrupted characters. Explanation:

Then set the volume definition, preferably to ~/.pam_mount.conf.xml:

"noroot" is needed because the encryption key will be added to the user's keyring.

Finally, edit /etc/pam.d/system-login as described in the pam_mount article.

To avoid needlessly wasting time unwrapping the passphrase, you can create a script that will check pmvarrun to see the number of open sessions:

With the following line added before the eCryptfs unwrap module in your PAM stack:

The article suggests adding these to /etc/pam.d/login, but the changes will need to be added to all other places you login, such as /etc/pam.d/kde.

This article or section needs expansion.

- point to the above "Setup & Mounting" section for how to mount and unmount [this section here will cover all other (i.e. setup-independent) usage info] - reference ecryptfs tools not used/mentioned in the prior sections (e.g. with a short link to the online manpages and mention of the other tools usage, as it seems useful (not covered yet are, e.g. ecryptfs-stat, ecryptfs-find, ecryptfs-rewrite-file.) - mention the options to share an encrypted folder between users and to place non-encrypted files or folders in the encrypted container ("pass-through") (references for the points: [5] and (maybe) [6])

Besides using your private directory as storage for sensitive files, and private data, you can also use it to protect application data. Firefox for example has an internal password manager, but the browsing history and cache can also be sensitive. Protecting it is easy:

There are no special steps involved, if you want to remove your private directory. Make sure it is un-mounted and delete the respective lower directory (e.g. ~/.Private/), along with all the encrypted files. After also removing the related encryption signatures and configuration in ~/.ecryptfs/, all is gone.

If you were using the #Ubuntu tools to setup a single directory encryption, you can directly follow the steps detailed by:

and follow the instructions.

If you want to move a file out of the private directory just move it to the new destination while ~/Private/ is mounted.

With eCryptfs the cryptographic metadata is stored in the header of the files. Setup variants explained in this article separate the directory with encrypted data from the mount point. The unencrypted mount point is fully transparent and available for a backup. Obviously this has to be considered for automated backups, if one has to avoid leaking sensitive unencrypted data into a backup.

You can do backups, or incremental backups, of the encrypted (e.g. ~/.Private/) directory, treating it like any other directory.

Further points to note:

This is a known issue of Mosh server, which does not keep the eCryptfs /home directory mounted.

**Examples:**

Example 1 (unknown):
```unknown
truncate -s 1G file.img
```

Example 2 (unknown):
```unknown
# modprobe ecryptfs
```

Example 3 (unknown):
```unknown
~/.Private/
```

Example 4 (unknown):
```unknown
~/.Private/
```

---

## Core utilities

**URL:** https://wiki.archlinux.org/title/Find

**Contents:**
- Essentials
  - Preventing data loss
- Nonessentials
- Alternatives
  - cat alternatives
  - cd alternatives
  - date alternatives
  - cp alternatives
  - ls alternatives
  - find alternatives

Core utilities are the basic, fundamental tools of a GNU/Linux system. This article provides an incomplete overview of them, links their documentation and describes useful alternatives. The scope of this article includes, but is not limited to, the GNU Core Utilities. Most core utilities are traditional Unix tools and many were standardized by POSIX but have been developed further to provide more features.

Most command-line interfaces are documented in man pages, utilities by the GNU Project are documented primarily in Info manuals, some shells provide a help command for shell builtin commands. Additionally most utilities print their usage when run with the --help flag.

The following table lists some important utilities which Arch Linux users should be familiar with. See also intro(1).

rm, mv, cp and shell redirections happily delete or overwrite files without asking. rm, mv, and cp all support the -i flag to prompt the user before every removal / overwrite. Some users like to enable the -i flag by default using aliases. Relying upon these shell options can be dangerous, because you get used to them, resulting in potential data loss when you use another system or user that does not have them. The best way to prevent data loss is to create backups.

This table lists core utilities that often come in handy.

The moreutils package provides useful tools like sponge(1) that are missing from the GNU coreutils.

Alternative core utilities are provided by the following packages:

See also Bash#Auto "cd" when entering just a path and Zsh#Remembering recent directories.

This article or section is a candidate for moving to List of applications/Other.

Using rsync#As cp/mv alternative allows you to resume a failed transfer, to show the transfer status, to skip already existing files and to make sure of the destination files integrity using checksums.

For graphical file searchers, see List of applications/Utilities#File searching.

While diffutils does not provide a word-wise diff, several other programs do:

See also List of applications/Utilities#Comparison, diff, merge.

These tools aim to replace grep for code search. They do recursive search by default, skip binary files and respect .gitignore.

This article or section needs expansion.

See also: dd and ddrescue

This subsection lists dd implementations whose interface and default behaviour is mostly compliant with the POSIX specification of dd(1p).

The GNU implementation of dd found in coreutils also conforms to POSIX. This subsection lists its forks.

This subsection lists dd alternatives that do not conform to POSIX (in terms of the JCL-resembling command-line syntax and default behaviour).

This subsection lists forks of bufferAUR, a general-purpose I/O buffering utility similar to dd but has a dynamic-sized buffer. It supports blockwise I/O and can be used when dumping from/to an LTO-tape to avoid shoe shining.

See also List of applications/Utilities#Disk usage display.

Many common packages already install most popular POSIX utilities as dependencies, but the posix metapackage can be installed to ensure all of them being always present.

Beside mandatory utilities, there are also metapackages for some of the optional categories:

Some commands (arch, kill, etc.) are missing from coreutils or taken from other packages. To complete them for compatibility, install uutils-coreutils and do:

**Examples:**

Example 1 (unknown):
```unknown
--color-words
```

Example 2 (unknown):
```unknown
# ln -sf /usr/bin/uu-coreutils /usr/local/bin/arch
# echo -e "#compdef arch=uu-arch\n_uu-arch" > /usr/local/share/zsh/site-functions/_arch
# echo "complete -c arch -w uu-arch" > /usr/local/share/fish/vendor_completions.d/arch.fish
```

---

## syslog-ng

**URL:** https://wiki.archlinux.org/title/Syslog-ng

**Contents:**
- Overview
- Installation
  - systemd/journald integration
- Sources
  - syslog-ng and systemd journal
- Destinations
- Creating filters for messages
- Log paths
- Tips and tricks
  - Have syslog-ng reload the configuration file

syslog-ng is a syslog implementation which can take log messages from sources and forward them to destinations, based on powerful filter directives. Although its origins are syslog, it is a pretty generic log management tool, being able to consume structured and unstructured log messages, parsing and transforming them if necessary.

syslog-ng takes incoming log messages from defined 'sources' and forwards them to the appropriate destinations, based on powerful filter directives. In a typical simple set-up, syslog-ng will read messages from three sources:

Sources are defined using the "source" directive. These incoming messages are then filtered according to defined filters ("filter" keyword), i.e. according to originating program or log level, and sent to the appropriate "destination".

Destinations include log files (e.g. /var/log/messages.log), printing messages on a console and remote servers.

The pivotal function is log. This function defines which filters should be applied to a certain source, and where the resulting messages should be sent to.

Apart from local sources, as explained above, syslog-ng is also able to work with various sources over the network. To work with these, you will have to create a network() or udp() or tcp() source as explained below.

Install the syslog-ng package.

To use syslog-ng, start/enable syslog-ng@default.service.

syslog-ng pulls in the messages from the systemd journal by default. Keeping ForwardToSyslog=no in /etc/systemd/journald.conf is recommended in order to avoid the overhead associated with the socket and to avoid needless error messages in the log. If on the other hand you do not want to store your logs twice and turn journald's Storage=none, you will need ForwardToSyslog=yes, as syslog-ng tries to follow the 'journald' journal file.

See #syslog-ng and systemd journal for more details.

syslog-ng receives log messages from a source. To define a source you should follow the following syntax:

In the simplest case, you will just need a single system() driver.

This will automatically detect the best way to collect local logs and will make sure kernel, application and internal logs from syslog-ng are all collected.

You can look at the identifiers and source-drivers in the alternative syslog-ng manuals (as the ones on syslog-ng.com were unreachable).

The system() driver is actually a higher-level construct that expands to various sources as needed for the local system. But you can acquire more control by removing the system() source and use the lower level drivers directly.

The unix-stream() source-driver opens the given AF_UNIX socket and starts listening on it for messages.

The internal() source-driver "receives" messages generated internally, by syslog-ng itself.

Therefore, the following means: src gets messages from the /dev/log socket and syslog-ng.

The kernel sends log messages to /proc/kmsg and the file() driver reads log messages from files. Therefore, the following means kernsrc gets messages from file /proc/kmsg:

To open a port to read data from a remote server a source must be defined with this syntax:

to receive log messages via TCP. Both listen on port 514, unless overridden with the port() parameter.

Starting with syslog-ng version 3.6.1 the default system() source on Linux systems using systemd uses journald as its standard system() source.

If you wish to use both the journald and syslog-ng, ensure the following settings are in effect. For systemd-journald, in the /etc/systemd/journald.conf file, Storage= either set to auto or unset (which defaults to auto) and ForwardToSyslog= set to yes. For /etc/syslog-ng/syslog-ng.conf, you need the following source stanza:

If, on the other hand, you wish not to retain the journald logs, but only syslog-ng's text logs, set Storage=volatile in /etc/systemd/journald.conf. This will store journald in ram. As of syslog-ng 3.6.3, syslog-ng is using journald as the system(); source so if you set Storage=none, the systemd journal will drop all messages and not forward them to syslog-ng.

After the change restart the systemd-journald.service and syslog-ng@default.service daemons.

In syslog-ng, log messages are sent to files. The syntax is very similar to sources:

You will be normally logging to a file, but you could log to a different destination-driver: pipe, Unix socket, TCP-UDP ports, terminals or to specific programs. Simply declaring a destination will not cause messages to be delivered to that destination: that will only happen as soon as you connect sources and destinations using log statements. Log statements can also include filters, thereby implementing a flexible log routing functionality.

This declaration instructs syslog-ng to send messages to /var/log/auth.log:

If the user is logged in, usertty() sends messages to the terminal of the specified user. If you want to send console messages to root's terminal if it is logged in:

Messages can be sent to a pipe with pipe(). The following sends xconsole messages to the pipe /dev/xconsole. This needs some more configuration, so you could look at the sub-section xconsole below.

To send messages on the network, use udp(). The following will send your log data out to another server.

You can also use the newer network() driver syntax for the same:

The syntax for the filter statement is:

Functions can be used in the expression, such as the function facility() which selects messages based on the syslog facility codes (kern, mail, auth, etc). Apart from facility codes, each log message is associated with a severity value; where debug is the most verbose, and panic only shows serious errors. You can find the facilities, log levels and priority names in /usr/include/sys/syslog.h or in RFC 3164. To filter those messages coming from authorization, like su(pam_unix)[18569]: session opened for user root by (uid=1000) use the following:

The facility expression can use the boolean operators and, or, and not, so the following filter selects those messages not coming from authorization, network news or mail:

The function severity() selects messages based on its severity level, so if you want to select informational levels:

Functions and boolean operators can be combined in more complex expressions. The following line filters messages with a priority level from informational to warning not coming from auth, authpriv, mail and news facilities:

Messages can also be selected by matching a regular expression in the message with the function match("regex" value("<macro>")). For example this would match the main part of the message against the regexp "failed":

In filter expressions, you can use both pre-defined and user defined macros. These are also called "hard" and "soft" macros respectively.

A list and documentation of all macros can be found in the syslog-ng documentation:

To filter messages received from a particular remote host (as declared in the incoming message itself and not by its IP address), the host() function must be used:

If you would rather filter on sending IP address, you can use the netmask() filter:

syslog-ng connects sources, filters and destinations with log statements. The syntax is:

The following for example sends messages from src source to mailinfo destination filtered by f_info filter:

A log statement describes a pipeline: it tells syslog-ng to take messages from a source (or multiple sources) and deliver them to a destination (or multiple destinations) assuming the associated filters are matching.

If you have multiple log statements that take messages from the same source, messages will be duplicated along with all of those pipelines. Of course, you can apply a different set of filters and therefore route messages selectively to multiple destinations.

Apart from filtering, syslog-ng can apply parsing or rewriting of messages. Parsing means the extraction of information from the message text, whereas rewriting a message means that syslog-ng can change/reformat messages as needed.

The log statement has a lot more to offer:

You can read more about log statements in a chapter of the syslog-ng documentation.

After understanding the logic behind syslog-ng, many possible and complex configuration are possible. Here there are some examples.

You can make syslog-ng re-evaluate the configuration file. You can do so manually by sending a SIGHUP to the process, or reload syslog-ng@default.service.

This setup shows how to send the default unencrypted syslog packets across both TCP and UDP protocols, using the standard port (514) and an alternate port. This is sending the same output to the same machine 4 different ways to try and make sure packets make it. Mostly useful if you are debugging a remote server that fails to reboot. The different ports and protocols are to make it past any firewall filters or other network problems. Also useful for port-forwarding and using tunnels. Something like this setup is ideal to tunnel across an ssh connection that the prone-to-failover host initiates through a reverse connection.

And then on the loghost receiving these logs:

In order to move some log from /var/log/messages to another file.

Make sure you add this block above your usual log statements. Due to flags(final) in the log statement, anything that matches the "sshd" filter would only be sent to ssh.log and processing of the message would stop at that point.

Configuring your system to be a loghost is quite simple. Drop the following into your configuration, and create the needed directory. With this simple configuration, log filenames will be based on the FQDN of the remote host, and located in /var/log/remote/. After creating the remote directory, reload your syslog-ng configuration.

You could also consider the default-network-drivers() source that will open multiple ports, accepting messages over multiple different syslog protocols that are usually deployed in the field.

syslog-ng's performance can be improved in different ways:

It seems that the old sync(X) option is called flush_lines(X) now, where the writing to the file is buffered for X lines. Default is 100.

syslog-ng is doing message processing in parallel, as streams of messages are received using the many different source mechanisms. To avoid starving one source connection over the other, syslog-ng both uses threading and imposes limits on how much messages would it process from a single source connection at a time.

This means that even though a source application might have sent 1000 messages in a tight loop, syslog-ng would process it 100 pieces at a time (the exact limit is specified by log-fetch-limit()) and after every 100, it would re-check if other connections are also in need of processing. This has some overhead, and syslog-ng performance can be increased significantly by increasing log-fetch-limit()

The other mechanism that can use tuning based on a specific use-case is the window size setting used for backpressure propagation. This is the log-iw-size() parameter that controls how many messages can be in-flight before a destination acknowledges them. By increasing the log-iw-size() you can let it work on more messages before it would stop to allow destinations to consume messages.

By increasing log-iw-size() your memory/disk buffer use will increase, as syslog-ng will have to put the messages somewhere.

A single log message can be sent to different log files several times. For example, in the initial configuration file, we have the following definitions:

The same message from the cron facility will end up in both the cron.log and messages files. To change this behavior we can use the final flag, ending up further processing with the message. Therefore, in this example, if we want messages from the cron facility not ending up in the messages file, we should change the cron's log sentence by:

another way is to exclude the cron facility from f_messages filter:

This section will use two roles: syslog and logwriter. syslog will be the administrator of the database syslog and logwriter will only be able to add records to the logs table.

No longer needed to create table for logs. syslog-ng will create automatically.

Edit pg_hba.conf to allow syslog and logwriter to establish a connection to PostgreSQL.

Then reload postgresql.service.

Edit /etc/syslog-ng/syslog-ng.conf so that it knows where and how to write to PostgreSQL. syslog-ng will utilize the logwriter role.

Finally, restart syslog-ng.service.

And check to see if things are being logged.

Add ts_format(iso); to /etc/syslog-ng/syslog-ng.conf in the options section. Example:

Then reload syslog-ng.service.

Same as above, except use rfc3339 instead of iso for ts_format

Log levels are defined separately for each logged facility in syslog-ng config. Available log levels are listed in /usr/include/sys/syslog.h:

Macros can be used in both templates, and in destination file names. Macros of syslog-ng OSE.

The following code will write the log lines to /var/log/test.log in the format of macroname=value@.

You can create your own value list as below once syslog-ng is restarted with: tail /var/log/test.log|tr "@" "\n"

Starting from version 3.16 syslog-ng is capable to receive and parse messages on the most common ports with the most common parsers using the default-network-drivers() source driver.

**Examples:**

Example 1 (unknown):
```unknown
/var/log/messages.log
```

Example 2 (unknown):
```unknown
syslog-ng@default.service
```

Example 3 (unknown):
```unknown
ForwardToSyslog=no
```

Example 4 (unknown):
```unknown
/etc/systemd/journald.conf
```

---

## Extended attributes

**URL:** https://wiki.archlinux.org/title/Extended_attributes

**Contents:**
- User extended attributes
- Preserving extended attributes
- Support
  - File systems
  - Software
- Other tagging systems
  - gvfs
- See also

From xattr(7): "Extended attributes are name:value pairs associated permanently with files and directories". There are four extended attribute classes: security, system, trusted and user.

Extended attributes are also used to set Capabilities.

User extended attributes can be used to store arbitrary information about a file. To create one:

Use getfattr to display extended attributes:

To remove an extended attribute:

To find files with certain extended attributes use rawhideAUR:

Some other user extended attributes include:

XDG also proposes a set of standardized extended attributes to be used by programs:

user.xdg.tags is not part of the official standard, but it has become a "de facto" standard as several popular programs have implemented support for it (see #Software). It is implemented as a CSV list of user-specified tags for each file.

To preserve extended attributes with text editors you need to configure them to truncate files on saving instead of using rename(2). [1]

Just like you should do for any other data you do not want to lose, you should make regular backups of your extended attributes. To make a full backup of the extended attributes of all files in the current directory (recursively):

All major Linux file systems including Ext4, Btrfs, ZFS, and XFS support extended attributes. The kernel allows to have extended attribute names of up to 255 bytes and values of up to 64 KiB, but Ext4 and Btrfs might impose smaller limits, requiring extended attributes to be within a "filesystem block".

NTFS uses Alternative Data Streams to store user. The mount option user_xattr or streams_interface=xattr should be used by default. However, it might not be supported if mount option streams_interface=windows is used. ntfs-3g supports mapping Alternative Data Streams to extended attributes in FUSE.

NFS supports extended attributes since Linux 5.9.

It might not be possible to use extended attributes due to lack of support of either the file system or software. For this reason, many media formats store metadata included in the file format that can be viewed using programs like Exiftool or more specified ones like id3AUR for audio.

Another filesystem-independent workaround is Gnome virtual filesystem: gvfs which is used to store metadata (gvfsd-metadata). For example, Firefox stores metadata this way and can be viewed with:

Other programs that use this approach include:

**Examples:**

Example 1 (unknown):
```unknown
$ setfattr --name=user.checksum --value="3baf9ebce4c664ca8d9e5f6314fb47fb" file.txt
```

Example 2 (unknown):
```unknown
$ getfattr --encoding=text --dump file.txt
```

Example 3 (unknown):
```unknown
# file: file.txt
user.checksum="3baf9ebce4c664ca8d9e5f6314fb47fb"
```

Example 4 (unknown):
```unknown
$ setfattr --remove=user.checksum file.txt
```

---

## Internet sharing

**URL:** https://wiki.archlinux.org/title/Internet_sharing

**Contents:**
- Requirements
- Configuration
  - Static IP address
  - Enable packet forwarding
    - Packet forwarding with systemd-networkd
  - Enable NAT
    - With iptables
    - With nftables
    - With firewalld
  - Assigning IP addresses to the client PC(s)

This article explains how to share the internet connection from one machine to other(s).

The machine acting as server should have an additional network device, aka network interface. That network device requires a functional data link layer to the machine(s) that are going to receive internet access:

All configuration is done on the server computer, except for the final step of #Assigning IP addresses to the client PC(s).

On the server computer, assign a static IPv4 address to the interface connected to the other machines. The first 3 bytes of this address cannot be exactly the same as those of another interface, unless both interfaces have netmasks strictly greater than /24.

To have your static IP assigned at boot, you can use a network manager.

To check the current packet forwarding settings, run:

You will note options for controlling forwarding per default, per interface, as well as separate options for IPv4/IPv6 per interface. For detailed description of all available options, see the kernel documentation.

To enable IPv4 and IPv6 packet forwarding, configure sysctl to set these settings:

This article or section is out of date.

If the system uses systemd-networkd to control the network interfaces, a per-interface setting for IPv4 is not possible, i.e. systemd logic propagates any configured forwarding into a global (for all interfaces) setting for IPv4. The advised work-around is to use a firewall to forbid forwarding again on selective interfaces. See the systemd.network(5) manual page for more information. The IPForward=kernel semantics introduced in a previous systemd release 220/221 to honor kernel settings does not apply anymore.[1] [2]

To make changes persistent across reboots, see Sysctl#Configuration. You might consider writing settings to a file with a descriptive filename, such as /etc/sysctl.d/30-ipforward.conf.

Afterwards it is advisable to double-check forwarding is enabled as required after a reboot.

This article or section is a candidate for merging with systemd-networkd#[Network].

If you are using systemd-networkd to manage your network configuration, you can also persist those settings across reboots:

This essentially sets the same net.ipv[46].conf.interface_name.forwarding=1 as mentioned in previous section. For IPv6 the configuration is IPv6Forwarding=yes.

This sets up packet forwarding for the specific interface only. For internet sharing to properly work, you need to enable packet forwarding on both (all) interfaces where traffic should be routed between. Typically your lan and wan interfaces.

See also systemd-networkd#[Network][broken link: invalid section].

Besides the methods listed here, its also possible to use ufw to set up a NAT.

Install the iptables package. Use iptables to enable NAT:

Use -I DOCKER-USER instead of -A FORWARD if you installed docker. [4]

If connected via PPPoE, clamp mss to pmtu in order to prevent fragmentation:

Read the iptables article for more information (especially saving the rule and applying it automatically on boot). There is also an excellent guide on iptables Simple stateful firewall.

Install the nftables package. To enable NAT with nftables, you will have to create the postrouting chain in a new/existing table:

After that, you have to masquerade the net0 addresses for internet0:

Many firewall configurations, like the default /etc/nftables.conf, set the default policy of the 'filter' table's 'forward' chain to 'drop'. In such cases, you will need rules to allow forwarding NAT traffic:

You can find more information on NAT in nftables in the nftables Wiki. If you want to make these changes permanent, follow the instructions on nftables.

Install the firewalld package. firewalld is a firewall daemon which relies on nftables or iptables. First change the firewalld zones of network interfaces:

Then add a new policy to let traffic flow between the internal and external zone:

For example, to allow only nodes in 192.168.2.0/24 to access the internet, do:

firewall-cmd --permanent --policy int2ext --add-rich-rule='rule family=ipv4 source address=192.168.2.0/24 accept' Do not forget to reload rules afterwards:

If you are planning to regularly have several machines using the internet shared by this machine, then is a good idea to install a DHCP server. See Router#DNS and DHCP for the available options. Then configure a DHCP client on every client PC, see Network configuration#Network managers.

This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.

Incoming connections to UDP port 67 has to be allowed for DHCP server. It also necessary to allow incoming connections to UDP/TCP port 53 for DNS requests.

Alternatively using firewalld

If you are not planning to use this setup regularly, you can manually add an IP to each client instead.

Instead of using DHCP, a static IP address and a default route via 192.168.123.100 can also be configured manually. There are many tools available to configure the network accordingly. One prominent example of such a tool is ip(8), see Network configuration#Network management. Alternatively, one can use a .network file, see systemd-networkd#Wired adapter using a static IP to setup a static IP.

Configure a DNS server for each client, see Domain name resolution for details.

That is it. The client PC should now have Internet.

If you are able to connect the two PCs but cannot send data (for example, if the client PC makes a DHCP request to the server PC, the server PC receives the request and offers an IP to the client, but the client does not accept it, timing out instead), check that you do not have other iptables rules interfering.

Symptoms might also include: Clients get host is down when pinging host, gets no route to host or Destination Host Unreachable when pinging devices outside the LAN (that should be forwarded by NAT), DHCP offers not crossing a bridge, ...

It is known that docker may cause these problems. Simply disabling docker.service and docker.socket solves this problem.

First PC have two LANs. Second PC have one LAN and connected to first PC. Lets go second PC to give all access to LAN after bridged interface:

This article or section needs expansion.

**Examples:**

Example 1 (unknown):
```unknown
ethtool interface | grep MDI
```

Example 2 (unknown):
```unknown
# ip link set up dev net0
# ip addr add 192.168.123.100/24 dev net0 # arbitrary address
```

Example 3 (unknown):
```unknown
# sysctl -a | grep forward
```

Example 4 (unknown):
```unknown
net.ipv4.ip_forward = 1
net.ipv4.conf.all.forwarding = 1
net.ipv6.conf.all.forwarding = 1
```

---

## Logwatch

**URL:** https://wiki.archlinux.org/title/Logwatch

**Contents:**
- Installation
- Configuration

Logwatch is a powerful and versatile log parser and analyzer. Logwatch is designed to give a unified report of all activity on a server, which can be delivered through the command line or email.

In addition to the logwatch binaries, scripts and configuration files, the package used to include a cron job that was installed as /etc/cron.daily/0logwatch. You need to start/enable logwatch.timer to generate regular logwatch reports.

Logwatch has a tiered configuration approach. There are several locations where configuration details can be specified, with each one superseding the previous one:

Logwatch will parse all these location when called.

Within these directories, there are several areas of configuration. The logwatch.conf files are where most of the high-level settings are, which allow you to set where your reports are sent, how they are formatted, etc. The configuration file at /usr/share/logwatch/default.conf/logwatch.conf contains all the default settings and comments on what they do. It is recommended to leave the default configuration alone and instead re-define a setting variable you want to change in /etc/logwatch/conf/logwatch.conf.

Within the logfiles directory of any locations are configuration files detailing specific log files. By default, most of the common log files found in a Linux system are already accounted for. If you have some esoteric application that does not have a log file configuration already, copy an existing one from the default.conf/logfiles directory and customize it for your application.

The services folder contains similar definitions, but these one define the various services reported by logwatch. This is necessary because often multiple services will report to the same log (e.g. messages, dmesg, boot, etc.). For more information, examine some of the default services files.

Note that if you want logwatch messages delivered by email, you need to install a package that provides a sendmail frontend. Postfix is a good choice.

There is a helpful document supplied with the package to give further information on configuration. It is located at /usr/share/logwatch/HOWTO-Customize-LogWatch.

**Examples:**

Example 1 (unknown):
```unknown
/etc/cron.daily/0logwatch
```

Example 2 (unknown):
```unknown
logwatch.timer
```

Example 3 (unknown):
```unknown
/usr/share/logwatch/default.conf/*
```

Example 4 (unknown):
```unknown
/etc/logwatch/conf/dist.conf/*
```

---

## procfs

**URL:** https://wiki.archlinux.org/title/Procfs

**Contents:**
- Content
  - Kernel & system information
  - Processes
- Usage
- See also

This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.

The proc file system, also called procfs, is a pseudo file system that is usually mounted at /proc and contains information about the running system:

There are many files under /proc which provide a lot of information about the system as well as the kernel. There are too many to cover them all here, but some of them are listed below with brief information about what they are.

Inside /proc/pid is stored information about every process currently running. Below is an example showing some of the PIDs currently running:

Lets take for example pid 1057 and see what is inside:

You can interact with /proc contents as with regular files.

**Examples:**

Example 1 (unknown):
```unknown
/proc/cpuinfo
```

Example 2 (unknown):
```unknown
/proc/meminfo
```

Example 3 (unknown):
```unknown
/proc/vmstat
```

Example 4 (unknown):
```unknown
/proc/mounts
```

---

## Keyboard input

**URL:** https://wiki.archlinux.org/title/Keyboard_input

**Contents:**
- Identifying scancodes
  - Using evtest
  - Using showkey
  - Using dmesg
- Identifying keycodes
  - Identifying keycodes in console
  - Identifying keycodes in Xorg
  - Identifying keycodes in Wayland
- Tips and tricks
  - Configuration of VIA compatible keyboards

Prerequisite for modifying the key mapping is knowing how a key press results in a symbol:

Most of your keys should already have a keycode, or at least a scancode. Keys without a scancode are not recognized by the kernel; these can include additional keys from "gaming" keyboards, etc.

In Xorg, some keysyms (e.g. XF86AudioPlay, XF86AudioRaiseVolume etc.) can be mapped to actions (i.e. launching an external application). See Keyboard shortcuts#Xorg for details.

In Linux console, some keysyms (e.g. F1 to F246) can be mapped to certain actions (e.g. switch to other console or print some sequence of characters). See Console keyboard configuration#Creating a custom keymap for details.

The most reliable way to obtain a scancode is to reference the MSC_SCAN evdev event produced when the key is pressed [1]. There are multiple evdev API testers, but the most straightforward is evtest(1) from the evtest package:

Use the "value" field of MSC_SCAN. This example shows that NumLock has scancode 70053 and keycode 69.

The traditional way to get a scancode is to use the showkey(1) utility. showkey waits for a key to be pressed, or exits if no keys are pressed within 10 seconds. For showkey to work you need to be in a virtual console, not in a graphical environment or logged in via a network connection. Run the following command:

and try to push keyboard keys; you should see scancodes being printed to the output.

You can get the scancode of a key by pressing the desired key and looking at the output of dmesg. For example, if you get:

then the scancode you need is 0xa0.

The Linux keycodes are defined in /usr/include/linux/input-event-codes.h (see the KEY_ variables).

The keycodes for virtual console are reported by the showkey(1) utility. showkey waits for a key to be pressed and if none are, in a span of 10 seconds, it quits. To execute showkey, you need to be in a virtual console, not in a graphical environment. Run the following command:

and try to push keyboard keys; you should see keycodes being printed to the output.

This article or section needs expansion.

The keycodes used by Xorg are reported by a utility called xev(1), which is provided by the xorg-xev package. Of course to execute xev, you need to be in a graphical environment, not in the console.

With the following command you can start xev and show only the relevant parts:

Xbindkeys is another wrapper to xev that reports keycodes.

If you press a key and nothing appears in the terminal, it means that either the key does not have a scancode, the scancode is not mapped to a keycode, or some other process is capturing the keypress. If you suspect that a process listening to X server is capturing the keypress, you can try running xev from a clean X session:

Although xev works through xwayland, you can also use wev to access keycodes under pure Wayland.

For example, this command lets you retrieve only key names and their UTF-8 equivalent:

VIA is a program to remap keys directly into compatible keyboards. In case you have one of those, in order for the keyboard to be picked up by the browser and configure it online, you need to add a custom udev rule changing the permissions of devices accessed through the hidraw driver.

Create the following udev rule:

Then reload the rules to take effect.

**Examples:**

Example 1 (unknown):
```unknown
XF86AudioPlay
```

Example 2 (unknown):
```unknown
XF86AudioRaiseVolume
```

Example 3 (unknown):
```unknown
# evtest /dev/input/event12
```

Example 4 (unknown):
```unknown
...
Event: time 1434666536.001123, type 4 (EV_MSC), code 4 (MSC_SCAN), value 70053
Event: time 1434666536.001123, type 1 (EV_KEY), code 69 (KEY_NUMLOCK), value 0
Event: time 1434666536.001123, -------------- EV_SYN ------------
```

---

## xrandr

**URL:** https://wiki.archlinux.org/title/Xrandr

**Contents:**
- Installation
  - Graphical front-ends
  - CLI front-ends
- Testing configuration
- Configuration
  - Scripts
    - Toggle external monitor
    - Manage 2-monitors
    - Avoid X crash with xrasengan
  - Configuration using arandr

xrandr(1) is an official configuration utility to the RandR (Resize and Rotate) X Window System extension. It can be used to set the size, orientation or reflection of the outputs for a screen. For configuring multiple monitors see the Multihead page.

When run without any option, xrandr shows the names of different outputs available on the system (VGA-1, HDMI-1, etc.) and resolutions available on each, with a * after the current one and a + after the preferred one:

You can use xrandr to set different resolution (must be present in the above list) on some output:

When multiple refresh rates are present in the list, it may be changed by the --rate option, either at the same time or independently. For example:

The --auto option will turn the specified output on if it is off and set the preferred (maximum) resolution:

It is possible to specify multiple outputs in one command, e.g. to turn off HDMI-1 and turn on HDMI-2 with preferred resolution:

xrandr is just a simple interface to the RandR extension and has no configuration file. However, there are multiple ways of achieving persistent configuration:

This script toggles between an external monitor (specified by $extern) and a default monitor (specified by $intern), so that only one monitor is active at a time.

The default monitor should be connected when running the script, which is always true for a laptop.

monsAUR is a POSIX-compliant shell script to quickly manage 2-monitor displays.

It provides well-known modes like computer, duplicate, extend and projector mode as well as selecting and positioning one or two monitors among those plugged in (for more details, see mons).

Use this workaround to turn on connected outputs that may be in suspend mode and hence shown as disconnected, as is often the case of DisplayPort monitors:

xrasenganAUR is an xrandr wrapper with this workaround built in.

With the --force option, xrasengan will update status of all outputs before HDMI-0 is turned off, avoiding an X crash if they were the only connected/active outputs.

To force reload current settings, xrasengan provides a --try-reload-active-layout option, which uses --force and unxrandr from the arandr package to assemble the command line:

This can be used in systemd unit or in a keyboard binding to avoid blank screen when resuming DisplayPort monitors from suspend.

arandr can graphically arrange your monitors, change resolutions, and save a script to duplicate your setup. By default, if you "Save As" it will be saved in ~/.screenlayout/. These files can then be autostarted. Sometimes problems arise from running the arandr script too soon after login, add a sleep command if needed.

For some LCD screens (e.g. Samsung 2343NW, Acer XB280HK and Iiyama ProLite XUB3490WQSU-B1) the command cvt -r can be used to calculate standardized modelines with reduced blanking, allowing for higher frequency signals.

For example: an external monitor ProLite XUB3490WQSU-B1 connected to a Dell laptop through a Thunderbolt-HDMI 2.0 adapter, using 59.97Hz refresh rate with a blinking problem:

Calculating the reduced modelines for the desired resolution 3440x1440 gives:

With this information we can use xrandr to create a new mode:

And add it to the set of valid modes for the corresponding output to make it selectable:

Due to buggy hardware or drivers, your monitor's correct resolutions may not always be detected by xrandr. For example, the EDID data block queried from the monitor may be incorrect. To fix this at a low level, see Kernel mode setting#Forcing modes and EDID. This section will describe how to address this at a higher level by adding the desired resolutions to xrandr. This same procedure can be used to add refresh rates you know are supported, but not enabled by your driver.

First we run gtf or cvt to get the Modeline for the resolution we want:

An example for the AMDGPU video driver (xf86-video-amdgpu):

Then we create a new xrandr mode. Note that the Modeline keyword needs to be omitted.

After creating it we need an extra step to add this new mode to our current output (VGA1). We use just the name of the mode, since the parameters have been set previously.

Now we change the resolution of the screen to the one we just added:

Note that these settings only take effect during this session. See Autostarting#On Xorg startup for a way to automatically apply them on startup.

If you are not sure about the resolution you will test, you may add a sleep 5 and a safe resolution command line following, like this:

Also, change VGA1 to correct output name.

If the previous method results in an *ERROR* EDID checksum is invalid error during boot, see KMS#Forcing modes and EDID and [2].

Or xrandr --addmode might give you the error X Error of failed request: BadMatch. NVIDIA users should read NVIDIA/Troubleshooting#xrandr BadMatch. BadMatch could indicate an invalid EDID checksum. To verify that this is the case, run X in verbose mode (e.g. startx -- -logverbose 6) and check your Xorg log for messages about a bad EDID.

If you use GNOME and your monitor does not have an EDID, above #Adding undetected resolutions might not work, with your screen just blinking once, after xrandr --output.

Poke around with ~/.config/monitors.xml, or delete the file completely, and then reboot.

It is better explained in this article.

Once a suitable resolution is found using xrandr, the mode can be permanently added by creating an entry in /etc/X11/xorg.conf.d/:

Replace intel with the right driver, e.g. nvidia. When the X server is restarted, you should be able to set the new resolution.

If this does not work for you, try removing the Screen and Device sections and just leaving the Monitor section. [3]

If your video card is recognized but the resolution is lower than you expect, you may try this.

Background: ATI X1550 based video card and two LCD monitors DELL 2408(up to 1920x1200) and Samsung 206BW(up to 1680x1050). Upon first login after installation, the resolution default to 1152x864. xrandr does not list any resolution higher than 1152x864. You may want to try editing /etc/X11/xorg.conf, add a section about virtual screen, logout, login and see if this helps. If not then read on.

About the numbers: DELL on the left and Samsung on the right. So the virtual width is of sum of both LCD width 3600=1920+1680; Height then is figured as the max of them, which is max(1200,1050)=1200. If you put one LCD above the other, use this calculation instead: (max(width1, width2), height1+height2).

DDX drivers other than that of the modesetting(4) driver may take time to properly enumerate the modes of attached devices, to where xrandr may not work right away. This seems to be the case for the xf86-video-intel driver, with which using xrandr early in the startup sets the incorrect resolution. Possible remedies include:

This does the waiting in the background, as to not block the rest of the startup. If this not desirable, e.g. your window manager configuration depends on the display being arranged correctly, you can execute the commands in the foreground:

With a flat panel TV, overscan looks like the picture is "zoomed in" so the edges are cut off.

Check your TV if there is a parameter to change. If not check if the output has support for the underscan property (xrandr --prop), if so apply an underscan and change border values. The required underscan vborder and underscan hborder values can be different for you, just check it and change it by more or less.

If underscan is not available another solution is using xrandr --transform a,b,c,d,e,f,g,h,i, which applies a transformation matrix on the output. See the xrandr(1)  RandR_version_1.3_options manual page for the explanation of the transformation.

For example, the transformation scaling horizontal coordinates by 0.8, vertical coordinates by 1.04 and moving the screen by 35 pixels right and 19 pixels down, is:

In some cases, a non-existent monitor may be detected by the system. To disable it, find the name of the phantom output, e.g. VGA1, and turn it off with

To make this permanent, add the following to an entry in /etc/X11/xorg.conf.d/:

If you are seeing very prominent interlace pattern artifacts (mesh or grid) when you see movement on the screen with this monitor, it might be happening because of a low refresh rate. Switching to a higher refresh rate (from 60 Hz to 119.98 Hz and perhaps even higher) might help reduce the effect.

Sample xrandr output for this monitor over HDMI:

As can be seen in the output above, the preferred refresh rate reported by xrandr is 60.00, but the artifacts are very visible with this refresh rate. Switching to 119.98 should help reduce the effect considerably.

**Examples:**

Example 1 (unknown):
```unknown
Screen 0: minimum 320 x 200, current 3200 x 1080, maximum 8192 x 8192
VGA-1 disconnected (normal left inverted right x axis y axis)
HDMI-1 connected primary 1920x1080+0+0 (normal left inverted right x axis y axis) 531mm x 299mm
   1920x1080     59.93 +  60.00*   50.00    59.94  
   1920x1080i    60.00    50.00    59.94  
   1680x1050     59.88  

```

Example 2 (unknown):
```unknown
$ xrandr --output HDMI-1 --mode 1920x1080
```

Example 3 (unknown):
```unknown
$ xrandr --output HDMI-1 --mode 1920x1080 --rate 60
```

Example 4 (unknown):
```unknown
$ xrandr --output HDMI-1 --auto
```

---

## Display Power Management Signaling

**URL:** https://wiki.archlinux.org/title/DPMS

**Contents:**
- Linux console
- Xorg
  - Configuration
  - Runtime settings
- See also

This article or section needs expansion.

VESA Display Power Management Signaling (DPMS) enables power saving behaviour of monitors when the computer is not in use. The time of inactivity before the monitor enters into a given saving power levelstandby, suspend or offcan be set as described in DPMSSetTimeouts(3).

To alter the terminal, use the setterm command. Its syntax (where 0 disables):

Some commands just write the terminal sequences to the current terminal device, whether that be in screen, a remote ssh terminal, console mode, serial consoles, etc.

To see the escape codes used, pipe the output as follows:

To modify a specific terminal, redirect the escape codes to it (with write permission):

To fully disable DPMS and screen blanking on the X Window System, create configuration files:

If you simply want to adjust the delays, change the duration (in minutes):

It is possible to turn off your monitor with the xset command which is provided by the xorg-xset package.

To query the current settings:

See xset(1) for all available commands.

**Examples:**

Example 1 (unknown):
```unknown
$ setterm --blank [0-60|force|poke]
$ setterm --powersave [on|vsync|hsync|powerdown|off]
$ setterm --powerdown [0-60]
```

Example 2 (unknown):
```unknown
setterm --powerdown
```

Example 3 (unknown):
```unknown
APM_DISPLAY_BLANK
```

Example 4 (unknown):
```unknown
consoleblank
```

---

## ClamAV

**URL:** https://wiki.archlinux.org/title/ClamAV

**Contents:**
- Installation
- Configuration
  - Enabling real-time protection OnAccessScan
    - Creating notification popups for alerts
- Updating database
- Starting the ClamAV + OnAccessScanning daemon
- Testing the software
  - Real-time protection
- Adding more databases/signatures repositories
  - Option #1: Set up Fangfrisch

Clam AntiVirus is an open source (GPL) anti-virus toolkit for UNIX. It provides a number of utilities including a flexible and scalable multi-threaded daemon, a command line scanner and advanced tool for automatic database updates. Because ClamAV's main use is on file/mail servers, it primarily detects malware with its built-in signatures and is not a traditional endpoint security suite.

The current situation of anti-malware products on Linux is inadequate due to several factors:

This is especially bad because the amount of malware on Linux is increasing just as the possible attack surface due to the increasing number of Linux-based servers and IoT devices.

Currently on Linux one of the few existing and actively developed anti-malware solutions is ClamAV.

Install the clamav package.

This will install the following tools:

All ClamAV related tools, services and daemons communicate with clamd via a socket.

By default this is done via a local socket, "LocalSocket."

ClamAV also provides the possibility to enable communication via remote locations by using a network socket which is configured and names as "TCPSocket".y exists.

For more details see:

https://blog.clamav.net/2016/06/regarding-use-of-clamav-daemons-tcp.html

Another important thing to note is that when using LocalSocket, then clamd will need to be run under a user with the right permissions to scan the files you plan on including in your monitoring.

Default configuration files should already exist. Otherwise, you can manually create them by using clamconf:

The following files contain the relevant configuration options.

Last but not least you can check your config files by running clamconf

The default installation will create "sane" default configurations such as: a clamav system user, a clamav group, and the required clamd configuration files.

Additional recommended configurations can be set:

On-access scanning is the real-time protection daemon which will scan the file while reading, writing or executing it. It can be configured to either notify on detection or prevent/block on detection.

Configuration OnAccessScan is done via editing the /etc/clamav/clamd.conf configuration file.

The following changes are required for OnAccessScan to work:

The following additional changes are recommended and will put the On-Access Scanner into notify-only mode:

So far ClamAV will silently log any detection but not alert the user. A pop-up to alert the user on any detection can be added.

First add the following line to your clamd configuration:

Next, allow the clamav user to run notify-send as any user with custom environment variables via sudo:

Next, create the file /etc/clamav/virus-event.bash, make it executable and add the following:

This allows you to change/specify the message when a virus has been detected by clamd's on-access scanning service.

By default, clamonacc passes clamav the names of just-accessed files for scanning. This is a problem, because files inaccessible to the clamav user cannot be scanned this way. Instead, it is possible to instruct clamonacc (which always runs as root) to use file descriptor passing. Edit clamav-clamonacc.service with the following:

Lastly, you will need to start/enable or restart the clamav-clamonacc.service as well as the clamav-daemon.service

See: #Starting the ClamAV + OnAccessScanning daemon

If you get AppArmor denials about clamd, set the profile to a complain-only mode:

Update the virus definitions with:

If you are behind a proxy, edit /etc/clamav/freshclam.conf and update HTTPProxyServer, HTTPProxyPort, HTTPProxyUsername and HTTPProxyPassword.

The database files are saved in:

For automatic updates first create and set the required freshclam.log file:

Start/enable clamav-freshclam.service or clamav-freshclam-once.timer so that the virus definitions are kept recent.

The clamav-freshclam.service launches freshclam in daemon mode, defaulting to 12 checks per day (every 2 hours). The frequency can be changed in /etc/clamav/freshclam.conf.

The clamav-freshclam-once.timer launches the freshclam check once per day. The frequency can be changed in /usr/lib/systemd/system/clamav-freshclam-once.timer.

This will load all virus signatures in RAM. As of February 2024 these signatures require at least 1.6GB of free RAM. Twice that amount of RAM is used shortly, during the periodic update of the signatures.

The service is called clamav-daemon.service. Start it and enable it to start at boot.

Additionally start and enable clamav-clamonacc.service for real-time on access protection.

In order to make sure ClamAV and the definitions are installed correctly, scan the EICAR test file (a harmless signature with no virus code) with clamscan.

The output must include:

You can download and save a the eicar file in one of the directories you configured clamonacc to monitor. For example:

ClamAV can use databases/signature from other repositories or security vendors.

To add the most important ones in a single step, install either clamav-unofficial-sigsAUR (see GitHub description) or python-fangfrischAUR (see online documentation). Both will add signatures/databases from popular providers, e.g. MalwarePatrol, SecuriteInfo, Yara, Linux Malware Detect, etc.

Fangfrisch was designed as a more secure, flexible and convenient replacement for clamav-unofficial-sigs, and requires very little configuration (/etc/fangfrisch/fangfrisch.conf).

Most importantly, Fangfrisch never needs to be run with root permissions, unlike clamav-unofficial-sigs.

Create database structure by running:

Enable the fangfrisch.timer (system-level).

Enable the clamav-unofficial-sigs.timer.

This will regularly update the unofficial signatures based on the configuration files in the directory /etc/clamav-unofficial-sigs.

To update signatures manually, run the following:

To change any default settings, refer and modify /etc/clamav-unofficial-sigs/user.conf.

If you would like to use the MalwarePatrol database, sign up for an account at https://malwareblocklist.org/ (for a fee).

In /etc/clamav-unofficial-sigs/user.conf, change the following to enable this functionality:

Source: https://www.malwarepatrol.net/clamav-configuration-guide/

There are two options for on-demand scanning:

clamscan can be used to scan certain files, home directories, or an entire system:

If you would like clamscan to remove the infected file add to the command the --remove option, or you can use --move=/dir to quarantine them.

You may also want clamscan to scan larger files. In this case, append the options --max-filesize=4000M and --max-scansize=4000M to the command. '4000M' is the largest possible value, and may be lowered as necessary.

Using the -l /path/to/file option will print the clamscan logs to a text file for locating reported infections.

clamdscan is similar to the above but utilizes the daemon, which must be running for the command to work. Most options are ignored since the daemon reads the settings specified in /etc/clamav/clamd.conf.

Milter will scan your sendmail server for email containing virus. Adjust /etc/clamav/clamav-milter.conf to your needs. For example:

Create /etc/systemd/system/clamav-milter.service:

Your system may require a different Restart= directive. It is needed, for example, when an automatism like logrotate stops the service.

Enable and start clamav-milter.service.

For Postfix add the following lines to /etc/postfix/main.cf:

Check journalctl if the permission to access clamav-milter.socket for postfix is set accordingly, if not, add user postfix to group clamav.

When scanning a file or directory from command line using clamscan only single CPU thread is used. This may be ok in cases when timing is not critical or you do not want computer to become sluggish. If there is a need to scan large directory or USB drive quickly you may want to use all available CPUs to speed up the process.

clamscan is designed to be single-threaded, so xargs can be used to run the scan in parallel:

In this example the -P parameter for xargs runs clamscan in as many processes as there are CPUs (reported by nproc) at the same time. --max-lines and --max-args options will allow even finer control of batching the workload across the threads.

This will consume loads of RAM as all processes are individual and will load the signature files. A single thread will consume around 1G (or more) of RAM, and may hang your computer unless OOM is clever enough. You may want to consider using clamdscan instead.

If you already have clamd daemon running clamdscan can be used instead (see #Starting the ClamAV + OnAccessScanning daemon):

Here the --multiscan parameter enables clamd to scan the contents of the directory in parallel using available threads. --fdpass parameter is required to pass the file descriptor permissions to clamd as the daemon is running under clamav user and group.

The number of available threads for clamdscan is determined in /etc/clamav/clamd.conf via MaxThreads parameter clamd.conf(5). Even though you may see that the number of MaxThreads specified is more than one (current default is 10), when you start the scan using clamdscan from command line and do not specify --multiscan option, only one effective CPU thread will be used for scanning.

If you enable TCPSocket in /etc/clamav/clamd.conf, you must edit clamav-daemon.socket too (see FS#57669). The systemd socket file needs to be configured with a matching port and IP address:

And finally restart clamav-daemon.socket to see a Clamd binding at TCP port 3310:

If you get the following messages after running freshclam:

Add a sock file for ClamAV:

Then, edit /etc/clamav/clamd.conf - uncomment this line:

Save the file and restart clamav-daemon.service.

If you get the next error when starting the daemon:

This happens because of mismatch between /etc/clamav/freshclam.conf setting DatabaseDirectory and /etc/clamav/clamd.conf setting DatabaseDirectory. /etc/clamav/freshclam.conf pointing to /var/lib/clamav, but /etc/clamav/clamd.conf (default directory) pointing to /usr/share/clamav, or other directory. Edit in /etc/clamav/clamd.conf and replace with the same DatabaseDirectory as in /etc/clamav/freshclam.conf. After that clamav will start up successfully.

If you get the following error, along with a 'HINT' containing a UID and a GID number:

**Examples:**

Example 1 (unknown):
```unknown
clamd: ClamAV Daemon
clamonacc: On-Access real-time protection
clamdscan: A simple scanning client
clamdtop: A resource monitoring interface for clamd
freshclam: Daemon for virus signature updates
clamconf: Tool to create and check configuration files
```

Example 2 (unknown):
```unknown
# clamconf -g freshclam.conf > freshclam.conf
# clamconf -g clamd.conf > clamd.conf
# clamconf -g clamav-milter.conf > clamav-milter.conf
```

Example 3 (unknown):
```unknown
/etc/clamav/freshclam.conf
```

Example 4 (unknown):
```unknown
/etc/clamav/clamd.conf
```

---

## Persistent block device naming

**URL:** https://wiki.archlinux.org/title/UUID

**Contents:**
- Persistent naming methods
  - by-label
  - by-uuid
  - by-id and by-path
    - World Wide Name
  - by-partlabel
  - by-partuuid
  - by-designator and gpt-auto
- Using persistent naming
  - fstab

This article describes how to use persistent names for your block devices. This has been made possible by the introduction of udev and has some advantages over bus-based naming. If your machine has more than one drive sharing a naming scheme, the order in which their corresponding device nodes are added is arbitrary. This may result in block device names (e.g. /dev/sda and /dev/sdb, /dev/nvme0n1 and /dev/nvme1n1, /dev/mmcblk0 and /dev/mmcblk1) switching around on each boot, culminating in an unbootable system, kernel panic, or a block device disappearing. Persistent naming solves these issues.

There are different schemes providing persistent naming managed by udev.

The directories in /dev/disk/ are created and destroyed dynamically, depending on whether there are devices in them.

The following sections describe what the different persistent naming methods are and how they are used.

The lsblk command can be used for viewing graphically the first persistent schemes:

For those using GPT, use the blkid command instead. The latter is more convenient for scripts, but more difficult to read.

Almost every file system type can have a label. All your volumes that have one are listed in the /dev/disk/by-label directory.

Most file systems support setting the label upon file system creation, see the man page of the relevant mkfs.* utility. For some file systems it is also possible to change the labels. Following are some methods for changing labels on common file systems:

The label of a device can be obtained with lsblk:

UUID is a mechanism to give each filesystem a unique identifier. These identifiers are generated by filesystem utilities (e.g. mkfs.*) when the device gets formatted and are designed so that collisions are unlikely. All GNU/Linux filesystems (including swap and LUKS headers of raw encrypted devices) support UUID. FAT, exFAT and NTFS filesystems do not support UUID, but are still listed in /dev/disk/by-uuid/ with a shorter UID (unique identifier):

The UUID of a device can be obtained with lsblk:

The advantage of using the UUID method is that it is much less likely that name collisions occur than with labels. Further, it is generated automatically on creation of the filesystem. It will, for example, stay unique even if the device is plugged into another system (which may perhaps have a device with the same label).

The disadvantage is that UUIDs make long code lines hard to read and break formatting in many configuration files (e.g. fstab or crypttab). Also every time a volume is reformatted a new UUID is generated and configuration files have to get manually adjusted.

by-id creates a unique name depending on the hardware serial number, by-path depending on the shortest physical path (according to sysfs). Both contain strings to indicate which subsystem they belong to (i.e. pci- for by-path, and ata- for by-id), so they are linked to the hardware controlling the device. This implies different levels of persistence: the by-path will already change when the device is plugged into a different port of the controller, the by-id will change when the device is plugged into a port of a hardware controller subject to another subsystem. [1] Thus, both are not suitable to achieve persistent naming tolerant to hardware changes.

However, both provide important information to find a particular device in a large hardware infrastructure. For example, if you do not manually assign persistent labels (by-label or by-partlabel) and keep a directory with hardware port usage, by-id and by-path can be used to find a particular device.[2] [3]

by-id also creates World Wide Name (WWN) links of storage devices that support it. Unlike other by-id links, WWNs are fully persistent and will not change depending on the used subsystem.

SATA and SAS devices have a wwn- prefix while NVMe devices use a different WWN format and are prefixed with nvme-eui..[4]

GPT partition labels can be defined in the header of the partition entry on GPT disks.

This method is very similar to the filesystem labels, except the partition labels do not get affected if the file system on the partition is changed.

All partitions that have partition labels are listed in the /dev/disk/by-partlabel directory.

The partition label of a device can be obtained with lsblk:

Like GPT partition labels, GPT partition UUIDs are defined in the partition entry on GPT disks.

MBR does not support partition UUIDs, but Linux[5] and software using libblkid[6] (e.g. udev[7]) are capable of generating pseudo PARTUUIDs for MBR partitions. The format is SSSSSSSS-PP, where SSSSSSSS is a zero-filled 32-bit MBR disk signature, and PP is a zero-filled partition number in hexadecimal form. Unlike a regular PARTUUID of a GPT partition, MBR's pseudo PARTUUID can change if the partition number changes.

The dynamic directory is similar to other methods and, like filesystem UUIDs, using UUIDs is preferred over labels.

The partition UUID of a device can be obtained with lsblk:

If all prerequisites of systemd-gpt-auto-generator are met, udev creates symlinks in /dev/disk/by-designator/ based on the partition type. The links are named: root, home, srv, esp, xbootldr, var, etc. See systemd.image-filter(7) for a full list.

For LUKS encrypted partitions, the aforementioned links will point to the unlocked/mapped volumes and additional links with a -luks suffix will point to the encrypted partitions. As long as the LUKS device mapper name matches a designator, links to unlocked volumes will be created even for encrypted partitions on other disks, but -luks suffixed links will not be created in this case.

The /dev/gpt-auto-root symlink points to the root volume block device. If the root partition is encrypted with LUKS, /dev/gpt-auto-root will point to the unlocked/mapped volume and /dev/gpt-auto-root-luks will point to the encrypted partition.

There are various applications that can be configured using persistent naming. Following are some examples of how to configure them.

See the main article: fstab#Identifying file systems.

To use persistent names in kernel parameters, the following prerequisites must be met. On a standard installation following the installation guide both prerequisites are met:

The location of the root filesystem is given by the parameter root on the kernel command line. The kernel command line is configured from the boot loader, see Kernel parameters#Boot loader configuration. To change to persistent device naming, only change the parameters which specify block devices, e.g. root and resume, while leaving other parameters as is. Various naming schemes are supported:

Persistent device naming using label and the LABEL= format, in this example Arch Linux is the LABEL of the root file system.

Persistent device naming using UUID and the UUID= format, in this example 0a3407de-014b-458b-b5c1-848e92a327a3 is the UUID of the root file system.

Persistent device naming using disk id and the /dev path format, in this example wwn-0x60015ee0000b237f-part2 is the id of the root partition.

Persistent device naming using GPT partition UUID and the PARTUUID= format, in this example 98a81274-10f7-40db-872a-03df048df366 is the PARTUUID of the root partition.

Persistent device naming using GPT partition label and the PARTLABEL= format, in this example GNU/Linux is the PARTLABEL of the root partition.

When using GPT partition automounting, the root= parameter can be omitted entirely. If an explicit configuration is desired, use:

**Examples:**

Example 1 (unknown):
```unknown
/dev/nvme0n1
```

Example 2 (unknown):
```unknown
/dev/nvme1n1
```

Example 3 (unknown):
```unknown
/dev/mmcblk0
```

Example 4 (unknown):
```unknown
/dev/mmcblk1
```

---

## Core utilities

**URL:** https://wiki.archlinux.org/title/Tee

**Contents:**
- Essentials
  - Preventing data loss
- Nonessentials
- Alternatives
  - cat alternatives
  - cd alternatives
  - date alternatives
  - cp alternatives
  - ls alternatives
  - find alternatives

Core utilities are the basic, fundamental tools of a GNU/Linux system. This article provides an incomplete overview of them, links their documentation and describes useful alternatives. The scope of this article includes, but is not limited to, the GNU Core Utilities. Most core utilities are traditional Unix tools and many were standardized by POSIX but have been developed further to provide more features.

Most command-line interfaces are documented in man pages, utilities by the GNU Project are documented primarily in Info manuals, some shells provide a help command for shell builtin commands. Additionally most utilities print their usage when run with the --help flag.

The following table lists some important utilities which Arch Linux users should be familiar with. See also intro(1).

rm, mv, cp and shell redirections happily delete or overwrite files without asking. rm, mv, and cp all support the -i flag to prompt the user before every removal / overwrite. Some users like to enable the -i flag by default using aliases. Relying upon these shell options can be dangerous, because you get used to them, resulting in potential data loss when you use another system or user that does not have them. The best way to prevent data loss is to create backups.

This table lists core utilities that often come in handy.

The moreutils package provides useful tools like sponge(1) that are missing from the GNU coreutils.

Alternative core utilities are provided by the following packages:

See also Bash#Auto "cd" when entering just a path and Zsh#Remembering recent directories.

This article or section is a candidate for moving to List of applications/Other.

Using rsync#As cp/mv alternative allows you to resume a failed transfer, to show the transfer status, to skip already existing files and to make sure of the destination files integrity using checksums.

For graphical file searchers, see List of applications/Utilities#File searching.

While diffutils does not provide a word-wise diff, several other programs do:

See also List of applications/Utilities#Comparison, diff, merge.

These tools aim to replace grep for code search. They do recursive search by default, skip binary files and respect .gitignore.

This article or section needs expansion.

See also: dd and ddrescue

This subsection lists dd implementations whose interface and default behaviour is mostly compliant with the POSIX specification of dd(1p).

The GNU implementation of dd found in coreutils also conforms to POSIX. This subsection lists its forks.

This subsection lists dd alternatives that do not conform to POSIX (in terms of the JCL-resembling command-line syntax and default behaviour).

This subsection lists forks of bufferAUR, a general-purpose I/O buffering utility similar to dd but has a dynamic-sized buffer. It supports blockwise I/O and can be used when dumping from/to an LTO-tape to avoid shoe shining.

See also List of applications/Utilities#Disk usage display.

Many common packages already install most popular POSIX utilities as dependencies, but the posix metapackage can be installed to ensure all of them being always present.

Beside mandatory utilities, there are also metapackages for some of the optional categories:

Some commands (arch, kill, etc.) are missing from coreutils or taken from other packages. To complete them for compatibility, install uutils-coreutils and do:

**Examples:**

Example 1 (unknown):
```unknown
--color-words
```

Example 2 (unknown):
```unknown
# ln -sf /usr/bin/uu-coreutils /usr/local/bin/arch
# echo -e "#compdef arch=uu-arch\n_uu-arch" > /usr/local/share/zsh/site-functions/_arch
# echo "complete -c arch -w uu-arch" > /usr/local/share/fish/vendor_completions.d/arch.fish
```

---

## Thunderbolt

**URL:** https://wiki.archlinux.org/title/Thunderbolt

**Contents:**
- Obtain firmware updates
- User device authorization
  - Graphical front-ends
  - Automatically connect any device
  - Forcing power
- Troubleshooting
  - PCI buses are not registered
    - Automatic PCI bus rescan
  - Increasing hot-plug bus size and memory
- See also

Thunderbolt 3 works out of the box with recent Linux kernel versions [1]. The Linux kernel, starting with version 4.13, supports Thunderbolt Security as well.

Manufacturers often release firmware updates for Thunderbolt ports and devices to function properly, visit https://thunderbolttechnology.net/updates for more details how to obtain upgrades for certain vendors.

Modern Thunderbolt devices implement security modes that require user authorization when connecting devices - this is to protect from malicious devices performing DMA attacks or otherwise interfering with the hardware (see Thunderstrike 2).

The modes currently supported on Linux are:

The security level is normally configured at firmware level; it is recommended to set it to at least secure. The state of this setting can be queried with:

Users who just want to connect any device without any sort of manual work can create a udev rule as in 99-removable.rules:

Many OEMs include a method that can be used to force the power of a Thunderbolt controller to an On state. If supported by the machine this will be exposed by the WMI bus with a sysfs attribute called force_power [2].

Forcing power may especially be useful when a connected device loses connection or the controller that switches itself off.

To force the power to be on/off, write 1 or 0 to this attribute, e.g. to force power:

Sometimes when connecting a Thunderbolt device PCI buses might not be registered. This is apparent by having screens working while USB devices fail to register on your computer. This can be solved by issuing a PCI rescan:

For persistent issues with PCI buses not being registered, an automatic rescan can be configured using udev rules. This will trigger a rescan whenever a Thunderbolt device is connected.

Create the PCI rescan bash script:

Following up with a udev rule that triggers on thunderbolt connection:

Some motherboards' firmware does not report enough bus and memory sizes to the kernel, causing drivers loading to fail. Add the following to kernel command line to manually set the size.

**Examples:**

Example 1 (unknown):
```unknown
$ cat /sys/bus/thunderbolt/devices/domain0/security
```

Example 2 (unknown):
```unknown
99-removable.rules
```

Example 3 (unknown):
```unknown
/etc/udev/rules.d/99-removable.rules
```

Example 4 (unknown):
```unknown
ACTION=="add", SUBSYSTEM=="thunderbolt", ATTR{authorized}=="0", ATTR{authorized}="1"
```

---

## Help:Reading

**URL:** https://wiki.archlinux.org/title/Unit_status

**Contents:**
- Organization
- Formatting
- Root, regular user or another user
- Append, add, create, edit
  - Make executable
- Source
- Installation of packages
  - Official packages
  - Arch User Repository
- Control of systemd units

Because the vast majority of the ArchWiki contains indications that may need clarification for users new to Arch Linux (or GNU/Linux in general), this rundown of basic procedures was written both to avoid confusion in the assimilation of the articles and to deter repetition in the content itself.

Most articles on the ArchWiki do not attempt to provide a holistic introduction to a single topic; they are instead written in adherence to the "Don't Repeat Yourself" principle, under the assumption that the user will seek out and read any supporting material that they do not yet understand. Where possible, such supporting material is indicated in the article via special formatting, see #Formatting.

Because of this organization, it may be necessary to examine several related sources in order to fully understand an ArchWiki article. In particular, users who are new to Arch (or GNU/Linux in general) should expect to end up reading a great number of articles even when solving simple problems. It is especially important to study the supporting material before seeking additional help from other users.

Some lines are written like so:

Others have a different prefix:

The numeral or hash sign (#) indicates that the command needs to be run as root, whereas the dollar sign ($) shows that the command should be run as a regular user.

When the commands need to run as a specific user, they will be prefixed by the username in square brackets, for example:

This means you should use a privilege elevation tool, e.g. with sudo:

A notable exception to watch out for:

In this example, the context surrounding the numeral sign communicates that this is not to be run as a command; it should be edited into a file instead. So in this case, the numeral sign denotes a comment. A comment can be explanatory text that will not be interpreted by the associated program. Bash scripts denotation for comments happens to coincide with the root PS1.

After further examination, "give away" signs include the uppercase character following the # sign. Usually, Unix commands are not written this way and most of the time they are short abbreviations instead of full-blown English words (e.g., Copy becomes cp).

Regardless, most articles make this easy to discern by notifying the reader:

Append to ~/path/to/file:

When prompted to append to, add to, create, or edit one or more files, it is implied that you should use one of the following methods.

To create or modify multiline files, it is suggested to use a text editor. For example, using the nano command to edit the file /etc/bash.bashrc is:

To create or overwrite a file from a string, it may be simpler to use output redirection. The following example creates or overwrites the contents of the file /etc/hostname with the text myhostname.

Output redirection can also be used to append a string to a file. The following example appends the text [custom-repo] to the file /etc/pacman.conf.

When prompted to create directories, use the mkdir command:

After creating a file, if it is meant to be run as a script (whether manually or called by another program), it needs to be set as executable, for example with:

See chmod. Some applications such as file managers may provide graphical interfaces to do the same.

Some applications, notably command-line shells, use scripts for their configuration: after modifying them, they must be sourced in order for the changes to be applied. In the case of bash, for example, this is done by running (you can also replace source with .):

When the wiki suggests modifying such a configuration script, it will not explicitly remind you to source the file, and only in some cases will it point to this section with a reminder link.

When an article invites you to install some packages in the conventional way, it will not indicate the detailed instructions to do so; instead, it will simply mention the names of the packages to be installed.

The subsections below give an overview of the generic installation procedures depending on the package type.

For packages from the official repositories, you will read something like:

This means that you have to run:

The pacman article contains detailed explanations to deal with package management in Arch Linux proficiently.

For packages from the Arch User Repository (AUR), you will read something like:

This means that in general you have to follow the foobarAUR link, download the PKGBUILD archive, extract it, verify the content and finally run, in the same folder:

The Arch User Repository article contains all the detailed explanations and best practices to deal with AUR packages.

When an article invites to start, enable, etc., some systemd unit (e.g. a service), it will not indicate the detailed instructions to do so, but instead you will read something like:

This means that you have to run:

A notable command that does not follow this exact pattern is systemctl daemon-reload which will be called without arguments.

The systemd#Using units section contains structured list of available actions (like start, enable, enable and start, etc.) with their corresponding systemctl commands.

It is important to remember that there are two different kinds of configurations on a GNU/Linux system. System-wide configuration affects all users. Since system-wide settings are generally located in the /etc directory, root privileges are required in order to alter them. For example, to apply a Bash setting that affects all users, /etc/bash.bashrc should be modified.

User-specific configuration affects only a single user. Dotfiles are used for user-specific configuration. For example, the file ~/.bashrc is the user-specific configuration file. The idea is that each user can define their own settings, such as aliases, functions and other interactive features like the prompt, without affecting other users' preferences.

Bash and other Bourne-compatible shells, such as Zsh, also source files depending on whether the shell is a login shell or an interactive shell. See Bash#Configuration files and Zsh#Startup/Shutdown files for details.

Some code blocks may contain so-called pseudo-variables, which, as the name says, are not actual variables used in the code. Instead they are generic placeholders and have to be manually replaced with system-specific configuration items before the code may be run or parsed. Common shells such as bash and zsh provide tab-completion to auto-complete parameters for common commands such as systemctl.

In the articles that comply with Help:Style/Formatting and punctuation, pseudo-variables are formatted in italics. For example:

In this case interface_name is used as a pseudo-variable placeholder in a systemd template unit. All systemd template units, identifiable by the @ sign, require a system-specific configuration item as argument. See systemd#Using units.

In this case the pseudo-variables are used to describe the parameters that must be substituted for them. Details on how to gather them are elaborated on in the section Securely wipe disk#Calculate blocks to wipe manually, which features the command.

This article or section needs expansion.

In case of file examples, pasting pseudo-variables in real configuration files might break the programs that use them.

In most cases, ellipses (...) are not part of the actual file content or code output, and instead represent omitted or optional text that is not relevant for the discussed subject.

For example HOOKS=(... encrypt ... filesystems ...) or:

Be aware though that, in a few instances, ellipses may be a meaningful part of the code syntax: attentive users should be able to recognize these cases by the context.

**Examples:**

Example 1 (unknown):
```unknown
# mkinitcpio -p linux
```

Example 2 (unknown):
```unknown
$ makepkg -s
```

Example 3 (unknown):
```unknown
sudo command
```

Example 4 (unknown):
```unknown
[postgres]$ initdb -D /var/lib/postgres/data
```

---

## xdg-utils

**URL:** https://wiki.archlinux.org/title/Xdg-open

**Contents:**
- Installation
- Usage
  - Environment variables
  - xdg-mime
  - xdg-open
  - xdg-settings
- Tips and Tricks
  - URL scheme handlers

xdg-utils provides the official utilities for managing XDG MIME Applications.

Install the xdg-utils package.

xdg-utils attempts to integrate with your desktop environment by invoking the specialized programs it provides, where applicable. The evaluation of the current environment is as follows [1]:

During this process, if any match is found, the DE variable is internally overwritten with the detected desktop environment's normalized value. Hence, DE is both a legacy environment variable, and an internal state variable for xdg-utils. As an example, if XDG_CURRENT_DESKTOP is KDE, xdg-utils will internally set DE to kde. If no match is found, then any pre-existing DE value will be used, in such a way that XDG_CURRENT_DESKTOP=KDE is equivalent to XDG_CURRENT_DESKTOP being unset, and having DE=kde. This implementation detail is worth noting because it has the consequence that a pre-set DE is ignored if a desktop environment is otherwise detected.

Values of the variables that xdg-utils recognizes are:

Note that this is only a list of what the scripts provided by xdg-utils are capable of detecting. The scripts will still perform generic, environment-agnostic actions under the following conditions:

xdg-mime(1) is a script for directly querying and modifying default MIME applications. It is used within other scripts, such as xdg-open, and is also a useful troubleshooting tool.

Determine a file's MIME type:

Determine the default application for a MIME type:

Change the default application for a MIME type:

To set a file manager as the default file manager (for ex -Thunar) type:

Debug default application for MIME type:

When it is necessary to determine the MIME type of a file, xdg-mime attempts to use the right program for the desktop environment:

In the generic case, xdg-mime will:

xdg-open(1) is a resource opener used by many applications, implementing the XDG MIME Applications standard while integrating with the system's desktop environment as much as possible.

If a desktop environment was detected, its provided handler will be invoked [2]:

In the generic case, xdg-open will:

Since xdg-mime relies on perl-file-mimeinfo package to implement the XDG MIME Applications standard, if you are not using a desktop environment, you should either install perl-file-mimeinfo, or consider a different resource opener.

Shortcut to open all web MIME types with a single application:

Shortcut for setting the default application for a URL scheme:

To set the default application for a URL scheme you may also need to change the default application for the x-scheme-handler/* MIME types:

**Examples:**

Example 1 (unknown):
```unknown
KDE_FULL_SESSION
```

Example 2 (unknown):
```unknown
XDG_CURRENT_DESKTOP
```

Example 3 (unknown):
```unknown
XDG_CURRENT_DESKTOP=KDE
```

Example 4 (unknown):
```unknown
XDG_CURRENT_DESKTOP
```

---

## Realtime kernel

**URL:** https://wiki.archlinux.org/title/Realtime_kernel

**Contents:**
- What is realtime?
- How does the realtime patch work
- Installation
- Scheduling latency
- Latency testing utilities
  - cyclictest
  - hackbench
  - hwlatdetect
- See also

This article or section is out of date.

This article describes the Linux kernel realtime patch set, and some utilities useful for trouble shooting scheduling latencies.

Realtime applications have operational deadlines between some triggering event and the application's response to that event. To meet these operational deadlines, programmers use realtime operating systems (RTOS) on which the maximum response time can be calculated or measured reliably for the given application and environment. A typical RTOS uses priorities. The highest priority task wanting the CPU always gets the CPU within a fixed amount of time after the event waking the task has taken place. On such an RTOS the latency of a task only depends on the tasks running at equal or higher priorities; tasks running at lower priorities may be ignored. On a non-realtime OS (most GNU/Linux distributions running their default kernels), since latencies depend on each process running on the system, it is obviously much harder to ensure deadlines will be met every time, and this difficulty scales nonlinearly with system complexity. Determinism in scheduling becomes yet more difficult to achieve because preemption can be switched off for an arbitrary amount of time. A high priority task wanting to run can thus be delayed indefinitely by lower priority tasks with preemption disabled.

The RT-Preempt patch converts Linux into a fully preemptible kernel. This is done through:

There are two realtime patched kernels available: linux-rt and linux-rt-lts, which both have a configuration based on the main linux kernel package. linux-rt follows the development branch of the -rt patch, while linux-rt-lts tracks a stable branch of the rt patchset.

In the context of the scheduler, latency is the time that passes from the occurrence of an event until the handling of said event. Often the delay from the firing of an interrupt until the interrupt handler starts running, but could also be from the expiration of a timer, etc.

There can be many varied causes for high scheduling latencies. Some worth mentioning (in no particular order) are: a misconfigured system, bad hardware, badly programmed kernel modules, CPU power management, faulty hardware timers, SMIs and SMT.

When trying to determine a system's maximum scheduling latency, the system needs to be put under load. A busy system will tend to experience greater latencies than an idle one. To sufficiently characterize latencies of interest, it would be prudent to run tests for a long time and under a variety of nominal and worst-case load conditions. Further, since many subsystems such as disks, network devices, USB and graphics may be used sparsely after a system is brought online, care should be taken to characterize latency with these subsystems active as well.

Understanding latency is non-intuitive. In measuring and interpreting latency, errors are common and very likely to happen even with experienced computer scientists. Popular tools are often incorrect. This talk explains some common pitfalls. There are several tools available to check kernel scheduling latencies, and to track down the causes of latency spikes. One set of tools comes in a package called rt-tests.

One of the programs in rt-tests is called cyclictest, which can be used to verify the maximum scheduling latency, and for tracking down the causes of latency spikes. cyclictest works by measuring the time between the expiration of a timer a thread sets and when the thread starts running again.

Here is the result of a typical test run:

It shows a four CPU core system running one thread (SCHED_FIFO) per core at priority 98, with memory locked, the system is also under a high load due to running hackbench in a separate terminal. What is most interesting is the max schedling latency detected, in this case 32 usecs on core 3.

See cyclictest(8) man page.

An idle kernel will tend to show much lower scheduling latencies, it is essential to put some load on it to get a realistic result. This can be done with another utility in the rt-tests package called hackbench. It works by creating multiple pairs of threads or processes, that pass data between themselves either over sockets or pipes. To make it run longer add the -l parameter: hackbench -l 1000000.

See hackbench(8) man page.

hwlatdetect can be used to detect SMIs taking an inordinate time, thus introducing latency by blocking normal kernel execution. It consists of a kernel module (present in both linux-rt and linux-rt-lts), and a python script to launch the process and report the results back to the user. To check if the system uses NMIs run the following command:

The hwlatdetect kernel module works by turning everything running on the CPUs off through the stop_machine() call. It then polls the TSC (Time Stamp Counter) looking for gaps in the generated data stream. Any gaps indicates that it was interrupted by a NMI, as they are the only possible mechanism (apart from a broken TSC implementation). To run the program for 120 secs, with a detection threshold of 15 usecs, execute the following:

The result shows 16 NMIs detected that exceeded the 15 usecs threshold specified, the maximum latency detected was 21 usecs.

See hwlatdetect(8) man page.

**Examples:**

Example 1 (unknown):
```unknown
# cyclictest --smp -p98 -m
```

Example 2 (unknown):
```unknown
# /dev/cpu_dma_latency set to 0us
policy: fifo: loadavg: 239.09 220.49 134.53 142/1304 23799          

T: 0 (23124) P:98 I:1000 C: 645663 Min:      2 Act:    4 Avg:    4 Max:      23
T: 1 (23125) P:98 I:1500 C: 430429 Min:      2 Act:    5 Avg:    3 Max:      23
T: 2 (23126) P:98 I:2000 C: 322819 Min:      2 Act:    4 Avg:    3 Max:      15
T: 3 (23127) P:98 I:2500 C: 258247 Min:      2 Act:    5 Avg:    4 Max:      32
^C
```

Example 3 (unknown):
```unknown
hackbench -l 1000000
```

Example 4 (unknown):
```unknown
$ grep NMI /proc/interrupts
```

---

## Nmap

**URL:** https://wiki.archlinux.org/title/Nmap

**Contents:**
- Installation
- Usage
  - Specifying the target
    - Specifying multiple targets
  - List scan
  - Default options
- Ping scan
  - Ping scan types
- Port scan
  - Scan types

This article or section needs expansion.

From the official website:

Install the nmap package.

Nmap has a GUI called zenmap that can be installed separately, but this article will cover only command-line usage.

There are a number of ways to tell Nmap the list of IP addresses to scan. The simplest form is to just pass the address or domain name:

Using CIDR notation, for example to scan all 256 addresses beginning with 10.1.1:

Using the dash, for example to scan 10.1.50.1, 10.1.51.1 and 10.1.52.1:

Using commas (does what you expect):

The list scan option (-sL) is useful for making sure that correct addresses are specified before doing the real scan:

List scan simply prints the specified addresses without sending a single packet to the target.

If you specify only an IP address or domain name and no other options:

Nmap will do the following:

Ping scanning (host discovery) is a technique for determining whether the specified computers are up and running. Nmap performs ping scan by default before port scan to avoid wasting time on hosts that are not even connected. To instruct Nmap to only perform ping scan:

This will cause Nmap to ping every one of the specified addresses and then report the list of hosts which did respond to the ping.

Nmap uses different kinds of ping packets when run with user or root privileges and when scanning the same or different subnets:

-Pn is useful when the machine is heavily firewalled, TCP 80 and 443 ports and IGMP requests are blocked, but the IP address might still have a machine listening on other less common ports.

There are 3 main states a port can be in:

In addition to these there are 3 more states that Nmap can classify a port. These are used when Nmap cannot reliably determine the state but suspects two of the three possible states:

By default Nmap scans the 1000 most popular ports found in /usr/share/nmap/nmap-services. To specify a different number of common ports:

To specify custom port numbers, use -p:

Dashes and commas work just like in #Specifying the target. In addition, it is possible to specify all ports before/after given one by skipping the starting/ending port when using a dash. For example to scan all possible 65535 ports (except port number 0):

PSD is an extension module of iptables. It is used on some linux-based commercial routers as well.

The principle behind PSD is simple. If requests from a single IP have gained a value more than threshold in delay seconds, then the IP is classified as a port scanner. In a math expression:

Here are some examples:

One of the simplest ways to avoid PSD is to simply scan slowly. For default values this parameter would suffice:

Another interesing fact about PSD is that it does not detect a request as a port scan when the ack or rst flags are set (See the function is_portscan in xt_psd.c)

Also, if you are port scanning a host and the latter has an HTTP(S) service running on it, nmap will use Mozilla/5.0 (compatible; Nmap Scripting Engine; https://nmap.org/book/nse.html) as default user agent. Your action will thus be easily detected, especially if an administrator or a robot are taking measures if such an user agent appears in the logs. Hopefully, nmap allows us to change that string easily: just pass -script-args http.useragent="user agent you want". Source

Nmap scans are fast. While this is often a desirable feature, it can be counter-productive as well. For example when you want to test your system's firewall without disabling any activated flood detection rules, or when you want to run a long-term test for a specific port/service. The following options specify how fast Nmap sends packets.

To send a packet at most every 3.333 seconds:

Alternatively, to send a packet every 3.1 seconds:

For other timing and parallelization options, see nmap(1).

Often it is necessary to scan a large number of non-adjacent addresses. Passing them on the command line is usually not convenient. For this reason Nmap supports input from a list file (-iL):

Addresses in the file must be separated with a whitespace.

Alternatively, Nmap can read the list from standard input (the - means standard input in many command-line programs):

The same from a file:

To spoof the source MAC address:

To spoof source port:

By default, Nmap performs DNS/reverse-DNS resolution on targets. To tell Nmap never do any DNS resolution, pass the -n option:

This will speed the scan about 2 times.

By default port 0 is skipped from scans, even if -p - is specified. To scan it, it must be explicitly specified. For example to scan every possible port:

Remember that this port number is invalid in RFC standards. However it can be used by malware and the like to avoid more naive port scanners.

Nmap has built-in support for for file output alongside with terminal output:

For example to output to the terminal, to file and to XML file:

**Examples:**

Example 1 (unknown):
```unknown
--packet-trace
```

Example 2 (unknown):
```unknown
$ nmap scanme.nmap.org
$ nmap 74.207.244.221
```

Example 3 (unknown):
```unknown
$ nmap 10.1.1.0/24
```

Example 4 (unknown):
```unknown
nmap 10.1.1.0/24
```

---

## zswap

**URL:** https://wiki.archlinux.org/title/Zswap

**Contents:**
- Toggling zswap
- Customizing zswap
  - Current parameters
  - Set parameters
    - Using sysfs
    - Using kernel boot parameters
  - Maximum pool size
  - Shrinker
  - Compressed memory pool allocator
  - Compression algorithm

zswap is a kernel feature that provides a compressed RAM cache for swap pages. Pages which would otherwise be swapped out to disk are instead compressed and stored into a memory pool in RAM. Once the pool is full or the RAM is exhausted, the least recently used (LRU) page is decompressed and written to disk, as if it had not been intercepted. After the page has been decompressed into the swap cache, the compressed version in the pool can be freed.

The difference compared to zram is that zswap works in conjunction with a swap device while zram with swap created on top of it is a swap device in RAM that does not require a backing swap device.

zswap can be toggled at runtime, by writing either 1 (to enable) or 0 (to disable) to /sys/module/zswap/parameters/enabled. For example, to disable it at runtime:

To disable zswap permanently on kernels where it is enabled by default, add zswap.enabled=0 to your kernel parameters.

zswap has several customizable parameters. The live settings can be displayed using:

See the zswap documentation for the description of the different parameters.

The boot time load message showing the initial configuration can be retrieved with:

Each setting can be changed at runtime via the sysfs interface. For example, to change the compressor parameter:

To persist the parameter change, the corresponding option, for example zswap.compressor=lz4, must be added to the kernel boot parameter. Therefore to set permanently all the above settings, the following kernel parameters must be added:

When changing the compression algorithm via a boot parameter, ensure the corresponding compression module is loaded early during boot (refer to #Compression algorithm).

The memory pool is not preallocated, it is allowed to grow up to a certain limit in percentage of the total memory available, by default up to 20% of the total RAM. Once this threshold is reached, pages are evicted from the pool into the swap device. The maximum compressed pool size is controlled with the parameter max_pool_percent.

The shrinker, when enabled, causes zswap to shrink the pool by evicting cold pages to swap when memory pressure is high. This reduces the amount of cold data in the pool and, in the author's synthetic benchmark, helps avoid wasting CPU time on compressing and decompressing cold pages. It can be turned on with the parameter shrinker_enabled.

The zpool parameter controls the management of the compressed memory pool.

A zpool of type zsmalloc is created by default. Use the kernel parameter zswap.zpool to select another allocator at boot time (although as of 6.15 there is no other allocator left). The data allocator can also be changed at a later stage via the sysfs interface.

For page compression, zswap uses compressor modules provided by the kernel's cryptographic API. In official kernels the zstd compression algorithm is used by default but this can be changed with zswap.compressor= at boot time. Other options include deflate, lzo, 842, lz4 and lz4hc.

There is no issue changing the compression at runtime using sysfs but zswap starts in this case with zstd and switches at a later stage to the defined algorithm. To start zswap with another algorithm straight away, this must be set via the kernel boot parameters and the corresponding module must be loaded early by the kernel. This can be achieved by following these steps:

On next boot, see #Current parameters to check if zswap now uses the requested compressor.

zswap has a per-cgroup option to disable writeback (i.e. to prevent writes to disk).

See Power management/Suspend and hibernate#Disable zswap writeback to use the swap space only for hibernation for an example use case.

To see zswap statistics you can run this:

**Examples:**

Example 1 (unknown):
```unknown
zgrep CONFIG_ZSWAP_DEFAULT_ON /proc/config.gz
```

Example 2 (unknown):
```unknown
/sys/module/zswap/parameters/enabled
```

Example 3 (unknown):
```unknown
# echo 0 > /sys/module/zswap/parameters/enabled
```

Example 4 (unknown):
```unknown
zswap.enabled=0
```

---

## xprofile

**URL:** https://wiki.archlinux.org/title/Xprofile

**Contents:**
- Compatibility
  - Sourcing xprofile from a session started with xinit
- Configuration

An xprofile file, ~/.xprofile and /etc/xprofile, allows you to execute commands at the beginning of the X user session - before the window manager is started.

The xprofile file is similar in style to xinitrc.

The xprofile files are natively sourced by the following display managers:

It is possible to source xprofile from a session started with one of the following programs:

All of these execute, directly or indirectly, ~/.xinitrc or /etc/X11/xinit/xinitrc if it does not exist. That is why xprofile has to be sourced from these files.

Firstly, create the file ~/.xprofile if it does not exist already. Then, simply add the commands for the programs you wish to start with the session. See below:

**Examples:**

Example 1 (unknown):
```unknown
~/.xprofile
```

Example 2 (unknown):
```unknown
/etc/xprofile
```

Example 3 (unknown):
```unknown
/etc/gdm/Xsession
```

Example 4 (unknown):
```unknown
/etc/lightdm/Xsession
```

---

## Dell Latitude 3420

**URL:** https://wiki.archlinux.org/title/Dell_Latitude_3420

**Contents:**
- Performance
- Accessibility
- Firmware
- Video
  - Tearing when using Xorg and modesetting driver
- Audio
- Fingerprint reader
- Function keys

This device can be throttled and have very bad performance, even if setting on BIOS the Ultra Performance option.

Installing and enabling thermald seems to fix the issue, allowing the processor to reach higher speeds, while also running on higher CPU temperature.

The appearance of the BIOS is pretty simple and not very colorful, so it might work well with OCR software. However, it requires the user to use a mouse.

From the official manual see Navigation keys.

fwupd supports update for those devices:

For better video support install mesa, vulkan-intel. As per Intel graphics#Installation, it is not recommended to install xf86-video-intel.

However, if you experience too much tearing you could add back xf86-video-intel driver to alleviate tearing when using Xorg.

You can additionally enable TearFree (see Intel graphics#Tearing), but be aware that there is a noticeable impact on performance.

Sound and microphone requires the installation of Sound Open Firmware to work, else no card / output device will be detected.

The built-in fingerprint reader (27c6:639c) is supported and works with fprint.

**Examples:**

Example 1 (unknown):
```unknown
XF86AudioMute
```

Example 2 (unknown):
```unknown
XF86AudioLowerVolume
```

Example 3 (unknown):
```unknown
XF86AudioRaiseVolume
```

Example 4 (unknown):
```unknown
XF86AudioPlay
```

---

## LXDE

**URL:** https://wiki.archlinux.org/title/LXDE

**Contents:**
- Installation
- Starting the desktop
  - Graphical log-in
  - Console
- Tips and tricks
  - Application menu editing
  - Autostart
  - Keyboard shortcuts
  - Cursors
  - Digital clock applet time

From project home page:

LXDE requires at least lxde-common, lxsession and Openbox (or another window manager) to be installed. The lxde group contains the full desktop.

LXDM is the default display manager for LXDE and is installed as part of the lxde group. See also Display manager.

To use startx, add to xinitrc:

See also Start X at login.

The application menu works by resolving the .desktop files located in /usr/share/applications/ and ~/.local/share/applications/. To add or edit a menu item, see desktop entries. Third party menu editors can be found in the AUR (e.g. lxmedAUR). There are also official ones like alacarte (GNOME), mozo (MATE), etc.

LXDE implements XDG Autostart. Applications can be automatically started in a couple of ways:

Each line in ~/.config/lxsession/LXDE/autostart represents a command to be executed. If a line starts with @, and the command following it crashes, the command is automatically re-executed. For example:

Mouse and key bindings (i.e. keyboard shortcuts) are implemented with Openbox. LXDE users should follow the Openbox wiki to edit ~/.config/openbox/lxde-rc.xml.

An optional GUI for editing the key bindings is provided by the obkeyAUR package. While it edits rc.xml by default, you can direct it to the LXDE configuration as follows:

See [1] for more information.

lxappearance is a graphical tool to set GTK look and feel, including the cursor theme. Settings configured with LXAppearance are written to ~/.gtkrc-2.0, ~/.config/gtk-3.0/settings.ini and ~/.icons/default/index.theme. See also Cursor themes.

You can right click on the digital clock applet on the panel and set how it displays the current time using the strftime format. See strftime(3) for details.

lxappearance-obconf configures Openbox settings. See also Font configuration.

lxpanel includes a keyboard layout applet. See Keyboard configuration in Xorg for generic instructions and #Autostart to automatically start setxkbmap in LXDE.

LXDE does not come with a screen locker of its own. See List of applications/Security#Screen lockers and #Autostart on how to start them.

The Screen Lock icon executes a script (located at /usr/bin/lxlock) which searches for a number of well known screen lockers and uses the first one it finds to lock the screen. See lxlock on GitHub.

/etc/xdg/lxsession/LXDE/autostart (from the lxde-common package) lists XScreenSaver which will be launched automatically.

See DPMS on how to control the screen saver without external programs.

To change default icons for applications, see Desktop entries#Icons.

The panel's menus can be configured in /etc/xdg/menus/lxde-applications.menu as per the xdg-menu format to work with applications from other sessions (notably MATE) to add some of the function-ability that LXDE lacks.

LXsession uses the window manager defined in ~/.config/lxsession/LXDE/desktop.conf (Openbox by default). If this file does not exist, it searches in /etc/xdg/lxsession/LXDE/desktop.conf instead.

Replace openbox-lxde in either file with a window manager of your choice:

Alternatively use WM --replace as defined in #Autostart, where WM is the name of the window manager executable being started. This means that openbox will be started first on each login and will then immediately be replaced. Note that Openbox and LXDE do not share the same rc.xml and keyboard shortcuts may differ. See xbindkeys.

LXDE does not enable compositing by default, which can lead to screen tearing problems. These can be remedied at the cost of a some graphical performance by installing a composite manager, such as picom.

The wallpaper, GTK theme, and icons from Lubuntu 18.04 can be used with LXDE to replicate the look of Lubuntu 18.04. The package lubuntu-artwork-18-04AUR provides the files necessary for this. Install the package, then open lxappearance. Select the "Widget" tab and choose "Lubuntu-default", then choose the "Icon Theme" tab and select "Lubuntu". Last, select "Window Border" and select "Lubuntu-default". Click "Apply" to save. Additionally, to theme the taskbar, right click the taskbar and select "Panel Settings". In the "Appearance" tab, select "System theme" and ensure the font custom color box is unchecked, then select the "Geometry" tab and ensure the "Height" is set to 24 pixels. Note that all of these settings may be adjusted as needed, but the above instructions should provide the default look of Lubuntu 18.04.

With some GTK themes, launching lxpanel will lead to the following error:

In this case install ttf-dejavu.

If lxpanel crashes when browsing particular unicode web pages, install ttf-droid.

The icons of running applications do not match the set Icon size in Panel Settings > Geometry but are 4px smaller which makes some of them blurry. To have clear looking 32px icons in the Task Bar the set Icon size has to be 36px which would blur the icons of the rest of your active Panel Applets. To get around this create additional panel(s) and have them collectively make a single continuous looking panel by adjusting the Alignment and Margin in Panel Settings > Geometry.

Newer versions of the VTE terminal widget library require a compositing window manager for background transparency. The unmaintained, legacy GTK 2 version of VTE has fake transparency, where the desktop background image will show through the terminal. It you prefer fake transparency, the GTK 2 version of LXTerminal can be installed with the lxterminal-gtk2AUR package.

LXDE overrides the SAL_USE_VCLPLUGIN environment variable used for theming LibreOffice in /usr/bin/startlxde. Change the line in that file to set the theme. Upstream bug

**Examples:**

Example 1 (unknown):
```unknown
exec startlxde
```

Example 2 (unknown):
```unknown
/usr/share/applications/
```

Example 3 (unknown):
```unknown
~/.local/share/applications/
```

Example 4 (unknown):
```unknown
~/.config/lxsession/LXDE/autostart
```

---

## HiDPI

**URL:** https://wiki.archlinux.org/title/HiDPI

**Contents:**
- Background
- Desktop environments
  - GNOME
    - Fractional scaling
      - Wayland
        - Xwayland
      - Xorg
    - Text Scaling
      - GTK+ vs Gnome Shell elements on Xorg
  - KDE Plasma

HiDPI (High Dots Per Inch) displays, also known by Apple's "Retina display" marketing name, are screens with a high resolution in a relatively small format. They are mostly found in high-end laptops and monitors.

Not all software behaves well in high-resolution mode yet. Here are listed most common tweaks which make work on a HiDPI screen more pleasant.

The terminology in this space can be misleading. Prior to HiDPI, the terms were:

Every display has an intrinsic PPI as the ratio of native screen resolution to physical screen size. [3] Although some technical sources use the term "PPI", [4] it is much more common to (inaccurately) refer to this ratio as DPI. 96 DPI screens are regarded as comfortable for most people to read ~12pt font on, and is about where most "low-DPI" monitors fall. HiDPI screens are around 192 DPI and greater, with some screens falling in the middle ("medium PPI").

When using a HiDPI screen on a DPI-unaware system which assumes a DPI of ~96, small fonts will be uncomfortable to read. Since font rendering is relatively easy to adjust, even "DPI-unaware" systems (such as #Wine applications) often provide a knob for adjusting the font DPI. Whenever a DPI setting is exposed as a number (as opposed to a multiplier or a percentage), it is likely referring to the text rendering alone.

Most modern GUI toolkits are capable of "integer scaling", rendering the UI at least 2x size. This is achieved by applying a different font DPI as well as providing HiDPI versions of assets. Some toolkits also support fractional scaling, with GTK #Fractional scaling using a combination of applying arbitrary font DPI and downscaling graphical resources.

On a desktop which is otherwise using UI scaling, applications that lack resolution independence (such as #Xwayland) may render at 1x scale and then be scaled up by the display server.

To enable HiDPI, navigate to Settings > Devices > Displays > Scale and choose an appropriate value. Or, use gsettings:

A setting of 2, 3, etc, which is all you can do with scaling-factor, may not be ideal for certain HiDPI displays and smaller screens (e.g. small tablets). Fractional scaling is possible on both Wayland and Xorg, though the process differs.

Implementation was mainly discussed and decided in GNOME fractional scaling hackfest 2017, check [5] for more technical details.

The factual accuracy of this article or section is disputed.

Currently, GTK only supports fractional scaling for fonts. On the other hand, widgets, like buttons or labels, may only use integer (DPI) scaling. As such, fractional scaling for most native GNOME applications requires first rendering at a higher resolution, then downscaling to the requested resolution. GTK utilizes this technique in both Wayland and Xorg sessions.

For some setups running GTK 3 applications, this can increase CPU and GPU usage and power usage, resulting in a less responsive experience - particularly in Xorg. If these issues are considerable in your use case, please consider using another desktop environment or deactivating fractional scaling.

Enable the experimental fractional scaling feature:

then open Settings > Devices > Displays (the new options may only appear after a restart).

To enable the option for all users, create the following three files with the corresponding content

Then run dconf update and restart the machine. This will permanently lock the option.

As of Mutter 47.0 it is possible to enable native fractional scaling as an experimental feature.

Enable it by issuing:

This article or section is being considered for removal.

Ubuntu has provided a patch[6] to scale with Randr in GNOME Settings. This patch is already provided by mutter-x11-scalingAUR[broken link: package not found]. After installing it, you can set:

Then open Settings > Devices > Displays to set the scale.

You can also manually achieve any non-integer scale factor by using a combination of GNOME's scaling-factor and xrandr. This combination keeps the TTF fonts properly scaled so that they do not become blurry if using xrandr alone. You specify zoom-in factor with gsettings and zoom-out factor with xrandr.

First scale GNOME up to the minimum size which is too big. Usually "2" is already too big, otherwise try "3" etc. Then start scaling down by setting zoom-out factor with xrandr. First get the relevant output name, the examples below use eDP1. Start e.g. with zoom-out 1.25 times. If the UI is still too big, increase the scale factor; if it is too small decrease the scale factor.

To ensure that the settings persist across reboots, you may choose to use autorandr. Refer to this StackOverflow for more information.

The factual accuracy of this article or section is disputed.

GNOME ignores X settings due to its xsettings Plugin in Gnome Settings Daemon, where DPI setting is hard coded. There is blog entry for recompiling Gnome Settings Daemon[dead link 2022-09-18domain name not resolved]. In the source documentation there is another way mentioned to set X settings DPI:

You can use the gsettings, just make sure to read previous setting first and merge it. In just simply set it with this command:

From README.xsettings

Noting that variants must be specified in the usual way (wrapped in <>).

Note also that DPI in the above example is expressed in 1024ths of an inch.

Alternatively, or in addition to changing the display scaling, you can separately scale text. This can be done by navigating to Fonts > Scaling Factor in Gnome Tweaks, or using gsettings. Note that the text scaling factor need not be limited to whole integers, for example:

This article or section is out of date.

Adjusting the text scaling as per the above only affects GTK+ elements of the GNOME desktop. This should cover everything on Wayland. However, those on an Xorg session may find that they need to make further adjustments on HiDPI environments, since the GNOME Shell UI (including the top bar, dock, application menus, etc.) relies separately on the St[dead link 2021-11-11HTTP 404] toolkit. Note that this is a long-standing issue to which a patch has been merged and available for Gnome Shell 3.35 onward. For older releases, Xorg users can resolve most of the Gnome shell scaling problems by manually editing the shell theme that they are currently using. The relevant CSS files are normally located at /usr/share/themes/YOUR-THEME/gnome-shell/gnome-shell.css. Users should increase all "font-size" elements in this file in proportion to their display scaling (doubling font sizes for 200% scaling, etc.) For example, the top of an edited CSS file for the Adapta shell theme might look like:

Once these changes have been saved, activate them by switching to another theme (for example, using gnome-tweaks) and then reverting back again. The top bar, application menus, calendar, and other shell elements should now be correctly scaled.

In addition to editing the relevant shell theme's CSS file, users on Xorg may also wish to increase the title bar font at the top of open applications. This can be done through the dconf editor (org > gnome > desktop > wm > preferences :: titlebar-font). Note that the title-bar-uses-system-fonts option should also be turned off. Alternatively, use gsettings:

You can use Plasma's settings to fine tune font, icon, and widget scaling. This solution affects both Qt and GTK applications.

To adjust font, widget, and icon scaling together:

However, using X11 session, Plasma ignores the Qt scaling settings by default, which affects panels and other desktop elements. To make Plasma respect the Qt settings, set PLASMA_USE_QT_SCALING=1.

To adjust cursor size:

To adjust only font scaling:

To adjust only icon scaling:

To adjust only panel scaling:

As of Plasma 5.26, the Xwayland scale method can be chosen at the bottom of the System Settings > Display and Monitor > Display Configuration page.

In "Scaled by the system" mode, the X application will be rendered at 1x and then magnified (scaled) by KDE. This works for all applications, but causes blurriness due to the magnification.

In "Apply scaling themselves" mode, the X application will have to render itself at the appropriate size. This will avoid blurriness, but applications which aren't HiDPI-aware will render themselves at 1x scale and therefore will appear small.

See this blog post for details.

Xfce supports HiDPI scaling which can be enabled using the settings manager:

Alternatively, it is possible to do the same from command line using xfconf-query:

After either of the above changes, fonts in some GTK applications may still not be scaled; you may additionally do the following (see #GDK 3 (GTK 3)):

The steps above would set 2x scaled resolution for Xfce and other GTK 3 applications.

Scaling for Qt 5 applications should be set manually, see #Qt 5. Note that if you set a Custom DPI for fonts above, you likely need to set QT_FONT_DPI=96 to avoid double-scaling of fonts in Qt applications.

Has good support out of the box.

For E18, go to the E Setting panel. In Look > Scaling, you can control the UI scaling ratios. A ratio of 1.2 seems to work well for the native resolution of the MacBook Pro 15" screen.

See Hyprland#Setting screen resolution.

The factual accuracy of this article or section is disputed.

If you are not using a desktop environment such as KDE, Xfce, or other that manipulates the X settings for you, you can set the desired DPI setting manually via the Xft.dpi variable in Xresources:

For Xft.dpi, using integer multiples of 96 usually works best, e.g. 192 for 200% scaling.

Make sure the settings are loaded properly when X starts, for instance in your ~/.xinitrc with xrdb -merge ~/.Xresources (see Xresources for more information).

This will make the font render properly in most toolkits and applications, it will however not affect things such as icon size! Setting Xft.dpi at the same time as toolkit scale (e.g. GDK_SCALE) may cause interface elements to be much larger than intended in some programs like firefox.

Some programs may still interpret the DPI given by the X server (most interpret X Resources, though, directly or indirectly). Older versions of i3 (before 2017) and Chromium (before 2017) used to do this.

To verify that the X Server has properly detected the physical dimensions of your monitor, use the xdpyinfo utility from the xorg-xdpyinfo package:

This example uses inaccurate dimensions (423mm x 238mm, even though the Dell XPS 9530 has 346mm x 194mm) to have a clean multiple of 96 dpi, in this case 192 dpi. This tends to work better than using the correct DPI  Pango renders fonts crisper in i3 for example.

If the DPI displayed by xdpyinfo is not correct, see Xorg#Display size and DPI for how to fix it.

Since Qt 5.6, Qt 5 applications can be instructed to honor screen DPI by setting the QT_AUTO_SCREEN_SCALE_FACTOR environment variable. Qt 5.14 introduced a new environment variable QT_ENABLE_HIGHDPI_SCALING which replaces the QT_AUTO_SCREEN_SCALE_FACTOR variable. It is recommended to set both environment variables for maximum compatibility:

If automatic detection of DPI does not produce the desired effect, scaling can be set manually per-screen (QT_SCREEN_SCALE_FACTORS/QT_ENABLE_HIGHDPI_SCALING) or globally (QT_SCALE_FACTOR). For more details see the Qt blog post or Qt developer documentation.

An alternative is e.g.:

Setting the GDK scale (in X11, not Wayland) will scale the UI; however, it will not scale icons. If you are using a minimal window manager where you are setting the dpi via Xft.dpi, GDK should scale perfectly fine with it. In other cases, do the following:

To scale UI elements by an integer only factor:

GTK3 does not support fractional scaling currently, so fractional factors will be ignored. This environment variable is also ignored in mutter wayland sessions.

GTK4 supports fractional scaling since 4.14 under Wayland.

GDK_DPI_SCALE can be used to scale text only. To undo scaling of text, fractional scale can be used:

Under GTK3/4 it not currently possible to scale icon sizes, unless the application explicitly implements a way to do so. See bug report #4528. If you need this feature, use Qt when possible.

Scaling of UI elements is not supported by the toolkit itself, however it is possible to generate a theme with elements pre-scaled for HiDPI display using themix-full-gitAUR.

Electron applications (e.g. slack-desktopAUR, signal-desktop, etc.) can be configured to always use a custom scaling value by adding a --force-device-scale-factor flag to the .desktop file. Electron packages in the official repositories and packages that use them, can be configured with a configuration file.

Desktop files are normally located at /usr/share/applications/, and can normally be overridden on a per-user basis by copying it to ~/.local/share/applications/. The flag should be added to the line beginning with "Exec=". For example:

To scale UI elements by a factor of 1.5:

For more details see https://phab.enlightenment.org/w/elementary/[dead link 2024-10-12domain name not resolved]

GNUstep applications that use its gui (AppKit) library accept a GSScaleFactor property in their defaults (STEP preferences). To define a scaling factor of 1.5 for all applications:

Note that you must also disable font hinting by setting the value of GSFontHinting to 17, else text rendering will look broken when rendering long lines.

Some automatic detection was possible back in 2011, but the code responsible for the X11 backend was commented out thereafter.

FLTK 1.3, the default FLTK version available in Arch Linux, does not support resolution scaling. Support will arrive when applications start using FLTK 1.4.

Set a lower resolution for the framebuffer as explained in GRUB/Tips and tricks#Setting the framebuffer resolution.

Find a ttf font that you like in /usr/share/fonts/.

Convert the font to a format that GRUB can utilize:

Edit /etc/default/grub to set the new font as shown in GRUB/Tips and tricks#Background image and bitmap fonts:

Finally regenerate the main configuration file.

Set a lower resolution for the console through console-mode as explained in systemd-boot#Loader configuration and loader.conf(5)  OPTIONS.

If you are running a Wayland session, but application is running via Xwayland (either because it does not support Wayland natively or because it uses X11 by default), you could still get blurry fonts and interface, even if the application supports HiDPI. See this bug report. See also Wayland#Detect Xwayland applications.

Firefox should use the #GDK 3 (GTK 3) settings. However, the suggested GDK_SCALE suggestion does not consistently scale the entirety of Firefox, and does not work for fractional values (e.g., a factor of 158DPI/96DPI = 1.65 for a 1080p 14" laptop). You may want to use GDK_DPI_SCALE instead. Another option, which will avoid Firefox-specific settings in many setups is to use the settings in #X Resources as Firefox should respect the Xft.dpi value defined there.

To view the internal UI scaling settings of Firefox, open the advanced preferences page (about:config) and check those parameters:

If you use a HiDPI monitor such as Retina display together with another monitor, you can use the ffreszoom add-on, which will adjust the page zoom if it detects you are using a large monitor (zoom level and threshold are configurable). Modifying the internal CSS DPI setting from an extension is currently unsupported [7].

Chromium should use the #GDK 3 (GTK 3) settings.

To override those, use the --force-device-scale-factor flag with a scaling value. This will scale all content and ui, including tab and font size. For example chromium --force-device-scale-factor=2.

Using this option, a scaling factor of 1 would be normal scaling. Floating point values can be used. To make the change permanent, for Chromium, you can add it to ~/.config/chromium-flags.conf:

To make this work for Chrome, add the same option to ~/.config/chrome-flags.conf instead.

If you are using Wayland and the setting the above flag doesn't seem to work on Chrome (not Chromium), you might need to explicitly set the following 'experimental' settings via accessing the url from the address bar: chrome://flags/ (What are Chrome flags?):

If you use a HiDPI monitor such as Retina display together with another monitor, you can use the reszoom extension in order to automatically adjust the zoom level for the active screen.

If using Wayland session, you should enable native wayland support to avoid blurriness. See also Chromium#Incorrect HiDPI rendering.

Opera should use the #GDK 3 (GTK 3) settings.

To override those, use the --alt-high-dpi-setting=X command line option, where X is the desired DPI. For example, with --alt-high-dpi-setting=144 Opera will assume that DPI is 144. Newer versions of opera will auto detect the DPI using the font DPI setting (in KDE: the force font DPI setting.)

To scale the icons to a "usable" size go to Preferences > Interface and set the icon size to Large or Larger[8][9].

Java applications using the AWT/Swing framework can be scaled by defining the sun.java2d.uiScale VM property when invoking java. The value can be an integer percentage value, or a float value. For example,

Since Java 9 the GDK_SCALE environment variable is used to scale Swing applications accordingly.

Note that at this point, Java AWT/Swing (up to including OpenJDK 13) only effectively supports integer values. A setting of -Dsun.java2d.uiScale=250% or GDK_SCALE=2.5 will be treated as if it were set to -Dsun.java2d.uiScale=2 resp. GDK_SCALE=2.

Java applications using JavaFX can be scaled by defining the glass.gtk.uiScale VM property when invoking java. The value can be an integer percentage value, an integer DPI value (where 96dpi represents a scale factor of 100%, and for example 192dpi represents a scale factor of 200%), or a float value. For example,

JavaFX perfectly well supports fractions. Using values like -Dglass.gtk.uiScale=250% or -Dglass.gtk.uiScale=2.5 will deliver the expected result.

Some Java applications mix JavaFX and AWT/Swing (via javafx.embed.swing.JFXPanel). In that case, the settings for AWT/Swing will also affect JavaFX, and setting -Dglass.gtk.uiScale will have no effect.

On Wayland, HiDPI with fractional scaling is experimentally supported since version 2024.2. The Wayland support preview can be enabled, by adding -Dawt.toolkit.name=WLToolkit to the VM options (Help > Edit custom VM options).

JetBrains products (IntelliJ IDEA and other IDEs) support two HiDPI modes (JRE-managed and IDE-managed). The sequence for determining system scale factor is well documented at [10]:

For troubleshooting, consult the "Show HiDPI Info" dialog via search everywhere "Shift Shift".

When using per-monitor scaling, an issue might occur where IntelliJ fails to recognize the real, original monitor resolution. To remediate this problem some people have success by adding the -Dsun.java2d.uiScale.enabled=true option to the ide_name.vmoptions file (Help > Edit custom VM options).

If this does not work, the experimental GTK option scale-monitor-framebuffer might be enabled on Wayland (see above) and the Wayland support preview might be disabled (see above). Currently JetBrains products run on Xwayland and thus have no full native Wayland support yet. This makes the rendering in JetBrains products incompatible with the monitor scaling framebuffer. Disabling the framebuffer thus might solve blurry font/rendering issues for JB products, but alas results in disabled fractional scaling. Another options is to enable the Wayland support preview.

Maple can be scaled for HiDPI monitors using the AWT/Swing solution. But it has to be added inside your Maple installation directory to maple-directory/bin/maple to the JVM_OPTIONS:

Alternatively, the GDK_SCALE environment variable can be used to start the application scaled:

Recent versions (since R2017b) of MATLAB allow to set the scale factor[11]:

To adjust the scale factor, execute the following instructions in a MATLAB command window:

The settings take effect after MATLAB is restarted.

This can become tedious if you need to change the scaling frequently. To simplify this, consider using the following script:

To change the display scaling to 3:

In the latest MATLAB versions, changing the DisplayScaleFactor property often has no effect. However, a newly introduced parameter enables varying the scale factor in real-time, with no need for restarting MATLAB:

According to [12][dead link 2023-04-23domain name not resolved], Mono applications should be scalable like GTK 3 applications. The precise method depends on the GUI library: GtkSharp obviouslys points back to Gtk, while the usual Windows Forms (libgdiplus) simply detects Xft settings.

NetBeans allows the font size of its interface to be controlled using the --fontsize parameter during startup. To make this change permanent edit the /usr/share/netbeans/etc/netbeans.conf file and append the --fontsize parameter to the netbeans_default_options property.[13]

The editor fontsize can be controlled from Tools > Option > Fonts & Colors.

The output window fontsize can be controlled from Tools > Options > Miscellaneous > Output

OBS 29 supports HiDPI setups without any extra configuration.

For older versions of OBS, the recommendation was to set the environment variable QT_AUTO_SCREEN_SCALE_FACTOR=0 to disable Qts hi-dpi migration mode and install the Yami theme. Do not use the Yami theme with OBS 29 or newer, as it is not necessary anymore and will cause buggy behavior.

Rofi defaults to 96 DPI and relies on its own configuration only

You can change scale factor by simple Ctrl++ for zoom in, Ctrl+- for zoom out and Ctrl+0 for default scale. Scaling setting will be saved in ~/.config/spotify/Users/YOUR-SPOTIFY-USER-NAME/prefs, you may have to create this file by yourself:

Also Spotify can be launched with a custom scaling factor which will be multiplied with setting specified in ~/.config/spotify/Users/YOUR-SPOTIFY-USER-NAME/prefs, for example

The HiDPI-Steam-Skin can be installed to increase the font size of the interface. While not perfect, it does improve usability.

MetroSkin Unofficial Patch also helps with HiDPI on Steam with Linux.

Sublime Text 3 has full support for display scaling. Go to Preferences > Settings > User Settings and add "ui_scale": 2.0 to your settings.

See #Firefox. To access about:config, go to Edit > Preferences > Advanced >Config editor.

VirtualBox also applies the system-wide scaling to the virtual monitor, which reduces the maximum resolution inside VMs by your scaling factor (see [15]).

This can be worked around by calculating the inverse of your scaling factor and manually setting this new scaling factor for the VirtualBox execution, e.g.

Text in the VMware application is rendered at an appropriate size following the system configuration, but icons are small and UI elements have little padding between them.

As described in #GDK 3 (GTK 3), you can use GDK_SCALE to further scale up the entire UI (including icons & padding) and then use GDK_DPI_SCALE to scale only the text back down to a reasonable size.

For example, to get a final 2x scale factor:

and change the "dpi" setting found in the "Graphics" tab. This only affects the font size.

No modifications required for document viewing.

UI text scaling is specified via configuration file (note that "font" is a girara option):

Set the scaleFactor variable in ~/.config/zoomus.conf.

For the Flatpak version, set the environment variable QT_SCALE_FACTOR (e.g. to 0.5 [16]). This can be easily done with Flatseal, if using a GUI tool is preferred.

Gazebo only renders an upper left of a view instead of the whole view. To fix this a Qt environment variable must be set.

To run a ROS simulation:

Making an alias such as gazebo="QT_SCREEN_SCALE_FACTORS=[1.0] gazebo" works for the first case but not for the second.

Fcitx preedit FontSize can be changed in ~/.config/fcitx/conf/fcitx-classic-ui.config.

For Fcitx5, set Font with a size inside double quotes in ~/.config/fcitx5/conf/classicui.conf.

Synthesizer V Studio Pro has support for UI scaling. It can setup the scaling automatically, but if it fails the scale can be adjusted with option --with-scaling:

xpra includes a run_scaled script which can be used to scale applications.

Another approach is to run the application full screen and without decoration in its own VNC desktop. Then scale the viewer. With vncdesk-gitAUR you can set up a desktop per application, then start server and client with a simple command such as vncdesk 2.

x11vnc has an experimental option -appshare, which opens one viewer per application window. Perhaps something could be hacked up with that.

There is a no-network, potentially GPU-accelerated solution to scale old/unsupported applications via Weston. The basic example goes as:

Note 1: You can make it look nicer. Create a dedicated weston.ini and use it with weston --config:

Note 2: Adjust your DISPLAY according to your system, :1 is simply the default that comes after the main :0. Check files created in /tmp/.X11-unix to do that.

Note 3: If you want a separate window per each scaled app, adjust the --socket parameter to weston and WAYLAND_DISPLAY + DISPLAY for each started app. Scripting that is not easy because Xorg display has to be a small-ish integer, but you can create a semi-safe script to infer it.

Note 4: It is not fully tested yet whether weston and xwayland truly off-board the heavy parts to the GPU. At least weston advertises to do so, but no tests on that were done yet. Please edit if you make the GPU usage tests.

The HiDPI setting applies to the whole desktop, so non-HiDPI external displays show everything too large. However, note that setting different scaling factors for different monitors is already supported in Wayland.

One workaround is to use xrandr's scale option. To have a non-HiDPI monitor (on DP1) right of an internal HiDPI display (eDP1), one could run:

When extending above the internal display, you may see part of the internal display on the external monitor. In that case, specify the position manually.

You may adjust the sharpness parameter on your monitor settings to adjust the blur level introduced with scaling.

There might be some problems in scaling more than one external monitors which have lower dpi than the built-in HiDPI display. In that case, you may want to try downscaling the HiDPI display instead, with e.g.

In addition, when you downscale the HiDPI display, the font on the HiDPI display will be slightly blurry, but it is a different kind of blurriness compared with the one introduced by upscaling the external displays. You may compare and see which kind of blurriness is less problematic for you.

If all you want is to mirror ("unify") displays, this is easy as well:

With AxB your native HiDPI resolution (for ex 3200x1800) and CxD your external screen resolution (e.g. 1920x1200)

In this example which is QHD (3200/1920 = 1.66 and 1800/1200 = 1.5)

For UHD to 1080p (3840/1920=2 2160/1080=2)

You may adjust the sharpness parameter on your monitor settings to adjust the blur level introduced with scaling.

There are several tools which automate the commands described above.

The Linux console changes the font to TER16x32 (based on ter-i32b from terminus-font[19]) based on the vertical and horizontal pixel count of the display[20] regardless of its physical size. If your monitor is not recognised as HiDPI, the default font can be changed. In that case, specify fbcon=font:TER16x32 in the kernel command line.

The largest fonts present in the kbd package are latarcyrheb-sun32 and solar24x32. Other packages like terminus-font contain further alternatives, such as ter-132n (normal) and ter-132b (bold). See Linux console#Fonts for configuration details and Linux console#Persistent configuration in particular for applying the font setting during the early userspace boot sequence.

After changing the font, it is often garbled and unreadable when changing to other virtual consoles (tty26). To fix this you can force specific mode for KMS, such as video=2560x1600@60 (substitute in the native resolution of your HiDPI display), and reboot. Using small resolutions will make the text look bigger, but also pixelated.

Users booting through UEFI may experience the console and boot loader being constrained to a low resolution despite correct KMS settings being set. This can be caused by legacy/BIOS boot being enabled in UEFI settings. Disabling legacy boot to bypass the compatibility layer should allow the system to boot at the correct resolution.

For real HiDPI support, see KMSCON instead of changing the font size in the tty.

**Examples:**

Example 1 (unknown):
```unknown
(number of pixels in image) / (physical size of the photo)
```

Example 2 (unknown):
```unknown
$ gsettings set org.gnome.settings-daemon.plugins.xsettings overrides "[{'Gdk/WindowScalingFactor', <2>}]"
$ gsettings set org.gnome.desktop.interface scaling-factor 2
```

Example 3 (unknown):
```unknown
scaling-factor
```

Example 4 (unknown):
```unknown
$ gsettings set org.gnome.mutter experimental-features "['scale-monitor-framebuffer']"
```

---

## Help:Reading

**URL:** https://wiki.archlinux.org/title/Stop

**Contents:**
- Organization
- Formatting
- Root, regular user or another user
- Append, add, create, edit
  - Make executable
- Source
- Installation of packages
  - Official packages
  - Arch User Repository
- Control of systemd units

Because the vast majority of the ArchWiki contains indications that may need clarification for users new to Arch Linux (or GNU/Linux in general), this rundown of basic procedures was written both to avoid confusion in the assimilation of the articles and to deter repetition in the content itself.

Most articles on the ArchWiki do not attempt to provide a holistic introduction to a single topic; they are instead written in adherence to the "Don't Repeat Yourself" principle, under the assumption that the user will seek out and read any supporting material that they do not yet understand. Where possible, such supporting material is indicated in the article via special formatting, see #Formatting.

Because of this organization, it may be necessary to examine several related sources in order to fully understand an ArchWiki article. In particular, users who are new to Arch (or GNU/Linux in general) should expect to end up reading a great number of articles even when solving simple problems. It is especially important to study the supporting material before seeking additional help from other users.

Some lines are written like so:

Others have a different prefix:

The numeral or hash sign (#) indicates that the command needs to be run as root, whereas the dollar sign ($) shows that the command should be run as a regular user.

When the commands need to run as a specific user, they will be prefixed by the username in square brackets, for example:

This means you should use a privilege elevation tool, e.g. with sudo:

A notable exception to watch out for:

In this example, the context surrounding the numeral sign communicates that this is not to be run as a command; it should be edited into a file instead. So in this case, the numeral sign denotes a comment. A comment can be explanatory text that will not be interpreted by the associated program. Bash scripts denotation for comments happens to coincide with the root PS1.

After further examination, "give away" signs include the uppercase character following the # sign. Usually, Unix commands are not written this way and most of the time they are short abbreviations instead of full-blown English words (e.g., Copy becomes cp).

Regardless, most articles make this easy to discern by notifying the reader:

Append to ~/path/to/file:

When prompted to append to, add to, create, or edit one or more files, it is implied that you should use one of the following methods.

To create or modify multiline files, it is suggested to use a text editor. For example, using the nano command to edit the file /etc/bash.bashrc is:

To create or overwrite a file from a string, it may be simpler to use output redirection. The following example creates or overwrites the contents of the file /etc/hostname with the text myhostname.

Output redirection can also be used to append a string to a file. The following example appends the text [custom-repo] to the file /etc/pacman.conf.

When prompted to create directories, use the mkdir command:

After creating a file, if it is meant to be run as a script (whether manually or called by another program), it needs to be set as executable, for example with:

See chmod. Some applications such as file managers may provide graphical interfaces to do the same.

Some applications, notably command-line shells, use scripts for their configuration: after modifying them, they must be sourced in order for the changes to be applied. In the case of bash, for example, this is done by running (you can also replace source with .):

When the wiki suggests modifying such a configuration script, it will not explicitly remind you to source the file, and only in some cases will it point to this section with a reminder link.

When an article invites you to install some packages in the conventional way, it will not indicate the detailed instructions to do so; instead, it will simply mention the names of the packages to be installed.

The subsections below give an overview of the generic installation procedures depending on the package type.

For packages from the official repositories, you will read something like:

This means that you have to run:

The pacman article contains detailed explanations to deal with package management in Arch Linux proficiently.

For packages from the Arch User Repository (AUR), you will read something like:

This means that in general you have to follow the foobarAUR link, download the PKGBUILD archive, extract it, verify the content and finally run, in the same folder:

The Arch User Repository article contains all the detailed explanations and best practices to deal with AUR packages.

When an article invites to start, enable, etc., some systemd unit (e.g. a service), it will not indicate the detailed instructions to do so, but instead you will read something like:

This means that you have to run:

A notable command that does not follow this exact pattern is systemctl daemon-reload which will be called without arguments.

The systemd#Using units section contains structured list of available actions (like start, enable, enable and start, etc.) with their corresponding systemctl commands.

It is important to remember that there are two different kinds of configurations on a GNU/Linux system. System-wide configuration affects all users. Since system-wide settings are generally located in the /etc directory, root privileges are required in order to alter them. For example, to apply a Bash setting that affects all users, /etc/bash.bashrc should be modified.

User-specific configuration affects only a single user. Dotfiles are used for user-specific configuration. For example, the file ~/.bashrc is the user-specific configuration file. The idea is that each user can define their own settings, such as aliases, functions and other interactive features like the prompt, without affecting other users' preferences.

Bash and other Bourne-compatible shells, such as Zsh, also source files depending on whether the shell is a login shell or an interactive shell. See Bash#Configuration files and Zsh#Startup/Shutdown files for details.

Some code blocks may contain so-called pseudo-variables, which, as the name says, are not actual variables used in the code. Instead they are generic placeholders and have to be manually replaced with system-specific configuration items before the code may be run or parsed. Common shells such as bash and zsh provide tab-completion to auto-complete parameters for common commands such as systemctl.

In the articles that comply with Help:Style/Formatting and punctuation, pseudo-variables are formatted in italics. For example:

In this case interface_name is used as a pseudo-variable placeholder in a systemd template unit. All systemd template units, identifiable by the @ sign, require a system-specific configuration item as argument. See systemd#Using units.

In this case the pseudo-variables are used to describe the parameters that must be substituted for them. Details on how to gather them are elaborated on in the section Securely wipe disk#Calculate blocks to wipe manually, which features the command.

This article or section needs expansion.

In case of file examples, pasting pseudo-variables in real configuration files might break the programs that use them.

In most cases, ellipses (...) are not part of the actual file content or code output, and instead represent omitted or optional text that is not relevant for the discussed subject.

For example HOOKS=(... encrypt ... filesystems ...) or:

Be aware though that, in a few instances, ellipses may be a meaningful part of the code syntax: attentive users should be able to recognize these cases by the context.

**Examples:**

Example 1 (unknown):
```unknown
# mkinitcpio -p linux
```

Example 2 (unknown):
```unknown
$ makepkg -s
```

Example 3 (unknown):
```unknown
sudo command
```

Example 4 (unknown):
```unknown
[postgres]$ initdb -D /var/lib/postgres/data
```

---

## Localization

**URL:** https://wiki.archlinux.org/title/Localization

**Contents:**
- Fonts
- Locale
- Input method
- Keyboard layouts
- See also

Localization (often abbreviated as l10n) and internationalization (often abbreviated as i18n) "are means of adapting computer software to different languages, regional peculiarities and technical requirements of a target locale."  Wikipedia:Internationalization and localization

Subpages listed below contain instructions on localization of your system to specific languages:

See Fonts#Non-latin scripts for a non-exhaustive list of available non-Latin fonts.

See Locale for help with adding non-Latin language support to your system.

See Input method for help with non-Latin text input.

See Keyboard configuration in console and Keyboard configuration in Xorg.

---

## nginx

**URL:** https://wiki.archlinux.org/title/Nginx

**Contents:**
- Installation
- Running
- Configuration
  - Configuration example
  - General configuration
    - Processes and connections
    - Running under different user
    - Server blocks
      - Managing server entries
    - TLS

nginx (pronounced "engine X"), is a free, open-source, high-performance HTTP web server and reverse proxy, as well as an IMAP/POP3 proxy server, written by Igor Sysoev in 2005. nginx is well known for its stability, rich feature set, simple configuration, and low resource consumption.

This article describes how to set up nginx and how to optionally integrate it with PHP via #FastCGI.

Install one of the following packages:

Using the mainline branch is recommended. The main reason to use the stable branch is that you are concerned about possible impacts of new features, such as incompatibility with third-party modules or the inadvertent introduction of bugs in new features.

For a chroot-based installation for additional security, see #Installation in a chroot.

Start/enable nginx.service or angie.service if you use Angie.

The default page served at http://127.0.0.1 is /usr/share/nginx/html/index.html.

First steps with nginx are described in the Beginners Guide. You can modify the configuration by editing the files in /etc/nginx/ The main configuration file is located at /etc/nginx/nginx.conf.

More details and examples can be found in the official documentation.

The examples below cover the most common use cases. It is assumed that you use the default location for documents (/usr/share/nginx/html). If that is not the case, substitute your path instead.

You should choose a fitting value for worker_processes. This setting ultimately defines how many connections nginx will accept and how many processors it will be able to make use of. Generally, making it the number of hardware threads in your system is a good start. Alternatively, worker_processes accepts the auto value since versions 1.3.8 and 1.2.5, which will try to autodetect the optimal value (source).

The maximum connections nginx will accept is given by max_clients = worker_processes * worker_connections.

By default, nginx runs the master process as root and worker processes as user http. To run worker processes as another user, change the user directive in nginx.conf:

If the group is omitted, a group whose name equals that of user is used.

It is possible to serve multiple domains using server blocks. These are comparable to "VirtualHosts" in Apache HTTP Server. Also see the upstream examples.

In the example below the server listens for incoming connections on IPv4 and IPv6 ports 80 for two domains, domainname1.tld and domainname2.tld:

Restart nginx.service to apply any changes.

It is possible to put different server blocks in different files. This allows you to easily enable or disable certain sites.

The factual accuracy of this article or section is disputed.

Instead, one can just create files inside etc/nginx/conf.d/ which adheres to the standard of drop in configuration files. Then, include include /etc/nginx/conf.d/*.conf in the main config file, similar to including other file patterns in other directories as shown below. This way, sites can be disabled just be renaming them to e.g. original_name.conf.disabled, since only files ending in .conf are included.

For using the sites-enabled and sites-available approach, create the following directories:

Create a file inside the sites-available directory that contains one or more server blocks:

Append include sites-enabled/*; to the end of the http block:

To enable a site, simply create a symlink:

To disable a site, unlink the active symlink:

Reload/restart nginx.service to enable changes to the site's configuration.

This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.

OpenSSL provides TLS support and is installed by default on Arch installations.

Create a private key and self-signed certificate. This is adequate for most installations that do not require a CSR:

If you need to create a CSR, follow these instructions instead of the above:

A starting point for a /etc/nginx/nginx.conf with TLS is Mozilla's SSL Configuration Generator.

Restart nginx.service to apply any changes.

To replicate Apache-style ~user URLs to users' ~/public_html directories, try the following. (Note: if both rules are used, below, the more-specific PHP rule must come first.)

See #PHP implementation for more information on PHP configuration with nginx.

Restart nginx.service to enable the new configuration.

FastCGI, also FCGI, is a protocol for interfacing interactive programs with a web server. FastCGI is a variation on the earlier Common Gateway Interface (CGI); FastCGI's main aim is to reduce the overhead associated with interfacing the web server and CGI programs, allowing servers to handle more web page requests at once.

FastCGI technology is introduced into nginx to work with many external tools, e.g. Perl, PHP and Python.

PHP-FPM is the recommended solution to run as FastCGI server for PHP.

Install php-fpm and make sure PHP has been installed and configured correctly. The main configuration file of PHP-FPM is /etc/php/php-fpm.conf. For basic usage the default configuration should be sufficient.

Finally, start/enable php-fpm.service.

You can also use php-legacy-fpm instead, see #Using php-legacy.

When serving a PHP web-application, a location for PHP-FPM should to be included in each server block [2], e.g.:

If it is needed to process other extensions with PHP (e.g. .html and .htm):

Non .php extension processing in PHP-FPM should also be explicitly added in /etc/php/php-fpm.d/www.conf:

You might use the common TCP socket, not default,

To enable PHP support for a particular server, simply include the php_fastcgi.conf configuration file:

You need to restart the php-fpm.service and nginx.service units if the configuration has been changed in order to apply changes.

To test the FastCGI implementation, create a new PHP file inside the root folder containing:

Navigate this file inside a browser and you should see the informational page with the current PHP configuration.

This implementation is needed for CGI applications.

Install fcgiwrap. The configuration is done by editing fcgiwrap.socket. Enable and start fcgiwrap.socket.

If you want to spawn multiple worker threads, it is recommended that you use multiwatchAUR, which will take care of restarting crashed children. You will need to use spawn-fcgi to create the Unix socket, as multiwatch seems unable to handle the systemd-created socket, even though fcgiwrap itself does not have any trouble if invoked directly in the unit file.

Override the unit fcgiwrap.service (and the fcgiwrap.socket unit, if present), and modify the ExecStart line to suit your needs. Here is a unit file that uses multiwatchAUR. Make sure fcgiwrap.socket is not started or enabled, because it will conflict with this unit:

Tweak -f 10 to change the number of children that are spawned.

In /etc/nginx, copy the file fastcgi_params to fcgiwrap_params. In fcgiwrap_params, comment or delete the lines which set SCRIPT_NAME and DOCUMENT_ROOT.

Inside each server block serving a CGI web application should appear a location block similar to:

The default socket file for fcgiwrap is /run/fcgiwrap.sock.

Using fastcgi_param SCRIPT_FILENAME /srv/www/cgi-bin/myscript.cgi is a shortcut alternative to setting DOCUMENT_ROOT and SCRIPT_NAME. If you use SCRIPT_FILENAME, you also will not need to copy fastcgi_params to fcgiwrap_params and comment out the DOCUMENT_ROOT and SCRIPT_NAME lines.

If you keep getting a 502 - bad Gateway error, you should check if your CGI-application first announces the mime-type of the following content. For HTML this needs to be Content-type: text/html.

If you get 403 errors, make sure that the CGI executable is readable and executable by the http user and that every parent folder is readable by the http user.

The factual accuracy of this article or section is disputed.

Installing nginx in a chroot adds an additional layer of security. For maximum security the chroot should include only the files needed to run the nginx server and all files should have the most restrictive permissions possible, e.g., as much as possible should be owned by root, directories such as /usr/bin should be unreadable and unwritable, etc.

Arch comes with an http user and group by default which will run the server. The chroot will be in /srv/http.

A Perl script to create this jail is available at jail.pl gist. You can either use that or follow the instructions in this article. It expects to be run as root. You will need to uncomment a line before it makes any changes.

nginx needs /dev/null, /dev/random, and /dev/urandom. To install these in the chroot create the /dev/ directory and add the devices with mknod. Avoid mounting all of /dev/ to ensure that, even if the chroot is compromised, an attacker must break out of the chroot to access important devices like /dev/sda1.

nginx requires a bunch of files to run properly. Before copying them over, create the folders to store them. This assumes your nginx document root will be /srv/http/www.

Then mount $JAIL/tmp and $JAIL/run as tmpfs's. The size should be limited to ensure an attacker cannot eat all the RAM.

In order to preserve the mounts across reboots, the following entries should be added to /etc/fstab:

First copy over the easy files.

Now copy over required libraries. Use ldd to list them and then copy them all to the correct location. Copying is preferred over hardlinks to ensure that even if an attacker gains write access to the files they cannot destroy or alter the true system files.

For files residing in /usr/lib you may try the following one-liner:

And the following for ld-linux-x86-64.so:

Copy over some miscellaneous but necessary libraries and system files.

Create restricted user/group files for the chroot. This way only the users needed for the chroot to function exist as far as the chroot knows, and none of the system users/groups are leaked to attackers should they gain access to the chroot.

Finally, make set very restrictive permissions. As much as possible should be owned by root and set unwritable.

If your server will bind port 80 (or any other port in range [1-1023]), give the chrooted executable permission to bind these ports without root.

Override the unit nginx.service. Upgrading nginx will not modify your custom .service file.

The systemd unit must be changed to start up nginx in the chroot, as the http user, and store the PID file in the chroot.

You can now safely get rid of the non-chrooted nginx installation.

If you do not remove the non-chrooted nginx installation, you may want to make sure that the running nginx process is in fact the chrooted one. You can do so by checking where /proc/PID/root symlinks to. It should link to /srv/http instead of /.

Use a drop-in unit file for nginx.service and set the User and optionally Group options under [Service]:

We can harden the service against ever elevating privileges:

Then we need to ensure that user has access to everything it needs. Follow the subsections below and then start nginx.

Linux does not permit non-root processes to bind to ports below 1024 by default. A port above 1024 can be used:

Or you may grant the nginx process the CAP_NET_BIND_SERVICE capability which allows it to bind to ports below 1024:

Alternatively, you can use systemd socket activation. In this case, systemd will listen on the ports and, when a connection is made, spawn nginx passing the socket as a file descriptor. This means nginx requires no special capabilities as the socket already exists when it is started. This relies on an internal environment variable that nginx uses for passing sockets [3] and is therefore not officially supported. Instead of setting CapabilityBoundingSet and AmbientCapabilities, edit the service override to set the NGINX environment variable to tell nginx which file descriptors the sockets will be passed as:

There will be one socket per listening port starting at file descriptor 3, so in this example we are telling nginx to expect two sockets. Now create an nginx.socket unit specifying what ports to listen on:

The sockets will be passed in the order defined in this unit, so port 80 will be file descriptor 3 and port 443 will be file descriptor 4. If you previously enabled or started the service, you should now stop it, and enable nginx.socket instead. When your system starts, nginx will not be running, but will be started when you access the website in a browser. With this you can harden the service further; for example, in many cases you can now set PrivateNetwork=True in the service file, blocking nginx from the external network, since the socket created by systemd is sufficient to serve the website over. Note that this will print a warning in the logs of the nginx service: 2020/08/29 19:33:20 [notice] 254#254: using inherited sockets from "3:4;"

nginx is compiled to use /run/nginx.pid by default, which user cannot write to. We can create a directory that user can write to and place the PID file there. This can for example be done with RuntimeDirectory (systemd.exec(5)).

Edit nginx.service to configure the PID file:

nginx is compiled to store temp files in /var/lib/nginx by default.

You can give user write access to this directory by for example using StateDirectory (systemd.exec(5)):

nginx is compiled to store access logs in /var/log/nginx by default.

You can give user write access to this directory by for example using LogsDirectory (systemd.exec(5)):

If you want to run a server instance fully controlled and configurable by unprivileged user, here is an example of a systemd user service.

It reads config from $XDG_CONFIG_HOME/nginx/nginx.conf and uses $XDG_DATA_HOME/nginx as a working directory.

On pure systemd you can get advantages of chroot + systemd. [4] Based on set user group and pid with:

the absolute path of the file is /srv/http/etc/nginx/nginx.conf.

It is not necessary to set the default location, nginx loads at default -c /etc/nginx/nginx.conf, but it is a good idea.

Alternatively you can run only ExecStart as chroot with parameter RootDirectoryStartOnly set as yes (see systemd.service(5)) or start it before mount point as effective or a systemd path (see systemd.path(5)) is available.

Enable the created nginx.path and change the WantedBy=default.target to WantedBy=nginx.path in /etc/systemd/system/nginx.service.

The PIDFile in unit file allows systemd to monitor process (absolute path required). If it is undesired, you can change to default one-shot type, and delete the reference from the unit file.

nginxbeautifierAUR is a commandline tool used to beautify and format nginx configuration files.

Nginx has a rather unintuitive header management system where headers can only be defined in one context, any other headers are ignored. To remedy this we can install the headers-more-nginx module.

Install the package nginx-mod-headers-more package. This will install the module to /usr/lib/nginx/modules directory.

To load the module add the following to the top of your main nginx configuration file.

Basic authentication requires creation of a password file. The password file can be managed using htpasswd program provided by the apache package or using nginx_passwdAUR which provides nginx-passwd - details available on GitHub source

Install php-legacy-fpm instead of php-fpm and make sure PHP has been installed and configured correctly.

The main configuration file of PHP-LEGACY-FPM is /etc/php-legacy/php-fpm.conf. For basic usage the default configuration should be sufficient.

The Unix socket for the fastcgi_pass argument also needs to be adjusted, usually it is:

Then start/enable php-legacy-fpm.service.

This is because the FastCGI server has not been started, or the socket used has wrong permissions.

Try out this answer to fix the 502 error.

In Arch Linux, the configuration file mentioned in above link is /etc/php/php-fpm.conf.

1. Verify that variable open_basedir in /etc/php/php.ini contains the correct path specified as root argument in nginx.conf (usually /usr/share/nginx/). When using PHP-FPM as FastCGI server for PHP, you may add fastcgi_param PHP_ADMIN_VALUE "open_basedir=$document_root/:/tmp/:/proc/"; in the location block which aims for processing PHP file in nginx.conf.

2. Another occasion is that, wrong root argument in the location ~ \.php$ section in nginx.conf. Make sure the root points to the same directory as it in location / in the same server. Or you may just set root as global, do not define it in any location section.

3. Check permissions: e.g. http for user/group, 755 for directories and 644 for files. Remember the entire path to the html directory should have the correct permissions. See File permissions and attributes#Bulk chmod to bulk modify a directory tree.

4. You do not have the SCRIPT_FILENAME containing the full path to your scripts. If the configuration of nginx (fastcgi_param SCRIPT_FILENAME) is correct, this kind of error means PHP failed to load the requested script. Usually it is simply a permissions issue, you can just run php-cgi as root:

or you should create a group and user to start the php-cgi:

5. If you are running php-fpm with chrooted nginx ensure chroot is set correctly within /etc/php-fpm/php-fpm.d/www.conf (or /etc/php-fpm/php-fpm.conf if working on older version)

When starting the nginx.service, the process might log the message:

To fix this warning, increase the values for these keys inside the http block [5] [6]:

The full error from nginx.service unit status is

Even if your nginx unit-file is configured to run after network.target with systemd, nginx may attempt to listen at an address that is configured but not added to any interface yet. Verify that this the case by manually running start for nginx (thereby showing the IP address is configured properly). Configuring nginx to listen to any address will resolve this issue. Now if your use case requires listening to a specific address, one possible solution is to reconfigure systemd.

To start nginx after all configured network devices are up and assigned an IP address, append network-online.target to After= within nginx.service and start/enable systemd-networkd-wait-online.service.

**Examples:**

Example 1 (unknown):
```unknown
nginx.service
```

Example 2 (unknown):
```unknown
angie.service
```

Example 3 (unknown):
```unknown
/usr/share/nginx/html/index.html
```

Example 4 (unknown):
```unknown
/etc/nginx/
```

---

## iOS

**URL:** https://wiki.archlinux.org/title/IOS

**Contents:**
- Installation
- Connecting to a device
  - Usbmux daemon
  - Pairing
  - Transferring data
    - Using a graphical file manager
    - Manual mounting
- Importing videos and pictures
- Backups
  - Creating a backup

iOS is an operating system created by Apple Inc. for use in the iPhone series of smartphones. Although connecting iOS devices to Linux is not supported by Apple, the libimobiledevice project provides libraries and tools to connect and transfer data between iOS devices and Linux machines.

Install the libimobiledevice and usbmuxd packages.

usbmuxd is responsible for performing the low-level connection to iOS devices. The usbmuxd package also includes an udev rule that automatically starts and stops the daemon whenever a device is connected or disconnected.

Connect the iOS device and verify that usbmuxd.service is automatically started.

After connecting your iOS device and unlocking the screen, you should be presented with a "Trust This Computer?" popup on the device. Tap "Trust", then enter your device passcode to complete the pairing process.

If you do not see the popup, you can start the pairing process manually. Connect the device, unlock the screen and run:

If you have multiple iOS devices connected --udid ios_udid parameter can be passed to target specific device.

You can verify the pairing has succeeded by running:

After pairing, iOS exposes two different filesystems to the computer. One is the media filesystem, containing the device's photos, videos and music. The second filesystem is used for sharing files directly to certain apps. This is sometimes called "iTunes document sharing". [1] [2]

File managers which use GVFS can interact with iOS devices. To access the media filesystem, install gvfs-gphoto2. To access the app document filesystem, install gvfs-afc.

Dolphin support for iOS devices is included in the kio-extras package, which is already a dependency for Dolphin. [3]

Install the ifuse package. You can then run the following command to mount your iPhone's media filesystem:

You can use this to access the device's photos inside mountpoint/DCIM.

To access an app's document filesystem, first you need to identify the app:

You can then mount an application's files using:

Where APPID is the bundle identifier of the desired application, such as org.videolan.vlc-ios.

After you're done, unmount the filesystem:

Both photos and videos can typically be found in <mountpoint>/DCIM/100APPLE.

iOS tends to use formats that are not as well supported, but it can be configured to use formats with better compatibility.[4] Furthermore, you may wish to convert media that was already saved in one of the less compatible Apple formats.

You can create a full backup of your iOS device using libimobiledevice by running the following command, where backup_to is the folder you want the backup to be placed in:

You may need to authorize the backup request on your iOS device by entering your device passcode.

A backup located in restore_from can be restored by running:

Due to Apple having changed the backup format, idevicebackup2's info, list and unback subcommands are broken. However, other tools like this one can be used to view and export contents of a backup.

idevicerestore-gitAUR can be used to update (reinstall the operating system without touching user data) or restore (reinstall the operating system and erase user data) iOS devices.

When using ifuse to mount application directories, you may run into the following error when trying to list the contents of the mountpoint:

This is a known issue that has been fixed in the current development version of libimobiledevice, but has not been released in a stable version yet. A workaround is to install libimobiledevice-gitAUR.

If you use a Windows virtual machine to sync your device via USB, trying to redirect it may fail with a "device is in use by another application" message. This is due to usbmuxd.service starting automatically when the device is connected. This can be solved by either stopping or masking usbmuxd.service.

**Examples:**

Example 1 (unknown):
```unknown
usbmuxd.service
```

Example 2 (unknown):
```unknown
$ systemctl status usbmuxd.service
```

Example 3 (unknown):
```unknown
...
Active: active (running) since Sun 2020-01-19 19:23:18 UTC; 22s ago
...
```

Example 4 (unknown):
```unknown
$ idevicepair pair
```

---

## nano

**URL:** https://wiki.archlinux.org/title/Nano

**Contents:**
- Installation
- Configuration
  - Syntax highlighting
    - Forth
    - PKGBUILD
  - Suspension
- Usage
  - Special functions
- Tips and tricks
  - Replacing vi with nano

GNU nano (or nano) is a text editor which aims to introduce a simple interface and intuitive command options to console based text editing. nano supports features including colorized syntax highlighting, DOS/Mac file type conversions, spellchecking and UTF-8 encoding. nano opened with an empty buffer typically occupies a few MiB of resident memory.

Install the nano package.

The look, feel, and function of nano is typically controlled by way of either command-line arguments, or configuration commands within the file ~/.config/nano/nanorc.

A sample configuration file is installed upon program installation and is located at /etc/nanorc. To customize your nano configuration, first create a local copy at ~/.config/nano/nanorc:

Proceed to establish the nano console environment by setting and/or unsetting commands within ~/.config/nano/nanorc file.

Nano ships with predefined syntax highlighting rules, defined in /usr/share/nano/*.nanorc and /usr/share/nano/extra/*.nanorc. To enable them, add the following line to your ~/.config/nano/nanorc or to /etc/nanorc:

For syntax highlighting enhancements which replace and expand the defaults, install nano-syntax-highlighting or nano-syntax-highlighting-gitAUR and, additionally to the above setting, also add:

See https://paste.xinu.at/wc17YG/ for Forth highlighting.

Save https://paste.xinu.at/4ss/ (similar to Arch's old svntogit server) to /etc/nano/pkgbuild.nanorc and include it:

Suspending (i.e. sending nano to the background) is enabled by default, however the default keybind is changed from Ctrl+z to Ctrl+t Ctrl+z and it must be changed if the old behavior is desired. This can be done by checking the Key bindings section.

Shortcuts can be viewed from inside nano. See the nano online help files via Ctrl+g within nano and the nano Command Manual for complete descriptions and additional support.

See also the cheatsheet for nano.

Keyboard shortcuts representing commonly used functions are listed along the bottom two lines of the nano screen.

They can be toggled by:

To replace vi with nano as the default text editor for commands such as visudo, set the VISUAL and EDITOR environment variables, for example:

Some window managers have keybindings that conflict with nano, for example Alt+Enter. Remove or remap them to e.g Super (with dconf for mutter, muffin and marco) and restart the window manager.

**Examples:**

Example 1 (unknown):
```unknown
~/.config/nano/nanorc
```

Example 2 (unknown):
```unknown
/etc/nanorc
```

Example 3 (unknown):
```unknown
~/.config/nano/nanorc
```

Example 4 (unknown):
```unknown
$ mkdir ~/.config/nano
$ cp /etc/nanorc ~/.config/nano/nanorc
```

---

## Mobile broadband modem

**URL:** https://wiki.archlinux.org/title/Mobile_broadband_modem

**Contents:**
- Device identification
- Mode switching
  - From mass storage mode
  - From router mode
- Modem mode
  - Remove the PIN
    - Using mmcli
    - Using AT commands
  - Connection
    - ModemManager

This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.

A number of mobile phone carriers around the world offer internet access via mobile broadband (e.g. LTE, UMTS, EDGE, GSM, etc.).

This article focuses on mobile broadband modems in the format of portable USB devices and mini PCIe cards. For standalone mobile broadband routers, simply connect to them using an interface they provide (e.g. Ethernet or Wi-Fi).

Examine the output of:

which will show the vendor and product IDs of the device. Note that some devices will show two different product IDs at different times as explained below.

Often these devices will have two modes (1) USB flash memory storage (2) USB modem. The first mode, sometimes known as ZeroCD, is often used to deliver an internet communications program for another operating system and is generally of no interest to Linux users. Additionally some have a slot into which the user can insert an additional flash memory card.

A useful utility for switching these devices into modem mode is usb_modeswitch. It ships with udev rules /usr/lib/udev/rules.d/40-usb_modeswitch.rules that contain entries for many devices, which it will switch to modem mode upon insertion.

When a device is switched, its product ID may change to a different value. The vendor ID will remain unchanged. This can be seen in the output of lsusb.

Some devices are supported in the USB serial kernel module called option (named after the "Option" devices, but not limited to just those) and may be used without usb_modeswitch.

This article or section needs expansion.

Depending on the device, it may expose an Ethernet network interface or provide Wi-Fi. In that case you will need to have the interface up. If the device has a DHCP server, you can use a DHCP client to match it. Otherwise, you will have to have some knowledge about the network the device expects. Such information might be obtained from its behavior in another OS. Or by searching the web. Or from the drivers, and other information, stored in the initial USB flash memory storage (ZeroCD). Some Huawei HiLink devices, for example, sometime operate at 192.168.8.0/24, with a gateway at 192.168.8.1. They also might have a web interface at http://192.168.8.1.

In general, at this point you should note if mode switching left you with additional /dev/ttyUSB* serial device and a ww* network interface. You can do that with journalctl or by shell commands such as:

First of all use your SIM card in a normal phone and disable the PIN request if present. If the SIM card asks the PIN wvdial will not work.

Failing that, you can use mmcli (provided by modemmanager) or AT commands, to unlock the SIM card.

First, list the modems and find the modem's index:

Look for /org/freedesktop/ModemManager1/Modem/MODEM_INDEX.

Find the SIM card index:

Just as with the modem index, look for primary sim path: /org/freedesktop/ModemManager1/SIM/SIM_INDEX.

Remove the requirement for PIN:

Follow the instructions in https://unix.stackexchange.com/a/313878.

To connect to the mobile network, use one of the following methods.

This article or section needs expansion.

ModemManager is a system daemon which controls WWAN devices and connections.

Install the modemmanager and usb_modeswitch packages.

Start and enable ModemManager.service.

Use mmcli(1) to communicate with the modem.

The simplest way to establish a connection is to use mmcli's --simple-connect option.

First, list the modems and find the modem's index:

Look for /org/freedesktop/ModemManager1/Modem/MODEM_INDEX.

Next connect to the mobile network. For example:

Replace internet.myisp.example with your ISP's provided APN. If a user name and password is required, set them accordingly:

To disconnect from the mobile network run:

See also mmcli(1)  EXAMPLES.

NetworkManager uses ModemManager to work with mobile broadband modems. See NetworkManager#Mobile broadband support.

Install libmbim. To bring up the modem you can use mbim-network which is a wrapper for mbimcli calls. First create a profile for mbim-network.

Now connect to the network with:

Then follow Network configuration to bring up the ww* interface and get an IP address using DHCP.

pppd can be used to configure 3g connections. Step by step instruction is available on 3G and GPRS modems with pppd. Optionally, pppconfigAUR can be used to simplify the pppd configuration using dialog interface.

See main article: wvdial

Netctl can be used to establish a connection using a USB modem. An example configuration file provided by netctl is located at /etc/netctl/examples/mobile_ppp. Minimally you will probably have to specify

See the netctl article and netctl.profile(5) for more information.

Some ways to disable usb_modeswitch from operating on a device before the device was inserted, for example to be able to read the initial flash memory (ZeroCD), are:

Masking the udev rule the package is using can be achieved with

There are some useful commands:

Encode *100# to PDU format:

Decode AA180C3602 from PDU format:

Answer decoding (this example is balance response: 151.25):

Some operators return USSD result in PDU encoding, so you should check proper decoding method.

Frequently a 3G connection obtained via a mobile phone operator comes with restricted bandwidth, so that you are only allowed to use a certain bandwidth per time (e.g. 1GB per month). While it is quite straight-forward to know which type of network applications are pretty bandwidth extensive (e.g. video streaming, gaming, torrent, etc.), it may be difficult to keep an overview about overall consumed bandwidth.

A number of tools are available to help with that. Two console tools are vnstat, which allows to keep track of bandwith over time, and iftop to monitor bandwidth of individual sessions.

The internal web server found in some devices, such as some Huawei HiLink, might also show information about bandwidth usage.

This was tested on a Huawei EM770W (GTM382E) 3g card integrated into an Acer Aspire AS3810TG laptop. Install gnokiiAUR, then:

Usually the configuration directory is ~/.config/gnokii.

Edit ~/.config/gnokii/config as follows:

You may have to use a different port depending on your configuration, for example /dev/ttyUSB1 or something else:

You need to be part of the uucp group to use /dev/ttyUSB0.

Click on the "SMS" icon button, a window opens up. Then click: "messages->activate sms reading". Your messages will show up in the window.

A small command line script using gnokii to read SMS on your SIM card (not phone memory) without having to start a GUI:

Granted this does not work very well if your SMS contains the word "Text", but you may adapt the script to your liking.

Another option is to use mmcli, you can use simple bash script like this one[dead link 2024-10-12HTTP 404] that is also used to write messages or this one below:

Some Devices, such as some Huawei HiLink, include an email like web interface for SMS. It is included in the device internal web server, which is used for other purposes too.

You may need give permission by creating file with content like

Unplugging, and plugging, the device is sometimes used to restart the USB device. The following describes how to do that from the shell. Doing that from the shell might be useful, if, for example, the plug is at the rear side of the PC. The method described is not just for USB modems. It should be good for many other USB devices.

The important part is that the requirements are for the USB bus, and the port, the device is attached to. There could be one, or more, sub ports too. Suppose I obtained bus 2 and port 4, without sub ports, for my device from the output of lsusb -t. This information is also recorded in the journal. With

I can verify it is the intended device.

The following sequence will restart the device:

Some more comments are at, for example, https://askubuntu.com/questions/1036341/unplug-and-plug-in-again-a-usb-device-in-the-terminal.

This problem commonly occurs on some modems which locked by a mobile operator. You can successfully connect to the internet but after few minutes connection halts and your modem reboots. That happens because an operator built a some checks into modem firmware so a modem checks if a branded software is running on your pc, but usually that software is Windows-only, and obviously you do not use it. Fix (it works on ZTE-mf190 at least) is simple - send this command through serial port (use minicom or similar soft):

This command will delete a NODOWNLOAD.FLG file in the modem's filesystem - it will disable such checks.

Another possibility for such disconnections is to help the customer save bandwidth, which might be expensive. With Huawei HiLink devices with a web interface, there might be an option there to set a longer period of inactivity before the connection hangs up.

Someone claims that the connection speed under Linux is lower than Windows [3]. This is a short summary for possible solutions which are not fully verified.

In most of conditions, the low speed is caused by bad receiver signals and too many people in cell. But you still could use the following method to try to improve the connection speed:

It is advisable to see the baud rate set by the official modem application for Windows (possibly 9600 on Vista).

If you are getting low quality images while browsing the web over a mobile broadband connection with the hints shift+r improves the quality of this image and shift+a improves the quality of all images on this page, follow these instructions:

Edit /etc/tinyproxy/tinyproxy.conf and insert the following two lines:

Start tinyproxy.service

Configure your browser to use localhost:8888 as a proxy server and you are all done. This is especially useful if you are using, for example, Google Chrome which, unlike Firefox, does not allow you to modify the Pragma and Cache-Control headers.

In case ModemManager does not recognize the modem, check the unit status of ModemManager.service. If you get error messages such as Couldn't check support for device and not supported by any plugin, you may have to whitelist your device using the ModemManager filter rules.

The FCC lock is a software lock integrated in WWAN modules shipped by several different laptop manufacturers like Lenovo, Dell, or HP. This lock prevents the WWAN module from being put online until some specific unlock procedure (usually a magic command sent to the module) is performed.

Since release 1.18.4, the ModemManager daemon no longer automatically performs the FCC unlock procedure [4].

ModemManager will keep on providing support for the known FCC unlock procedures, but no longer automatically: the user must install and select the FCC unlock procedure needed in the specific laptop being used. This applies to: EM7355, MC8805, MC7355, EM7455, SDX55, EM120.

The modemmanager package ships several scripts installed under /usr/share/ModemManager/fcc-unlock.available.d/ and named as vid:pid with either the PCI or USB vendor and product IDs. However, they are not used if they are not in the /etc/ModemManager directory.

For each device the vid:pid can be found in the brackets at the end of the line:

To enable unlock script for the device it must be symlinked like so:

For a Quectel EM120 modem that would be:

See the ModemManager documentation for more information.

If NetworkManager persists on that the device (e.g. /dev/cdc-wdm0) is not available while ModemManager can use it, it could either be, that the device is blocked using a hardware switch, by rfkill or just NetworkManager believes that.

The wwan device should be listed as unblocked for both SOFT & HARD. If it is HARD blocked, a hardware switch blocks the device. If it is SOFT blocked, unblock it using:

If NetworkManager still declares the device not available, it could be that NetworkManager is not synced with rfkill. Check that using:

If WWAN is listed as disabled, enable it using:

This has been reported to happen on some LTE modems with buggy or incompatible firmware versions. In this scenario, when inspecting the bearer with:

It can be seen how the IPv4 configuration section shows no IP address, and may show dhcp as the method despite the associated interface (e.g. wwan0) not being dhcp-capable. In this cases, the modem firmware is not behaving correctly and it should be upgraded.

**Examples:**

Example 1 (unknown):
```unknown
$ lspci -nn
```

Example 2 (unknown):
```unknown
/usr/lib/udev/rules.d/40-usb_modeswitch.rules
```

Example 3 (unknown):
```unknown
/dev/ttyUSB*
```

Example 4 (unknown):
```unknown
$ ls /dev/ttyUSB*
$ ip link
```

---

## Archiving and compression

**URL:** https://wiki.archlinux.org/title/Archiver

**Contents:**
- Archiving only
- Compression tools
  - Compression only
  - Archiving and compression
  - Feature charts
    - Decompress
- Usage comparison
  - Archiving only usage
  - Compression only usage
  - Archiving and compression usage

The traditional Unix archiving and compression tools are separated according to the Unix philosophy:

These tools are often used in sequence by firstly creating an archive file and then compressing it.

Of course there are also tools that do both, which tend to additionally offer encryption, error detection and recovery.

See also #Archiving only usage.

These compression programs implement their own file format.

See also #Archiving and compression usage.

Some of the tools above are capable of handling multiple formats, allowing for fewer installed packages.

See also Bash/Functions#Extract.

To extract an archive, its file format needs to be determined. If the file is properly named you can deduce its format from the file extension.

Otherwise you can use the file tool, see file(1).

Some file systems support on-the-fly compression of file data:

The open-sourced VDO project was integrated into the Linux kernel project, which provides a deduplication and compression device mapper layer in the interest of increasing storage efficiency. A userspace tools for managing VDO volumes is available in the AUR: vdoAUR

See Character encoding#Troubleshooting.

**Examples:**

Example 1 (unknown):
```unknown
--use-compress-program=lz4
```

Example 2 (unknown):
```unknown
tar cfv archive.tar file1 file2
```

Example 3 (unknown):
```unknown
tar xfv archive.tar
```

Example 4 (unknown):
```unknown
tar -tvf archive.tar
```

---

## Clipboard

**URL:** https://wiki.archlinux.org/title/Clipboard

**Contents:**
- History
- Selections
- Disabling middle-click paste
  - Globally
    - Using sxhkd
    - Using xsel
  - Application-specific
- Tools
- Managers
- See also

This article or section needs expansion.

According to Wikipedia:

In X10 (1985), cut buffers were introduced. These were limited buffers that stored arbitrary text and were used by most applications. However, they were inefficient and implementation of them varied, so selections were introduced. Cut buffers are long deprecated, and although some applications (such as xterm) may have legacy support for them, it is both not likely and not recommended that they be used.

Freedesktop.org describes the two main selections as follows:[4]

The majority of programs for Xorg, including Qt and GTK applications, follow this behavior. While ICCCM also defines a SECONDARY selection, it does not have a consensually agreed upon purpose. Despite the naming, all three selections are basically "clipboards". Rather than the old "cut buffers" system where arbitrary applications could modify data stored in the cut buffers, only one application may control or "own" a selection at one time. This prevents inconsistencies in the operation of the selections.

See the Keyboard shortcuts page which lists the default shortcuts in many programs.

It is also important to realize that according to the selection protocols, nothing is copied until it is pasted. For example, if you select some word in a terminal window, close the terminal and then want to paste it somewhere else, it will not work because the terminal is gone and the text has not been copied anywhere. If you want the word to be preserved after closing terminal window, consider installing a clipboard manager.

The following disables the middle-click pasting behavior by automatically clearing PRIMARY, without disabling the middle-click button or altering its other functionalities (like opening in a new tab or scrolling).

Using sxhkd, add the following to the configuration file:

The command makes use of xclip. See sxhkd#Usage for configuring sxhkd to autostart.

This section lists command-line tools to manipulate the clipboards.

This section lists clipboard managers which provide additional features such as clipboard history or synchronization.

**Examples:**

Example 1 (unknown):
```unknown
~button2
    ;echo -n | xclip -in
```

Example 2 (unknown):
```unknown
#!/bin/sh
while true; do
    xsel --follow --input --nodetach </dev/null
done
```

Example 3 (unknown):
```unknown
wl-paste --primary --watch wl-copy
```

---

## YubiKey

**URL:** https://wiki.archlinux.org/title/YubiKey

**Contents:**
- Installation
  - Management tools
  - Authentication tools
- Inputs
- Outputs
- USB connection modes
  - Get enabled modes
  - Set modes
- One-time password
  - Factory configuration

The YubiKey is a small USB Security token. Depending on the model, it can:

While offering many features, newer versions of the YubiKey are not released as open source. Alternatives are the Solo, TKey or Nitrokey.

The YubiKey takes inputs in the form of API calls over USB and button presses.

The button is very sensitive. Depending on the context, touching it does one of these things:

The YubiKey transforms these inputs into outputs:

Depending on the YubiKey model, the device provides up to three different USB interfaces. Two of the interfaces implement the USB HID (Human Interface Device) device class; the third is a smart card interface (CCID). All three can be enabled or disabled independently, allowing control of their associated protocols.

The following table shows which protocols use which interfaces:

ykman uses the term "modes", named OTP, FIDO, and CCID.

For YubiKey prior to version 5:

For YubiKey version 5:

All modes are enabled from the factory. To change them:

Here is a table of mode-numbers, if you care to use them:

For more information, see ykman mode --help.

This feature has a somewhat misleading name, because it also encompasses the static password and challenge-response functions.

2 slots are provided for this feature, accessible by short and long button presses respectively. Each can be configured with one of the following:

Each function has several configuration options provided at the time of creation, but once set they cannot be read back. It is possible to swap slots 1 and 2, with ykman otp swap.

On a new YubiKey, Yubico OTP is preconfigured on slot 1. This initial AES symmetric key is stored in the YubiKey and on the Yubico Authentication server. This allows validating against YubiCloud, allowing the use of Yubico OTP in combination with the Yubico Forum website for instance or on https://demo.yubico.com).

The Yubico OTP is based on symmetric cryptography. More specifically, each YubiKey contains a 128-bit AES key unique to that device, which is also stored on a validation server. When asked for a password, the YubiKey will create a token by concatenating different fields such as the ID of the key, a counter, and a random number, and encrypting the result.

This OTP is sent to the target system, which passes it to a validation server. The validation server (also in posession of the secret key) decrypts it and verifies the information inside. The result is returned to the target system, which can then decide whether to grant access.

Yubico provides a validation server with free unlimited access, called YubiCloud. YubiCloud knows the factory configuration of all YubiKeys, and is the "default" validation service used by (for example) yubico-pam. Yubico also offers open-source implementations[dead link 2025-08-16HTTP 404] of the server.

Generate a new key in slot 2, and upload it to YubiCloud (opens in a browser):

For more information, see ykman otp yubiotp --help.

As you can imagine, the AES key should be kept secret. It cannot be retrieved from the YubiKey itself (or it should not, at least not with software). It is also present in the validation server, so the security of this server is very important.

Since the target system relies on a validation server, a possible attack would be to impersonate it. To prevent this, the target system needs to authenticate the validation server, either using HMAC or HTTPS.

A challenge is sent to the YubiKey, which calculates a response based on some secret. The same challenge always results in the same response. Without the secret this calculation is not feasible, even with many challenge-response pairs.

There are two Challenge-Response algorithms:

You can set them up with a GUI using the yubikey-personalization-gui, or with the following instructions:

Set up slot 2 in challenge response mode with a generated key:

You can omit the --generate flag in order to provide a key, see ykman otp chalresp --help. A main advantage of providing a key is that it can be used to setup a second device as a backup. The command openssl rand -hex 20 generates a suitable key, for example.

ykman Does not appear to support setting the chal-yubico algorithm, but you can use ykpersonalize. Generate a random key in slot 2:

For more information, see ykpersonalize(1).

To send a challenge and get a response, the ykchalresp -slot challenge command can be used. For example,

returns a 40-byte SHA1-hash unique to the programmed slot 2. A different challenge produces another unique response.

You can either generate a static password:

You have several options; you can set the length and character set of the generated password, and whether or not to send an Enter keystroke. See ykman otp static --help for more.

In order for the YubiKey to work with most keyboard layouts, passwords are by default limited to the ModHex alphabet (cbdefghijklnrtuv), digits 0-9, and !. These characters use the same scan codes across a very large number of keyboard layouts, ensuring compatibility with most computers.

Yubico has provided a whitepaper on the subject.

The YubiKey offers 2 OATH implementations:

If you prefer a GUI, you can use yubioath-desktopAUR[broken link: package not found].

ykman can add codes in the URI format with ykman oath uri. Here is a one-liner that will add a credential from an image of a QR code:

You can also do things manually. Program a TOTP key, requiring a button touch to generate a code:

To see all available subcommands see ykman oath --help. To see information about each, use ykman oath subcommand --help.

Program an HOTP in slot 2:

See also: ykman otp --help and https://developers.yubico.com/OATH/

Universal 2nd Factor (U2F) with a YubiKey is very simple, requiring no configuration for the key itself. Note that this mode is also referred to as 'FIDO' in some documentation and utilities. You have a few limited management options through the ykman utility:

To use U2F for authentication, see the instructions in U2F.

CCID (Chip Card Interface Device) is a USB standard device class for use by USB devices that act as smart card readers or with security tokens that connect directly via USB, like the YubiKey. HID (Human Interface Device) and CCID are both USB device classes, i.e. they are in the same category of USB specifications. HID is a specification for computer peripherals, like keyboards. The YubiKey works like a USB (HID) keyboard when used in the OTP and FIDO modes, but switches to the CCID protocol when using the PIV application, or as an OpenPGP device.

CCID mode should be enabled by default on all YubiKeys shipped since November 2015 [1][dead link 2025-04-06HTTP 404]. Enable at least the CCID mode. Please see #Get enabled modes.

Starting with the YubiKey NEO, the YubiKeys contain a PIV (Personal Identity Verification) application on the chip. PIV is a US government standard (FIPS 201) that specifies how a token using RSA or ECC (Elliptic Curve Cryptography) is used for personal electronic identification. The YubiKey NEO only supports RSA encryption, later models (YubiKey 4 and 5) support both RSA and ECC. The exact algorithms supported depends on the firmware. For example, only YubiKeys with firmware 5.7 and up support RSA 3072, RSA 4096, Ed25519, and X25519 keys [2]. The distinguishing characteristic of a PIV token is that it is built to protect private keys and operate on-chip. A private key never leaves the token after it has been installed on it. Optionally, the private key can even be generated on-chip with the aid of an on-chip random number generator. If generated on-chip, the private key is never handled outside of the chip, and there is no way to recover it from the token. When using the PIV mechanism, the YubiKey functions as a CCID device.

The YubiKey can act as a standard OpenPGP smartcard; see GnuPG#Smartcards for instructions on how to set up and use it with GnuPG. Yubico also provides some documentation in https://developers.yubico.com/PGP/.

If you do not want to use the other features (U2F and OTP), the button can be configured to insert and eject it, and an auto-eject timeout can be set as well. See #USB connection modes for more.

The default user pin is 123456 and the default admin pin is 12345678. The default PUK is also 12345678. Remember to change all 3.

This section details how to use your YubiKey for various authentication purposes. It is by no means an exhaustive list.

You have several options:

See yubikey-full-disk-encryption's official documentation for complete instructions. Broadly:

There are a few variations available:

You must regenerate the initramfs for any configuration changes to take effect.

Users of the sd-encrypt hook may install mkinitcpio-ykfdeAUR or mkinitcpio-ykfde-gitAUR and follow the instruction in the project documentation. The procedure is broadly similar to yubikey-full-disk-encryption.

One tool to accomplish this is initramfs-scencrypt; see its docs for complete instructions. Note that as of October 2022 this package is not in the AUR and is not thoroughly tested, though the GitHub repository offers a PKGBUILD.

The dm-crypt pages offer a few alternatives, though they are mostly links to old forum posts.

Yet another way of using YubiKey for full disk encryption is to utilize HMAC Secret Extension to retrieve the LUKS password from YubiKey. This can be protected by a passphrase. This functionality requires at least YubiKey 5 with firmware 5.2.3+. For a passphrase protected solution, install khefinAUR and follow instructions available in project documentation. For single factor (optionally PIN-protected) solution and starting with systemd 248, it is possible to use your FIDO2 key as LUKS2 keyslot. Instructions available in the author's blog post.

KeePass can be configured for YubiKey support; see the YubiKey section for instructions.

If your YubiKey supports CCID smartcards, you can use it as a hardware-backed SSH key, either based on GPG or PIV keys. Yubico offers good documentation:

You may also use the U2F feature of the YubiKey to create hardware-backed SSH keys. See SSH keys#FIDO/U2F for instructions.

yubikey-agentAUR stores the SSH key as PIV token. See https://github.com/FiloSottile/yubikey-agent#readme for a setup guide.

PAM, and therefore anything which uses PAM for user authentication, can be configured to use a YubiKey as a factor of its user authentication process. This includes sudo, su, ssh, screen lockers, display managers, and nearly every other instance where a Linux system needs to authenticate a user. Its flexible configuration allows you to set whichever authentication requirements fit your needs, for the entire system, a specific application, or for groups of applications. For example, you could accept the YubiKey as an alternative to a password for local sessions, while requiring both for remote sessions. In addition to the Arch Wiki, You are encouraged to read pam(8) and pam.conf(5) to understand how it works and how to configure it.

There are several modules available which integrate YubiKey-supported protocols into PAM:

PAM configuration is beyond the scope of this article, but for a brief overview:

Many web services are beginning to support FIDO hardware tokens. See the U2F and WebAuthn pages for more information, but usually the only thing you need to do is to install libfido2 and try it.

For example, you want to perform an action when you pull your YubiKey out of the USB slot, create /etc/udev/rules.d/80-yubikey-actions.rules and add the following contents:

Please note, most keys are covered within this example but it may not work for all versions of YubiKey. You will have to look at the output of lsusb to get the vendor and model ID's, along with the description of the device or you could use udevadm to get information. Of course, to execute a script on insertion, you would change the action to 'add' instead of remove.

The authenticator is a long-running GUI process. If run directly in a udev rule, the process would block udev's processing. If forked, udev would unconditionally kill the process after the event handling finishes. Thus you cannot start the authenticator from udev rules. However, systemd.device may be used to handle this case.

Similar to above, create /etc/udev/rules.d/80-yubikey-actions.rules and add the following contents:

Then create a new systemd user unit:

and enable it. systemctl would warn that it is added as a dependency to a non-existent unit dev-yubikey.device. But it is okay. Such unit will start existing once the YubiKey is plugged in.

These steps will allow you to install the OATH applet onto your YubiKey NEO. This allows the use of Yubico Authenticator in the Google Play Store.

You can get the desktop version of the Yubico Authenticator by installing yubioath-desktopAUR[broken link: package not found].

While pcscd.service is running, run yubioath-desktop and insert your YubiKey when prompted.

Restart, especially if you have completed updates since your YubiKey last worked. Do this even if some functions appear to be functioning.

Add udev rule as described in this article:

Run udevadm trigger afterwards.

If the manager fails to connect to the YubiKey, make sure you have started pcscd.service or pcscd.socket.

This can occur when using ykman to access the oath credentials on the device if scdaemon has already taken exclusive control of the device. [3]

To fix this you can set the reader-port option with the correct value for your device in ~/.gnupg/scdaemon.conf. [4]

For YubiKey NEO and YubiKey 4:

Assuming the YubiKey is available to the guest, the issue results from a driver binding to the device on the host. To unbind the device, the bus and port information is needed from dmesg on the host:

The resulting USB id should be of the form X-Y.Z or X-Y. Then, on the host, use find to search /sys/bus/usb/drivers for which driver the YubiKey is binded to (e.g. usbhid or usbfs).

To unbind the device, use the result from the previous command (i.e. /sys/bus/usb/drivers/DRIVER/X-Y.Z:1.0):

Occurs when attempting to sign keys on a non-standard keyring while a YubiKey is plugged in, e.g. as Pacman does in pacman-key --populate. The solution is to remove the offending YubiKey and start over.

This happens when the CCID driver is not installed. You may need to install the ccid package.

You are probably using the wrong slot. Try the other one.

Because gpg(scdaemon) tries to acquire exclusive access to the yubikey. It needs to be configured to use pscs and use shared access.[5][6]

Your file ~/.gnupg/scdaemon.conf should contain:

For old versions of GnuPG, the pcsc-shared option is not available. Only keep disable-ccid and restart pcscd.service as a workaround.

**Examples:**

Example 1 (unknown):
```unknown
pcscd.service
```

Example 2 (unknown):
```unknown
pcscd.socket
```

Example 3 (unknown):
```unknown
$ ykman config mode
```

Example 4 (unknown):
```unknown
Current connection mode is: OTP+FIDO+CCID
```

---

## ATI

**URL:** https://wiki.archlinux.org/title/ATI

**Contents:**
- Selecting the right driver
- Installation
- Loading
- Xorg configuration
- Performance tuning
  - Enabling video acceleration
  - Graphical tools
  - Driver options
  - Kernel parameters
    - Deactivating PCIe 2.0

This article covers the radeon open source driver which supports older AMD (previously ATI) GPUs.

Depending on the card you have, find the right driver in Xorg#AMD. This page has instructions for ATI.

If unsure, try the AMDGPU driver first, it will suit most needs for cards released since 2015. See the feature matrix to know what is supported by this driver and the decoding table to translate marketing names (e.g. Radeon HD4330) to chip names (e.g. R700).

Install the mesa package, which provides both the DRI driver for 3D acceleration and VA-API/VDPAU drivers for accelerated video decoding. Or mesa-amber for very old GPUs (R200 and prior).

The factual accuracy of this article or section is disputed.

The radeon kernel module should load fine automatically on system boot.

If it does not happen, then:

Xorg will automatically load the driver and it will use your monitor's EDID to set the native resolution. Configuration is only required for tuning the driver.

If you want manual configuration, create /etc/X11/xorg.conf.d/20-radeon.conf, and add the following:

Using this section, you can enable features and tweak the driver settings.

See Hardware video acceleration#AMD/ATI.

The following options apply to /etc/X11/xorg.conf.d/20-radeon.conf.

Please read radeon(4) and RadeonFeature first before applying driver options.

Acceleration architecture; Glamor is available as a 2D acceleration method implemented through OpenGL, and it is the default for R600 (Radeon HD2000 series) and newer graphic cards. Older cards use EXA.

DRI3 is enabled by default since xf86-video-ati 7.8.0.

TearFree is a tearing prevention option which prevents tearing by using the hardware page flipping mechanism:

ColorTiling and ColorTiling2D are supposed to be enabled by default. Tiled mode can provide significant performance benefits with 3D applications. It is disabled if the DRM module is too old or if the current display configuration does not support it. KMS ColorTiling2D is only supported on R600 (Radeon HD2000 series) and newer chips:

When using Glamor as acceleration architecture, it is possible to enable the ShadowPrimary option, which enables a so-called "shadow primary" buffer for fast CPU access to pixel data, and separate scanout buffers for each display controller (CRTC). This may improve performance for some 2D workloads, potentially at the expense of other (e.g. 3D, video) workloads. Note that enabling this option currently disables Option "EnablePageFlip":

EXAVSync is only available when using EXA and can be enabled to avoid tearing by stalling the engine until the display controller has passed the destination region. It reduces tearing at the cost of performance and has been known to cause instability on some chips:

Below is a sample configuration file of /etc/X11/xorg.conf.d/20-radeon.conf:

Defining the gartsize, if not autodetected, can be done by adding radeon.gartsize=32 as a kernel parameter.

The changes take effect at the next reboot.

Since kernel 3.6, PCI Express 2.0 in radeon is turned on by default.

It may be unstable with some motherboards. It can be deactivated by adding radeon.pcie_gen2=0 as a kernel parameter.

See Phoronix article for more information.

The radeon driver supports the activation of a heads-up display (HUD) which can draw transparent graphs and text on top of applications that are rendering, such as games. These can show values such as the current frame rate or the CPU load for each CPU core or an average of all of them. The HUD is controlled by the GALLIUM_HUD environment variable, and can be passed the following list of parameters among others:

To see a full list of parameters, as well as some notes on operating GALLIUM_HUD, you can also pass the "help" parameter to a simple application such as glxgears and see the corresponding terminal output:

More information can be found from this mailing list post or this blog post.

It is the technology used on recent laptops equiped with two GPUs, one power-efficent (generally Intel integrated card) and one more powerful and more power-hungry (generally Radeon or Nvidia). There are two ways to get it work:

You can choose between three different methods:

See https://www.x.org/wiki/RadeonFeature/#index3h2 for more details.

Since kernel 3.13, DPM is enabled by default for lots of AMD Radeon hardware. If you want to disable it and use another method instead, add the parameter radeon.dpm=0 to the kernel parameters.

Unlike dynpm, the "dpm" method uses hardware on the GPU to dynamically change the clocks and voltage based on GPU load. It also enables clock and power gating.

There are 3 operation modes to choose from:

They can be changed via sysfs:

For testing or debugging purposes, you can force the card to run in a set performance mode:

This method dynamically changes the frequency depending on GPU load, so performance is ramped up when running GPU intensive applications, and ramped down when the GPU is idle. The re-clocking is attempted during vertical blanking periods, but due to the timing of the re-clocking functions, does not always complete in the blanking period, which can lead to flicker in the display. Due to this, dynamic power management only works when a single head is active.

It can be activated by simply running the following command:

This method will allow you to select one of the five profiles (described below). Different profiles, for the most part, end up changing the frequency/voltage of the GPU. This method is not as aggressive, but is more stable and flicker free and works with multiple heads active.

To activate the method, run the following command:

Select one of the available profiles:

As an example, we will activate the low profile (replace low with any of the aforementioned profiles as necessary):

The methods described above are not persistent. To make them persistent, you may create a udev rule (example for #Profile-based frequency switching):

As another example, dynamic power management can be permanently forced to a certain performance level:

To view the speed that the GPU is running at, perform the following command and you will get something like this output:

It depends on which GPU line yours is, however. Along with the radeon driver versions, kernel versions, etc. So it may not have much/any voltage regulation at all.

Thermal sensors are implemented via external i2c chips or via the internal thermal sensor (rv6xx-evergreen only). To get the temperature on asics that use i2c chips, you need to load the appropriate hwmon driver for the sensor used on your board (lm63, lm64, etc.). The drm will attempt to load the appropriate hwmon driver. On boards that use the internal thermal sensor, the drm will set up the hwmon interface automatically. When the appropriate driver is loaded, the temperatures can be accessed via lm_sensors tools or via sysfs in /sys/class/hwmon.

While the power saving features above should handle fan speeds quite well, some cards may still be too noisy in their idle state. In this case, and when your card supports it, you can change the fan speed manually.

To control the GPU fan, see Fan speed control#AMDGPU sysfs fan control (amdgpu and radeon share the same controls for this).

For persistence, see the example in #Persistent configuration.

If a fixed value is not desired, there are possibilities to define a custom fan curve manually by, for example, writing a script in which fan speeds are set depending on the current temperature (current value in /sys/class/drm/card0/device/hwmon/hwmon0/temp1_input).

A GUI solution is available by installing radeon-profile-gitAUR.

First, check that you have an S-video output: xrandr should give you something like

Now we should tell Xorg that it is actually connected (it is, right?)

Setting TV standard to use:

Adding a mode for it (currently supports only 800x600):

Now let us try to see what we have:

At this point, you should see a 800x600 version of your desktop on your TV.

To disable the output, do

The kernel can recognize video= parameter in following form (see KMS for more details):

Parameters with whitespaces must be quoted:

You can get list of your video outputs with following command:

HDMI audio is supported in the xf86-video-ati video driver. To disable HDMI audio add radeon.audio=0 to your kernel parameters.

If there is no video after boot up, the driver option has to be disabled.

See Multihead#RandR how to setup multiple monitors by using RandR.

Independent dual-headed setups can be configured the usual way. However you might want to know that the radeon driver has a "ZaphodHeads" option which allows you to bind a specific device section to an output of your choice:

This can be a life-saver, when using videocards that have more than two outputs. For instance one HDMI out, one DVI, one VGA, will only select and use HDMI+DVI outputs for the dual-head setup, unless you explicitly specify "ZaphodHeads" "VGA-0".

The radeon driver will probably enable vsync by default, which is perfectly fine except for benchmarking. To turn it off, try the vblank_mode=0 environment variable or create ~/.drirc (edit it if it already exists) and add the following:

If vsync is still enabled, you can disable it by editing /etc/X11/xorg.conf.d/20-radeon.conf. See #Driver options.

If having 2D performance issues, like slow scrolling in a terminal or webbrowser, adding Option "MigrationHeuristic" "greedy" as device option may solve the issue.

In addition disabling EXAPixmaps may solve artifacts issues, although this is generally not recommended and may cause other issues.

See Xrandr#Adding undetected resolutions.

When connecting a TV using the HDMI port, the TV may show a blurry picture with a 2-3cm border around it. This protects against overscanning (see Wikipedia:Overscan), but can be turned off using xrandr:

This is a solution to the no-console problem that might come up, when using two or more ATI cards on the same PC. Fujitsu Siemens Amilo PA 3553 laptop for example has this problem. This is due to fbcon console driver mapping itself to the wrong framebuffer device that exists on the wrong card. This can be fixed by using the following kernel parameter:

This will tell the fbcon to map itself to the /dev/fb1 framebuffer and not the /dev/fb0, that in our case exists on the wrong graphics card. If that does not fix your problem, try booting with

There are three possible solutions:

If the cursor becomes corrupted (e.g. repeating itself vertically after the monitor(s) comes out of sleep) set "SWCursor" "True" in the "OutputClass" section of the /etc/X11/xorg.conf.d/20-radeon.conf configuration file.

Try booting with the kernel parameter radeon.audio=0.

Firmware issues with R9-390 series cards include poor performance and crashes (frequently caused by gaming or using Google Maps) possibly related DPM. There has been a comment on a bug report with instructions for a fix.

Older cards have their pixel clock limited to 165MHz for HDMI. Hence, they do not support QHD or 4k only via dual-link DVI but not over HDMI.

One possibility to work around this is to use custom modes with lower refresh rate, e.g. 30Hz.

Another one is a kernel patch removing the pixel clock limit, but this may damage the card!

Official kernel bug ticket with patch for 4.8: https://bugzilla.kernel.org/show_bug.cgi?id=172421

The patch introduces a new kernel parameter radeon.hdmimhz which alters the pixel clock limit.

Be sure to use a high speed HDMI cable for this.

If you use 390X (or perhaps similar models) and the 4k output from DP, you may experiencing occasional horizontal artifacts / flickering (i.e. every half an hour or so, a horizontal strip of pixels with a height of ~100 pixels across the whole screen's width shaking up and down for a few seconds). This might be a bug of the radeon driver. Changing to AMDGPU seems to fix it.

Benchmark showing the open source driver is on par performance-wise with the proprietary driver for many cards.

**Examples:**

Example 1 (unknown):
```unknown
/etc/X11/xorg.conf.d/20-radeon.conf
```

Example 2 (unknown):
```unknown
Section "OutputClass"
    Identifier "Radeon"
    MatchDriver "radeon"
    Driver "radeon"
EndSection
```

Example 3 (unknown):
```unknown
/etc/X11/xorg.conf.d/20-radeon.conf
```

Example 4 (unknown):
```unknown
Option "AccelMethod" "glamor"
```

---

## Help:Reading

**URL:** https://wiki.archlinux.org/title/Starting

**Contents:**
- Organization
- Formatting
- Root, regular user or another user
- Append, add, create, edit
  - Make executable
- Source
- Installation of packages
  - Official packages
  - Arch User Repository
- Control of systemd units

Because the vast majority of the ArchWiki contains indications that may need clarification for users new to Arch Linux (or GNU/Linux in general), this rundown of basic procedures was written both to avoid confusion in the assimilation of the articles and to deter repetition in the content itself.

Most articles on the ArchWiki do not attempt to provide a holistic introduction to a single topic; they are instead written in adherence to the "Don't Repeat Yourself" principle, under the assumption that the user will seek out and read any supporting material that they do not yet understand. Where possible, such supporting material is indicated in the article via special formatting, see #Formatting.

Because of this organization, it may be necessary to examine several related sources in order to fully understand an ArchWiki article. In particular, users who are new to Arch (or GNU/Linux in general) should expect to end up reading a great number of articles even when solving simple problems. It is especially important to study the supporting material before seeking additional help from other users.

Some lines are written like so:

Others have a different prefix:

The numeral or hash sign (#) indicates that the command needs to be run as root, whereas the dollar sign ($) shows that the command should be run as a regular user.

When the commands need to run as a specific user, they will be prefixed by the username in square brackets, for example:

This means you should use a privilege elevation tool, e.g. with sudo:

A notable exception to watch out for:

In this example, the context surrounding the numeral sign communicates that this is not to be run as a command; it should be edited into a file instead. So in this case, the numeral sign denotes a comment. A comment can be explanatory text that will not be interpreted by the associated program. Bash scripts denotation for comments happens to coincide with the root PS1.

After further examination, "give away" signs include the uppercase character following the # sign. Usually, Unix commands are not written this way and most of the time they are short abbreviations instead of full-blown English words (e.g., Copy becomes cp).

Regardless, most articles make this easy to discern by notifying the reader:

Append to ~/path/to/file:

When prompted to append to, add to, create, or edit one or more files, it is implied that you should use one of the following methods.

To create or modify multiline files, it is suggested to use a text editor. For example, using the nano command to edit the file /etc/bash.bashrc is:

To create or overwrite a file from a string, it may be simpler to use output redirection. The following example creates or overwrites the contents of the file /etc/hostname with the text myhostname.

Output redirection can also be used to append a string to a file. The following example appends the text [custom-repo] to the file /etc/pacman.conf.

When prompted to create directories, use the mkdir command:

After creating a file, if it is meant to be run as a script (whether manually or called by another program), it needs to be set as executable, for example with:

See chmod. Some applications such as file managers may provide graphical interfaces to do the same.

Some applications, notably command-line shells, use scripts for their configuration: after modifying them, they must be sourced in order for the changes to be applied. In the case of bash, for example, this is done by running (you can also replace source with .):

When the wiki suggests modifying such a configuration script, it will not explicitly remind you to source the file, and only in some cases will it point to this section with a reminder link.

When an article invites you to install some packages in the conventional way, it will not indicate the detailed instructions to do so; instead, it will simply mention the names of the packages to be installed.

The subsections below give an overview of the generic installation procedures depending on the package type.

For packages from the official repositories, you will read something like:

This means that you have to run:

The pacman article contains detailed explanations to deal with package management in Arch Linux proficiently.

For packages from the Arch User Repository (AUR), you will read something like:

This means that in general you have to follow the foobarAUR link, download the PKGBUILD archive, extract it, verify the content and finally run, in the same folder:

The Arch User Repository article contains all the detailed explanations and best practices to deal with AUR packages.

When an article invites to start, enable, etc., some systemd unit (e.g. a service), it will not indicate the detailed instructions to do so, but instead you will read something like:

This means that you have to run:

A notable command that does not follow this exact pattern is systemctl daemon-reload which will be called without arguments.

The systemd#Using units section contains structured list of available actions (like start, enable, enable and start, etc.) with their corresponding systemctl commands.

It is important to remember that there are two different kinds of configurations on a GNU/Linux system. System-wide configuration affects all users. Since system-wide settings are generally located in the /etc directory, root privileges are required in order to alter them. For example, to apply a Bash setting that affects all users, /etc/bash.bashrc should be modified.

User-specific configuration affects only a single user. Dotfiles are used for user-specific configuration. For example, the file ~/.bashrc is the user-specific configuration file. The idea is that each user can define their own settings, such as aliases, functions and other interactive features like the prompt, without affecting other users' preferences.

Bash and other Bourne-compatible shells, such as Zsh, also source files depending on whether the shell is a login shell or an interactive shell. See Bash#Configuration files and Zsh#Startup/Shutdown files for details.

Some code blocks may contain so-called pseudo-variables, which, as the name says, are not actual variables used in the code. Instead they are generic placeholders and have to be manually replaced with system-specific configuration items before the code may be run or parsed. Common shells such as bash and zsh provide tab-completion to auto-complete parameters for common commands such as systemctl.

In the articles that comply with Help:Style/Formatting and punctuation, pseudo-variables are formatted in italics. For example:

In this case interface_name is used as a pseudo-variable placeholder in a systemd template unit. All systemd template units, identifiable by the @ sign, require a system-specific configuration item as argument. See systemd#Using units.

In this case the pseudo-variables are used to describe the parameters that must be substituted for them. Details on how to gather them are elaborated on in the section Securely wipe disk#Calculate blocks to wipe manually, which features the command.

This article or section needs expansion.

In case of file examples, pasting pseudo-variables in real configuration files might break the programs that use them.

In most cases, ellipses (...) are not part of the actual file content or code output, and instead represent omitted or optional text that is not relevant for the discussed subject.

For example HOOKS=(... encrypt ... filesystems ...) or:

Be aware though that, in a few instances, ellipses may be a meaningful part of the code syntax: attentive users should be able to recognize these cases by the context.

**Examples:**

Example 1 (unknown):
```unknown
# mkinitcpio -p linux
```

Example 2 (unknown):
```unknown
$ makepkg -s
```

Example 3 (unknown):
```unknown
sudo command
```

Example 4 (unknown):
```unknown
[postgres]$ initdb -D /var/lib/postgres/data
```

---

## dk

**URL:** https://wiki.archlinux.org/title/Dk

**Contents:**
- Installation
- Starting
- Configuration

This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.

dk is a list-based tiling window manager in the vein of dwm, bspwm and xmonad. It describes itself as heavily scriptable and tinker friendly, featuring dynamic workspaces, diverse layouts, sane support for ICCCM, EWMH, motif, and more.

Install dkAUR for the window manager and sxhkd for the X hotkey daemon, since dk has no keybind support by itself.

The example configurations for dk and sxhkd respectively are situated in /usr/share/doc/dk/ as dkrc and sxhkdrc. Both have to be under $XDG_CONFIG_HOME/dk/ and be executable. You may edit these files to your liking.

Read dk(1) and sxhkd(1) for further documentation on configuration.

**Examples:**

Example 1 (unknown):
```unknown
/usr/share/doc/dk/
```

Example 2 (unknown):
```unknown
$XDG_CONFIG_HOME/dk/
```

---

## Xephyr

**URL:** https://wiki.archlinux.org/title/Xephyr

**Contents:**
- Installation
- Usage
  - Launching window managers
  - Grabbing and un-grabbing user input
  - Sending Alt+Tab
- Tips and tricks

This article or section needs expansion.

Xephyr is a nested X server that runs as an X application.

This may be useful to workaround a badly written application. For example, Supermicro servers may be controlled with a java ipmi kvm viewer application. While the server is rebooting, the application frequently recreates its window and steals focus from your current window. This happens several times per minute, and makes your work impossible. It is not obvious how to make a window rule that prevents such application's window from gaining focus when created, because focus must be given when launched for the first time. Using Xephyr allows you to keep these window recreations inside a separate window, which does not steal focus from your currently opened window(s).

Install xorg-server-xephyr.

If you wish to run a nested X window, you will need to specify a new display:

This will launch a new Xephyr window with a DISPLAY of ":1". In order to launch an application in that window, you would need to specify that display:

If you want to launch a specific WM, spectrwm for example, you would type:

You can also launch Xephyr with your xinitrc using startx:

Pressing Ctrl+Shift should lock/unlock your mouse pointer and your keystrokes inside Xephyr window exclusively if possible.

This article or section needs expansion.

If using KDE, create a window rule to ignore global shortcuts. Then you can use Alt+Tab inside Xephyr.

Other examples for situations where Xephyr can be useful are:

**Examples:**

Example 1 (unknown):
```unknown
$ Xephyr -br -ac -noreset -screen 800x600 :1
```

Example 2 (unknown):
```unknown
$ DISPLAY=:1 xterm
```

Example 3 (unknown):
```unknown
$ DISPLAY=:1 spectrwm
```

Example 4 (unknown):
```unknown
$ startx -- /usr/bin/Xephyr :1
```

---

## xdg-utils

**URL:** https://wiki.archlinux.org/title/Xdg-utils

**Contents:**
- Installation
- Usage
  - Environment variables
  - xdg-mime
  - xdg-open
  - xdg-settings
- Tips and Tricks
  - URL scheme handlers

xdg-utils provides the official utilities for managing XDG MIME Applications.

Install the xdg-utils package.

xdg-utils attempts to integrate with your desktop environment by invoking the specialized programs it provides, where applicable. The evaluation of the current environment is as follows [1]:

During this process, if any match is found, the DE variable is internally overwritten with the detected desktop environment's normalized value. Hence, DE is both a legacy environment variable, and an internal state variable for xdg-utils. As an example, if XDG_CURRENT_DESKTOP is KDE, xdg-utils will internally set DE to kde. If no match is found, then any pre-existing DE value will be used, in such a way that XDG_CURRENT_DESKTOP=KDE is equivalent to XDG_CURRENT_DESKTOP being unset, and having DE=kde. This implementation detail is worth noting because it has the consequence that a pre-set DE is ignored if a desktop environment is otherwise detected.

Values of the variables that xdg-utils recognizes are:

Note that this is only a list of what the scripts provided by xdg-utils are capable of detecting. The scripts will still perform generic, environment-agnostic actions under the following conditions:

xdg-mime(1) is a script for directly querying and modifying default MIME applications. It is used within other scripts, such as xdg-open, and is also a useful troubleshooting tool.

Determine a file's MIME type:

Determine the default application for a MIME type:

Change the default application for a MIME type:

To set a file manager as the default file manager (for ex -Thunar) type:

Debug default application for MIME type:

When it is necessary to determine the MIME type of a file, xdg-mime attempts to use the right program for the desktop environment:

In the generic case, xdg-mime will:

xdg-open(1) is a resource opener used by many applications, implementing the XDG MIME Applications standard while integrating with the system's desktop environment as much as possible.

If a desktop environment was detected, its provided handler will be invoked [2]:

In the generic case, xdg-open will:

Since xdg-mime relies on perl-file-mimeinfo package to implement the XDG MIME Applications standard, if you are not using a desktop environment, you should either install perl-file-mimeinfo, or consider a different resource opener.

Shortcut to open all web MIME types with a single application:

Shortcut for setting the default application for a URL scheme:

To set the default application for a URL scheme you may also need to change the default application for the x-scheme-handler/* MIME types:

**Examples:**

Example 1 (unknown):
```unknown
KDE_FULL_SESSION
```

Example 2 (unknown):
```unknown
XDG_CURRENT_DESKTOP
```

Example 3 (unknown):
```unknown
XDG_CURRENT_DESKTOP=KDE
```

Example 4 (unknown):
```unknown
XDG_CURRENT_DESKTOP
```

---

## Power management

**URL:** https://wiki.archlinux.org/title/Power_saving

**Contents:**
- Userspace tools
  - Console
  - Graphical
- ACPI events
  - Power managers
  - xss-lock
- Power saving
  - Print power settings
  - Processors with Intel Hardware P-state support
  - Audio

Power management is a feature that turns off the power or switches system components to a low-power state when inactive.

In Arch Linux, power management consists of two main parts:

These tools allow you to change a lot of settings without the need to edit config files by hand. Only run one of these tools to avoid possible conflicts as they all work more or less similarly. Have a look at the power management category to get an overview on what power management options exist in Arch Linux.

These are the more popular scripts and tools designed to help power saving:

systemd handles some power-related ACPI events, whose actions can be configured in /etc/systemd/logind.conf or /etc/systemd/logind.conf.d/*.conf  see logind.conf(5). On systems with no dedicated power manager, this may replace the acpid daemon which is usually used to react to these ACPI events.

The specified action for each event can be one of ignore, poweroff, reboot, halt, suspend, hibernate, hybrid-sleep, suspend-then-hibernate, lock or kexec. In case of hibernation and suspension, they must be properly set up. If an event is not configured, systemd will use a default action.

To apply changes, reload systemd-logind.service.

Some desktop environments include power managers which inhibit (temporarily turn off) some or all of the systemd ACPI settings. If such a power manager is running, then the actions for ACPI events can be configured in the power manager alone. Changes to /etc/systemd/logind.conf or /etc/systemd/logind.conf.d/*.conf need be made only if you wish to configure behaviour for a particular event that is not inhibited by the power manager.

Note that if the power manager does not inhibit systemd for the appropriate events you can end up with a situation where systemd suspends your system and then when the system is woken up the other power manager suspends it again. The power managers of GNOME, MATE, Plasma and Xfce issue the necessary inhibited commands. If the inhibited commands are not being issued, such as when using acpid or others to handle ACPI events, set the Handle options to ignore. See also systemd-inhibit(1).

xss-lock subscribes to the systemd-events suspend, hibernate, lock-session, and unlock-session with appropriate actions (run locker and wait for user to unlock or kill locker). xss-lock also reacts to DPMS events and runs or kills the locker in response.

Autostarting the following for example:

This section is a reference for creating custom scripts and power saving settings such as by udev rules. Make sure that the settings are not managed by some other utility to avoid conflicts.

Almost all of the features listed here are worth using whether or not the computer is on AC or battery power. Most have negligible performance impact and are just not enabled by default because of commonly broken hardware/drivers. Reducing power usage means reducing heat, which can even lead to higher performance on a modern Intel or AMD CPU, thanks to dynamic overclocking.

This script prints power settings and a variety of other properties for USB and PCI devices. Note that root permissions are needed to see all settings.

This article or section is a candidate for merging with CPU frequency scaling.

The available energy preferences of an Intel Hardware P-state (HWP) supported processor are default, performance, balance_performance, balance_power, power.

This can be validated by running

To conserve more energy, you can edit the configuration by creating the following file:

See the x86_energy_perf_policy(8) man page for more details on energy-performance policy in Intel processors. Also see systemd-tmpfiles(8) and tmpfiles.d(5) man pages for temporary files/directories details.

Whether power saving is turned on by default depends on a given driver, e.g. it is on for HD Audio. Identify the module in use, then run

and look for a kernel module parameter (like power_save) that adjusts or disables power-saving feature.

To disable Bluetooth completely, blacklist the btusb and bluetooth modules.

Alternatively, create the following udev rules:

To turn off Bluetooth only temporarily, use rfkill(8):

If you will not use integrated web camera then blacklist the uvcvideo module.

This section uses configurations in /etc/sysctl.d/, which is "a drop-in directory for kernel sysctl parameters." See The New Configuration Files and more specifically sysctl.d(5) for more information.

This article or section needs expansion.

The NMI watchdog is a debugging feature to catch hardware hangs that cause a kernel panic. On some systems it can generate a lot of interrupts, causing a noticeable increase in power usage. To list these interrupts per CPU core since last boot, you can use:

To turn the hardlockup detector off, use:

or add nmi_watchdog=0 to the kernel line.

Alternatively add nowatchdog to the kernel line to disable both hard and soft lockup detectors. See [3]

Increasing the virtual memory dirty writeback time helps to aggregate disk I/O together, thus reducing spanned disk writes, and increasing power saving. To set the value to 60 seconds (default is 5 seconds):

To do the same for journal commits on supported filesystems (e.g. ext4, btrfs...), use commit=60 as an option in fstab.

Note that this value is modified as a side effect of the Laptop Mode setting below. See also sysctl#Virtual memory for other parameters affecting I/O performance and power saving.

See the kernel documentation on the laptop mode "knob" - "A sensible value for the knob is 5 seconds".

Wake-on-LAN can be a useful feature, but if you are not making use of it then it is simply draining extra power waiting for a magic packet while in suspend. You can adapt the Wake-on-LAN#udev rule to disable the feature for all ethernet interfaces. To enable powersaving with iw on all wireless interfaces:

The name of the configuration file is important. With the use of persistent device names in systemd, the above network rule, named lexicographically after 80-net-setup-link.rules, is applied after the device is renamed with a persistent name e.g. wlan0 renamed wlp3s0. Be aware that the RUN command is executed after all rules have been processed and must anyway use the persistent name, available in $name for the matched device.

Additional power saving functions of Intel wireless cards with iwlwifi driver can be enabled by passing the correct parameters to the kernel module. Making them persistent can be achieved by adding the lines below to the /etc/modprobe.d/iwlwifi.conf file:

This option will probably increase your median latency:

On kernels < 5.4 you can use this option, but it will probably decrease your maximum throughput:

Depending on your wireless card one of these two options will apply.

You can check which one is relevant by checking which of these modules is running using

Keep in mind that these power saving options are experimental and can cause an unstable system.

If using iwd, power-saving can be disabled for all Wi-Fi devices with the following config file:

You can also replace * with a specific driver name, see iwd.config(5)  SETTINGS.

If using NetworkManager, power-saving can be disabled globally for every connection with a config file, for example:

At boot, the BIOS enables or disables ASPM based on hardware support. To check for support:

Fetch available ASPM policies and the current system default using the following:

ASPM might be disabled for the following reasons [4]:

If you believe that your hardware has support for ASPM despite the above, it can be force-enabled for the kernel to handle with the pcie_aspm=force kernel parameter.

As long as ASPM is supported and enabled, it is possible to select a desired policy for the current session. For example, switch to powersupersave for the current session by doing the following:

To configure a specific ASPM state to enable upon system boot (using powersupersave as an example), add pcie_aspm.policy=powersupersave as a kernel parameter.

The rule above powers down unused devices.

Some devices will not wake up again. To allow runtime power management only for devices that are known to work, use simple matching against vendor and device IDs (use lspci -nn to get these values):

Alternatively, to blacklist devices that are not working with PCI runtime power management and enable it for all other devices:

The Linux kernel can automatically suspend USB devices when they are not in use. This can sometimes save quite a bit of power, however some USB devices are not compatible with USB power saving and start to misbehave (common for USB mice/keyboards). udev rules based on whitelist or blacklist filtering can help to mitigate the problem.

The example is enabling autosuspend for all USB devices except for keyboards and mice:

To allow autosuspend only for devices that are known to work, use simple matching against vendor and product IDs (use lsusb to get these values):

Alternatively, to blacklist devices that are not working with USB autosuspend and enable it for all other devices:

The default autosuspend idle delay time is controlled by the autosuspend parameter of the usbcore built-in kernel module. To set the delay to 5 seconds instead of the default 2 seconds, add the following kernel parameter for your boot loader.

Similarly to power/control, the delay time can be fine-tuned per device by setting the power/autosuspend attribute. This means, alternatively, autosuspend can be disabled by setting power/autosuspend to -1 (i.e., never autosuspend):

See the Linux kernel documentation for more information on USB power management.

The current setting can be read from or written to /sys/class/scsi_host/host*/link_power_management_policy as follows:

You can configure link_power_management_policy settings persistently by adding a udev rules file, for example:

See hdparm#Power management configuration for drive parameters that can be set.

Power saving is not effective when too many programs are frequently writing to the disk. Tracking all programs, and how and when they write to disk is the way to limit disk usage. Use iotop to see which programs use the disk frequently. See Improving performance#Storage devices for other tips.

Small adjustments such as setting the noatime option can also help. If enough RAM is available, consider disabling or limiting swappiness as it has the possibility to limit a good number of disk writes.

For Seagate drives with PowerChoice technology, tricks setting APM via hdparm will not work due to the EPC (Extended Power Conditions) feature. Rather than setting APM, you can install openseachestAUR and fully disable EPC like so (replace X with actual drive letter):

Last invocation will give the following summary:

Zeroes in the first column confirm that parking and spindown were disabled successfully

This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.

This article or section is a candidate for merging with Laptop#Power management.

Since systemd users can suspend and hibernate through systemctl suspend or systemctl hibernate and handle acpi events with /etc/systemd/logind.conf, it might be interesting to remove pm-utils and acpid. There is just one thing systemd cannot do (as of systemd-204): power management depending on whether the system is running on AC or battery. To fill this gap, you can create a single udev rule that runs a script when the AC adapter is plugged and unplugged:

Examples of powersave scripts:

The above udev rule should work as expected, but if your power settings are not updated after a suspend or hibernate cycle, you should add a script in /usr/lib/systemd/system-sleep/ with the following contents:

Do not forget to make it executable!

This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.

The suspend, poweroff and hibernate button presses and lid close events are handled by logind as described in #ACPI events.

If you are using polkit, users with non-remote session can issue power-related commands as long as the session is not broken.

To check if your session is active:

The user can then use systemctl commands in the command line, or add them to menus:

Other commands can be used as well, including systemctl suspend and systemctl hibernate. See the System Commands section in systemctl(1).

Install sudo, and configure it to give the user root privileges. The user will then be able to use the sudo systemctl commands (e.g. sudo systemctl poweroff, sudo systemctl reboot, sudo systemctl suspend and sudo systemctl hibernate). See the System Commands section in systemctl(1)

If users should only be allowed to use shutdown commands, but not have other privileges, add the following to the end of /etc/sudoers using the visudo command as root. Substitute user for your username and hostname for the machine's hostname.

Now your user can shutdown with sudo systemctl poweroff, and reboot with sudo systemctl reboot. Users wishing to power down a system can also use sudo systemctl halt. Use the NOPASSWD: tag only if you do not want to be prompted for your password.

**Examples:**

Example 1 (unknown):
```unknown
/etc/systemd/logind.conf
```

Example 2 (unknown):
```unknown
/etc/systemd/logind.conf.d/*.conf
```

Example 3 (unknown):
```unknown
hybrid-sleep
```

Example 4 (unknown):
```unknown
suspend-then-hibernate
```

---

## Flatpak

**URL:** https://wiki.archlinux.org/title/Flatpak

**Contents:**
- Installation
  - Desktop integration
  - Application management
  - Permission management
- Managing repositories
  - Add a repository
  - Delete a repository
  - List repositories
  - Set repository priorities
  - Change the repository subset

From the project README: "Flatpak is a system for building, distributing and running sandboxed desktop applications on Linux."

Install the flatpak package. If you want to build flatpaks, install flatpak-builder too.

For flatpak applications to interact with your desktop (i.e. allow applications to open URLs, share your screen and more), make sure to set up the xdg-desktop-portal. Depending on the implementation for your desktop, there is a confirmation dialog before the application is able to access some portals.

To add a remote flatpak repository do:

where name is the name for the new remote, and location is the path or URL for the repository.

The installation of flatpak will, by default, add the official Flathub repository as a system-wide installation. To add the official repository with a per-user configuration:

To delete a remote flatpak repository do:

where name is the name of the remote repository to be deleted.

To list all the added repositories do:

To change the default priority of Flathub repo to 3:

To select verified subset of Flathub repo:

Refer to the documentation for the desired Flatpak repository to find the available subsets and their descriptions.

Before being able to search for a runtime or application in a newly added remote repository, we need to retrieve the appstream data for it:

Then we can proceed to search for a package with flatpak search packagename, e.g. to look for the package libreoffice with the flathub remote configured:

To list all available runtimes and applications in a remote repository named remote do:

To install a runtime or application do:

where remote is the name of the remote repository, and name is the name of the application or runtime to install.

To list installed runtimes and applications do:

Binaries are available in /var/lib/flatpak/exports/bin, which is automatically added to $PATH by /etc/profile.d/flatpak-bindir.sh. You may have to re-login to apply the change.

Flatpak applications can also be run with the command line:

List runtimes and applications that have updates available:

To update a runtime or application named name do:

To update all applications and runtimes:

To update your system runtimes and applications automatically, create the following files:

Afterwards, do a daemon-reload and enable/start the flatpak-update.timer unit.

To uninstall a runtime or application named name do:

To delete app data from ~/.var/app and from the permission store while uninstalling, use:

To downgrade a runtime or application, first look for the associated commit ID:

Where remote is the repository (such as flathub), and name is the name of the application or runtime. Then, deploy the commit:

where commit is the commit for the desired version, and name is as before.

This procedure can also be used to selectively upgrade a package to a desired version that is not the latest version.

To exclude flatpak update from updating this package, see #Prevent updates to a runtime or application.

To prevent automatic and manual updates to a runtime or application, use the flatpak mask command:

This also prevents selective upgrades and downgrades.

To reverse the mask and re-enable updates, use flatpak mask --remove:

Flatpak expects window managers to respect the XDG_DATA_DIRS environment variable to discover applications. This variable is set by the script /etc/profile.d/flatpak.sh. Updating the environment may require restarting the session. If the launcher does not support XDG_DATA_DIRS, you can edit the list of directories scanned and add these to it:

This is known to be necessary in Awesome.

Flatpak applications come with predefined sandbox rules which define the resources and file system paths the application is allowed to access. To view the specific application permissions do:

The reference of the sandbox permission names can be found on official flatpak documentation.

If you find the predefined permissions of the application too lax or too restrictive you can change to anything you want using flatpak override command. For example:

This will prevent the application access to your home folder.

Every type of permission, such as device, filesystem or socket, has a command line option that allows that particular permission and a separate option that denies permission. For example, in case of device access --device=device_name allows access, --nodevice=device_name denies the permission to access device.

For all permission types commands consult the manual page: flatpak-override(1)

Permission overrides can be reset to defaults with command:

Flatseal is a GUI permissions manager which offers simple point-and-click permissions operations. In KDE Plasma, Flatpak Permissions Management KCM provides a similar GUI for the system settings application: System Settings > Applications > Flatpak Permission Settings.

This article or section needs expansion.

You can create a custom Arch-based base runtime and base SDK for Flatpak using pacman. You can then use it for building and packaging applications. This is an alternative for personal use to the default org.freedesktop.BasePlatform and org.freedesktop.BaseSdk runtimes.

In addition to flatpak, you need to have installed fakeroot and for pacman hooks support also fakechroot.

First, start by creating a directory for building the runtime and possibly applications.

You can then prepare a directory for building the runtime base platform. The files subdirectory will contain what will later be the /usr directory in the sandbox. Therefore you will need to create symbolic links so the default /usr/share etc. from Arch can still be accessed at the usual path.

Make your host OS fonts available to the Arch runtime:

You need and may want to adapt your pacman.conf before installing packages to the runtime. Copy /etc/pacman.conf to your build directory and then make the following changes:

Now install the packages for the runtime.

Set up the locales to be used by editing myruntime/files/etc/locale.gen. Then regenerate the runtimes locales.

The base SDK can be created from the base runtime with added applications needed for building packages and running pacman.

Insert metadata about runtime and SDK.

Add base runtime and SDK to a local repository in the current directory. You may want to give them appropriate commit messages such as My Arch base runtime and My Arch base SDK.

Install the runtime and SDK.

As an alternative to building applications the usual way, we can use pacman to create a containerized version of the regular Arch packages. Note that /usr is read-only when creating apps, so we can not use Archs packages when building an app. To create a real app with pacman, we can either

For doing the latter, first create a runtime using pacman such as this one for gedit. The runtime is first initialized and prepared for use with pacman.

Then the package is installed. The hosts network connection must be made available to pacman.

You can test the installation before finishing the runtime (without proper sandboxing).

Now finish building the runtime and export it to a new local repository. pacmans GnuPG keys have permissions that may interfere and need to be removed first.

Then create a dummy app.

Now finish the dummy app. You can fine-tune the apps access permissions when sandboxed by giving additional options when finishing the build. For possible options see the Flatpak documentation and the GNOME manifest files. Alternatively, adapt geditapp/metadata to your needs after finishing the build but before exporting. When the metadata file is complete, export the app to the repository.

Install it along with the runtime.

The linux-hardened kernel sets kernel.unprivileged_userns_clone to 0, so only privileged users can create new user namespaces.

One method to fix this is to install bubblewrap-suid. This package provides a version of bwrap(1) with the setuid bit enabled, allowing bubblewrap elevate itself and create new namespaces.

Alternatively, set kernel.unprivileged_userns_clone to 1 using sysctl(8), allowing unprivileged users to create new user namespaces:

To make this change persist across reboots, add a configuration file to sysctl.d(5):

For more information, see the note in Bubblewrap#Installation.

If the application doesn't properly open and you get messages such as Failed to connect to Wayland display: No such file or directory on flatpak run: This may be because some other setting such as ELECTRON_OZONE_PLATFORM_HINT="auto" makes the Flatpak application choose Wayland while access to Wayland isn't whitelisted for this application.

This can be fixed by whitelisting access to socket=wayland with e.g. Flatseal.

If you are starting X with manually-configured run commands, ensure you are including all essential components of the reference `xinitrc`. One of which sources a script which runs an update of the environment used for D-Bus session services.

There is no ideal way to apply system themes in flatpak apps as mentioned in the flatpak documentation [2] [3]. The easiest solution is using themes that are available in Flathub. However there is a workaround which can be used to apply themes to your flatpak apps. stylepak-gitAUR automates this workaround.

By default, the Flatpak version of Firefox will display a "File not found" error page when opening a local HTML. This is because permission must be granted to the app for accessing the folder containing the file.

However, note that when granting permission to access the entire Home folder, Firefox will then check for an existing profile in ~/.mozilla and load it instead of those previously in use from the sandboxed folder ~/.var/app/org.mozilla.firefox/cache/mozilla/. If your previous session's tabs and browsing history is missing after changing a permission (e.g. with Flatseal), either modify the permission to exclude access to ~/.mozilla, or consider copying the profile from ~/.var/app/org.mozilla.firefox/cache/mozilla/ to ~/.mozilla.

Flatpak applications that attempt to open URIs make use of the org.freedesktop.portal.OpenURI.OpenURI D-Bus interface exposed by xdg-desktop-portal. The xdg-desktop-portal-wlr backend does not support this call and therefore you will need an additional backend to fill the gap, for example xdg-desktop-portal-gtk.

There is no single standard to set the cursor properly. Some programs only need read access to the cursors directory, others also rely on other mechanisms. For GTK applications, ensure that xdg-desktop-portal-gtk is installed.

Otherwise, the following overrides should work for most common desktop applications.

In some cases you may also need to override the environment variables XCURSOR_THEME and XCURSOR_SIZE:

See this discussion for additional details.

Apparently it is not possible anymore to enable access to applications to directories under /usr/. The following hints at this when launching a program:

One possible workaround would be to manually copy your icon theme from /usr/share/icons to /home/$USER/.icons/.

If you switched your theme to Adwaita-dark and Flatpak Qt applications still use the light version, install the required KStyle:

Flatpak applications will not run if the mount point that contains the folder in which the application is stored, typically /var/lib/flatpak/ for system wide installations, and ~/.local/share/flatpak/ for user-specific installations, is mounted with the noexec option.

With noexec set you will get errors such as this:

**Examples:**

Example 1 (unknown):
```unknown
flatpak remote-add --user name location
```

Example 2 (unknown):
```unknown
$ flatpak remote-add name location
```

Example 3 (unknown):
```unknown
$ flatpak remote-add --if-not-exists --user flathub https://dl.flathub.org/repo/flathub.flatpakrepo
```

Example 4 (unknown):
```unknown
$ flatpak remote-delete name
```

---

## Kernel parameters

**URL:** https://wiki.archlinux.org/title/Kernel_parameters

**Contents:**
- Boot loader configuration
  - Clover
  - GRUB
  - GRUB Legacy
  - LILO
  - Limine
  - rEFInd
  - Syslinux
  - systemd-boot
- dracut

There are three ways to pass options to the kernel and thus control its behaviour:

Between the three methods, the configurable options differ in availability, their name and the method in which they are specified. This page only explains the second method (kernel command line parameters) and shows a list of the most used kernel parameters in Arch Linux.

Most parameters are associated with subsystems and work only if the kernel is configured with those subsystems built in. They also depend on the presence of the hardware they are associated with.

Kernel command line parameters either have the format parameter, or parameter=value, or module.parameter=value.

Kernel parameters can be set either temporarily by editing the boot entry in the boot loader boot selection menu, or permanently by modifying the boot loader configuration file.

The following examples add the quiet and splash parameters to the Clover, GRUB, GRUB Legacy, LILO, Limine, rEFInd, Syslinux and systemd-boot boot loaders.

dracut is capable of embedding the kernel parameters in the initramfs, thus allowing to omit them from the boot loader configuration. See dracut#Kernel command line options. Note that this only works for parameters understood by dracut, like root= and rd.*. They do not become real kernel parameters.

See EFI boot stub#Using UEFI directly.

Even without access to your boot loader it is possible to change your kernel parameters to enable debugging (if you have root access). This can be accomplished by overwriting /proc/cmdline which stores the kernel parameters. However /proc/cmdline is not writable even as root, so this hack is accomplished by using a bind mount to mask the path.

First create a file containing the desired kernel parameters:

Then use a bind mount to overwrite the parameters:

You can cat /proc/cmdline to confirm that your change was successful.

This list is not comprehensive. In addition to the kernel itself, other programs can also read parameters from /proc/cmdline and change their behavior.

**Examples:**

Example 1 (unknown):
```unknown
/etc/modprobe.d/
```

Example 2 (unknown):
```unknown
parameter=value
```

Example 3 (unknown):
```unknown
module.parameter=value
```

Example 4 (unknown):
```unknown
cat /proc/cmdline
```

---

## Archiving and compression

**URL:** https://wiki.archlinux.org/title/Tar

**Contents:**
- Archiving only
- Compression tools
  - Compression only
  - Archiving and compression
  - Feature charts
    - Decompress
- Usage comparison
  - Archiving only usage
  - Compression only usage
  - Archiving and compression usage

The traditional Unix archiving and compression tools are separated according to the Unix philosophy:

These tools are often used in sequence by firstly creating an archive file and then compressing it.

Of course there are also tools that do both, which tend to additionally offer encryption, error detection and recovery.

See also #Archiving only usage.

These compression programs implement their own file format.

See also #Archiving and compression usage.

Some of the tools above are capable of handling multiple formats, allowing for fewer installed packages.

See also Bash/Functions#Extract.

To extract an archive, its file format needs to be determined. If the file is properly named you can deduce its format from the file extension.

Otherwise you can use the file tool, see file(1).

Some file systems support on-the-fly compression of file data:

The open-sourced VDO project was integrated into the Linux kernel project, which provides a deduplication and compression device mapper layer in the interest of increasing storage efficiency. A userspace tools for managing VDO volumes is available in the AUR: vdoAUR

See Character encoding#Troubleshooting.

**Examples:**

Example 1 (unknown):
```unknown
--use-compress-program=lz4
```

Example 2 (unknown):
```unknown
tar cfv archive.tar file1 file2
```

Example 3 (unknown):
```unknown
tar xfv archive.tar
```

Example 4 (unknown):
```unknown
tar -tvf archive.tar
```

---

## Kernel parameters

**URL:** https://wiki.archlinux.org/title/Kernel_command_line

**Contents:**
- Boot loader configuration
  - Clover
  - GRUB
  - GRUB Legacy
  - LILO
  - Limine
  - rEFInd
  - Syslinux
  - systemd-boot
- dracut

There are three ways to pass options to the kernel and thus control its behaviour:

Between the three methods, the configurable options differ in availability, their name and the method in which they are specified. This page only explains the second method (kernel command line parameters) and shows a list of the most used kernel parameters in Arch Linux.

Most parameters are associated with subsystems and work only if the kernel is configured with those subsystems built in. They also depend on the presence of the hardware they are associated with.

Kernel command line parameters either have the format parameter, or parameter=value, or module.parameter=value.

Kernel parameters can be set either temporarily by editing the boot entry in the boot loader boot selection menu, or permanently by modifying the boot loader configuration file.

The following examples add the quiet and splash parameters to the Clover, GRUB, GRUB Legacy, LILO, Limine, rEFInd, Syslinux and systemd-boot boot loaders.

dracut is capable of embedding the kernel parameters in the initramfs, thus allowing to omit them from the boot loader configuration. See dracut#Kernel command line options. Note that this only works for parameters understood by dracut, like root= and rd.*. They do not become real kernel parameters.

See EFI boot stub#Using UEFI directly.

Even without access to your boot loader it is possible to change your kernel parameters to enable debugging (if you have root access). This can be accomplished by overwriting /proc/cmdline which stores the kernel parameters. However /proc/cmdline is not writable even as root, so this hack is accomplished by using a bind mount to mask the path.

First create a file containing the desired kernel parameters:

Then use a bind mount to overwrite the parameters:

You can cat /proc/cmdline to confirm that your change was successful.

This list is not comprehensive. In addition to the kernel itself, other programs can also read parameters from /proc/cmdline and change their behavior.

**Examples:**

Example 1 (unknown):
```unknown
/etc/modprobe.d/
```

Example 2 (unknown):
```unknown
parameter=value
```

Example 3 (unknown):
```unknown
module.parameter=value
```

Example 4 (unknown):
```unknown
cat /proc/cmdline
```

---

## Parted

**URL:** https://wiki.archlinux.org/title/GParted

**Contents:**
- Installation
- Usage
  - Command line mode
  - Interactive mode
- Rounding
- Partitioning
  - Create new partition table
  - Partition schemes
    - UEFI/GPT examples
    - BIOS/MBR examples

GNU Parted is a program for creating and manipulating partition tables. GParted is a GUI frontend.

Install one of the following packages:

Parted has two modes: command line and interactive. Parted should always be started with:

where device is block device name like /dev/sda, /dev/nvme0n1, /dev/mmcblk0, etc. If you omit the device argument, parted will attempt to guess which device you want.

In command line mode, this is followed by one or more commands. For example:

Interactive mode simplifies the partitioning process and reduces unnecessary repetition by automatically applying all partitioning commands to the specified device.

In order to start operating on a device, execute:

You will notice that the command-line prompt changes from a hash (#) to (parted): this also means that the new prompt is not a command to be manually entered when running the commands in the examples.

To see a list of the available commands, enter:

When finished, or if wishing to implement a partition table or scheme for another device, exit from parted with:

After exiting, the command-line prompt will change back to #.

If you do not give a parameter to a command, Parted will prompt you for it. For example:

Since many partitioning systems have complicated constraints, Parted will usually do something slightly different to what you asked. (For example, create a partition starting at 10.352Mb, not 10.4Mb) If the calculated values differ too much, Parted will ask you for confirmation. If you know exactly what you want, or to see exactly what Parted is doing, it helps to specify partition endpoints in sectors (with the "s" suffix) and give the "unit s" command so that the partition endpoints are displayed in sectors.

As of parted-2.4, when you specify start and/or end values using IEC binary units like MiB, GiB, TiB, etc., parted treats those values as exact, and equivalent to the same number specified in bytes (i.e., with the B suffix), in that it provides no helpful range of sloppiness. Contrast that with a partition start request of 4GB, which may actually resolve to some sector up to 500MB before or after that point. Thus, when creating a partition, you should prefer to specify units of bytes (B), sectors (s), or IEC binary units like MiB, but not MB, GB, etc.

You need to (re)create the partition table of a device when it has never been partitioned before, or when you want to change the type of its partition table. Recreating the partition table of a device is also useful when the partition scheme needs to be restructured from scratch.

Open each device whose partition table must be (re)created with:

To then create a new GUID Partition Table, use the following command:

To create a new Master Boot Record/MS-DOS partition table instead, use:

This article or section needs expansion.

You can decide the number and size of the partitions the devices should be split into, and which directories will be used to mount the partitions in the installed system (also known as mount points). See Partitioning#Partition scheme for the required partitions.

The following command will be used to create partitions:

The following command will be used to flag the partition that contains the /boot directory as bootable:

This article or section needs expansion.

In every instance, a special bootable EFI system partition is required.

If creating a new EFI system partition, use the following commands (the recommended size is at least 1 GiB):

The remaining partition scheme is entirely up to you. For one root partition using 100% of remaining space:

For separate swap (4 GiB) and / (all remaining space) partitions:

And for separate swap (4 GiB), / (32 GiB) and /home (all remaining space) partitions:

For a minimum single primary partition using all available disk space, the following command would be used:

In the following instance, a 4 GiB swap partition will be created, followed by a / partition using all the remaining space:

In the final example below, separate /boot (1 GiB), swap (4 GiB), / (32 GiB), and /home (all remaining space) partitions will be created:

If you are growing a partition, you have to first resize the partition and then resize the filesystem on it, while for shrinking the filesystem must be resized before the partition to avoid data loss.

To grow a partition (in parted interactive mode):

Where number is the number of the partition you are growing, and end is the new end of the partition (which needs to be larger than the old end).

Then, to grow the (ext2/3/4) filesystem on the partition (if size is not specified, it will default to the size of the partition):

Or to grow a Btrfs filesystem:

Where /path/to/mount/point stands for the mount point of the partition you are growing, and size in the form 16G or +1G is the new size or modification of the partition. Use max to fill the remaining space on the partition.

To shrink an ext2/3/4 filesystem on the partition:

To shrink a Btrfs filesystem:

Where /path/to/mount/point stands for the mount point of the partition you are shrinking, and size is the new size of the partition.

Then shrink the partition (in parted interactive mode):

Where number is the number of the partition you are shrinking, and end is the new end of the partition (which needs to be smaller than the old end).

When done, use the resizepart command from util-linux to tell the kernel about the new size:

Where device is the device that holds the partition, number is the number of the partition and size is the new size of the partition, in 512-byte sectors.

Parted will always warn you before doing something that is potentially dangerous, unless the command is one of those that is inherently dangerous (e.g. rm, mklabel and mkpart).

When creating a partition, parted might warn about improper partition alignment but does not hint about proper alignment. For example:

The warning means the partition start is not aligned. Enter "Ignore" to go ahead anyway, print the partition table in sectors to see where it starts, and remove/recreate the partition with the start sector rounded up to increasing powers of 2 until the warning stops. As one example, on a flash drive with 512B sectors, Parted wanted partitions to start on sectors that were a multiple of 2048, which is 1 MiB alignment.

If you want parted to attempt to calculate the correct alignment for you, specify the start position as 0% instead of some concrete value. To make one large ext4 partition, your command would look like this:

On an already partitioned disk, you can use parted to verify the alignment of a partition on a device. For instance, to verify alignment of partition 1 on /dev/sda:

**Examples:**

Example 1 (unknown):
```unknown
# parted device
```

Example 2 (unknown):
```unknown
/dev/nvme0n1
```

Example 3 (unknown):
```unknown
/dev/mmcblk0
```

Example 4 (unknown):
```unknown
# parted /dev/sda --script mklabel gpt mkpart P1 ext3 1MiB 8MiB
```

---

## vi

**URL:** https://wiki.archlinux.org/title/Vi

**Contents:**
- Installation
- Vi-style software
- See also

According to Wikipedia:

Install the vi package.

The package also provides line-oriented text editors edit and ex, as well as the vedit and view commands.

The key bindings and modes of vi have been recreated in various other software:

---

## Tor

**URL:** https://wiki.archlinux.org/title/Tor

**Contents:**
- Installation
- Usage
- Configuration
  - Tor ControlPort
    - Nyx
    - Set a Tor Control cookie file
    - Set a Tor Control password
    - Open Tor ControlSocket
    - Testing
- Web browsing

The Tor Project (The onion routing) is an open source implementation of onion routing that provides free access to an anonymous proxy network. Its primary goal is to enable online anonymity by protecting against traffic analysis attacks.

Users of the Tor network run an onion proxy software on their machines, which presents a SOCKS interface to its clients. This software connects out to Tor, periodically negotiating a virtual circuit through the Tor network. Tor employs cryptography in a layered manner (hence the 'onion' analogy), ensuring forward secrecy between routers.

Through this process the onion proxy manages networking traffic for end-user anonymity. It keeps a user anonymous by encrypting traffic, sending it through other nodes of the Tor network, and decrypting it at the last node to receive your traffic before forwarding it to the server you specified. One trade off is that using Tor can be considerably slower than a regular direct connection, due to the large amount of traffic re-routing. Additionally, although Tor provides protection against traffic analysis it cannot prevent traffic confirmation at the boundaries of the Tor network (i.e. the traffic entering and exiting the network). See Wikipedia:Tor (anonymity network) for more information.

Install the torbrowser-launcher package to use the Tor Browser, which is the only supported way to browse the web anonymously using Tor.

Users intending to manually use Tor with other software, run relays, or host onion services should install the tor package. The majority of this article covers this usage.

Nyx is a command line monitor for Tor, it provides bandwidth usage, connection details and on-the-fly configuration editing. To use it, install the nyx package.

Start/enable tor.service. Alternatively, launch it manually as the tor user:

To use a program over Tor, configure it to use 127.0.0.1 or localhost as a SOCKS5 proxy, with port 9050 for plain Tor with standard settings.

The proxy supports remote DNS resolution: use socks5h://localhost:9050 for DNS resolution from the exit node (instead of socks5 for a local DNS resolution).

To check if Tor is functioning properly, visit https://check.torproject.org/ with Tor Browser or CURL:

Tor reads its configurations from the file /etc/tor/torrc by default, or if the latter is not found, from $HOME/.torrc. The configuration options are explained in tor(1).

Drop-in files are supported by enabling:

To reload the configuration after a change, reload tor.service.

Some programs may require access to your Tor ControlPort to gain low-level control over your Tor node.

The ControlPort allows other programs to monitor and modify your Tor node's configuration while it is running, or get details about the status of the Tor network and its circuits.

To enable it, add to your torrc:

From The Tor Control Protocol:

To enhance security, restrict access to the ControlPort using a cookie file, control password, or both.

Assuming the ControlPort 9051 is set in torrc, you can start nyx by running:

To watch Tor connections in nyx [1], add to your torrc:

Restart tor.service to apply the change.

Enabling CookieAuthentication restricts access to the ControlPort by enforcing file permissions on the Tor cookie file and the Tor data directory.

Add users to the tor user group to give them access to the Tor cookie file.

You can use this command to check the permissions:

Convert your password from plaintext to hash:

Add the generated hash to your torrc:

If a program requires access to your Tor ControlSocket, such as a Unix domain Socket, add the following to your torrc:

Add the user who will run the program to the tor user group.

Restart tor.service and relaunch the program.

To verify the ControlSocket permissions:

To test your ControlPort, run openbsd-netcat with:

To test your ControlSocket, run socat with:

Both commands should print:

See The Tor Control Protocol for more commands.

The only way to browse anonymously is with the supported Tor Browser, which uses a patched version of Firefox. It can be installed with the torbrowser-launcher package.

Tor offers a built-in tunneled HTTP proxy and can also be used with an HTTP proxy like Privoxy; however, using the SOCKS5 library is generally recommended.

Add following line to your torrc file to set port 8118 on your localhost as HTTP proxy:

Refer to Tor manual for further information.

The FoxyProxy add-on allows you to specify multiple proxies for different URLs or for all your browsing. After restarting Firefox manually set Firefox to port 8118 on localhost, which is where Privoxy are running. These settings can be access under Add > Standard proxy type. Select a proxy label (e.g Tor) and enter the port and host into the HTTP Proxy and SSL Proxy fields. To check if Tor is functioning properly visit the Tor Check website and toggle Tor.

You can also use this setup in other applications like messaging (e.g. Jabber, IRC clients). Applications that support HTTP proxies you can connect to Privoxy (i.e. 127.0.0.1:8118). To use SOCKS proxy directly, you can point your application at Tor (i.e. 127.0.0.1:9050). A problem with this method though is that applications doing DNS resolves by themselves may leak information. Consider using Socks4A (e.g. with Privoxy) instead.

This article or section is being considered for removal.

In order to use an instant messaging client with tor, we do not need an HTTP proxy like privoxy. We will be using tor's daemon directly which listens to port 9050 by default.

You can set up Pidgin to use Tor globally, or per account. To use Tor globally, go to Tools -> Preferences -> Proxy. To use Tor for specific accounts, go to Accounts > Manage Accounts, select the desired account, click Modify, then go to the Proxy tab. The proxy settings are as follows:

This article or section is out of date.

Libera Chat recommends connecting to .onion directly. It also requires SASL to identify to NickServ during connection; see Irssi#Authenticating with SASL. Start irssi:

Set your identification to nickserv, which will be read when connecting. Supported mechanisms are ECDSA-NIST256P-CHALLENGE (see ecdsatool) and PLAIN. DH-BLOWFISH is not supported.

Disable CTCP and DCC and set a different hostname to prevent information disclosure: [3]

Connect to Libera Chat:

For more information check Accessing Libera.Chat Via Tor, Using SASL or IRC/SILC Wiki article.

Pacman download operations (repository databases, packages, and public keys) can be done using the Tor network.

Reliability with Tor:

Note on GPG: On stock Arch, pacman only trust keys which are either signed by you (that can be done with pacman-key --lsign-key) or signed by 3 of 5 Arch master keys. If a malicious exit node replaces packages with ones signed by its key, pacman will not let the user install the package.

The Tor network is reliant on people contributing bandwidth and setting up services. There are several ways to contribute to the network.

A usual Tor circuit consists of:

See the official documentation and Expectations for Relay Operators for more information.

Also known as non-exit relays: A guard relay is the first hop in a Tor circuit, while a middle relay acts as the second hop.

This means that your machine will act as an entry node or forwarding relay and, unlike a bridge, it will be listed in the public Tor directory. Your IP address will be publicly visible in the Tor directory but the relay will only forward to other relays or Tor exit nodes, not directly to the internet.

To setup a non-exit relay, see [4] and [5].

A Tor bridge is a Tor relay that is not listed in the public Tor directory, thus making it possible for people to connect to the Tor network when governments or ISPs block all public Tor relays. Visit https://bridges.torproject.org/ for more information and instructions on how to get bridge addresses.

To setup a Tor bridge, see [6] and [7].

Any requests from a Tor user to the regular internet must exit the Tor network at some point, and exit relays provide this essential service. To the destination host, these requests will appear to originate from your machine. This means that running an exit relay is typically viewed as more legally burdensome than running other types of Tor relays.

Before becoming an exit relay, it is strongly recommended to read Legal resources and tips for running an exit node.

To setup an exit relay, see [8] and [9].

This article or section is being considered for removal.

Using the torrc, you can configure which services you wish to allow through your exit relay.

Make the relay an exit relay:

Allow only IRC ports 6660-6667 but nothing else to exit from relay:

By default, Tor will block certain ports. You can use the torrc to override this, for example accepting NNTP:

This article or section is out of date.

If you run a fast exit relay (+100Mbps) with ORPort 443 and DirPort 80, the following configuration changes might serve as inspiration to setup Tor alongside iptables firewall and pdnsd as DNS cache. It is important to first read Relay Post-install and good practices.

To handle more than 32768 connections, LimitNOFILE can be raised [12]:

To successfully raise nofile limit, you may also have to append the following:

Check if the nofile (filedescriptor) limit is successfully raised with ulimit -Hn as the tor user.

To bind Tor to privileged ports the service must be started as root. Please specify User tor option in /etc/tor/torrc.

An example configuration:

See tor(1) for details.

Tor opens a socks proxy on port 9050 by default  even if you do not configure one. Set SOCKSPort 0 if you plan to run Tor only as a relay, and not make any local application connections yourself.

Log notice stdout changes logging to stdout, which is also the Tor default.

ControlPort 9051, CookieAuthentication 1 and DisableDebuggerAttachment 0 enables nyx to connect to Tor and display connections.

ORPort 443 and DirPort 80 lets Tor listen on port 443 and 80.

DirPortFrontPage displays tor-exit-notice.html on port 80.

ExitPolicy reject XXX.XXX.XXX.XXX/XX:* should reflect your public IP and netmask, which can be obtained with the command ip addr, so exit connections cannot connect to the host or neighboring machines public IP and circumvent firewalls.

AvoidDiskWrites 1 reduces disk writes and wear on SSD.

DisableAllSwap 1 "will attempt to lock all current and future memory pages, so that memory cannot be paged out".

If grep aes /proc/cpuinfo returns that your CPU supports AES instructions and lsmod | grep aes returns that the module is loaded, you can specify HardwareAccel 1 which tries "to use built-in (static) crypto hardware acceleration when available", see https://www.torservers.net/wiki/setup/server#aes-ni_crypto_acceleration.

ORPort 443, DirPort 80 and DisableAllSwap 1 require that you start the Tor service as root as described in #Start tor.service as root to bind Tor to privileged ports.

Use the User tor option to properly reduce Tors privileges.

Setup and learn to use iptables. Instead of being a Simple stateful firewall where connection tracking would have to track thousands of connections on a tor exit relay this firewall configuration is stateless.

-A PREROUTING -j NOTRACK and -A OUTPUT -j NOTRACK disables connection tracking in the raw table.

:INPUT DROP [0:0] is the default INPUT target and drops input traffic we do not specifically ACCEPT.

:FORWARD DROP [0:0] is the default FORWARD target and only relevant if the host is a normal router, not when the host is an onion router.

:OUTPUT ACCEPT [0:0] is the default OUTPUT target and allows all outgoing connections.

-A INPUT -p tcp ! --syn -j ACCEPT allow already established incoming TCP connections per the rules below and TCP connections established from the exit node.

-A INPUT -p udp -j ACCEPT allow all incoming UDP connections because we do not use connection tracking.

-A INPUT -p icmp -j ACCEPT allow ICMP.

-A INPUT -p tcp --dport 443 -j ACCEPT allow incoming connections to the ORPort.

-A INPUT -p tcp --dport 80 -j ACCEPT allow incoming connections to the DirPort.

-A INPUT -i lo -j ACCEPT allows all connections on the loopback interface.

You can use pdnsd to cache DNS queries locally, so the exit relay can resolve DNS faster and the exit relay does not forward all DNS queries to an external DNS recursor.

This configuration stub shows how to cache queries to your normal DNS recursor locally and increase pdnsd cache size to 100MB.

If your local DNS recursor is in some way censored or interferes with DNS queries, see Alternative DNS services for alternatives and add them in a separate server-section in /etc/pdnsd.conf as per Pdnsd#DNS servers.

To verify the function of your tor relay, check that tor.service started correctly with the journal or the unit status. If there are no errors, run nyx to ensure your relay is making connections. Do not be concerned if your new relay is slow at first; this is normal. After approximately 3 hours, your relay should be published and searchable on Relay Search.

DNS queries can be performed through the command-line interface by using tor-resolve. For example:

The tor 0.2.x series also provides a built-in DNS forwarder. To enable it, add the following lines to the Tor configuration file and restart the daemon:

This will allow tor to accept DNS requests (listening on port 9053) like a regular DNS server, and resolve the domain via the Tor network.

A downside of both methods is that they are only able to resolve DNS queries for A, AAAA and PTR records; MX and NS queries are never answered. For more information see this Debian-based introduction.

It is possible to configure your system to use Tor DNS for any A, AAAA, and PTR queries your system makes, regardless of whether you eventually use Tor to connect to your final destination. To do this, configure your system to use 127.0.0.1 as its DNS server and edit the DNSPort line in /etc/tor/torrc to show:

Alternatively, you can use a local caching DNS server, such as dnsmasq or pdnsd, which will also compensate for Tor DNS being a little slower than traditional DNS servers. The following instructions will show how to set up dnsmasq for this purpose. Note, if you are using NetworkManager you will need to add your configuration file to the location outlined in NetworkManager#dnsmasq.

Change the tor setting to listen for the DNS request in port 9053 and install dnsmasq.

Modify its configuration file so that it contains:

These configurations set dnsmasq to listen only for requests from the local computer, and to use Tor DNS at its sole upstream provider. It is now necessary to edit /etc/resolv.conf so that your system will query only the dnsmasq server:

Start the dnsmasq.service.

Finally, if you use dhcpcd you need to change its settings so that it does not alter the resolv.conf file. Add this line in the configuration file:

If you already have an nohook line, add resolv.conf separated by a comma.

torsocks allows you to use an application through the Tor network without requiring any configuration changes to the application itself. From torsocks(1):

For a comparison of torsocks with its predecessor, see [14].

The factual accuracy of this article or section is disputed.

In some cases it is more secure and often easier to transparently torify an entire system instead of configuring individual applications to use Tor's socks port, not to mention preventing DNS leaks. Transparent torification can be done with iptables in such a way that all outbound packets are redirected through Tor's TransPort, except the Tor traffic itself. Once in place, applications do not need to be configured to use Tor, though Tor's SOCKSPort will still work. This also works for DNS via Tor's DNSPort, but realize that Tor only supports TCP, thus UDP packets other than DNS cannot be sent through Tor and therefore must be blocked entirely to prevent leaks.

Using iptables to transparently torify a system affords comparatively strong leak protection, but it is not a substitute for virtualized torification applications such as Whonix, or TorVM [16]. Transparent torification also will not protect against fingerprinting attacks on its own, so it is recommended to use an amnesic solution like Tails instead. Applications can still learn your computer's hostname, MAC address, serial number, timezone, etc. and those with root privileges can disable the firewall entirely. In other words, transparent torification with iptables protects against accidental connections and DNS leaks by misconfigured software, it is not sufficient to protect against malware or software with serious security vulnerabilities.

When a transparent proxy is used, it is possible to start a Tor session from the client as well as from the transparent proxy, creating a "Tor over Tor" scenario. Doing so produces undefined and potentially unsafe behavior. In theory, the user could get six hops instead of three in the Tor network. However, it is not guaranteed that the three additional hops received are different; the user could end up with the same hops, possibly in reverse or mixed order. The Tor Project opinion is that this is unsafe [17] [18].

To enable transparent torification, use the following file for iptables-restore and ip6tables-restore (internally used by systemd's iptables.service and ip6tables.service).

Make sure your torrc contains the following lines:

This file also works for ip6tables-restore, so you may symlink it:

Then make sure Tor is running, and start/enable the iptables and ip6tables systemd units.

You may want to add Requires=iptables.service and Requires=ip6tables.service to whatever systemd unit logs your user in (most likely a display manager), to prevent any user processes from being started before the firewall up. See systemd.

To run tor as a non-root user and use a port lower than 1024, you can use kernel capabilities to allow it to bind to privileged ports:

If you use tor.service, it is also possible to use systemd to grant tor the appropriate permissions. This has the benefit that permissions do not need to be reapplied after every tor upgrade:

Refer to superuser.com for further explanations.

When using the Tor Browser, it is possible to use the running tor.service instead of establishing a second connection to the Tor network. Instructions are provided in the starter file for the browser, which by default is located at ~/.local/share/torbrowser/tbb/x86_64/tor-browser/Browser/start-tor-browser.

As of version 0.3.7, you can follow these steps:

This article or section is being considered for removal.

For security purposes, it may be desirable to run Tor in a chroot. The following script will create an appropriate chroot in /opt/torchroot:

After running the script as root, Tor can be launched in the chroot with the command:

or, if you use systemd, overload the service:

This article or section needs expansion.

In this example we will create a systemd-nspawn container named tor-exit with a virtual macvlan network interface.

See systemd-nspawn and systemd-networkd for full documentation.

In this example the container will reside in /srv/container:

Install the arch-install-scripts.

Install base, tor and nyx as per systemd-nspawn#Create and boot a minimal Arch Linux container:

Symlink to register the container on the host, as per systemd-nspawn#Management:

Create a drop-in configuration file for the container:

MACVLAN=interface creates a "macvlan" interface named mv-interface and assigns it to the container, see systemd-nspawn#Use a "macvlan" or "ipvlan" interface for details. This is advisable for security as it will allow you to give a private IP to the container, and it will not know what your machine's IP is. This can help obscure DNS requests.

LimitNOFILE=65536 per #Raise maximum number of open file descriptors.

Set up systemd-networkd according to your network in /srv/container/tor-exit/etc/systemd/network/mv-interface.network.

Start/enable systemd-nspawn@tor-exit.service.

Login to the container (see systemd-nspawn#machinectl):

Start and enable systemd-networkd.service. networkctl displays if systemd-networkd is correctly configured.

See #Running a Tor relay.

One can ensure a java application proxies its connections through Tor by defining its environment variable:

Tor Browser typically works without significant customization. If the bundled proxy fails with proxy server is refusing connections for any website, consider reinstallation by removing the ~/.local/share/torbrowser/ directory (make sure to back up important files).

**Examples:**

Example 1 (unknown):
```unknown
tor.service
```

Example 2 (unknown):
```unknown
[tor]$ /usr/bin/tor
```

Example 3 (unknown):
```unknown
socks5h://localhost:9050
```

Example 4 (unknown):
```unknown
$ curl -x socks5h://localhost:9050 -s https://check.torproject.org
```

---

## Samba

**URL:** https://wiki.archlinux.org/title/Samba

**Contents:**
- Server
  - Installation
    - Enabling and starting services
    - Make the server discoverable
    - Configure firewall
      - UFW Rule
      - firewalld service
  - Basic configuration
    - User management
      - Adding a user

Samba is the standard Windows interoperability suite of programs for Linux and Unix. Since 1992, Samba has provided secure, stable and fast file and print services for all clients using the SMB/CIFS protocol, such as all versions of DOS and Windows, OS/2, Linux and many others.

To share files through Samba, see #Server section; to access files shared through Samba on other machines, please see #Client section.

Install the samba package.

Samba is configured in the /etc/samba/smb.conf configuration file, which is extensively documented in smb.conf(5).

Because the samba package does not provide this file, one needs to create it before starting smb.service.

A documented example as in smb.conf.default from the Samba git repository may be used to setup /etc/samba/smb.conf.

This article or section is out of date.

To provide basic file sharing through SMB, enable/start smb.service. See smbd(8) for details.

If you want to make your server accessible via NetBIOS host name, set the desired name in the netbios name option in smb.conf and enable/start nmb.service. See nmbd(8) for details.

Install the avahi package, then enable/start avahi-daemon.service to make the samba server discoverable with Zeroconf. It should work for most non-Windows file managers (macOS Finder, various GUI-based file managers on Linux & BSD etc.)

If avahi-daemon.service is not running, the server will still be accessible, just not discoverable, i.e. it will not show up in file managers, but you can still connect to the server directly by IP or domain.

Instead of installing the avahi package, systemd-resolved provides similar Zeroconf functionalities. Ensure its support for mDNS is enabled:

And then create a file that defines a network service. See systemd.dnssd(5).

Run systemctl reload systemd-resolved as root to apply changes.

Using the systemd implementation works around a bug in Avahi that causes hostnames to be unstable (Avahi#Hostname changes with appending incrementing numbers).

Windows Explorer relies on the WS-Discovery protocol instead; see #Windows 1709 or up does not discover the samba server in Network view.

If you are using a firewall, do not forget to open required ports (usually 137-139 + 445). For a complete list, see Samba port usage.

A Ufw App Profile for SMB/CIFS is included by default with the default installation of UFW in ufw-fileserver.

Allow Samba by running ufw allow CIFS as root.

If you deleted the profile, create/edit /etc/ufw/applications.d/samba and add the following content:

Then load the profile into UFW run ufw app update Samba as root.

Then finally, allow Samba by running ufw allow Samba as root.

To configure firewalld to allow Samba in the home zone, run:

The three services listed are:

--permanent ensures the changes remain after firewalld.service is restarted.

The following section describes creating a local (tdbsam) database of Samba users. For user authentication and other purposes, Samba can also be bound to an Active Directory domain, can itself serve as an Active Directory domain controller, or can be used with an LDAP server.

Samba requires a Linux user account - you may use an existing user account or create a new one.

Although the user name is shared with Linux system, Samba uses a password separate from that of the Linux user accounts. Replace samba_user with the chosen Samba user account:

Depending on the server role, existing File permissions and attributes may need to be altered for the Samba user account.

If you want the new user only to be allowed to remotely access the file server shares through Samba, you can restrict other login options

Also see Security for hardening your system.

Samba users can be listed using the pdbedit(8) command:

To change a user password, use smbpasswd:

1. Create a Linux user which anonymous Samba users will be mapped to.

2. Add the following to /etc/samba/smb.conf:

Anonymous users will now be mapped to the Linux user guest and have the ability to access any directories defined in guest_share.path, which is configured to be /tmp/ in the example above.

Make sure that the Linux user guest has the proper permissions to access files in guest_share.path.

Also, make sure shares have been properly defined as per the Share Definitions section of smb.conf.default.

Then, restart smb.service.

Server-side copy eliminates the need to transfer data between the server and the client when copying files on the server. This is enabled by default, but it doesn't work with macOS clients. If you have macOS clients, you need to add the following configuration to smb.conf and then restart smb.service.

Usershares is a feature that gives non-root users the capability to add, modify, and delete their own share definitions. See smb.conf(5)  USERSHARES.

Set the following parameters in the smb.conf configuration file:

Add the user to the sambashare group. Replace your_username with the name of your user:

Restart smb.service and nmb.service services.

Log out and log back in.

If you want to share paths inside your home directory you must make it accessible for the group others.

The factual accuracy of this article or section is disputed.

In the GUI, you can use Thunar or Dolphin - right click on any directory and share it on the network.

In the CLI, use one of the following commands, replacing italic sharename, user, ... :

Permissions may be applied to both the server and shares:

See smb.conf(5) for a full overview of possible permission flags and settings.

Append server min protocol and server max protocol in /etc/samba/smb.conf to force usage of a minimum and maximum protocol:

See server max protocol in smb.conf(5) for an overview of supported protocols. For compatibility with older clients and/or servers, you might need to set client min protocol or server min protocol to an older protocol, but please note that this makes you vulnerable to exploits.

Clients using mount.cifs may need to specify the correct vers=*, e.g.:

See mount.cifs(8) for more information.

Native SMB transport encryption is available in SMB version 3.0 or newer. Clients supporting this type of encryption include Windows 8 and newer, Windows server 2012 and newer, and smbclient of Samba 4.1 and newer.

To use native SMB transport encryption by default, set the server smb encrypt parameter globally and/or by share. Possible values are off, enabled (default value), desired, or required:

To configure encryption for on the client side, use the option client smb encrypt.

See smb.conf(5) for more information, especially the paragraphs Effects for SMB1 and Effects for SMB2.

By default Samba shares printers configured using CUPS.

If you do not want printers to be shared, use the following settings:

Samba offers an option to block files with certain patterns, like file extensions. This option can be used to prevent dissemination of viruses or to dissuade users from wasting space with certain files. More information about this option can be found in smb.conf(5).

The default settings should be sufficient for most users. However setting the 'socket options' correct can improve performance, but getting them wrong can degrade it by just as much. Test the effect before making any large changes.

Read the smb.conf(5) man page before applying any of the options listed below.

The following settings should be appended to the [global] section of /etc/samba/smb.conf.

Setting a deadtime is useful to stop a server's resources from being exhausted by a large number of inactive connections:

The usage of sendfile may make more efficient use of the system CPU's and cause Samba to be faster:

Setting min receivefile size allows zero-copy writes directly from network socket buffers into the filesystem buffer cache (if available). It may improve performance but user testing is recommended:

Increasing the receive/send buffers size and socket optimize flags might be useful to improve throughput. It is recommended to test each flag separately as it may cause issues on some networks:

Latest versions of Samba no longer offer older authentication methods and protocols which are still used by some older clients (IP cameras, etc). These devices usually require Samba server to allow NTMLv1 authentication and NT1 version of the protocol, known as CIFS. For these devices to work with latest Samba, you need to add these two configuration parameters into [global] section:

Anonymous/guest access to a share requires just the first parameter. If the old device will access with username and password, you also need the add the second line too.

Spotlight allows supporting clients (e.g. MacOS Finder) to quickly search shared files.

Install and start/enable OpenSearch. Install fs2es-indexerAUR, configure the directories you want to index in /etc/fs2es-indexer/config.yml, and start/enable fs2es-indexer.service for periodic indexing.

Edit smb.conf as described in the Samba wiki to enable Spotlight per share, and restart smb.service to apply the changes.

Install smbclient for an ftp-like command line interface. See smbclient(1) for commonly used commands.

For a lightweight alternative (without support for listing public shares, etc.), install cifs-utils that provides /usr/bin/mount.cifs.

Depending on the desktop environment, GUI methods may be available. See #File manager configuration for use with a file manager.

The following command lists public shares on a server:

Alternatively, running $ smbtree -N will show a tree diagram of all the shares. It uses broadcast queries and is therefore not advisable on a network with a lot of computers, but can be helpful for diagnosing if you have the correct sharename. The -N (-no-pass) option suppresses the password prompt.

Samba clients handle NetBIOS host names automatically by default (the behavior is controlled by the name resolve order option in smb.conf). Other programs (including mount.cifs) typically use Name Service Switch, which does not handle NetBIOS by default.

The smbclient package provides a libnss driver to resolve NetBIOS host names. To use it, install it along with the samba package (which provides the winbindd daemon), start/enable winbind.service and add wins to the hosts line in nsswitch.conf(5):

Now, during host resolving (e.g. when using mount.cifs or just ping netbios-name), winbindd will resolve the host name by sending queries using NetBIOS Name Service (NBNS, also known as WINS) protocol.

By default it sends a broadcast query to your local network. If you have a WINS server, you can add wins server = wins-server-ip to smb.conf and restart winbind.service, then winbindd and other Samba clients will send unicast queries to the specified IP.

If you want to resolve your local host name (specified in the netbios name option in smb.conf), start/enable nmb.service, which will handle incoming queries.

You can test WINS resolution with nmblookup. By default it sends broadcast queries to your local network regardless of the wins server option.

Note that WINS resolution requires incoming traffic originating from port 137.

When not using NetBIOS/WINS host name resolution, it may be preferred to disable this protocol:

Finally disable/stop winbind.service.

Mount the share using mount.cifs as type. Not all the options listed below are needed or desirable:

The options uid and gid corresponds to the local (e.g. client) user/user group to have read/write access on the given path.

The factual accuracy of this article or section is disputed.

Storing passwords in a world readable file is not recommended. A safer method is to use a credentials file instead, e.g. inside /etc/samba/credentials:

For the mount command replace username=myuser,password=mypass with credentials=/etc/samba/credentials/share.

The credential file should explicitly readable/writeable to root:

NetworkManager can be configured to run a script on network status change. This script uses the gio command so that it mounts the Samba shares automatically, the same way your file manager does, as explained below. The script also safely unmounts the Samba shares before the relevant network connection is disabled by listening for the pre-down and vpn-pre-down events. Make the script executable after creating it.

Create a symlink inside /etc/NetworkManager/dispatcher.d/pre-down to catch the pre-down events:

This is a simple example of a cifs mount entry that requires authentication:

Create a new .mount file inside /etc/systemd/system, e.g. mnt-myshare.mount. See systemd.mount(5) for details.

Where= path to mount the share

Options= share mounting options

To use mnt-myshare.mount, start the unit and enable it to run on system boot.

To automatically mount a share (when accessed, like autofs), one may use the following automount unit:

Disable/stop the mnt-myshare.mount unit, and enable/start mnt-myshare.automount to automount the share when the mount path is being accessed.

First, check if you can see all the shares you are interested in mounting:

If that does not work, find and modify the following line in /etc/samba/smb.conf accordingly:

Now restart smb.service and nmb.service.

If everything works as expected, install smbnetfs.

Then, add the following line to /etc/fuse.conf:

Now copy the directory /etc/smbnetfs/.smb to your home directory:

Then create a link to smb.conf:

If a username and a password are required to access some of the shared folders, edit ~/.smb/smbnetfs.auth to include one or more entries like this:

It is also possible to add entries for specific hosts to be mounted by smbnetfs, if necessary. More details can be found in ~/.smb/smbnetfs.conf.

If you are using the Dolphin or GNOME Files, you may want to add the following to ~/.smb/smbnetfs.conf to avoid "Disk full" errors as smbnetfs by default will report 0 bytes of free space:

When you are done with the configuration, you need to run

Otherwise, smbnetfs complains about 'insecure config file permissions'.

Finally, to mount your Samba network neighbourhood to a directory of your choice, call

The Arch Linux package also maintains an additional system-wide operation mode for smbnetfs. To enable it, you need to make the said modifications in the directory /etc/smbnetfs/.smb.

Then, you can start and/or enable the smbnetfs daemon as usual. The system-wide mount point is at /mnt/smbnet/.

See Autofs for information on the kernel-based automounter for Linux.

In order to access samba shares through GNOME Files, Nemo, Caja, Thunar or PCManFM, install the gvfs-smb package.

Press Ctrl+l and enter smb://servername/share in the location bar to access your share.

The mounted share is likely to be present at /run/user/your_UID/gvfs or ~/.gvfs in the filesystem.

KDE applications (like Dolphin) has the ability to browse Samba shares built in. Use the path smb://servername/share to browse the files. If you want to access files from on non-KDE application, you can install kio-fuse.

To use a GUI in the KDE System Settings, you will need to install the kdenetwork-filesharing package.

There are a number of useful programs, but they may need to have packages created for them. This can be done with the Arch package build system. The good thing about these others is that they do not require a particular environment to be installed to support them, and so they bring along less baggage.

If nothing is known about other systems on the local network, and automated tools such as smbnetfs are not available, you can manually probe for Samba shares.

First, install the nmap and smbclient packages.

Use nmap to scan your local network to find systems with TCP port 445 open, which is the port used by the SMB protocol. Note that you may need to use -Pn or set a custom ping scan type (e.g. -PS445) because Windows systems are usually firewalled.

The first result is another system; the second happens to be the client from where this scan was performed.

Now you can connect to their IP addresses directly, but if you want to use NetBIOS host names, you can use nmblookup(1) to check for NetBIOS names. Note that this will not work if NetBIOS is disabled on the server.

Regardless of the output, look for <20>, which shows the host with open services.

Use smbclient(1) to list which services are shared on these systems. You can use NetBIOS host name (PUTER in this example) instead of IP when available. If prompted for a password, pressing enter should still display the list:

Samba offers a set of tools for communication with Windows. These can be handy if access to a Windows computer through remote desktop is not an option, as shown by some examples.

Send shutdown command with a comment:

A forced shutdown instead can be invoked by changing -C with comment to a single -f. For a restart, only add -r, followed by a -C or -f.

Stop and start services:

To see all possible net rpc command:

SELinux not allow samba to access user home directories by default, to solve this, run:

Similarly, samba_export_all_ro and samba_export_all_rw make Samba has the ability to read or "read and write" all files.

If using a share path located outside of a home or usershares directory, whitelist it in /etc/apparmor.d/local/usr.sbin.smbd. E.g.:

After editing, reload the AppArmor profile:

The client is using an unsupported SMB/CIFS version that is required by the server.

See #Restrict protocols for better security for more information.

The factual accuracy of this article or section is disputed.

Set map to guest inside the global section of /etc/samba/smb.conf:

If you are still using Samba < 4.10.10, use Bad User instead of Bad Password.

This error affects some machines running Windows 10 version 1709 and later. It is not related to SMB1 being disabled in this version but to the fact that Microsoft disabled insecure logons for guests on this version for some, but not others.

To fix, open Group Policy Editor (gpedit.msc). Navigate to Computer configuration\administrative templates\network\Lanman Workstation > Enable insecure guest logons and enable it. Alternatively,change the following value in the registry:

If you are a home user and using samba purely for file sharing from a server or NAS, you are probably not interested in sharing printers through it. If so, you can prevent this error from occurring by adding the following lines to your /etc/samba/smb.conf:

Restart the samba service, smb.service, and then check your logs:

and the error should now no longer be appearing.

It means that while you are sharing a folder from Dolphin (file manager) and everything seems ok at first, after restarting Dolphin the share icon is gone from the shared folder, and also some output like this in terminal (Konsole) output:

To fix it, enable usershare as described in #Enable Usershares.

And you are using a firewall (iptables) because you do not trust your local (school, university, hotel) network. This may be due to the following: When the smbclient is browsing the local network it sends out a broadcast request on udp port 137. The servers on the network then reply to your client but as the source address of this reply is different from the destination address iptables saw when sending the request for the listing out, iptables will not recognize the reply as being "ESTABLISHED" or "RELATED", and hence the packet is dropped. A possible solution is to add:

to your iptables setup.

For Uncomplicated Firewall, you need to add nf_conntrack_netbios_ns to the end of the following line in /etc/default/ufw

and then run the following commands as root:

To make this change persistent across reboots, add the following line at the end of /etc/ufw/sysctl.conf:

The client probably does not have access to shares. Make sure clients' IP address is in hosts allow = line in /etc/samba/smb.conf.

Another problem could be, that the client uses an invalid protocol version. To check this try to connect with the smbclient where you specify the maximum protocol version manually:

If the command was successful then create a configuration file:

You are probably passing a wrong server name to smbclient. To find out the server name, run hostnamectl on the server and look at "Transient hostname" line

Make sure that the server has started. The shared directories should exist and be accessible.

Probably the server is configured not to accept protocol SMB1. Add option client max protocol = SMB2 in /etc/samba/smb.conf. Or just pass argument -m SMB2 to smbclient.

Samba 4.5 has NTLMv1 authentication disabled by default. It is recommend to install the latest available upgrades on clients and deny access for unsupported clients.

If you still need support for very old clients without NTLMv2 support (e.g. Windows XP), it is possible force enable NTLMv1, although this is not recommend for security reasons:

If NTLMv2 clients are unable to authenticate when NTLMv1 has been enabled, create the following file on the client:

This change also affects samba shares mounted with mount.cifs. If after upgrade to Samba 4.5 your mount fails, add the sec=ntlmssp option to your mount command, e.g.

See the mount.cifs(8) man page: ntlmssp - Use NTLMv2 password hashing encapsulated in Raw NTLMSSP message. The default in mainline kernel versions prior to v3.8 was sec=ntlm. In v3.8, the default was changed to sec=ntlmssp.

Starting with kernel 3.18, the cifs module uses the "mapposix" option by default. When mounting a share using unix extensions and a default Samba configuration, files and directories containing one of the seven reserved Windows characters : \ * < > ? are listed but cannot be accessed.

Possible solutions are:

The latter approach (using catia or fruit) has the drawback of filtering files with unprintable characters.

This section presupposes:

For clarification purpose only, in the following sub-sections is assumed:

Run the following command from a terminal to test configuration file correctness:

Run the following commands from a terminal:

If everything is fine, you will notice a file named mysharedfiles

Read the file contents using the following command:

The terminal output should display something like this:

Run the following command from a terminal. If prompted for a password, just press Enter:

If everything is fine, MySharedFiles should be displayed under Sharename column

Run the following command in order to access the shared folder as guest (anonymous login)

If everything is fine samba client prompt will be displayed:

From samba prompt verify guest can list directory contents:

If the NTFS_STATUS_ACCESS_DENIED error is displayed, the issue is likely to be with Unix directory permissions. Ensure that your samba user has access to the folder and all parent folders. You can test this by sudoing to the user and attempting to list the mount directory, and all of its parents.

This error might be seen when mounting shares of Synology NAS servers. Use the mount option vers=1.0 to solve it.

File managers that utilizes gvfs-smb can show the error Software caused connection abort when writing a file to a share/server. This may be due to the server running SMB/CIFS version 1, which many routers use for USB drive sharing (e.g. Belkin routers). To write to these shares specify the CIFS version with the option vers=1.0. E.g.:

This can also happen after updating Samba to version 4.11, which deactivates SMB1 as default, and accessing any Samba share. You can reenable it by adding

Be sure that you do not leave any space characters before your username in Samba client configuration file as follows:

The correct format is:

With Windows 10 version 1511, support for SMBv1 and thus NetBIOS device discovery was disabled by default. Depending on the actual edition, later versions of Windows starting from version 1709 ("Fall Creators Update") do not allow the installation of the SMBv1 client anymore. This causes hosts running Samba not to be listed in the Explorer's "Network (Neighborhood)" views. While there is no connectivity problem and Samba will still run fine, users might want to have their Samba hosts to be listed by Windows automatically. wsdd implements a Web Service Discovery host daemon. This enables (Samba) hosts, like your local NAS device, to be found by Web Service Discovery Clients like Windows. The default settings should work for most installations, all you need to do is start enable wsdd.service.

If the default configuration (advertise itself as the machine hostname in group "WORKGROUP") should be all you need in most cases. If you need, you can change configuration options by passing additional arguments to wsdd by adding them in /etc/conf.d/wsdd (see the manual page for wsdd for details).

wsdd2AUR does the same thing, but is written in C instead of Python. By default, it will look for the netbios name and workgroup values in smb.conf.

See GNOME/Files#Windows machines (version 1709 or up) with shared folders don't show up in Network view.

Beginning with iOS/iPadOS 14.5 attempting to transfer from a device running iOS/iPadOS using the "Files" app to a samba share on Arch Linux will result in the error:

To correct this problem, add add the following to the global section of your smb.conf and restart smb.service. Comment optional:

See https://apple.stackexchange.com/q/424681 Apple.Stackexchange.com - "The operation couldn't be completed"/"Operation canceled" error message when saving to a Samba share via Files app.

Some SMB clients, such as Solid Explorer for Android, take significantly longer to connect to Samba if they fail to resolve the NetBIOS name. Enabling nmb.service will greatly speed up initial connections if this is the case. Since this is a bug in the client software, please report such cases to the authors of conflicting software.

When Samba is configured to use CUPS for printing

And the following symptoms occur:

A workaround is to launch /usr/libexec/samba/samba-bgqd manually (without parameters). Consider creating a systemd service to keep the binary running until the bug is fixed

Reference: Redhat Bug [1]

If you get this message with cifscreds add HOST, try

**Examples:**

Example 1 (unknown):
```unknown
/etc/samba/smb.conf
```

Example 2 (unknown):
```unknown
smb.service
```

Example 3 (unknown):
```unknown
smb.conf.default
```

Example 4 (unknown):
```unknown
/etc/samba/smb.conf
```

---

## Touchscreen

**URL:** https://wiki.archlinux.org/title/Touchscreen

**Contents:**
- Introduction
- Available X11 drivers
- Two-fingers scrolling
- evdev drivers
  - Calibration
- Using a touchscreen in a multi-head setup
  - Using xrandr-watch-git to automate map-to-output
  - Wayland/Weston
- Touchegg

If you ever tried to set up a touchscreen device in Linux, you might have noticed that it is either working out of the box (besides some calibration), or is very tedious, especially when it is not supported by the kernel.

This article assumes that your touchscreen device is supported by the kernel (e.g. by the usbtouchscreen module). That means there exists a /dev/input/event* node for your device. Check out

to see if your device is listed or try

for every of your event nodes while touching the display. If you found the corresponding node, it is likely that you will be able to get the device working.

There are a lot of touchscreen input drivers for X11 out there. The most common ones are in the extra repository:

Less common drivers, not contained in the repository, are:

Proprietary drivers for some devices (e.g. xf86-input-egalax), were available at one point but have become unmaintained: use the open source drivers.

Depending on your touchscreen device choose an appropriate driver. Again, evdev is likely to be the default if your touchscreen "just works."

The two-fingers scrolling has to be implemented on the application side (see this link). For Firefox, see Firefox/Tweaks#Enable touchscreen gestures.

There is a hack to emulates this scrolling behavior for every application in #Touchegg, but the X server still handles it as text selection (at least with Plasma).

Install xinput_calibratorAUR (AUR). Then, run xinput_calibrator and follow the instructions.

To use multiple displays (some of which are touchscreens), you need to tell Xorg the mapping between the touch surface and the screen. This can be achieved with xinput as follows.

Take for example the setup of having a wacom tablet and an external monitor; xrandr shows both displays:

You see we have two displays here. LVDS1 and VGA1. LVDS1 is the display internal to the tablet, and VGA1 is the external monitor. We wish to map our stylus input to LVDS1. So we have to find the ID of the stylus input:

We see that we have two stylus inputs. We now need to simply map our inputs to our output like so:

You can automate this by putting these commands in your ~/.xinitrc or similar. The mapping will be lost if the touchscreen is disconnected and re-connected, for example, when switching monitors via a KVM. In that case it is better to use a udev rule. The Calibrating Touchscreen page has an example udev rule for the case when a transformation matrix has been calculated manually and needs to be applied automatically.

There are xrandr events we can capture from a script. Install xrandr-watch-gitAUR, create a script ~/.xrandr-changed with execution permission to perform map-to-output, for example:

and start, test and enable the systemd/User service xrandr-watcher.service.

Wayland does not currently have a known method to lock touching to a specific display in any environment other than sway (or wlroots-based supported compositors). There are tools such as weston-touch-calibrator, but Gnome Wayland uses Xwayland leaving the calibrator unable to locate any touchscreen.

Wayland/Xwayland also masks the xinput list and funnels them down to generic xwayland devices such as "xwayland-pointer","xwayland-relative-pointer","xwayland-touch-pointer", etc. The Wayland method of "Xinput" is "Libinput", but does not have all the same functionality. The current known method to use touchscreens in a multi-head setup is to force Gnome or KDE to use X11. libinput currently assumes the touchscreen(s) covers all available monitors.

See Sway#Touch display mapping for settings in sway.

Touchegg is a multitouch gesture program, only compatible with X, that runs as a user in the background, recognizes gestures, and translates them to more conventional events such as mouse wheel movements, so that you can for example use two fingers to scroll. But it also interferes with applications or window managers which already do their own gesture recognition. If you have both a touchpad and a touchscreen, and if the touchpad driver (such as synaptics or libinput) has been configured not to recognize gestures itself, but to pass through the multi-touch events, then Touchegg will recognize gestures on both: this cannot be configured. In fact it does a better job of recognizing gestures than either the synaptics or libinput touchpad drivers; but on the touchscreen, it is generally better for applications to respond to touch in their own unique ways. Some Qt and GTK applications do that, but they will not be able to if you have Touchegg "eating" the touch events. So, Touchegg is useful when you are running mainly legacy applications which do not make their own use of touch events.

The two-fingers scrolling has been disabled in the recent rewrite of touchegg 2.0. To enable it, install xdotool and see this closed issue.

**Examples:**

Example 1 (unknown):
```unknown
/dev/input/event*
```

Example 2 (unknown):
```unknown
$ less /proc/bus/input/devices
```

Example 3 (unknown):
```unknown
# cat /dev/input/eventn
```

Example 4 (unknown):
```unknown
Screen 0: minimum 320 x 200, current 2944 x 1080, maximum 8192 x 8192
LVDS1 connected 1024x768+0+0 (normal left inverted right x axis y axis) 0mm x 0mm
   1024x768       60.0*+
   800x600        60.3     56.2  
   640x480        59.9  
VGA1 connected 1920x1080+1024+0 (normal left inverted right x axis y axis) 477mm x 268mm
   1920x1080      60.0*+
   1600x1200      60.0  
   1680x1050      60.0  
   1680x945       60.0
```

---

## Dolphin

**URL:** https://wiki.archlinux.org/title/Dolphin

**Contents:**
- Installation
  - Extensions
  - File previews
- Configuration
  - Single click to open folders/files
  - Change the default terminal emulator
  - KIO slaves
- Tips and Tricks
  - File tagging
  - Hiding custom files/directories

Dolphin is the default file manager of KDE. For the video game console emulator, see Dolphin emulator.

Install the dolphin package.

To configure Single-Click to open items, you need to install and run qt6ct from the terminal, which will then give you the option to enable Single-Click (and change the theme etc.) from the 'Interface' tab.

If you use Kvantum theme engine - use kvantum manager > Configure Active Theme > Miscellaneous > Click behavior.

Dolphin and other KDE applications use konsole by default. To change the default terminal emulator, run kcmshell6 componentchooser and select your Terminal Emulator or write the launch command into the selection field in the Other... popup. (The second option will create a new local desktop entry for this command.)

For example, to launch tmux in alacritty inside Dolphin, type alacritty -e tmux after selecting Other....

The setting can also be directly changed by modifying the configuration file ~/.config/kdeglobals. For example, to use alacritty add in the [General] section:

Where TerminalApplication takes a command and TerminalService a desktop entry (TerminalService seems to be optional). Note, that this does not influence the terminal within the dolphin window (opened with F4).

The factual accuracy of this article or section is disputed.

To change the internal Dolphin terminal window go to Keyboard > Shortcuts and choose your preferred terminal and set the Launch shortcut to Ctrl+Alt+T, overriding Konsole's shortcut. Note that terminals opened this way might not follow the background color as specified in the terminal's configuration file, but otherwise should be identical to an instance launched in a window.

Dolphin uses KIO slaves for network access, trash and other functionality, unlike GTK file managers which use GVFS. Available protocols are shown in the location bar (editable mode) [1]. To quickly bookmark them, right-click in the workspace, and select "Add to Places".

You can install KIO slaves manually. For example, to browse your Google Drive in Dolphin, install kio-gdrive.

Dolphin provides extensive support for file tagging. Add tags to a file by right-clicking the file and selecting Assign Tags. Tags on a file can be viewed in the Properties menu or in the Information panel. To show tags in a column in the Details View Mode, right-click on any header and choose Tags from the menu.

Dolphin uses the user.xdg.tags extended attribute to store tags directly along with each file. Baloo indexes these tags into its own database to allow for faster searching and maintains a list of all known tags. Activate Baloo to show a list of all indexed tags within the Places panel and make searching for files by their tags possible.

Files/directories can be hidden by creating a .hidden file (in the same directory) that contains the names of the files/directories that should be hidden (one per line).

Custom service menu entries can be added to dolphin with special *.desktop files in one of the following Paths (see [2]):

This adds a Run with Gamemode menu item on all application mime types.

Create a filesystem label, or a partition label, and Dolphin will show this label in the device list instead of the size. See Persistent block device naming#by-label.

For moving files into the Trash, it is required that the user has exclusive access rights to the Trash. The rationale being that you do not want that others can see what you deleted. For that, a folder such as .Trash-1000/ is created on the external drive, with permission mode 700.

If the correct access permissions cannot be set, dolphin will (unlike GNOME) move the files to the trash in the home directory, which takes time.

To mount USB-Sticks / external HDDs, Dolphin uses Udisks. FAT32 / EXFAT / NTFS do not support UNIX file permission, udisk mounts them by default with mode 755. To configure udisks to mount these drives with mode 700, have a look at the file /etc/udisks2/mount_options.conf.example. Copy the file (the name should end with .conf), uncomment the relevant part and add for the three filesystems to the lines with xyz_defaults the options fmask=177,dmask=077.

(Background information: [3], [4])

Fonts in selection frames may become transparent when using the GTK Qt style. Native Qt styles such as Cleanlooks and Oxygen are unaffected.

See Samba#Unable to overwrite files.

If icons are not displayed in Dolphin, install and run qt6ct, choose one icon theme at the Icon Theme tab, and Apply.

If icons are still not displayed in Dolphin, set the QT_QPA_PLATFORMTHEME variable to qt6ct in your xprofile. Alternatively, start Dolphin with the platform theme flag:

Also make sure to install and inherit a fallback icon theme like hicolor or Adwaita when you are using an uncommon, incomplete icon theme.

If icons are too big on Dolphin outside of a KDE environment, start it with:

When running Dolphin under something other than Plasma, it is possible the background color in the folder view pane will not match the system Qt theme. This is because Dolphin reads the folder view's background color from the [Colors:View] section in ~/.config/kdeglobals. Change the following line to the RGB value you prefer (it may be given in the form #RRGGBB or R,G,B):

If you get a blue border around the folder view pane (if you are in split view it will only be around the focused pane), you may get rid of it by applying the fusion-fixes.qss style sheet via the qt6ct app. This answer describes what to do to get the adwaita dark theme working for dolphin under Gnome.

Alternatively, use kvantum to manage your Qt6 theming. For instructions on usage see the Kvantum project homepage.

The factual accuracy of this article or section is disputed.

If your zsh profile is not loading, try editing your current profile. Right-click on the integrated terminal then Edit Current Profile... and edit the launch command to /bin/zsh --login

See Uniform look for Qt and GTK applications#Consistent file dialog under KDE Plasma.

When you go to Dolphin menu Settings > Configure Dolphin > Context Menu > Download New Services and try to install any service, you get this error message:

This can be solved by installing packagekit-qt6. Then restart Dolphin.

This can be resolved by installing the archlinux-xdg-menu package and running:

This updates the KService desktop-file system configuration cache (see kbuildsycoca6(8)), which many KDE-applications rely on for selecting desktop entries. The --noincremental argument is optional. XDG_MENU_PREFIX is needed, because archlinux-xdg-menu creates a XDG Desktop Menu with an arch- prefix (see xdg-menu).

The XDG Desktop Menu files can be found in /etc/xdg/menus/*-applications.menu.

Normally, kbuildsycoca6 does not need to be manually installed as it is part of the kservice package, which is a dependency of dolphin.

**Examples:**

Example 1 (unknown):
```unknown
kcmshell6 componentchooser
```

Example 2 (unknown):
```unknown
alacritty -e tmux
```

Example 3 (unknown):
```unknown
~/.config/kdeglobals
```

Example 4 (unknown):
```unknown
TerminalApplication=alacritty
TerminalService=Alacritty.desktop
```

---

## Enlightenment

**URL:** https://wiki.archlinux.org/title/Enlightenment

**Contents:**
- Enlightenment
  - Installation
  - Starting Enlightenment
    - Entrance
    - Manually
  - Configuration
    - Network
    - Polkit agent
    - GNOME Keyring integration
    - System tray

This comprises both the Enlightenment window manager and Enlightenment Foundation Libraries (EFL), which provide additional desktop environment features such as a toolkit, object canvas, and abstracted objects. It has been under development since 2005, but in February 2011 the core EFLs saw their first stable 1.0 release.

Install the enlightenment package.

You might also want to install some EFL-based applications that integrates well with Enlightenment:

The following are EFL-based applications, most in an early stage of development and not yet released:

Simply choose Enlightenment session from your favourite display manager or configure xinitrc to start it from the console.

Enlightenment has a new display manager called Entrance, which is provided by the entrance-gitAUR package. Entrance is quite sophisticated and its configuration is controlled by /etc/entrance/entrance.conf. It can be used by enabling entrance.service.

If you prefer to start Enlightenment manually, enter startx /usr/bin/enlightenment_start in the console. See xinitrc for details.

To try the Wayland compositor, enter enlightenment_start instead.

Enlightenment has a sophisticated configuration system that can be accessed from the Main menu's Settings submenu.

Enlightenment's preferred network manager is ConnMan which can be installed from the connman package. Follow the instructions on ConnMan to do the configuration.

For extended configuration, you may also install Econnman (available in AUR as econnmanAUR or econnman-gitAUR) and its associated dependencies. This is not required for general functionality though.

Adding the ConnMan Gadget to the Shelf

You can also use networkmanager to manage your network connections - see NetworkManager for more information.

Note however that the applet will need Appindicator support to show in Enlightenment's system tray. See NetworkManager#Appindicator. As an alternative to using the applet, NetworkManager includes both CLI and TUI interfaces for network configuration - see NetworkManager#Usage.

Enlightenment version DR 0.24.0 ships with a built-in polkit agent, and no extra polkit package is required to authenticate for privileged actions. Earlier versions of Enlightenment do not ship with a graphical polkit authentication agent. If you want to access privileged actions (e.g. mount a filesystem on a system device), you have to install one and autostart it. For that you should go to Settings Panel > Apps > Startup Applications > System and activate it.

It is possible to use gnome-keyring in Enlightenment. However, at the time of writing, you need a small hack to make it work in full. First, you must tell Enlightenment to autostart gnome-keyring. For that you should go to Settings Panel > Apps > Startup Applications > System and activate Certificate and Key Storage, GPG Password Agent, SSH Key Agent and "Secret Storage Service". After this, you should set the following:

This "hack" is used to override the automatic setting of the variable by "enlightenment-start" from "ssh-agent" to gnome-keyring.

More information on this topic in the GNOME Keyring article.

Enlightenment has support for a system tray but it is disabled by default. To enable the system tray, open the Enlightenment main menu, navigate to the Settings submenu and click on the Modules option. Scroll down until you see the Systray option. Highlight that option and click the Load button. Now that the module has been loaded, it can be added to the shelf. Right click on the shelf you wish to add the Systray to, hightlight the Shelf submenu and click on the Contents option. Scroll down until you see Systray. Highlight that option and click the Add button.

Enlightenment provides a notification server through its Notification extension.

More themes to customize the look of Enlightenment are available from:

You can install the themes (coming in .edj format) using the theme configuration dialog or by moving them to ~/.e/e/themes.

To alter the GTK theme, go to Settings > All > Look > Application Theme.

Many Modules provide Gadgets that can be added to your desktop or on a shelf. Some Modules (such as CPUFreq) only provide a single Gadget while others (such as Composite) provide additional features without any gadgets. Note that certain gadgets such as Systray can only be added to a shelf while others such as Moon can only be loaded on the desktop.

Beyond the modules described here, more "extra" modules are available from e-modules-extra-gitAUR.

The Scale Windows module, which requires compositing to be enabled, adds several features. The scale windows effect shrinks all open windows and brings them all into view. This is known in "Mission Control" in macOS. The scale pager effect zooms out and shows all desktops as a wall, like the compiz expo plugin. Both can be added to the desktop as a gadget or bound to a key binding, mouse binding or screen edge binding.

Some people like to change the standard window selection key binding ALT + Tab to use Scale Windows to select windows. To change this setting, you navigate to Menu > Settings > Settings Panel > Input > Keys. From here, you can set any key binding you would like.

To replace the window selection key binding functionality with Scale Windows, scroll through the left panel until you find the ALT section and then find and select ALT + Tab. Then, scroll through the right panel looking for the "Scale Windows" section and choose either Select Next or Select Next (All) depending on whether you would like to see windows from only the current desktop or from all desktops and click Apply to save the binding.

Available from upstream git.

If you find some unexpected behavior, there are a few things you can do:

If you are sure you found a bug open an issue for the relevant component at https://git.enlightenment.org/.

When the configuration needs to be reset and the settings windows can no longer be approached, configuration for the compositor can be reset using the hardcoded keybinding Ctrl + Alt + Shift + Home.

If fonts are too small and your screen is unreadable, be sure the right font packages are installed. ttf-dejavu and ttf-bitstream-vera are valid candidates.

You also should consider just increasing the scaling size under the Scaling. You can set scaling under Settings > Settings Panel > Look > Scaling.

You may find that Enlightenment routinely dims the backlight to 0% on logout and will only restore it to 100% when you log into another Enlightenment session. Enlightenment assumes that whatever comes after it will set the backlight to whatever it prefers, if anything as this is what Enlightenment does at start. This is especially problematic when using another desktop environment alongside Enlightenment that cannot control backlight as the backlight will not automatically be restored to its normal level when using that desktop environment. To fix this issue, open the Enlightenment Settings Panel and, under the Look tab, click on the Composite option. Tick the Don't fade backlight box and click OK.

You may find that the cursor theme for the desktop is different to the one used in applications such as Firefox. This is because desktop applications are using X cursor themes whilst Enlightenment has its own set of cursor themes. For consistency, you can set Enlightenment to always use the X cursor theme. To do this, open the Enlightenment Settings Panel and click on the Input tab. Click on the Mouse option. Change the theme from Enlightenment to X and click OK. You should now find that the same cursor theme is used everywhere. If the X cursor theme itself is not always consistent, see Cursor themes#The default cursor theme.

You can just select wallpapers in the wallpaper settings dialog and import any image with the provided settings dialog, or you can put desired wallpapers into ~/.e/e/backgrounds/

LMB anywhere on the desktop will give access to the settings, select /Desktop/Backgrounds/

Any new image copied in the ~/.e/e/backgrounds/ folder will get the list of available backgrounds auto-updated. You can drop animated gifs and even mp4 and other video files in here and use them as wallpapers if you want. Select desired wallpaper from drop-down menu. Inside the appropriate tabs in the global settings, you can adjust things like tiling of the background image, filling screen and such.

Enlightenment, Development Release 16 was first released in 2000, and reached version 1.0 in 2009. Originally, the DR16 stood for the 0.16 version of the Enlightenment project. You will find it as "Enlightenment16" now in the Arch repositories, it is still under development today, regularly updated by its maintainer Kim 'kwo' Woelders. With compositing, shadows and transparencies, E16 kept all of the speed that presided over its foundation by original author Carsten "Rasterman" Haitzler but with up to date refinement.

Install enlightenment16AUR.

See /usr/share/doc/e16/e16.html for in depth documentation.

Most configuration files for E16 reside in ~/.e16 and are text-based, editable at will. That includes the Menus too.

Shortcut keys can be either modified by hand, or with the e16keyedit software provided as source on the sourceforge page of the e16 project. Note that the keyboard shortcuts file is not created in ~/.e16 by default. You can copy the packaged version to your home directory if you wish to make changes:

Create an Init, a Start and a Stop folder in your ~/.e16 folder: any .sh script found there will either be executed at Startup (from Init folder), at each Restart (from Start folder), or at Shutdown (from Stop folder); provided you allowed it through the MMB / settings / session / <enable scripts> button and made them executable. Typical examples involves starting PulseAudio or your favorite network manager applet.

Shadows, Transparent effects et al can be found in MMB or RMB /Settings, under Composite .

**Examples:**

Example 1 (unknown):
```unknown
/etc/entrance/entrance.conf
```

Example 2 (unknown):
```unknown
entrance.service
```

Example 3 (unknown):
```unknown
startx /usr/bin/enlightenment_start
```

Example 4 (unknown):
```unknown
enlightenment_start
```

---

## Mouse acceleration

**URL:** https://wiki.archlinux.org/title/Mouse_acceleration

**Contents:**
- Disabling mouse acceleration
  - Disable mouse acceleration in GDM
- Configuring mouse acceleration
- Mouse acceleration with libinput
  - Changing the acceleration
  - Persistent configuration
- Setting mouse acceleration
  - In Xorg configuration
  - Using xinput
  - Configuration example

Using Xorg, libinput sets itself as the default driver with /usr/share/X11/xorg.conf.d/40-libinput.conf. To activate the flat profile for our pointer device, we need to set the flat profile to 1 and the adaptive and custom profile options to 0.

Find out your device id:

The first number indicates the default acceleration profile, the second number indicates the flat profile (No Acceleration), and the third number indicates a custom profile. To activate the flat profile:

Make it persistent by adding the option to the pointer section:

Disable mouse acceleration in GDM with Wayland:

The factual accuracy of this article or section is disputed.

Setting the mouse acceleration depends on the windowing protocol you are using: either Xorg or Wayland.

When using the adaptive pointer acceleration profile, libinput calculates the mouse acceleration depending on the DPI and the parameter Acceleration Speed [2]. libinput relies on the resolution reported by evdev [3]. Feedback settings set with xset m are effectively ignored. When using the flat pointer acceleration profile, the acceleration factor is constant regardless of the velocity of the pointer. This provides 1:1 movement between the device and the pointer on-screen.

Find the id of your device with xinput list and set the acceleration speed with the following command. Note that the acceleration speed has to be in the range of [-1,1]. Check this plot to see the impact of different acceleration speed values.

Confirm your changes with the following:

libinput does not store configuration options, it is up to the caller to manage these. Under Wayland configuration is restored by the desktop environment. Under X xf86-input-libinput reads the xorg configuration files and applies the options [4]. To make changes persistent under X create a file like this:

For further options see libinput(4).

This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.

See xorg.conf(5) for details.

You can also assign settings to specific hardware by using "MatchProduct", "MatchVendor" and other matches inside class sections. Run lsusb to find out the product name and vendor to match:

If you are unable to identify your device, try running xinput list. Some devices the use Logitech Unifying Recceiver share the same USB connection therefore, the mouse do not appear using lsusb

First, get a list of devices plugged in (ignore any virtual pointers):

Take note of the ID. You may also use the full name in commands if the ID is prone to changing.

Get a list of available properties and their current values available for modification with

where 9 is the ID of the device you wish to use. Or

where mouse name is the name of your mouse given by xinput list.

Example, changing the property of Constant Deceleration to 2:

To make it permanent, edit Xorg configuration (see above) or add commands to xprofile. The latter will not affect speed in a display manager.

You may need to resort to using more than one method to achieve your desired mouse settings. Here is what I did to configure a generic optical mouse: First, slow down the default movement speed 3 times so that it is more precise.

Then, enable acceleration and make it 3 times faster after moving past 6 units.

If you are satisfied with the results, store the preceding commands in ~/.xinitrc.

**Examples:**

Example 1 (unknown):
```unknown
/usr/share/X11/xorg.conf.d/40-libinput.conf
```

Example 2 (unknown):
```unknown
# xinput set-prop "deviceid" "libinput Accel Profile Enabled" 0 1 0
```

Example 3 (unknown):
```unknown
# xinput list-props "deviceid"
```

Example 4 (unknown):
```unknown
/etc/X11/xorg.conf.d/40-libinput.conf
```

---

## Rust

**URL:** https://wiki.archlinux.org/title/Rust

**Contents:**
- Core language
  - Rust Core Library
  - Rust Standard Library
  - Release cycle
- Installation
  - Native installation
  - Rustup
    - Arch Linux package
    - Building rust against a new version of llvm
    - Upstream installation script

The Rust Core Library is the dependency-free foundation of the Rust Standard Library. It interfaces directly with LLVM primitives, which allows Rust to be platform and hardware-agnostic. It is this integration with LLVM that allows Rust to obtain greater performance than equivalent C applications compiled with Clang, making Rust software designed with libcore lower level than C. It contains only basic platform-independent types such as Option, Result, and Iterator. Developers looking to target software for embedded platforms may forego the standard library with #![no_std] to exclusively use the no-batteries-included core library for smaller binary sizes and improved performance. However, using #![no_std] limits the amount of software support that you can get from the larger Rust community as a majority of libraries require the standard library.

The Rust Standard Library provides the convenient high level abstractions by which a majority of portable Rust software is created with. It includes the Vec and String types; a vast amount of methods for language primitives; a large number of standard macros; I/O and multithreading support; heap allocations with Box; and many more high level features not available in the core library.

Rust follows a regular six-week release cycle, similar to the release cycle of Firefox. With each new release, the core and standard libraries are improved to support more platforms, improve performance, and stabilize new features for use with stable Rust.

The two main ways to install Rust are:

To install the latest stable version of Rust from the official Arch Linux software repository, install the rust package. This will install the rustc compiler and Cargo.

There is also a development version of the Rust compiler available: rust-nightly-binAUR for prebuilt generic binaries or rust-gitAUR to build the compiler with system libraries.

The official and recommended method of installing Rust for the purpose of developing software is to use the Rustup toolchain manager, written in Rust.

The benefit of using the Rustup toolchain manager instead of the standalone prepackaged Rust in the software repository is the ability to install multiple toolchains (stable, beta, nightly) for multiple targets (windows, mac, android) and architectures (x86, x86_64, arm). It should be noted that installing rustup does not automatically install a rust toolchain with it, nor does updating rustup through any method automatically provide the latest toolchain release of rust. See #Usage or rustup toolchain documentation for more on toolchains.

There are two choices for a Rustup installation, one is supported by Arch Linux via pacman, while the other is officially supported by Rust via their installation script.

rustup is available on the Arch Linux software repository. Note that rustup self update will not work when installed this way, the package needs to be updated by pacman. However, this change does not extend to other rustup functionality, such as rustup update for updating rust toolchains.

This package has the advantage that the various Rust executables live in /usr/bin, instead of ~/.cargo/bin, removing the need to add another directory to your PATH.

In order to install the toolchain, you need to tell rustup which version to use, between stable and nightly:

Since rust builds using a bootstrap strategy, a functional rust package is required. In the case of building a version of llvm which is newer than the version in the official repos, users will encounter the need to have a shared object from a previous version of llvm-libs (one which was used to build the repo version of rust) in order to build against the newer version of llvm.

Example: the official repos offer llvm-18.1.8 and the goal is to build llvm-19.1.6.

The bootstrap step requires /usr/lib/libLLVM.so.18.1 from llvm-libs-18.1.8. This file can be manually placed in the build root or placed by a package such as llvm15-libs.

Rustup is also available to download and install manually via rustup's official web page.

Download the file with curl --proto '=https' --tlsv1.3 -sSf https://sh.rustup.rs -o rust.sh, view it: less ./rust.sh, and run the script ./rust.sh to start rustup installation. The script makes PATH changes only to login shell configuration files. You need to source ~/.cargo/env until you logout and login back into the system. To update rustup afterwards, run rustup self update.

The script installs and activates the default toolchain by default (the one used by the rust package), so there is no need to manually install it to start using Rust.

You might need to manually install a toolchain, e.g. stable, beta, nightly or 1.58.0. You also need to do this if you want to use/test another toolchain.

You can now run the Rust commands by running, rustup run toolchain command. However, to use these commands directly, you need to activate the toolchain:

Check the installed Rust version using rustc -V:

Rust does not do its own linking, and so youll need to have a linker installed. You can use gcc, otherwise Rust will generate the following error: linker `cc` not found.

Rustup does not automatically update provided toolchains. If users are wishing to use the latest releases of Rust, crates, and other relevant packages, they may occasionally wish to update their toolchains with rustup update. See Official rustup documentation for more.

Check that Rust is installed correctly by building a simple program, as follows:

You can compile it with rustc, then run it:

You can easily cross-compile using Rustup. Rustup supports many cross-compile targets. A full list can be found running rustup target list.

For instance, if you want to install Rust using the stable channel for Windows, using the GNU Compiler, you will need to do:

This will only install Rust and its tools for your target architecture, but some additional tools might be needed for cross-compiling.

In this section, $ARCH is the target architecture (either x86_64 or i686). It will explain how to cross compile using rustup.

Finally, you can cross compile for windows by passing the --target $ARCH-pc-windows-gnu to cargo:

Currently building executables using MinGW 6 and the toolchains installed by rustup is broken. To fix it, execute

where CHANNEL is the update channel (stable, beta or nightly)

The unofficial repository archlinuxcn has rust-nightly and Rust std library for i686, ARM, ARMv7, Windows 32 and 64 so you can just install the one you want then enjoy cross-compiling. However, you have to find an ARM toolchain by yourself. For Windows 32bit targets, you will need to get a libgcc_s_dw2-1.dll (provided by mingw-w64-gcc) to build and run.

Cargo, Rust's package manager, is part of the rust package. The nightly version is available in the AUR as part of rust-nightly-binAUR. If you use rustup, it already includes cargo.

Cargo is a tool that allows Rust projects to declare their various dependencies, and ensure that you will always get a repeatable build. You are encouraged to read the official guide.

To create a new project using Cargo:

This creates a directory with a default Cargo.toml file, set to build an executable.

In order to instruct Cargo to always compile optimal code for your CPU platform, you can achieve this by adding a flag to ~/.cargo/config.toml. Please be aware that the resulting binaries can not be distributed for use on other computers, and might even fail on your own system if you decide to change your CPU in the future.

Find out which target platform is used by default on your installation:

In this example, we are using stable Rust on the x86_64-unknown-linux-gnu platform.

Instruct Cargo to always compile code optimized for the native CPU platform:

Compilation times can be greatly reduced by using sccache (sccache package). This will maintain a local cache of compiler artifacts, eliminating the need to recompile code that has not changed since the last time it was compiled.

To enable sccache, you can use RUSTC_WRAPPER environment variable:

Alternatively, add the following configuration to ~/.cargo/config.toml:

See https://www.rust-lang.org/tools for the recommended tools of the Rust project.

rust-analyzer is the official Language Server Protocol implementation for Rust which has replaced RLS.

It is available as the rust-analyzer package, and the latest Git version is available as rust-analyzer-gitAUR. Alternatively, if you have rustup installed, you can install rust-analyzer with:

rust-analyzer needs the source code of the standard library. If it is not present, rust-analyzer will attempt to install it automatically using rustup. To install the source code manually using rustup, run the following command:

Clippy takes advantage of compiler plugin support to provide a large number of additional lints for detecting and warning about a larger variety of errors and non-idiomatic Rust.

Clippy is included in the rust package. To install it with rustup use:

Rustfmt is a tool to format Rust code according to the official style guidelines.

Rustfmt is included in the rust package. To install it with rustup use:

Emacs support for Rust can be obtained via the official rust-mode plugin.

GNOME Builder support for Rust is achieved using Language Server Protocol. It uses rust-analyzer by default; all you need to do is install it along with the Rust source.

Helix editor is written in Rust and has the Rust language server protocol included. Helix is inspired by Neovim and Kakoune.

Kate support for Rust is achieved using Language Server Protocol. It uses rust-analyzer by default; all you need to do is install it along with the Rust source.

IntelliJ IDEA has a Rust plugin. The same plugin also works with CLion.

If using rustup, use rustup to download the source (rustup component add rust-src), and then select ~/.rustup/toolchains/<your toolchain>/bin as the toolchain location.

If using Rust from the official Arch Linux software repository, select /usr/bin as the toolchain location and /usr/lib/rustlib/src/rust/library/ as the stdlib sources location.

Jetbrains is also working on a special Editor just for Rust. It can be found and downloaded under their official Website or in the AUR under rustroverAUR and rustrover-eapAUR.

Visual Studio Code support for Rust can be obtained via rust-analyzer with the rust-lang.rust-analyzer extension.

Vim support for Rust can be obtained via the official rust.vim plugin, which provides file detection, syntax highlighting, formatting and support for the Syntastic syntax checking plugin. Many completion engines have Rust support, like coc (via the coc-rust-analyzer plugin) and YouCompleteMe.

**Examples:**

Example 1 (unknown):
```unknown
rustup self update
```

Example 2 (unknown):
```unknown
rustup update
```

Example 3 (unknown):
```unknown
~/.cargo/bin
```

Example 4 (unknown):
```unknown
/usr/bin/rustup
```

---

## File permissions and attributes

**URL:** https://wiki.archlinux.org/title/Chown

**Contents:**
- Viewing permissions
  - Examples
- Changing permissions
  - Text method
    - Text method shortcuts
    - Copying permissions
  - Numeric method
  - Bulk chmod
- Changing ownership
- Access Control Lists

File systems use permissions and attributes to regulate the level of interaction that system processes can have with files and directories.

Use the ls command's -l option to view the permissions (or file mode) set for the contents of a directory, for example:

The first column is what we must focus on. Taking an example value of drwxrwxrwx+, the meaning of each character is explained in the following tables:

Each of the three permission triads (rwx in the example above) can be made up of the following characters:

See info Coreutils -n "Mode Structure" and chmod(1) for more details.

Let us see some examples to clarify:

Archie has full access to the Documents directory. They can list, create files and rename, delete any file in Documents, regardless of file permissions. Their ability to access a file depends on the file's permissions.

Archie has full access except they can not create, rename, delete any file. They can list the files and (if the file's permissions allow it) may access an existing file in Documents.

Archie can not do ls in the Documents directory but if they know the name of an existing file then they may list, rename, delete or (if the file's permissions allow it) access it. Also, they are able to create new files.

Archie is only capable of (if the file's permissions allow it) accessing those files the Documents directory which they know of. They can not list already existing files or create, rename, delete any of them.

You should keep in mind that we elaborate on directory permissions and it has nothing to do with the individual file permissions. When you create a new file it is the directory that changes. That is why you need write permission to the directory.

Let us look at another example, this time of a file, not a directory:

Here we can see the first letter is not d but -. So we know it is a file, not a directory. Next the owner's permissions are rw- so the owner has the ability to read and write but not execute. This may seem odd that the owner does not have all three permissions, but the x permission is not needed as it is a text/data file, to be read by a text editor such as Gedit, EMACS, or software like R, and not an executable in its own right (if it contained something like python programming code then it very well could be). The group's permissions are set to r--, so the group has the ability to read the file but not write/edit it in any way  it is essentially like setting something to read-only. We can see that the same permissions apply to everyone else as well.

chmod is a command in Linux and other Unix-like operating systems that allows to change the permissions (or access mode) of a file or directory.

To change the permissions  or access mode  of a file, use the chmod command in a terminal. Below is the command's general structure:

Where who is any from a range of letters, each signifying who is being given the permission. They are as follows:

The permissions are the same as discussed in #Viewing permissions (r, w and x).

Now have a look at some examples using this command. Suppose you became very protective of the Documents directory and wanted to deny everybody but yourself, permissions to read, write, and execute (or in this case search/look) in it:

Before: drwxr-xr-x 6 archie web 4096 Jul 5 17:37 Documents

After: drwx------ 6 archie web 4096 Jul 6 17:32 Documents

Here, because you want to deny permissions, you do not put any letters after the = where permissions would be entered. Now you can see that only the owner's permissions are rwx and all other permissions are -.

This can be reverted with:

Before: drwx------ 6 archie web 4096 Jul 6 17:32 Documents

After: drwxr-xr-x 6 archie web 4096 Jul 6 17:32 Documents

In the next example, you want to grant read and execute permissions to the group, and other users, so you put the letters for the permissions (r and x) after the =, with no spaces.

You can simplify this to put more than one who letter in the same command, e.g:

Now let us consider a second example, suppose you want to change a foobar file so that you have read and write permissions, and fellow users in the group web who may be colleagues working on foobar, can also read and write to it, but other users can only read it:

Before: -rw-r--r-- 1 archie web 5120 Jun 27 08:28 foobar

After: -rw-rw-r-- 1 archie web 5120 Jun 27 08:28 foobar

This is exactly like the first example, but with a file, not a directory, and you grant write permission (just so as to give an example of granting every permission).

The chmod command lets add and subtract permissions from an existing set using + or - instead of =. This is different from the above commands, which essentially re-write the permissions (e.g. to change a permission from r-- to rw-, you still need to include r as well as w after the = in the chmod command invocation. If you missed out r, it would take away the r permission as they are being re-written with the =. Using + and - avoids this by adding or taking away from the current set of permissions).

Let us try this + and - method with the previous example of adding write permissions to the group:

Before: -rw-r--r-- 1 archie web 5120 Jun 27 08:28 foobar

After: -rw-rw-r-- 1 archie web 5120 Jun 27 08:28 foobar

Another example, denying write permissions to all (a):

Before: -rw-rw-r-- 1 archie web 5120 Jun 27 08:28 foobar

After: -r--r--r-- 1 archie web 5120 Jun 27 08:28 foobar

A different shortcut is the special X mode: this is not an actual file mode, but it is often used in conjunction with the -R option to set the executable bit only for directories, and leave it unchanged for regular files, for example:

It is possible to tell chmod to copy the permissions from one class, say the owner, and give those same permissions to group or even all. To do this, instead of putting r, w, or x after the =, put another who letter. e.g:

Before: -rw-r--r-- 1 archie web 5120 Jun 27 08:28 foobar

After: -rw-rw-r-- 1 archie web 5120 Jun 27 08:28 foobar

This command essentially translates to "change the permissions of group (g=), to be the same as the owning user (=u). Note that you cannot copy a set of permissions as well as grant new ones e.g.:

In that case chmod throw an error.

chmod can also set permissions using numbers.

Using numbers is another method which allows you to edit the permissions for all three owner, group, and others at the same time, as well as the setuid, setgid, and sticky bits. This basic structure of the code is this:

Where xxx is a 3-digit number where each digit can be anything from 0 to 7. The first digit applies to permissions for owner, the second digit applies to permissions for group, and the third digit applies to permissions for all others.

In this number notation, the values r, w, and x have their own number value:

To come up with a 3-digit number you need to consider what permissions you want owner, group, and all others to have, and then total their values up. For example, if you want to grant the owner of a directory read write and execution permissions, and you want group and everyone else to have just read and execute permissions, you would come up with the numerical values like so:

This is the equivalent of using the following:

To view the existing permissions of a file or directory in numeric form, use the stat(1) command:

Where the %a option specifies output in numeric form.

Most directories are set to 755 to allow reading, writing and execution to the owner, but deny writing to everyone else, and files are normally 644 to allow reading and writing for the owner but just reading for everyone else; refer to the last note on the lack of x permissions with non executable files: it is the same thing here.

To see this in action with examples consider the previous example that has been used but with this numerical method applied instead:

Before: -rw-r--r-- 1 archie web 5120 Jun 27 08:28 foobar

After: -rw-rw-r-- 1 archie web 5120 Jun 27 08:28 foobar

If this were an executable the number would be 774 if you wanted to grant executable permission to the owner and group. Alternatively if you wanted everyone to only have read permission the number would be 444. Treating r as 4, w as 2, and x as 1 is probably the easiest way to work out the numerical values for using chmod xxx filename, but there is also a binary method, where each permission has a binary number, and then that is in turn converted to a number. It is a bit more convoluted, but here included for completeness.

Consider this permission set:

If you put a 1 under each permission granted, and a 0 for every one not granted, the result would be something like this:

You can then convert these binary numbers:

The value of the above would therefore be 775.

Consider we wanted to remove the writable permission from group:

The value would therefore be 755 and you would use chmod 755 filename to remove the writable permission. You will notice you get the same three digit number no matter which method you use. Whether you use text or numbers will depend on personal preference and typing speed. When you want to restore a directory or file to default permissions e.g. read and write (and execute) permission to the owner but deny write permission to everyone else, it may be faster to use chmod 755/644 filename. However if you are changing the permissions to something out of the norm, it may be simpler and quicker to use the text method as opposed to trying to convert it to numbers, which may lead to a mistake. It could be argued that there is not any real significant difference in the speed of either method for a user that only needs to use chmod on occasion.

You can also use the numeric method to set the setuid, setgid, and sticky bits by using four digits.

For example, chmod 2777 filename will set read/write/executable bits for everyone and also enable the setgid bit.

Generally directories and files should not have the same permissions. If it is necessary to bulk modify a directory tree, use find to selectively modify one or the other.

To chmod only directories to 755:

To chmod only files to 644:

chown changes the owner of a file or directory, which is quicker and easier than altering the permissions in some cases.

Consider the following example, making a new partition with GParted for backup data. Gparted does this all as root so everything belongs to root by default. This is all well and good but when it comes to writing data to the mounted partition, permission is denied for regular users.

As you can see the device in /dev is owned by root, as is the mount location (/media/Backup). To change the owner of the mount location one can do the following:

Before: drwxr-xr-x 5 root root 4096 Jul 6 16:01 Backup

After: drwxr-xr-x 5 archie root 4096 Jul 6 16:01 Backup

Now the partition can have data written to it by the new owner, archie, without altering the permissions (as the owner triad already had rwx permissions).

Access Control Lists provides an additional, more flexible permission mechanism for file systems by allowing to set permissions for any user or group to any file.

The umask utility is used to control the file-creation mode mask, which determines the initial value of file permission bits for newly created files.

Apart from the file mode bits that control user and group read, write and execute permissions, several file systems support file attributes that enable further customization of allowable file operations.

The e2fsprogs package contains the programs lsattr(1) and chattr(1) that list and change a file's attributes, respectively.

These are a few useful attributes. Not all filesystems support every attribute.

See chattr(1) for a complete list of attributes and for more info on what each attribute does.

For example, if you want to set the immutable bit on some file, use the following command:

To remove an attribute on a file just change + to -.

See Extended attributes.

Use the --preserve-root flag to prevent chmod from acting recursively on /. This can, for example, prevent one from removing the executable bit systemwide and thus breaking the system. To use this flag every time, set it within an alias. See also [1].

**Examples:**

Example 1 (unknown):
```unknown
$ ls -l /path/to/directory
```

Example 2 (unknown):
```unknown
total 128
drwxr-xr-x 2 archie archie  4096 Jul  5 21:03 Desktop
drwxr-xr-x 6 archie archie  4096 Jul  5 17:37 Documents
drwxr-xr-x 2 archie archie  4096 Jul  5 13:45 Downloads
-rw-rw-r-- 1 archie archie  5120 Jun 27 08:28 customers.ods
-rw-r--r-- 1 archie archie  3339 Jun 27 08:28 todo
-rwxr-xr-x 1 archie archie  2048 Jul  6 12:56 myscript.sh
```

Example 3 (unknown):
```unknown
drwxrwxrwx+
```

Example 4 (unknown):
```unknown
info ls -n "What information is listed"
```

---

## Perl

**URL:** https://wiki.archlinux.org/title/Perl

**Contents:**
- Installation
- Commands
- Package management
  - pacman and AUR
  - CPAN.pm
    - Configuring cpan
    - Usage examples
- Widget bindings
- Development in JetBrains IDE
- Tips and tricks

Install the perl package.

The Perl language interpreter:

Lookup the Perl documentation in POD format:

Send the Perl authors and maintainers a thank you message:

The Comprehensive Perl Archive Network (CPAN) is a repository of over 250,000 software modules and accompanying documentation written in the Perl programming language by over 12,000 contributors.

CPAN is also the name of a Perl module, CPAN.pm, which is used to download and install Perl software from the CPAN archive.

A number of popular CPAN modules are available as packages in the Arch repositories. There are further modules available in the AUR.

The CPAN.pm module is included with Perl. It can be used interactively from the shell or in Perl scripts.

Before first use, the module needs to be configured. This is done interactively from the shell with (some output omitted):

Automated configuration will suit most users. Answering yes, the configuration will continue with:

If you want cpan to install modules in your home directory choose local::lib. To install them system-wide choose sudo. Choosing sudo the configuration ends:

Choosing the local::lib option will result in addition modules being installed.

Choosing not to use automated configuration allows the user to set cpan options interactively in the shell. The table below shows some option names with a brief description and default value. More detailed information is displayed for each option during configuration.

The configuration file $HOME/.cpan/CPAN/MyConfig.pm can be edited with your text editor of choice.

To simply install a modules pass them as parameters to cpan (multiple module names are separated by spaces):

The following examples are all in the cpan interactive shell, started with:

Display information on a module:

The following widget toolkit bindings are available:

To use these with Perl, you may need to install the associated widget kits.

If you are using a JetBrains IDE, for example IntelliJ Idea, install perl-bundle-camelcadeAUR. Then install Perl plugin. Then go to the Settings > Languages & Frameworks > Perl5. In the Perl 5 Interpreter field select Add System Perl. Now you can make a run/debug configuration for your project and start debugging.

cpanminus extends module management and aims to be zero configuration and integrates with local::db.

Install the cpanminus package.

The cpanminus documentation gives examples.

ucpan is a nice, fully automatic updater for CPAN modules (especially installed in local-lib). To install use:

Installing the Bundle::CPAN distribution will add a lot of nice functionality to CPAN.pm.

Module::Starter helps you create your new module... Install:

This going to create your module tree...

To add dependencies, put module_name => version_or_0 inside $builder -> requires in file Build.PL

To install required dependencies for your project, run inside App-foobar folder:

To skip git folder, add under $builder

Adding scripts can by done like this:

You can change your project version here: lib/App/foobar.pm

To install your module run this command in module root folder:

See more on: cpan page, cookbook

**Examples:**

Example 1 (unknown):
```unknown
$ perlthanks
```

Example 2 (unknown):
```unknown
CPAN.pm requires configuration, but most of it can be done automatically.
If you answer 'no' below, you will enter an interactive dialog for each
configuration option instead.

Would you like to configure as much as possible automatically? [yes]
```

Example 3 (unknown):
```unknown
To install modules, you need to configure a local Perl library directory or
escalate your privileges. CPAN can help you by bootstrapping the local::lib
module or by configuring itself to use 'sudo' (if available). You may also
resolve this problem manually if you need to customize your setup.

What approach do you want?  (Choose 'local::lib', 'sudo' or 'manual')
 [local::lib]
```

Example 4 (unknown):
```unknown
Autoconfiguration complete.

commit: wrote '/home/username/.cpan/CPAN/MyConfig.pm'
```

---

## Main page

**URL:** https://wiki.archlinux.org/title/Main_page

**Contents:**
- The distribution
- Our community
- Wiki interaction

Welcome to the ArchWiki: your source for Arch Linux documentation on the web.

Visit the Table of contents for a listing of article categories.

---

## iptables

**URL:** https://wiki.archlinux.org/title/Iptables

**Contents:**
- Installation
  - Front-ends
    - Console
    - Graphical
- Basic concepts
  - Tables
  - Chains
  - Rules
  - Traversing Chains
  - Modules

iptables is a command line utility for configuring Linux kernel firewall implemented within the Netfilter project. The term iptables is also commonly used to refer to this kernel-level firewall. It can be configured directly with iptables, or by using one of the many console and graphical front-ends. iptables is used for IPv4 and ip6tables is used for IPv6. Both iptables and ip6tables have the same syntax, but some options are specific to either IPv4 or IPv6.

The stock Arch Linux kernel is compiled with iptables support. You will only need to install the userland utilities, which are provided by the package iptables. The iptables package is an indirect dependency of the base meta package, so it should be installed on your system by default.

iptables is used to inspect, modify, forward, redirect, and/or drop IP packets. The code for filtering IP packets is already built into the kernel and is organized into a collection of tables, each with a specific purpose. The tables are made up of a set of predefined chains, and the chains contain rules which are traversed in order. Each rule consists of a predicate of potential matches and a corresponding action (called a target) which is executed if the predicate is true; i.e. the conditions are matched. If the IP packet reaches the end of a built-in chain, including an empty chain, then the chain's policy target determines the final destination of the IP packet. iptables is the user utility which allows you to work with these chains/rules. Most new users find the complexities of linux IP routing quite daunting, but, in practice, the most common use cases (NAT and/or basic Internet firewall) are considerably less complex.

The key to understanding how iptables works is this chart. The lowercase word on top is the table and the uppercase word below is the chain. Every IP packet that comes in on any network interface passes through this flow chart from top to bottom. A common misconception is that packets entering from, say, an internal interface are handled differently than packets from an Internet-facing interface. All interfaces are handled the same way; it is up to you to define rules that treat them differently. Of course, some packets are intended for local processes, hence they come in from the top of the chart and stop at <Local Process>, while other packets are generated by local processes; hence they start at <Local Process> and proceed downward through the flowchart. A detailed explanation of how this flow chart works can be found here.

In the vast majority of use cases, you will not need to use the raw, mangle, or security tables at all. Consequently, the following chart depicts a simplified network packet flow through iptables:

iptables contains five tables:

In most common use cases, you will only use two of these: filter and nat. The other tables are aimed at complex configurations involving multiple routers and routing decisions and are in any case beyond the scope of these introductory remarks.

Tables consist of chains, which are lists of rules which are followed in order. The default table, filter, contains three built-in chains: INPUT, OUTPUT and FORWARD which are activated at different points of the packet filtering process, as illustrated in the flow chart. The nat table includes PREROUTING, POSTROUTING, and OUTPUT chains.

See iptables(8) for a description of built-in chains in other tables.

By default, none of the chains contain any rules. It is up to you to append rules to the chains that you want to use. Chains do have a default policy, which is generally set to ACCEPT, but can be reset to DROP, if you want to be sure that nothing slips through your ruleset. The default policy always applies at the end of a chain only. Hence, the packet has to pass through all existing rules in the chain before the default policy is applied.

User-defined chains can be added to make rulesets more efficient or more easily modifiable. See Simple stateful firewall for an example of how user-defined chains are used.

Packet filtering is based on rules, which are specified by multiple matches (conditions the packet must satisfy so that the rule can be applied), and one target (action taken when the packet matches all conditions). The typical things a rule might match on are what interface the packet came in on (e.g eth0 or eth1), what type of packet it is (ICMP, TCP, or UDP), or the destination port of the packet.

Targets are specified using the -j or --jump option. Targets can be either user-defined chains (i.e. if these conditions are matched, jump to the following user-defined chain and continue processing there), one of the special built-in targets, or a target extension. Built-in targets are ACCEPT, DROP, QUEUE and RETURN, target extensions are, for example, REJECT and LOG. If the target is a built-in target, the fate of the packet is decided immediately and processing of the packet in current table is stopped. If the target is a user-defined chain and the fate of the packet is not decided by this second chain, it will be filtered against the remaining rules of the original chain. Target extensions can be either terminating (as built-in targets) or non-terminating (as user-defined chains), see iptables-extensions(8) for details.

A network packet received on any interface traverses the traffic control chains of tables in the order shown in the flow chart. The first routing decision involves deciding if the final destination of the packet is the local machine (in which case the packet traverses through the INPUT chains) or elsewhere (in which case the packet traverses through the FORWARD chains). Subsequent routing decisions involve deciding what interface to assign to an outgoing packet. At each chain in the path, every rule in that chain is evaluated in order and whenever a rule matches, the corresponding target/jump action is executed. The 3 most commonly used targets are ACCEPT, DROP, and jump to a user-defined chain. While built-in chains can have default policies, user-defined chains can not. If every rule in a chain that you jumped fails to provide a complete match, the packet is dropped back into the calling chain; see the following illustration. If at any time a complete match is achieved for a rule with a DROP target, the packet is dropped and no further processing is done. If a packet reaches a jump to the ACCEPT target, it will not traverse any further rules of the chain and subsequent chains of the table. Its processing will jump to the first chain of the next table in order. See also Traversing tables and chains and Accept Target of the frozentux tutorial.

There are many modules which can be used to extend iptables such as connlimit, conntrack, limit and recent. These modules add extra functionality to allow complex filtering rules.

By default, iptables provides an empty set of rules in /etc/iptables/iptables.rules. To be able to load iptables rules automatically on boot, you can enable the iptables.service unit provided by iptables or start the systemd service unit to load iptables rules at once.

Similarly, iptables rules for IPv6 are, by default, stored in /etc/iptables/ip6tables.rules, which is read by ip6tables.service. You can start it the same way as above.

After adding rules via command-line as shown in the following sections, the configuration file is not changed automatically  you have to save it manually:

If you edit the configuration file manually, you have to reload iptables.

Or you can load it directly through iptables:

The basic command to list current rules is --list-rules (-S), which is similar in output format to the iptables-save utility. The main difference of the two is that the latter outputs the rules of all tables per default, while all iptables commands default to the filter table only.

When working with iptables on the command line, the --list (-L) command accepts more modifiers and shows more information. For example, you can check the current ruleset and the number of hits per rule by using the command:

If the output looks like the above, then there are no rules (i.e. nothing is blocked) in the default filter table. Other tables can be specified with the -t option.

To show the line numbers when listing rules, append --line-numbers to that input. The line numbers are a useful shorthand when #Editing rules on the command line.

You can flush and reset iptables to default using these commands:

The -F command with no arguments flushes all the chains in its current table. Similarly, -X deletes all empty non-default chains in a table.

Individual chains may be flushed or deleted by following -F and -X with a [chain] argument.

Rules can be edited by appending -A a rule to a chain, inserting -I it at a specific position on the chain, replacing -R an existing rule, or deleting -D it. The first three commands are exemplified in the following.

First of all, our computer is not a router (unless, of course, it is a router). We want to change the default policy on the FORWARD chain from ACCEPT to DROP.

The Dropbox LAN sync feature broadcasts packets every 30 seconds to all computers it can see. If we happen to be on a LAN with Dropbox clients and do not use this feature, then we might wish to reject those packets.

Now, say we change our mind about Dropbox and decide to install it on our computer. We also want to LAN sync, but only with one particular IP on our network. So we should use -R to replace our old rule. Where 10.0.0.85 is our other IP:

We have now replaced our original rule with one that allows 10.0.0.85 to access port 17500 on our computer. But now we realize that this is not scalable. If our friendly Dropbox user is attempting to access port 17500 on our device, we should allow them immediately, not test them against any firewall rules that might come afterwards!

So we write a new rule to allow our trusted user immediately. Using -I to insert the new rule before our old one:

And replace our second rule with one that rejects everything on port 17500:

Our final rule list now looks like this:

Protocols that use multicast identification (e.g. SANE searching for network scanners) will send traffic to the network's broadcast IP and responses will come back from a specific client's IP. Since these IP addresses are different, iptables will not recognize the response as RELATED or ESTABLISHED, and it will block the response. See [1] for how to accept multicast traffic without creating an overly-permissive firewall.

First, create an ipset hash container. The timeout is the window of time to accept client responses.

Second, create a rule to add outgoing multicast traffic to the ipset hash.

Third, create a rule to allow incoming traffic that matches against the ipset hash.

Finally, remember to save the new rules (see #Configuration and usage and ipset#Making ipset persistent), and ensure iptables.service and ipset.service are enabled so the rules load upon system start.

The LOG target can be used to log packets that hit a rule. Unlike other targets like ACCEPT or DROP, the packet will continue moving through the chain after hitting a LOG target. This means that in order to enable logging for all dropped packets, you would have to add a duplicate LOG rule before each DROP rule. Since this reduces efficiency and makes things less simple, a logdrop chain can be created instead.

Create the chain with:

And add the following rules to the newly created chain:

Explanation for limit and limit-burst options is given below.

Now whenever we want to drop a packet and log this event, we just jump to the logdrop chain, for example:

The above logdrop chain uses the limit module to prevent the iptables log from growing too large or causing needless hard drive writes. Without limiting an erroneously configured service trying to connect, or an attacker, could fill the drive (or at least the /var partition) by causing writes to the iptables log.

The limit module is called with -m limit. You can then use --limit to set an average rate and --limit-burst to set an initial burst rate. In the logdrop example above:

appends a rule which will log all packets that pass through it. The first 10 consecutive packets will be logged, and from then on only 5 packets per minute will be logged. The "limit burst" count is reset every time the "limit rate" is not broken, i.e. logging activity returns to normal automatically.

Logged packets are visible as kernel messages in the systemd journal.

To view all packets that were logged since the machine was last booted:

Assuming you are using syslog-ng, you can control where iptables' log output goes in syslog-ng.conf. Replace:

This will stop logging iptables output to /var/log/everything.log.

If you also want iptables to log to a different file than /var/log/iptables.log, you can simply change the file value of destination d_iptables here (still in syslog-ng.conf):

ulogd is a specialized userspace packet logging daemon for netfilter that can replace the default LOG target.

**Examples:**

Example 1 (unknown):
```unknown
XXXXXXXXXXXXXXXXXX
                             XXX     Network    XXX
                               XXXXXXXXXXXXXXXXXX
                                       +
                                       |
                                       v
 +-------------+              +------------------+
 |table: filter| <---+        | table: nat       |
 |chain: INPUT |     |        | chain: PREROUTING|
 +-----+-------+     |        +--------+---------+
       |             |                 |
       v             |                 v
 [local process]     |           ****************          +--------------+
       |             +---------+ Routing decision +------> |table: filter |
       v                         ****************          |chain: FORWARD|
****************                                           +------+-------+
Routing decision                                                  |
****************                                                  |
       |                                                          |
       v                        ****************                  |
+-------------+       +------>  Routing decision  <---------------+
|table: nat   |       |         ****************
|chain: OUTPUT|       |               +
+-----+-------+       |               |
      |               |               v
      v               |      +-------------------+
+--------------+      |      | table: nat        |
|table: filter | +----+      | chain: POSTROUTING|
|chain: OUTPUT |             +--------+----------+
+--------------+                      |
                                      v
                               XXXXXXXXXXXXXXXXXX
                             XXX    Network     XXX
                               XXXXXXXXXXXXXXXXXX
```

Example 2 (unknown):
```unknown
POSTROUTING
```

Example 3 (unknown):
```unknown
/etc/iptables/iptables.rules
```

Example 4 (unknown):
```unknown
iptables.service
```

---

## Resilio Sync

**URL:** https://wiki.archlinux.org/title/Resilio_Sync

**Contents:**
- Installation
- Usage
- Configuration
  - Automatic configuration file creation
- Troubleshooting
  - Missing storage path
  - Ignore some files/folders synchronization
  - Error while loading shared libraries: libcrypt.so.1
- See also

Resilio Sync (formerly: BitTorrent Sync or BTSync) is a proprietary file sharing system that relies on the BitTorrent Peer-to-peer (P2P) protocol.

Instead of having a central server which archives every file, it uses peer-to-peer connections between the devices themselves therefore there is no limit on data storage and/or transfer speed. The user's data is exclusively stored on the devices with which the user chose to be in sync with, hence it is required to have at least two user devices, or "nodes" to be online. If many devices are connected simultaneously, files are shared between them in a mesh networking topology.

All traffic between devices is encrypted with AES-128 in counter mode, using a unique session key. This key is derived from a 'secret' which itself is a random 21 Byte key Base32-encoded. By handing over the 'secret', files and folders can be shared with other users. When a device adds a folder for synchronization, a secret is generated. From now on, every device that wants to synchronize that folder must know the secret key. The synchronization has no speed or size limits, as long as both devices have enough disk space.

Install the rslsyncAUR package, which includes systemd service definitions for managing the rslsync daemon. This package creates a default /etc/rslsync.conf for system/root operation. Make the desired changes (e.g. login id and password) to those files prior to enabling rslsync.service.

Alternatively, the bare .tar.gz packaged executable can be downloaded from the official website. The rest of this guide assumes that you are using the rslsyncAUR package.

The Linux client of Resilio Sync does not use a typical GUI, instead it sets up a WebUI server accessible at localhost:8888. Shared folders can also be configured statically in a configuration file, but doing so disables the WebGUI.

Once installed, you will first need to create a configuration file at ~/.config/rslsync/rslsync.conf, see #Configuration. You will also need to create the storage_path directory. When that is done, start and (if you want it to start on boot) start/enable the user service rslsync (i.e., with the --user flag).

The service will run as the user invoking the command. Note that the above command is not run as root: doing so may lead to a cryptic error stating that D-Bus has refused the connection.

You can also run it as the rslsync.service system user (without the --user flag).

Configuration for this user is located at /etc/rslsync.conf, and metadata is saved in /var/lib/rslsync/ by default. You should review the configuration settings especially user and password, see below.

A sample configuration file can be created using:

You will probably want to change some of the settings, including:

The rslsyncAUR package provides a systemd user service called rslsync.service which, if enabled, triggers when a user's rslsync_user.service starts. Then a configuration file with default values will be created if it does not already exist.

The install script enables the service for all users by default. Although defeating most of its purpose, rslsync.service can be disabled.

Individual users can then enable the rslsync_user.service user unit.

rslsync_user.service creates ~/.config/rslsync/rslsync.conf if it does not exist, and guesses some default values of the settings:

The script also creates the storage_path directory set in the configuration file if it does not exist. This is done independently from the creation of the configuration file.

If you start the service but cannot reach the WebUI, check the unit status for the system-wide instance or the user unit.

A common error is Storage path specified in config file does not exist.. This is easily fixed by mkdir ~/.rslsync, or whatever your storage_path is set to.

If you dont want Resilio Sync to track some files in your sync folder, please use IgnoreList. IgnoreList is located in hidden .sync folder.

IgnoreList is a UTF-8 encoded .txt file that allows you to specify single files, paths or rules for ignoring during the synchronization job. Each line of the IgnoreList file represents a separate rule. All the files that fall under the ignore filter are not indexed and not counted in the "Size" column in Sync main view.

It supports '?' and '*' wildcard symbols.

Note that IgnoreList is applied only to the folder where it is contained and will not work with the files that have already been synced. If you add indexed files to IgnoreList, they will be deleted on other syncing devices. In order to avoid this:

For further details, please refer to Ignoring files in Sync (IgnoreList)

If you receive the error message error while loading shared libraries: libcrypt.so.1 starting rslsync with the binary or as a service, install the package libxcrypt-compat.

**Examples:**

Example 1 (unknown):
```unknown
/etc/rslsync.conf
```

Example 2 (unknown):
```unknown
rslsync.service
```

Example 3 (unknown):
```unknown
localhost:8888
```

Example 4 (unknown):
```unknown
~/.config/rslsync/rslsync.conf
```

---

## F2FS

**URL:** https://wiki.archlinux.org/title/F2FS

**Contents:**
- Known issues
  - fsck failures
  - Long running fsck delays boot
  - GRUB support
  - Cannot set file attributes for '/var/log/journal'
- Creating a F2FS file system
  - Compression
  - File-based encryption support
- Mounting a F2FS file system
  - Recommended mount options

F2FS (Flash-Friendly File System) is a file system intended for NAND-based flash memory equipped with Flash Translation Layer. Unlike JFFS or UBIFS it relies on a flash-transition layer (FTL) to handle write distribution. It is supported from kernel 3.8 onwards.

An FTL is found in all flash memory with a SCSI/SATA/PCIe/NVMe interface [1], opposed to bare NAND Flash and SmartMediaCards [2].

This article or section needs expansion.

F2FS has a weak fsck that can lead to data loss in case of a sudden power loss [3][4].

If power losses are frequent, consider an alternative file system.

This article or section is out of date.

If the kernel version has changed between boots, the fsck.f2fs utility will perform a full file system check which will take longer to finish[5].

This may be mitigated in the future thanks to a recent commit [6].

While GRUB supports F2FS since version 2.0.4, it cannot correctly read its boot files from an F2FS partition that was created with the extra_attr flag enabled (for more details, see GRUB#Unsupported file systems).

is present in your journal, it can be safely ignored: it is due to systemd-tmpfiles trying to use a file system feature that is not supported by F2FS.

This article assumes the device has partitions already setup. Install f2fs-tools. Use mkfs.f2fs to format the target partition referred to as /dev/sdxY:

To use compression, include the compression option. Example:

When mounting the filesystem, specify compress_algorithm=(lzo|lz4|zstd|lzo-rle). Using compress_extension=txt will cause all txt files to be compressed by default.

In order to tell F2FS to compress a file or a directory, use :

Since Linux 4.2, F2FS natively supports file encryption. Encryption is applied at the directory level, and different directories can use different encryption keys. This is different from both dm-crypt, which is block-device level encryption, and from eCryptfs, which is a stacked cryptographic filesystem. To use F2FS's native encryption support, see the fscrypt article. Create the file system with

or add encryption capability at a later time with fsck.f2fs -O encrypt /dev/sdxY.

The file system can then be mounted manually or via other mechanisms:

Since F2FS is designed to be used on flash devices, compression is a good idea. You have to enable it at mkfs.f2fs time. A few mount options can be used to improve things slightly.

By default, F2FS is mounted using a hybrid TRIM mode which behaves as continuous TRIM. This implementation creates asynchronous discard threads to alleviate long discarding latency among RW IOs. It keeps candidates in memory, and the thread issues them in idle time [7]. As a result of this, users wanting periodic TRIM will need to implicitly set the nodiscard mount option in /etc/fstab or pass it to mount if mounting manually.

Checking and repairs to F2FS file systems are accomplished with fsck.f2fs provided by f2fs-tools. To check a file system, execute

Depending on the result, see fsck.f2fs(8) for available switches to repair inconsistencies. For example:

When the filesystem is unmounted, it can be grown if the partition is expanded. Shrinking is not currently supported.

First use a partition tool to resize the partition: for example, suppose the output of the print command in the parted console is the following:

To resize the f2fs partition to occupy all the space up to the fourth one, just give resizepart 3 31GB and exit. Now expand the filesystem to fill the new partition using:

where /dev/sdxY is the target F2FS volume to grow. See resize.f2fs(8) for supported options.

As alluded to in Fsck#Boot time checking, the kernel keeps file systems in a read-only state by default, and it is either explicitly needed to pass the rw kernel parameter or have systemd remounting the filesystems as read-write with the systemd-remount-fs.service.

When remounting the file system from a read-only state to a read-write state, the mount -o remount,... / steps can fail for multiple reasons, which will either leave the root file system read-only or make the system unable to boot with the error message:

If the atgc option is specified in the fstab file but has not been passed as a kernel parameter, the F2FS kernel module will not permit re-mounting with an added or cleared atgc option[8][9], either:

Since linux 6.2, a check is made in the kernel to disable flush_merge when the file system is read-only, but it is the state of the file system before remounting that is checked. As explained in FS#77596, either:

ef47782 ("f2fs-tools: Refactor Summary block struct and friends")bb051c7 ("f2fs-tools: Refactor SIT/NAT block structs")32f5a37 ("f2fs-tools: Refactor f2fs_node struct and friends")b1aeb99 ("f2fs-tools: Refactor Orphan Block struct")30825b3 ("f2fs-tools: Define constants in terms of BLKSIZE")

**Examples:**

Example 1 (unknown):
```unknown
Cannot set file attributes for '/var/log/journal', maybe due to incompatibility in specified attributes, previous=0x10001000, current=0x10001000, expected=0x10801000, ignoring.
```

Example 2 (unknown):
```unknown
# mkfs.f2fs -l mylabel -i -O extra_attr,inode_checksum,sb_checksum /dev/sdxY
```

Example 3 (unknown):
```unknown
inode_checksum
```

Example 4 (unknown):
```unknown
sb_checksum
```

---

## DeveloperWiki:Project Leader

**URL:** https://wiki.archlinux.org/title/DeveloperWiki:Project_Leader

**Contents:**
- Project Leader
  - Roles
    - Decision making
    - Financial
    - Legal
    - Project management
  - Election
    - Nomination
    - Voting
    - Triggering of elections

The Arch Linux Leader will have the following roles:

The Leader will serve to make decisions on any aspect of the distribution that fails to reach consensus on the mailing lists. Here consensus is defined by the absence of sustained opposition to substantial issues by the concerned parties and by a process that involves seeking to take into account the views of all parties concerned and to reconcile any conflicting arguments. Consensus does not imply unanimity. A clear and concise description of the competing options will be provided to the leader by a single representative of each viewpoint prior to decisions being made, to ensure the Leader is well informed prior to decision making.

The Leader serves as the Arch Linux representative on the SPI, and approves all spending from the Arch Linux account. The Leader will inform the team yearly (to coincide with the release of the SPI report) on the status of Arch Linux finances.

The leader is the Arch Linux representative in all legal matters. The primary responsibility of the Leader in this role is holding the Arch Linux Trademark. They will be in charge of handling trademark requests and related issues.

The Leader oversees the progress of projects with distribution-wide effect. Regular census of planned and on-going projects will be performed, with action taken to ensure these projects run to completion, whether to full implementation or making a decision regarding abandonment.

The role of Arch Linux Leader is determined by a vote among eligible members of the Arch Linux Team.

Nomination of candidates will take place in a two week period prior to voting. Nominations require support of two members of the Arch Linux Team, which may include the nominee. If the nominee is neither of the nominators, then they must also agree to be nominated. All Team members are eligible for nomination.

When more than two candidates are nominated, voting will be conducted using the instant-runoff voting system. Ballots with candidates ranked in order of preference will be collected and tallied by a member of the Team who is neither a candidate, nor has nominated a candidate. Voting shall take place over a two week period, with no quorum in effect.

Team members eligible for voting are:

The complete list of individuals in these roles is defined on the Arch Linux website, fixed at the beginning of the voting period. People who have roles in multiple of these categories are only eligible to vote once.

After the winner has been determined, the Arch Linux Developers, can enact their right to veto. This veto decision must be made within 14 days of the election. In case of a veto, the current winner is removed from the list of candidates and a new winner is determined based on the remaining votes. Then the veto process is restarted with this new winner. If there are no more candidates on the list, the whole nomination and voting process is restarted.

A veto can be proposed by any developer on the arch-dev mailing list and needs initial support by at least one other developer. A decision on the veto is then reached by the usual developer decision making process. If two weeks pass without a veto from the developer team, the previous leader (or a representative) publicly announce the new leader. If no veto discussion has been initiated within one week of the election, the developer team can decide to announce the leader ahead of time, thereby refraining from their right to veto.

The election of the Arch Linux Leader can be triggered by any of the following conditions:

A successful vote for removal of a leader must satisfy the following conditions:

The term length for the Arch Linux Leader is set to a default of 2 years. A Leader may seek reelection an unlimited number of times. Calling an early election does not affect the Leader's ability to seek reelection.

---

## ccache

**URL:** https://wiki.archlinux.org/title/Ccache

**Contents:**
- Installation
- Configuration
  - Enable ccache for makepkg
  - Enable for command line
  - Enable with colorgcc
- Misc
  - Sloppiness
  - Change the cache directory
  - Set maximum cache size
  - Disable the cache via environment

ccache is a compiler wrapper that stores on disk the compiled binaries and offers them back to speed up any eventual recompilation of the same code. While it may take a few seconds longer to compile a program the first time, subsequent compiles will be much faster as no proper compilation is made, only a lookup through the previously stored binaries. ccache is compatible with GCC and Clang.

Install the ccache package.

The default behavior can be overridden by configuration files. Priority of the configuration settings is as follows (where 1 is highest):

See ccache(1) for details.

To enable ccache when using makepkg edit /etc/makepkg.conf. In BUILDENV uncomment ccache (remove the exclamation mark) to enable caching. For example:

If you are compiling your code from the command line, and not building packages, then you will still want to use ccache to help speed things up.

For that, you can prefix each compilation command with ccache.

Alternatively, change your $PATH to include ccache's binaries before the path to your compiler:

You may want to set this line as an environment variable for regular usage.

Since colorgcc is also a compiler wrapper, some care needs to be taken to ensure each wrapper is called in the correct sequence.

Then colorgcc needs to be told to call ccache instead of the real compiler. Edit /etc/colorgcc/colorgccrc and change the /usr/bin paths to /usr/lib/ccache/bin for all the compilers in /usr/lib/ccache/bin:

Newer versions of ccache will always enable color for GCC when GCC_COLORS is set. Color is enabled for Clang by default. If the output is not a TTY, ccache will ask the compiler to generate color, storing them in the cache, but stripping them from the output. There remains some issue in unifying -fdiagnostics-color.

ccache by default uses a very conservative comparison that minimizes both false positives and, for some projects, actual positives. Some of the comparisons are deemed useless and can be changed:

This tells ccache to ignore the __FILE__ and time-related macros, which usually invalidate the cache and are considered harmful in reproducible builds. Locale differences are also ignored; ccache cares about it mainly because it determines the language of diagnostic messages.

The CCACHE_SLOPPINESS environment variable can be exported to override any pre-existing sloppiness settings.

ccache also by default caches the current directory being used for each build, which means cache misses for build pipelines that use a new, random temporary directory each time it is called. See the Compiling in different directories section of the ccache manual.

You may want to move the cache directory to a faster location than the default ~/.cache/ccache directory, like an SSD or a ramdisk.

To change the cache location only in the current shell:

Or to change the location by default:

The default value is 5 gigabyte, however it is possible to use a lower or even a higher value:

If you wish to disable ccache, set the following environment variable:

You can use the command-line utility ccache to show a statistics summary:

Clear the cache completely:

It is also possible to use ccache with makechrootpkg from devtools package. To retain the cache when the chroot is cleaned the makechrootpkg option -d can be used to bind the cache directory from the regular system into the chroot, e.g.:

Then ccache can be configured for the chroot in the same way as explained above for the regular system.

ccache is effective only when compiling exactly identical sources. (More exactly, preprocessed sources.)

In the Gentoo Linux community, a source based distribution, ccache has been notorious for its placebo effect, compilation failure (due to undesirable leftover objects), etc. Gentoo requires to turn off ccache before reporting compilation failure. See Gentoo:Handbook:Parts/Working/Features#Caching compilation objects and the blog post titled "Debunking ccache myths" by Diego Petten, an ex-Gentoo developer.

**Examples:**

Example 1 (unknown):
```unknown
$HOME/.config/ccache/ccache.conf
```

Example 2 (unknown):
```unknown
/etc/ccache.conf
```

Example 3 (unknown):
```unknown
/etc/makepkg.conf
```

Example 4 (unknown):
```unknown
/etc/makepkg.conf
```

---

## i3

**URL:** https://wiki.archlinux.org/title/I3

**Contents:**
- Installation
- Starting
  - From tty
  - Display manager
- Usage
  - Keyboard shortcuts
  - Containers and layouts
  - Application launcher
    - KRunner as application launcher in KDE Plasma/i3
- Configuration

i3 is a tiling window manager inspired by wmii that is primarily targeted at developers and advanced users.

The stated goals for i3 include clear documentation, proper multi-monitor support, a tree structure for windows, and different modes like in vim.

i3 can be installed with the i3-wm package.

An i3 package group is also available. It includes the window manager, a screen locker and two programs which write a status line to i3bar through stdout.

i3-wm includes i3.desktop as Xsession which starts the window manager. i3-with-shmlog.desktop enables logs (useful for debugging).

i3-gnomeAUR integrates i3 into GNOME - but the underlying i3-gnome is not GNOME 45-compatible, and is not maintained anymore.

See the official documentation for more information, namely the i3 User's Guide.

In i3, commands are invoked with a modifier key, referred to as $mod. This is Alt (Mod1) by default, with Super (Mod4) being a popular alternative. Super is the key usually represented on a keyboard as a Windows icon, or on an Apple keyboard as a Command key.

See the i3 reference card and Using i3 for the defaults. See Keyboard bindings to add new shortcuts.

Users of non-Qwerty keyboard layouts may wish to circumvent the "configuration wizard" as described below.

If switching between multiple window managers or desktop environments, consider using sxhkd or another environment agnostic program to manage bindings. More information can be found in Keyboard shortcuts#Xorg.

i3 manages windows in a tree structure, with containers as building blocks. This structure branches with horizontal or vertical splits. Containers are tiled by default, but can be set to tabbed or stacking layouts, as well as made floating (such as for dialog windows). Floating windows are always on top.

See i3 Tree and Containers and the tree data structure for details.

i3 uses dmenu as an application launcher, which is bound by default to $mod+d. As it is an optional dependency dmenu must first be installed before this functionality can be used.

i3-wm contains i3-dmenu-desktop, a Perl wrapper for dmenu which uses desktop entries to create a list of all installed applications. Alternatively, j4-dmenu-desktop can be used.

rofi is a popular dmenu replacement and more that can list desktop entries.

It is possible to have i3 running alongside KDE Plasma as seen here: KDE#Use a different window manager

When running Plasma with KDEWM=/usr/bin/i3, one can set KRunner as an alternative application launcher with $mod+d by adding the following to the i3 config:

See Configuring i3 for details. The rest of this article assumes the i3 configuration file to be in the folder ~/.config/.

When i3 is first started, it offers to run the configuration wizard i3-config-wizard. This tool creates ~/.config/i3/config by rewriting a template configuration file in /etc/i3/config.keycodes. It makes two modifications to the default template:

Step 2 is designed to ensure that the four navigation shortcuts, j, k, l and ; on a Qwerty keyboard, will be mapped to keysyms which have the same location, e.g. h, t, n, s on a Dvorak keyboard. The side-effect of this magic is that up to fifteen other keysyms may be remapped in ways which break the mnemonics - so that, for a Dvorak user, "restart" is bound to $mod1+p instead of $mod1+r, "split horizontally" is bound to $mod1+d instead of $mod1+h, and so on.

Therefore, users of alternate keyboard layouts who want straightforward key bindings, which match the bindings given in tutorials, may prefer to circumvent the "config wizard". This can be done by just copying /etc/i3/config into ~/.config/i3/config (or ~/.i3/config), and editing that file.

Note that a keycode-based configuration is also possible, e.g. for users who often switch between keyboard layouts, but want the i3 bindings to stay the same.

Add an exec command in ~/.config/i3/config file to execute a command at startup. For example:

Using an exec_always command instead will ensure that the command is also run whenever i3 is restarted.

Alternatively, you can use a dedicated implementation of XDG Autostart.

In addition to showing workspace information, i3bar can act as an input for i3status or an alternative, such as those mentioned in the next section. For example:

See the Configuring i3bar for details.

Some users may prefer panels such as those provided by conventional Desktop Environments. This can be achieved within i3 by launching the panel application of choice during startup.

For example, to use the Xfce panel (xfce4-panel), add the following line anywhere in ~/.config/i3/config:

i3bar can be disabled by commenting the bar{ } section of ~/.config/i3/config, or defining a keybind to toggle the bar:

Replacements independent of the desktop environment are listed below:

Copy over the default configuration files to the home directory:

Not all plugins are defined in the default configuration and some configuration values may be invalid for your system, so they need to be updated accordingly. See i3status(1) for details.

This article or section is a candidate for merging with Fonts#Emoji and symbols.

See also Fonts#Emoji and symbols.

To combine fonts, define a font fallback sequence in your configuration file, separating fonts with , like so:

In accordance with pango syntax, font size is specified only once, at the end of the comma-separated list of font families. Setting a size for each font would cause all but the last font to be ignored.

Add icons to the format strings in ~/.config/i3status/config using the unicode numbers given in the cheatsheets linked above. The input method will vary between text editors. For instance, to insert the "heart" icon (unicode number f004):

This article or section is a candidate for merging with Input method.

By default when pressing $mod+Enter it launches the i3-sensible-terminal which is a script that invokes a terminal. See i3-sensible-terminal(1) for the order terminals are invoked in.

To instead launch a terminal of choice, modify this line in ~/.config/i3/config:

Alternatively, set the $TERMINAL environment variable.

Add bindsym button1 nop to not select a window when you click on its title frame. Useful if your default layout is tabbed and you often miss click i3's tabs instead of something in an application.

Add to ~/.config/i3/config: [1]

i3-wm provides the i3-save-tree script to save and restore workspace layouts. To use it, install its dependency perl-anyevent-i3.

To save the current window layout, follow these steps:

There are two ways to restore the layout of the workspace: by writing a script, or by editing ~/.config/i3/config to automatically load the layout. In this section only the first case will be considered, refer to the official documentation for the second case.

To restore the saved layout in the previous section, write a file named load_layout.sh with the following contents:

where M is the number of the workspace in which you would like to load the previously saved layout and N is the number of workspaces saved in the previous section.

For example, if the saved layout contained three uxterm windows:

Then set the file as executable. Finally, the layout of workspace N can be loaded onto to workspace M by running:

By default, scratchpads only contain a single window. However, containers can also be made a scratchpad.

Create a new container (for example, Mod+Enter), split it (Mod+v) and create another container. Focus the parent (Mod+a), split in the opposite direction (Mod+h), and create a container again.

Focus the first container (with focus parent as needed), make the window floating (Mod+Shift+Space), and move it to the scratchpad (Mod+Shift+-). You can now split containers to preference.

See also [2] for multiple scratchpads.

With Power management#xss-lock you can register a screenlocker for your i3 session. The -time option with xautolock locks the screen after a given time period:

A systemd service file can be used to lock the screen before the system is being sent to sleep or hibernation state. See Power management/Suspend and hibernate#Custom systemd units. Note that i3lock requires the type of service to be forking.

Another option is to use xidlehookAUR with betterlockscreenAUR or any other screensaver. xidlehook is a xautolock replacement written in Rust, but with a few extra features. This includes the option to disable locking when audio is playing or when the screen is in full screen. The --timer option is given in seconds:

Key combinations for shutdown, reboot and screenlock can be added to ~/.config/i3/config. The below example assumes you have polkit installed to allow unprivileged users to run power management commands.

Once completed, you will be presented with a prompt whenever you press $mod+pause. For more complex behavior, use a separate script, and refer to it in the mode. [3]

For a list of alternative screen lockers, see List of applications/Security#Screen lockers.

Similarly to dwm, i3 can "swallow" the current terminal window with the new GUI window launched from it. This can be done through the use of the i3-swallow-gitAUR package.

For example, to let mpv's window swallow the originating terminal:

Alternatively, terminal swallowing can be simulated by using a tabbed or stacked container in order to hide the terminal window when you are not using it. However, this comes with the drawback of always having a window list at the top of the container.

Thanks to xrandr there are many ways to easily manage systems displays. The below example integrates it in the i3 configuration file, and behave as the Power Management section above.

Here a laptop with both VGA and HDMI outputs will use a menu selection to switch them On/Off:

Any window that is still open in a switched Off display will automatically come back to the remaining active display.

The simplest way to determine names of your devices is to plug the device you wish to use and run:

which will output the available, recognized devices and their in-system names to set your configuration file appropriately.

Refer to the xrandr page or xrandr(1) for the complete list of available options, the i3 userguide and/or the i3 FAQ on reddit for more info.

Some web-browsers intentionally do not implement tabs, since managing tabs is considered to be the task of the window manager, not the task of the browser.

To let i3 manage your tab-less web-browser, in this example for uzbl, add the following line to your ~/.config/i3/config

This is for stacked web browsing, meaning that the windows will be shown vertically. The advantage over tabbed browsing is that the window-titles are fully visible, even if a lot of browser windows are open.

If you prefer tabbed browsing, with windows in horizontal direction ('tabs'), use

As workspaces are defined multiple times in i3, assigning workspace variables can be helpful. For example:

Then replace workspace names with their matching variables:

See Changing named workspaces for more information.

While dialogs should open in floating mode by default [6], many still open in tiling mode. To change this behaviour, check the dialog's WM_WINDOW_ROLE with xorg-xprop and add the correct rules to ~/.i3/config (using pcre syntax):

You can also use title rules and regular expressions:

You can use this upstream script. Change the ifaces variable if necessary, save the script in a suitable place (for example ~/.config/i3/), make it executable and point your status_command to it.

The autotiling package can be used for automatic switching horizontal / vertical window split orientation resulting in a similar behavior to the spiral tiling of bspwm. After installation add the following to your ~/.config/i3/config and reload i3.

In many cases, bugs are fixed in the development versions i3-gitAUR and i3status-gitAUR, and upstream will ask to reproduce any errors with this version. [7] See also Debugging/Getting traces#Compilation options.

Buttons such as "Edit config" in i3-nagbar call i3-sensible-terminal, so make sure your Terminal emulator is recognized by i3.

i3 v4.3 and higher ignore size increment hints for tiled windows [8]. This may cause terminals to wrap lines prematurely, amongst other issues. As a workaround, make the offending window floating, before tiling it again.

When starting a script or application which does not support startup notifications, the mouse cursor will remain in busy/watch/clock mode for 60 seconds.

To solve this for a particular application, use the --no-startup-id parameter, for example:

To disable this animation globally, see Cursor themes#Create links to missing cursors.

Some tools such as scrot may not work when used with a regular key binding (executed after key press). In those cases, execute commands after key release with the --release argument [9]:

i3 does not properly implement double buffering [10] hence tearing or flickering may occur. See picom, or TearFree option of modesetting driver.

The tray_output primary directive may require setting a primary output with xrandr, specifying the output explicitly or simply removing this directive. [11] See Xrandr for details. The default configuration created by i3-config-wizard no longer adds this directive to the configuration from i3 4.12.

To assign a default workspace for spotify windows one cannot use the standard route with assign and should rather use a for_window command, such as

To ensure for_window does not move the window if already in $ws10, one can instead use move --no-auto-back-and-forth.

**Examples:**

Example 1 (unknown):
```unknown
i3-with-shmlog.desktop
```

Example 2 (unknown):
```unknown
KDEWM=/usr/bin/i3
```

Example 3 (unknown):
```unknown
~/.config/i3/config
```

Example 4 (unknown):
```unknown
set $menu --no-startup-id qdbus6 org.kde.krunner /App display
bindsym $mod+d exec $menu
```

---

## Language checking

**URL:** https://wiki.archlinux.org/title/Spell_checker

**Contents:**
- Spell checkers
  - Language-specific
  - Enchant
  - Applications
- Grammar checkers
  - Applications

This article covers spell checking and grammar checking software.

Except for special cases, all spell checkers have in common that they consist of two parts: the logic unit and the dictionary. Make sure to install both.

Enchant is a wrapper library for generic spell checking, developed as part of AbiWord, supporting all above spell checkers apart from Ispell.

Enchant is available as the enchant package. For its usage and ordering file, see enchant-2(1).

Enchant is used by many applications through the following GTK libraries:

Firefox, Thunderbird, Chromium and LibreOffice can all use system-wide installed Hunspell dictionaries as well as dictionaries/other spell checkers installed through their own extension systems. See the following sections:

Firefox, Thunderbird, Chromium and LibreOffice all support grammar checking only through extensions. For LibreOffice, see LibreOffice#Grammar checking with LanguageTool.

AbiWord has a built-in grammar checker, see AbiWord#Grammar checking.

---

## Power management/Suspend and hibernate

**URL:** https://wiki.archlinux.org/title/Suspend_and_hibernate

**Contents:**
- Kernel interface (swsusp)
- High level interface (systemd)
- Changing suspend method
- Hibernation
  - About swap partition/file size
  - Configure the initramfs
  - Pass hibernate location to initramfs
    - Manually specify hibernate location
      - Acquire swap file offset
  - Change the image compression algorithm for hibernation

There are multiple methods of suspending available, notably:

The kernel provides basic functionality, and some high level interfaces provide tweaks to handle problematic hardware drivers/kernel modules (e.g. video card re-initialization).

It is possible to directly inform the in-kernel software suspend code (swsusp) to enter a suspended state; the exact method and state depends on the level of hardware support. On modern kernels, writing appropriate strings to /sys/power/state is the primary mechanism to trigger this suspend.

See kernel documentation for details.

systemd provides native commands for suspend, hibernate and a hybrid suspend. This is the default interface used in Arch Linux.

systemctl suspend should work out of the box. For systemctl hibernate to work on your system you might need to follow the instructions at #Hibernation.

There are also two modes combining suspend and hibernate:

See #Sleep hooks for additional information on configuring suspend/hibernate hooks. Also see systemctl(1), systemd-sleep(8), and systemd.special(7).

On systems where S0ix suspension does not provide the same energy savings as the regular S3 sleep, or when conserving energy is preferred to a quick resume time, changing the default suspend method is possible.

Run the following command to see all suspend methods hardware advertises support for (current method is shown in square brackets[1]):

If your hardware does not advertise the deep sleep status, check first if your UEFI advertises some settings for it, generally under Power or Sleep state or similar wording, with options named Windows 10, Windows and Linux or S3/Modern standby support for S0ix, and Legacy, Linux, Linux S3 or S3 enabled for S3 sleep. Failing that, you can keep using s2idle, consider using hibernation or try to patch the DSDT tables (or find a patched version online).

Confirm that your hardware does not exhibit issues with S3 sleep by testing a few sleep cycles after changing the sleep method:

If no issues have been found, you can make the change permanent through the MemorySleepMode directive in systemd-sleep.conf(5):

or through the mem_sleep_default=deep kernel parameter.

In some opposite situations, faulty firmware advertises support for deep sleep, while only s2idle is supported. In this case, an alternative method for using s2idle is available through the SuspendState setting:

In order to use hibernation, you must create a swap partition or file, configure the initramfs so that the resume process will be initiated in early userspace, and specify the location of the swap space in a way that is available to the initramfs, e.g. HibernateLocation EFI variable defined by systemd or resume= kernel parameter. These three steps are described in detail below.

Even if your swap partition is smaller than RAM, you still have a good chance of hibernating successfully. See "image_size" in the kernel documentation for information on the image_size sysfs(5) pseudo-file.

You may either decrease the value of /sys/power/image_size to make the suspend image as small as possible (for small swap partitions), or increase it to possibly speed up the hibernation process. For systems with a large amount of RAM, smaller values may drastically increase the speed of resuming a hibernating system. systemd#systemd-tmpfiles - temporary files can be used to make this change persistent:

The suspend image cannot span multiple swap partitions and/or swap files. It must fully fit in one swap partition or one swap file.[2]

When the system hibernates, the memory image is dumped to the swap space, which also includes the state of mounted file systems. Therefore, the hibernate location must be made available to the initramfs, i.e. before the root file system is mounted for resuming from hibernate to work.

When the system is running on UEFI, systemd-sleep(8) will automatically pick a suitable swap space to hibernate into, and the information of the used swap space is stored in HibernateLocation EFI variable. Upon next boot, systemd-hibernate-resume(8) reads the location off the EFI variable and the system resumes. This means the following steps are not necessary unless the system is using legacy BIOS or you want to choose a different swap space from the automatically-selected one.

The kernel parameter resume=swap_device can be used, where swap_device follows the persistent block device naming. For example:

The kernel parameters will only take effect after rebooting. To hibernate right away, obtain the volume's major and minor device numbers from lsblk and echo them in format major:minor to /sys/power/resume.

For example, if the swap device is 8:3:

If using a swap file, additionally follow the procedures in #Acquire swap file offset.

When using a swap file for hibernation, the block device on which the file system lies should be specified in resume=, and additionally the physical offset of swap file must be specified through resume_offset= kernel parameter. [3]

On file systems other than Btrfs, the value of resume_offset= can be obtained by running filefrag -v swap_file. The output is in a table format and the required value is in the first row of the physical_offset column.

In the example the value of resume_offset= is the first 38912.

Alternatively, to directly acquire the offset value:

For Btrfs, do not try to use the filefrag tool, since the "physical" offset you get from filefrag is not the real physical offset on disk; there is a virtual disk address space in order to support multiple devices.[4] Instead, use the btrfs-inspect-internal(8) command. E.g.:

In this example, the kernel parameter would be resume_offset=198122980.

To apply the change immediately (without rebooting), echo the resume offset to /sys/power/resume_offset. For example, if the offset is 38912:

Starting with Linux 6.9[5], the image compression algorithm for hibernation can be changed. The default compression algorithm is selected based on the compile time option CONFIG_HIBERNATION_DEF_COMP, but it can be overridden at boot time and runtime.

Different compression algorithms have different characteristics and hibernation may benefit when it uses any of these algorithms, especially when a secondary algorithm (LZ4) offers better decompression speeds over a default algorithm (LZO), which in turn reduces hibernation image restore time.

You can override the default algorithm in two ways:

1) Passing hibernate.compressor as a kernel parameter:

2) Specifying the algorithm at runtime:

Currently lzo and lz4 are the supported algorithms with LZO being the default.

It is possible to solve the hibernation problem with zram RAM-only swap by maintaining two or more swap spaces at the same time. systemd will always ignore zram block devices before triggering hibernation [6], therefore keeping both spaces enabled should work without further intervention.

After configuring the swap file, follow the zram page. Make sure zram has the higher swap priority (e.g. pri=100).

Hibernation into a thinly-provisioned LVM volume is possible, but you have to make sure that the volume is fully allocated. Otherwise resuming from it will fail, see FS#50703.

You can fully allocate the LVM volume by simply filling it with zeros. E.g.:

To verify the volume is fully allocated, you can use:

A fully allocated volume will show up as having 100% data usage.

In Linux 6.8, zswap gained a per-cgroup option to disable writeback. By using systemd unit setting MemoryZSwapWriteback (see systemd.resource-control(5)  Memory Accounting and Control) in all possible unit types, zswap writeback can be effectively disabled entirely. This allows to use zswap just like zram with the added benefit of supporting hibernation.

To avoid having to manually create twelve top level per-type drop-in files (for system and user scope, service, slice, socket, mount, swap units types), install zswap-disable-writebackAUR. Enable zswap and reboot for the settings to take effect.

Try to perform memory intensive tasks and confirm that zswap has not written anything to disk:

systemd starts suspend.target, hibernate.target, hybrid-sleep.target, or suspend-then-hibernate.target for each sleep state, respectively. All the aforementioned targets pull in sleep.target. Any of the targets can be used to invoke custom units before or after suspend/hibernate. Separate files should be created for user actions and root/system actions. Examples:

Enable user-suspend@user.service and/or user-resume@user.service for the change to take effect.

For root/system actions:

With the combined unit file, a single hook does all the work for different phases (sleep/resume) and for different targets.

Example and explanation:

systemd-sleep runs all executables in /usr/lib/systemd/system-sleep/, passing two arguments to each of them:

An environment variable called SYSTEMD_SLEEP_ACTION will be set and contain the sleep action that is processing. This is primarily helpful for suspend-then-hibernate where the value of the variable will be suspend, hibernate, or suspend-after-failed-hibernate in cases where hibernation has failed.

The output of any custom script will be logged by systemd-suspend.service, systemd-hibernate.service or systemd-hybrid-sleep.service. You can see its output in systemd's journalctl:

An example of a custom sleep script:

Do not forget to make your script executable.

When resuming, you can automatically unlock your system if it is connected to certain devices or trusted Wi-Fi networks.

Configure your desktop environment so that it locks on resume, and then create a sleep hook that runs the above script after resuming. You also need to install wireless_tools to read the connected Wi-Fi SSID. If you also want to test for connected USB devices, uncomment the lsusb -d ... line in the script and fill in the ID of your trusted device. You can get the ID of your device by running lsusb.

When using a device as e.g a server, suspending/hibernating might not be needed or it could even be undesired. Each sleep state can be disabled through systemd-sleep.conf(5):

Intel Rapid Start Technology is a firmware method of hibernation that allows hibernating from sleep after a predefined interval or according to battery state. This should be faster and more reliable than regular hibernation as it is done by firmware instead of at the operating system level. Generally it must enabled in the firmware, and the firmware also provides support for setting the duration after suspend/battery event triggering hibernation. However, some devicesdespite supporting IRST in the firmwareonly allow it to be configured via Intel's Windows drivers. In such cases the intel-rst kernel module described below should be able to configure the events under Linux.

With Intel Rapid Start Technology (IRST) enabled, resuming from a deep sleep takes "a few seconds longer than resuming from S3 but is far faster than resuming from hibernation".

Many Intel-based systems have firmware support for IRST but require a special partition on an SSD (rather than an HDD). OEM deployments of Windows may have a pre-existing IRST partition which can be retained during the Arch Linux installation process (rather than wiping and re-partitioning the whole SSD). It should show up as an unformatted partition equal in size to the system's RAM.

If you intend to wipe and re-partition the whole drive (or have already done so), then the IRST partition must be recreated if you also plan on using the technology. This can be done by creating an empty partition equal in size to the system's RAM and by setting its partition type to GUID D3BFE2DE-3DAF-11DF-BA40-E3A556D89593 for a GPT partition or ID 0x84 for an MBR partition. You may also need to enable support for IRST in your system's firmware settings.

The duration of the IRST hibernation process (i.e., copying the "entire contents of RAM to a special partition") is dependent on the system's RAM size and SSD speed and can thus take 2060 seconds. Some systems may indicate the process's completion with an LED indicator, e.g., when it stops blinking.

Configuring IRST hibernation events in the Linux kernel requires CONFIG_INTEL_RST built-in or as a module. Once loaded via modprobe intel_rst, it should create the files wakeup_events and wakeup_time under /sys/bus/acpi/drivers/intel_rapid_start/*/ that can be used for further configuration. This module is tersely documented, see the source drivers/platform/x86/intel/rst.c for more details.

See also the general Q&A and user guides for Intel Rapid Start Technology.

To measure power consumption in suspend states use Batenergy script to log battery changes to the system journal. This allows to compare power consumption in S3 / S0x states or check after BIOS and kernel updates for regressions and fixes. The script needs bc to be installed for calculation.

You might want to tweak your DSDT table to make it work. See DSDT.

The factual accuracy of this article or section is disputed.

There have been many reports about the screen going black without easily viewable errors or the ability to do anything when going into and coming back from suspend and/or hibernate. These problems have been seen on both laptops and desktops. This is not an official solution, but switching to an older kernel, especially the LTS-kernel, will probably fix this.

A problem may arise when using the hardware watchdog timer (disabled by default, see RuntimeWatchdogSec= in systemd-system.conf(5)  OPTIONS). A buggy watchdog timer may reset the computer before the system finishes creating the hibernation image.

Sometimes the screen goes black due to device initialization from within the initramfs. Removing any modules you might have in Mkinitcpio#MODULES, removing the kms hook and rebuilding the initramfs can possibly solve this issue, in particular with graphics drivers for early KMS. Initializing such devices before resuming can cause inconsistencies that prevents the system resuming from hibernation. This does not affect resuming from RAM. Also, check the blog article best practices to debug suspend issues.

Moving from the ATI video driver to the newer AMDGPU driver could also help to make the hibernation and awakening process successful.

With NVIDIA cards, the VRAM contents are saved to disk when suspending.[8] Make sure that you have enough disk space, otherwise you might get a blank screen when resuming. Another cause for this could be fixed by blacklisting the module nvidiafb. [9]

Laptops with an Intel CPU that load the intel_lpss_pci module for a touchpad may face kernel panic on resume (blinking caps lock) [10]. The module needs to be added to initramfs as:

Then regenerate the initramfs.

System may fail to suspend because of a USB device. You might see the following error:

lspci may give you more information on the failing device:

Try disconnecting devices on that port.

If Wake-on-LAN is active, the network interface card will consume power even if the computer is hibernated.

See Wakeup triggers#Instantaneous wakeup after suspending.

When you hibernate your system, the system should power off (after saving the state on the disk). On some firmware the S4 sleeping state does not work reliably. For example, instead of powering off, the system might reboot or stay on but unresponsive. If that happens, it might be instructive to set the HibernateMode to shutdown in sleep.conf.d(5):

With the above configuration, if everything else is set up correctly, on invocation of a systemctl hibernate the machine will shut down, saving state to disk as it does so.

This can happen when the boot disk is an external disk, and seems to be caused by a BIOS/firmware limitation. The BIOS/firmware tries to boot from an internal disk, while hibernation was done from an OS on an external (or other) disk.

Set HibernateMode=shutdown as shown in #System does not power off when hibernating to solve the problem permanently. If you have already locked yourself out, you can try rebooting your system 4 times (wait for the error to appear each time), which on some BIOS'es forces a normal boot procedure.

If the swap file is in /home/, systemd-logind will not be able to access it, giving the Call to Hibernate failed: No such file or directory warning message and resulting in a need for authentication on systemctl hibernate. This setup should be avoided, as it is considered unsupported upstream. See systemd issue 15354 for two workarounds.

On some motherboards with A520i and B550i chipsets, the system will not completely enter the sleep state or come out of it. Symptoms include the system entering sleep and the monitor turning off while internal LEDs on the motherboard or the power LED stay on. Subsequently, the system will not come back from this state and require a hard power off. If you have similar issues with AMD, first make sure your system is fully updated and check whether the AMD microcode package is installed.

Verify the line starting with GPP0 has the enabled status:

If that is enabled, you can run the following command to disable it:

Now test by running systemctl suspend and let the system go to sleep. Then try to wake the system after a few seconds. If it works, you can make the workaround permanent. Create a systemd unit file:

Do a daemon-reload and start/enable the newly created unit.

Alternatively, you can create a udev rule. Assuming GPP0s sysfs node is pci:0000:00:01.1 like in the example, run udevadm info -a -p /sys/bus/pci/devices/0000\:00\:01.1 to get the relevant information and create a udev rule like this one:

The udev daemon is already watching for changes in your system by default. If needed you can reload the rules manually.

If, regardless of the setting in logind.conf, the sleep button does not work (pressing it does not even produce a message in syslog), then logind is probably not watching the keyboard device. [11] Do:

You might see something like this:

Notice no keyboard device. List keyboard devices as follows:

Now obtain ATTRS{name} for the parent keyboard device [12]. As an example, on the above list this keyboard device has event6 as device input event, it can be used to search its respective attribute name:

Now write a custom udev rule to add the "power-switch" tag:

After reloading the udev rules and restarting systemd-logind.service, you should see Watching system buttons on /dev/input/event6 in the journal of logind.

Since systemd v256, systemd freezes user.slice before sleeping. This process can fail due to kernel bugs, particularly when KVM is in use.[13][14]

Messages in the logs will contain Failed to freeze unit 'user.slice' before sleep. When such an issue occurs, trying to login (start another session) would fail with:

To temporarily revert back to the old behavior, edit systemd-suspend.service, systemd-hibernate.service, systemd-hybrid-sleep.service, and systemd-suspend-then-hibernate.service with the following drop-in:

However, this drop-in can itself prevent the system from going to sleep.[15]

If you are running a multi boot system (including but not limited to dual boot with Windows) and want to be able to boot into your other system while your main Arch Linux is hibernated, you must take extra caution not to mount filesystems that are still in use by the hibernated system. Before attempting to mount such filesystem within another system, you must make sure to unmount this filesystem before hibernating the system. This can be achieved with sleep hooks.

This issue is particularly relevant for the EFI system partition, because the ESP is expected to be shared across multiple systems. Check the matching section in EFI system partition for mitigation strategies, which can be adapted to other filesystems as well.

**Examples:**

Example 1 (unknown):
```unknown
/sys/power/state
```

Example 2 (unknown):
```unknown
systemctl suspend
```

Example 3 (unknown):
```unknown
systemctl hibernate
```

Example 4 (unknown):
```unknown
systemctl hybrid-sleep
```

---

## GStreamer

**URL:** https://wiki.archlinux.org/title/GStreamer

**Contents:**
- Installation
- Usage
  - Using gst-launch-1.0
  - Using gst-discoverer-1.0
- Integration
  - PulseAudio
  - PipeWire
  - KDE / Phonon integration
  - Hardware video acceleration
    - Verify VA-API support

Install the gstreamer package.

To make GStreamer useful, install the plugins packages you require. See official documentation for list of features in each plugin.

A helpful tool of GStreamer is the gst-launch-1.0(1) command. It is an extremely versatile command line tool to create GStreamer pipelines. It is very similar to and can do many of the things the FFmpeg command can do. Here are some examples:

Convert an MP4 file to MKV:

Another helpful tool is gst-discoverer-1.0(1), which is the GStreamer equivalent of FFmpeg's ffprobe(1).

Get info on a video file:

PulseAudio support is provided by the gst-plugins-good package.

PipeWire support is provided by the gst-plugin-pipewire package.

See Hardware video acceleration.

GStreamer will automatically detect and use the correct API [1]. Depending on the system install:

If the new elements do not show up after installing the packages, you may want to delete and rebuild the plugin registry. Usually it suffices to

Gstreamer will then rebuild the registry on the next invocation, which usually takes a few seconds.

To verify VA-API support:

To verify NVDECODE/NVENCODE support:

For some NVIDIA users, gst-libav may prioritize the Libav decoder over nvcodec decoders which will inhibit hardware acceleration. The GST_PLUGIN_FEATURE_RANK environment variable can be used to rank decoders and thus alleviate this issue. See "GST_PLUGIN_FEATURE_RANK" in the documentation for more information. For example:

Those without AV1 hardware support may also want to disable AV1 decoders (e.g., for YouTube on webkit2gtk-based browsers) by appending avdec_av1:NONE and av1dec:NONE to the list above.

**Examples:**

Example 1 (unknown):
```unknown
$ gst-launch-1.0 filesrc location=source.mp4! qtdemux name=demux matroskamux name=mux! filesink location=dest.mkv  demux.audio_0! queue! aacparse! queue! mux.audio_0  demux.video_0! queue! h264parse! queue! mux.video_0
```

Example 2 (unknown):
```unknown
$ gst-discoverer-1.0 file.mp4
```

Example 3 (unknown):
```unknown
Properties:
  Duration: 0:02:55.613000000
  Seekable: yes
  Live: no
  container: Quicktime
    audio: MPEG-4 AAC
      Stream ID: c910ef2fa357f9f4ad365aebc98cfca88d23fdca99d832645f5113efa43b0cd3/002
      Language: <unknown>
      Channels: 2 (front-left, front-right)
      Sample rate: 44100
      Depth: 16
      Bitrate: 125588
      Max bitrate: 125588
    video: H.264 (Constrained Baseline Profile)
      Stream ID: c910ef2fa357f9f4ad365aebc98cfca88d23fdca99d832645f5113efa43b0cd3/001
      Width: 192
      Height: 144
      Depth: 24
      Frame rate: 15000/1001
      Pixel aspect ratio: 1/1
      Interlaced: false
      Bitrate: 107884
      Max bitrate: 107884
```

Example 4 (unknown):
```unknown
$ rm ~/.cache/gstreamer-1.0/registry.*.bin
```

---

## File permissions and attributes

**URL:** https://wiki.archlinux.org/title/File_attribute

**Contents:**
- Viewing permissions
  - Examples
- Changing permissions
  - Text method
    - Text method shortcuts
    - Copying permissions
  - Numeric method
  - Bulk chmod
- Changing ownership
- Access Control Lists

File systems use permissions and attributes to regulate the level of interaction that system processes can have with files and directories.

Use the ls command's -l option to view the permissions (or file mode) set for the contents of a directory, for example:

The first column is what we must focus on. Taking an example value of drwxrwxrwx+, the meaning of each character is explained in the following tables:

Each of the three permission triads (rwx in the example above) can be made up of the following characters:

See info Coreutils -n "Mode Structure" and chmod(1) for more details.

Let us see some examples to clarify:

Archie has full access to the Documents directory. They can list, create files and rename, delete any file in Documents, regardless of file permissions. Their ability to access a file depends on the file's permissions.

Archie has full access except they can not create, rename, delete any file. They can list the files and (if the file's permissions allow it) may access an existing file in Documents.

Archie can not do ls in the Documents directory but if they know the name of an existing file then they may list, rename, delete or (if the file's permissions allow it) access it. Also, they are able to create new files.

Archie is only capable of (if the file's permissions allow it) accessing those files the Documents directory which they know of. They can not list already existing files or create, rename, delete any of them.

You should keep in mind that we elaborate on directory permissions and it has nothing to do with the individual file permissions. When you create a new file it is the directory that changes. That is why you need write permission to the directory.

Let us look at another example, this time of a file, not a directory:

Here we can see the first letter is not d but -. So we know it is a file, not a directory. Next the owner's permissions are rw- so the owner has the ability to read and write but not execute. This may seem odd that the owner does not have all three permissions, but the x permission is not needed as it is a text/data file, to be read by a text editor such as Gedit, EMACS, or software like R, and not an executable in its own right (if it contained something like python programming code then it very well could be). The group's permissions are set to r--, so the group has the ability to read the file but not write/edit it in any way  it is essentially like setting something to read-only. We can see that the same permissions apply to everyone else as well.

chmod is a command in Linux and other Unix-like operating systems that allows to change the permissions (or access mode) of a file or directory.

To change the permissions  or access mode  of a file, use the chmod command in a terminal. Below is the command's general structure:

Where who is any from a range of letters, each signifying who is being given the permission. They are as follows:

The permissions are the same as discussed in #Viewing permissions (r, w and x).

Now have a look at some examples using this command. Suppose you became very protective of the Documents directory and wanted to deny everybody but yourself, permissions to read, write, and execute (or in this case search/look) in it:

Before: drwxr-xr-x 6 archie web 4096 Jul 5 17:37 Documents

After: drwx------ 6 archie web 4096 Jul 6 17:32 Documents

Here, because you want to deny permissions, you do not put any letters after the = where permissions would be entered. Now you can see that only the owner's permissions are rwx and all other permissions are -.

This can be reverted with:

Before: drwx------ 6 archie web 4096 Jul 6 17:32 Documents

After: drwxr-xr-x 6 archie web 4096 Jul 6 17:32 Documents

In the next example, you want to grant read and execute permissions to the group, and other users, so you put the letters for the permissions (r and x) after the =, with no spaces.

You can simplify this to put more than one who letter in the same command, e.g:

Now let us consider a second example, suppose you want to change a foobar file so that you have read and write permissions, and fellow users in the group web who may be colleagues working on foobar, can also read and write to it, but other users can only read it:

Before: -rw-r--r-- 1 archie web 5120 Jun 27 08:28 foobar

After: -rw-rw-r-- 1 archie web 5120 Jun 27 08:28 foobar

This is exactly like the first example, but with a file, not a directory, and you grant write permission (just so as to give an example of granting every permission).

The chmod command lets add and subtract permissions from an existing set using + or - instead of =. This is different from the above commands, which essentially re-write the permissions (e.g. to change a permission from r-- to rw-, you still need to include r as well as w after the = in the chmod command invocation. If you missed out r, it would take away the r permission as they are being re-written with the =. Using + and - avoids this by adding or taking away from the current set of permissions).

Let us try this + and - method with the previous example of adding write permissions to the group:

Before: -rw-r--r-- 1 archie web 5120 Jun 27 08:28 foobar

After: -rw-rw-r-- 1 archie web 5120 Jun 27 08:28 foobar

Another example, denying write permissions to all (a):

Before: -rw-rw-r-- 1 archie web 5120 Jun 27 08:28 foobar

After: -r--r--r-- 1 archie web 5120 Jun 27 08:28 foobar

A different shortcut is the special X mode: this is not an actual file mode, but it is often used in conjunction with the -R option to set the executable bit only for directories, and leave it unchanged for regular files, for example:

It is possible to tell chmod to copy the permissions from one class, say the owner, and give those same permissions to group or even all. To do this, instead of putting r, w, or x after the =, put another who letter. e.g:

Before: -rw-r--r-- 1 archie web 5120 Jun 27 08:28 foobar

After: -rw-rw-r-- 1 archie web 5120 Jun 27 08:28 foobar

This command essentially translates to "change the permissions of group (g=), to be the same as the owning user (=u). Note that you cannot copy a set of permissions as well as grant new ones e.g.:

In that case chmod throw an error.

chmod can also set permissions using numbers.

Using numbers is another method which allows you to edit the permissions for all three owner, group, and others at the same time, as well as the setuid, setgid, and sticky bits. This basic structure of the code is this:

Where xxx is a 3-digit number where each digit can be anything from 0 to 7. The first digit applies to permissions for owner, the second digit applies to permissions for group, and the third digit applies to permissions for all others.

In this number notation, the values r, w, and x have their own number value:

To come up with a 3-digit number you need to consider what permissions you want owner, group, and all others to have, and then total their values up. For example, if you want to grant the owner of a directory read write and execution permissions, and you want group and everyone else to have just read and execute permissions, you would come up with the numerical values like so:

This is the equivalent of using the following:

To view the existing permissions of a file or directory in numeric form, use the stat(1) command:

Where the %a option specifies output in numeric form.

Most directories are set to 755 to allow reading, writing and execution to the owner, but deny writing to everyone else, and files are normally 644 to allow reading and writing for the owner but just reading for everyone else; refer to the last note on the lack of x permissions with non executable files: it is the same thing here.

To see this in action with examples consider the previous example that has been used but with this numerical method applied instead:

Before: -rw-r--r-- 1 archie web 5120 Jun 27 08:28 foobar

After: -rw-rw-r-- 1 archie web 5120 Jun 27 08:28 foobar

If this were an executable the number would be 774 if you wanted to grant executable permission to the owner and group. Alternatively if you wanted everyone to only have read permission the number would be 444. Treating r as 4, w as 2, and x as 1 is probably the easiest way to work out the numerical values for using chmod xxx filename, but there is also a binary method, where each permission has a binary number, and then that is in turn converted to a number. It is a bit more convoluted, but here included for completeness.

Consider this permission set:

If you put a 1 under each permission granted, and a 0 for every one not granted, the result would be something like this:

You can then convert these binary numbers:

The value of the above would therefore be 775.

Consider we wanted to remove the writable permission from group:

The value would therefore be 755 and you would use chmod 755 filename to remove the writable permission. You will notice you get the same three digit number no matter which method you use. Whether you use text or numbers will depend on personal preference and typing speed. When you want to restore a directory or file to default permissions e.g. read and write (and execute) permission to the owner but deny write permission to everyone else, it may be faster to use chmod 755/644 filename. However if you are changing the permissions to something out of the norm, it may be simpler and quicker to use the text method as opposed to trying to convert it to numbers, which may lead to a mistake. It could be argued that there is not any real significant difference in the speed of either method for a user that only needs to use chmod on occasion.

You can also use the numeric method to set the setuid, setgid, and sticky bits by using four digits.

For example, chmod 2777 filename will set read/write/executable bits for everyone and also enable the setgid bit.

Generally directories and files should not have the same permissions. If it is necessary to bulk modify a directory tree, use find to selectively modify one or the other.

To chmod only directories to 755:

To chmod only files to 644:

chown changes the owner of a file or directory, which is quicker and easier than altering the permissions in some cases.

Consider the following example, making a new partition with GParted for backup data. Gparted does this all as root so everything belongs to root by default. This is all well and good but when it comes to writing data to the mounted partition, permission is denied for regular users.

As you can see the device in /dev is owned by root, as is the mount location (/media/Backup). To change the owner of the mount location one can do the following:

Before: drwxr-xr-x 5 root root 4096 Jul 6 16:01 Backup

After: drwxr-xr-x 5 archie root 4096 Jul 6 16:01 Backup

Now the partition can have data written to it by the new owner, archie, without altering the permissions (as the owner triad already had rwx permissions).

Access Control Lists provides an additional, more flexible permission mechanism for file systems by allowing to set permissions for any user or group to any file.

The umask utility is used to control the file-creation mode mask, which determines the initial value of file permission bits for newly created files.

Apart from the file mode bits that control user and group read, write and execute permissions, several file systems support file attributes that enable further customization of allowable file operations.

The e2fsprogs package contains the programs lsattr(1) and chattr(1) that list and change a file's attributes, respectively.

These are a few useful attributes. Not all filesystems support every attribute.

See chattr(1) for a complete list of attributes and for more info on what each attribute does.

For example, if you want to set the immutable bit on some file, use the following command:

To remove an attribute on a file just change + to -.

See Extended attributes.

Use the --preserve-root flag to prevent chmod from acting recursively on /. This can, for example, prevent one from removing the executable bit systemwide and thus breaking the system. To use this flag every time, set it within an alias. See also [1].

**Examples:**

Example 1 (unknown):
```unknown
$ ls -l /path/to/directory
```

Example 2 (unknown):
```unknown
total 128
drwxr-xr-x 2 archie archie  4096 Jul  5 21:03 Desktop
drwxr-xr-x 6 archie archie  4096 Jul  5 17:37 Documents
drwxr-xr-x 2 archie archie  4096 Jul  5 13:45 Downloads
-rw-rw-r-- 1 archie archie  5120 Jun 27 08:28 customers.ods
-rw-r--r-- 1 archie archie  3339 Jun 27 08:28 todo
-rwxr-xr-x 1 archie archie  2048 Jul  6 12:56 myscript.sh
```

Example 3 (unknown):
```unknown
drwxrwxrwx+
```

Example 4 (unknown):
```unknown
info ls -n "What information is listed"
```

---

## Extended attributes

**URL:** https://wiki.archlinux.org/title/Xattrs

**Contents:**
- User extended attributes
- Preserving extended attributes
- Support
  - File systems
  - Software
- Other tagging systems
  - gvfs
- See also

From xattr(7): "Extended attributes are name:value pairs associated permanently with files and directories". There are four extended attribute classes: security, system, trusted and user.

Extended attributes are also used to set Capabilities.

User extended attributes can be used to store arbitrary information about a file. To create one:

Use getfattr to display extended attributes:

To remove an extended attribute:

To find files with certain extended attributes use rawhideAUR:

Some other user extended attributes include:

XDG also proposes a set of standardized extended attributes to be used by programs:

user.xdg.tags is not part of the official standard, but it has become a "de facto" standard as several popular programs have implemented support for it (see #Software). It is implemented as a CSV list of user-specified tags for each file.

To preserve extended attributes with text editors you need to configure them to truncate files on saving instead of using rename(2). [1]

Just like you should do for any other data you do not want to lose, you should make regular backups of your extended attributes. To make a full backup of the extended attributes of all files in the current directory (recursively):

All major Linux file systems including Ext4, Btrfs, ZFS, and XFS support extended attributes. The kernel allows to have extended attribute names of up to 255 bytes and values of up to 64 KiB, but Ext4 and Btrfs might impose smaller limits, requiring extended attributes to be within a "filesystem block".

NTFS uses Alternative Data Streams to store user. The mount option user_xattr or streams_interface=xattr should be used by default. However, it might not be supported if mount option streams_interface=windows is used. ntfs-3g supports mapping Alternative Data Streams to extended attributes in FUSE.

NFS supports extended attributes since Linux 5.9.

It might not be possible to use extended attributes due to lack of support of either the file system or software. For this reason, many media formats store metadata included in the file format that can be viewed using programs like Exiftool or more specified ones like id3AUR for audio.

Another filesystem-independent workaround is Gnome virtual filesystem: gvfs which is used to store metadata (gvfsd-metadata). For example, Firefox stores metadata this way and can be viewed with:

Other programs that use this approach include:

**Examples:**

Example 1 (unknown):
```unknown
$ setfattr --name=user.checksum --value="3baf9ebce4c664ca8d9e5f6314fb47fb" file.txt
```

Example 2 (unknown):
```unknown
$ getfattr --encoding=text --dump file.txt
```

Example 3 (unknown):
```unknown
# file: file.txt
user.checksum="3baf9ebce4c664ca8d9e5f6314fb47fb"
```

Example 4 (unknown):
```unknown
$ setfattr --remove=user.checksum file.txt
```

---

## GIMP

**URL:** https://wiki.archlinux.org/title/GIMP

**Contents:**
- Installation
- Tips and tricks
  - Captions
  - Create a circle
  - Photoshop behaviour
- Plugins
  - Manual installation
  - Packaging
- Troubleshooting
  - Green text

GIMP is a powerful raster image editing program, and commonly used for photo retouching, image composition, and general graphic design work. It can be used as a simple paint program, an expert quality photo retouching program, an online batch processing system, a mass production image renderer, an image format converter, etc.

Install the gimp package.

To caption an image follow these steps after inputting the desired text:

To draw a circle in GIMP follow these steps:

Since GIMP is highly configurable, it is possible to change the keybinds, and even the UI, to be more familiar to those who are used to the raster image editing program Photoshop.

Specifically the GimpPs theme aims to make GIMP behave more like Photoshop, which you can install on top of GIMP.

Alternately, if you just want the keybinds, the relevant lines are included in the theme's menurc file, which you can then add to your local ~/.config/GIMP/2.10/menurc.

There is also the PhotoGIMP project, aiming to do to the same.

GIMP has an extensive plugin system, as well as a Scheme interpreter which can be used to write Script-Fu scripts.

Most plugins are distributed via the official repositories (for example, gimp-plugin-gmic) and the AUR; some, but not all, are listed as optional dependencies of the gimp package.

If not distributed via the repositories, plugins can be compiled and installed using the gimptool executable.

To install a plugin from source:

To install a pre-compiled plugin:

To install a Script-Fu script:

Further options may be found in the gimptool --help message.

Plugin packages should add their files to a directory with their name within /usr/lib/gimp/2.0/plug-ins/.

Add the following to ~/.config/GIMP/2.10/fonts.conf if you see a green tint around letters with antialiasing enabled:

Add the following to ~/.config/GIMP/2.10/fonts.conf if you have noto-fonts installed and would like to remove them from GIMP's fonts list:

See fonts-conf(5) and Font configuration#Whitelisting and blacklisting fonts for more information.

GIMP requires the poppler-glib library to open PDF files or it will report that they are unrecognised.

Since GIMP rasterizes PDF files right from the start, it will not exploit intrinsic PDF capabilities while editing them. Other programs (like LibreOffice Draw) can be used to better edit PDF files.

**Examples:**

Example 1 (unknown):
```unknown
~/.config/GIMP/2.10/menurc
```

Example 2 (unknown):
```unknown
$ gimptool --install source.c
```

Example 3 (unknown):
```unknown
$ gimptool --install-bin executable
```

Example 4 (unknown):
```unknown
$ gimptool --install-script script.scm
```

---

## USB storage devices

**URL:** https://wiki.archlinux.org/title/USB_storage_devices

**Contents:**
- Auto-mounting with udisks
- Manual mounting
  - Getting a kernel that supports usb_storage
  - Identifying device
  - Mounting USB memory
    - Allow writing by regular users
    - As normal user with fstab
    - Mount tools
- Troubleshooting
  - No USB storage devices are detected

This document describes how to use the popular USB memory sticks with Linux. However, it is also valid for other devices such as digital cameras that act as if they were just a USB storage device.

If you have an up-to-date system with the standard Arch kernel and a modern desktop environment, your device should just show up on your desktop, with no need to open a console.

This is the easiest and most frequently used method. It is used by many desktop environments, but can be used separately too.

See Udisks for detailed information, including a list of mount helpers.

If you do not use a custom-made kernel, you are ready to go; all Arch Linux stock kernels are properly configured. If you do use a custom-made kernel, ensure it is compiled with SCSI-Support, SCSI-Disk-Support and usb_storage. If you use the latest udev, you may just plug your device in and the system will automatically load all necessary kernel modules.

The first thing one needs to access a storage device is its identifier assigned by kernel. See File systems#Identify existing file systems for details.

Newly plugged-in devices are usually shown in the journal.

See File systems#Mount a file system.

If mount does not recognize the file system of the device you can try to use the -t argument, see mount(8) for details. If mounting does not work, you can try to recreate the file system or even repartition the disk.

If you want non-root users to be able to write to the USB stick, you can issue the following command:

If it does not work, make sure that the file system is mountable and writable as root, see the previous section for details.

See FAT#Writing to FAT32 as normal user if you want normal user to do the mount/unmount action.

Multiple mount tools facilitate mounting as a regular user.

If you have connected your USB storage device and it is not listed by lsblk but appears in the journal without being assigned a block device, see General troubleshooting#Cannot use some peripherals after kernel upgrade.

Also ensure that your BIOS has both XHCI Handoff and EHCI Handoff enabled, but this is usually not an issue with most modern devices.

This article or section needs expansion.

Failure to power off a device might result in:

When you unmount the partitions, the device is still powered on. You should ask the system to turn it off first in order to safely remove it: [4]

If you use udisks, you can use these commands: [5]

udev is shipped with a default set of rules, found in /usr/lib/udev/rules.d/, including one for ignoring some specific devices for various reasons. Some hardware devices, such as digital cameras, portable recorders, etc., may format usb storage in a way that results in the ignore rules to be triggered. You can check if one of these rules was applied with the following command and then find the corresponding conditions in the defaults:

If this is the case and your device has this property set to "1", you can override it with a custom rule, following udev#Example.

**Examples:**

Example 1 (unknown):
```unknown
# mount -o gid=users,fmask=113,dmask=002 /dev/sda1 /mnt/usbstick
```

Example 2 (unknown):
```unknown
# echo 1 > /sys/block/disk_name/device/delete
```

Example 3 (unknown):
```unknown
$ udisksctl unmount -b /dev/sdXY
$ udisksctl power-off -b /dev/sdX
```

Example 4 (unknown):
```unknown
/usr/lib/udev/rules.d/
```

---

## Parted

**URL:** https://wiki.archlinux.org/title/GNU_Parted

**Contents:**
- Installation
- Usage
  - Command line mode
  - Interactive mode
- Rounding
- Partitioning
  - Create new partition table
  - Partition schemes
    - UEFI/GPT examples
    - BIOS/MBR examples

GNU Parted is a program for creating and manipulating partition tables. GParted is a GUI frontend.

Install one of the following packages:

Parted has two modes: command line and interactive. Parted should always be started with:

where device is block device name like /dev/sda, /dev/nvme0n1, /dev/mmcblk0, etc. If you omit the device argument, parted will attempt to guess which device you want.

In command line mode, this is followed by one or more commands. For example:

Interactive mode simplifies the partitioning process and reduces unnecessary repetition by automatically applying all partitioning commands to the specified device.

In order to start operating on a device, execute:

You will notice that the command-line prompt changes from a hash (#) to (parted): this also means that the new prompt is not a command to be manually entered when running the commands in the examples.

To see a list of the available commands, enter:

When finished, or if wishing to implement a partition table or scheme for another device, exit from parted with:

After exiting, the command-line prompt will change back to #.

If you do not give a parameter to a command, Parted will prompt you for it. For example:

Since many partitioning systems have complicated constraints, Parted will usually do something slightly different to what you asked. (For example, create a partition starting at 10.352Mb, not 10.4Mb) If the calculated values differ too much, Parted will ask you for confirmation. If you know exactly what you want, or to see exactly what Parted is doing, it helps to specify partition endpoints in sectors (with the "s" suffix) and give the "unit s" command so that the partition endpoints are displayed in sectors.

As of parted-2.4, when you specify start and/or end values using IEC binary units like MiB, GiB, TiB, etc., parted treats those values as exact, and equivalent to the same number specified in bytes (i.e., with the B suffix), in that it provides no helpful range of sloppiness. Contrast that with a partition start request of 4GB, which may actually resolve to some sector up to 500MB before or after that point. Thus, when creating a partition, you should prefer to specify units of bytes (B), sectors (s), or IEC binary units like MiB, but not MB, GB, etc.

You need to (re)create the partition table of a device when it has never been partitioned before, or when you want to change the type of its partition table. Recreating the partition table of a device is also useful when the partition scheme needs to be restructured from scratch.

Open each device whose partition table must be (re)created with:

To then create a new GUID Partition Table, use the following command:

To create a new Master Boot Record/MS-DOS partition table instead, use:

This article or section needs expansion.

You can decide the number and size of the partitions the devices should be split into, and which directories will be used to mount the partitions in the installed system (also known as mount points). See Partitioning#Partition scheme for the required partitions.

The following command will be used to create partitions:

The following command will be used to flag the partition that contains the /boot directory as bootable:

This article or section needs expansion.

In every instance, a special bootable EFI system partition is required.

If creating a new EFI system partition, use the following commands (the recommended size is at least 1 GiB):

The remaining partition scheme is entirely up to you. For one root partition using 100% of remaining space:

For separate swap (4 GiB) and / (all remaining space) partitions:

And for separate swap (4 GiB), / (32 GiB) and /home (all remaining space) partitions:

For a minimum single primary partition using all available disk space, the following command would be used:

In the following instance, a 4 GiB swap partition will be created, followed by a / partition using all the remaining space:

In the final example below, separate /boot (1 GiB), swap (4 GiB), / (32 GiB), and /home (all remaining space) partitions will be created:

If you are growing a partition, you have to first resize the partition and then resize the filesystem on it, while for shrinking the filesystem must be resized before the partition to avoid data loss.

To grow a partition (in parted interactive mode):

Where number is the number of the partition you are growing, and end is the new end of the partition (which needs to be larger than the old end).

Then, to grow the (ext2/3/4) filesystem on the partition (if size is not specified, it will default to the size of the partition):

Or to grow a Btrfs filesystem:

Where /path/to/mount/point stands for the mount point of the partition you are growing, and size in the form 16G or +1G is the new size or modification of the partition. Use max to fill the remaining space on the partition.

To shrink an ext2/3/4 filesystem on the partition:

To shrink a Btrfs filesystem:

Where /path/to/mount/point stands for the mount point of the partition you are shrinking, and size is the new size of the partition.

Then shrink the partition (in parted interactive mode):

Where number is the number of the partition you are shrinking, and end is the new end of the partition (which needs to be smaller than the old end).

When done, use the resizepart command from util-linux to tell the kernel about the new size:

Where device is the device that holds the partition, number is the number of the partition and size is the new size of the partition, in 512-byte sectors.

Parted will always warn you before doing something that is potentially dangerous, unless the command is one of those that is inherently dangerous (e.g. rm, mklabel and mkpart).

When creating a partition, parted might warn about improper partition alignment but does not hint about proper alignment. For example:

The warning means the partition start is not aligned. Enter "Ignore" to go ahead anyway, print the partition table in sectors to see where it starts, and remove/recreate the partition with the start sector rounded up to increasing powers of 2 until the warning stops. As one example, on a flash drive with 512B sectors, Parted wanted partitions to start on sectors that were a multiple of 2048, which is 1 MiB alignment.

If you want parted to attempt to calculate the correct alignment for you, specify the start position as 0% instead of some concrete value. To make one large ext4 partition, your command would look like this:

On an already partitioned disk, you can use parted to verify the alignment of a partition on a device. For instance, to verify alignment of partition 1 on /dev/sda:

**Examples:**

Example 1 (unknown):
```unknown
# parted device
```

Example 2 (unknown):
```unknown
/dev/nvme0n1
```

Example 3 (unknown):
```unknown
/dev/mmcblk0
```

Example 4 (unknown):
```unknown
# parted /dev/sda --script mklabel gpt mkpart P1 ext3 1MiB 8MiB
```

---

## Notion

**URL:** https://wiki.archlinux.org/title/Notion

**Contents:**
- Installation
- Starting
  - With a display manager
- Usage
- Configuration
- See also

This article or section is out of date.

Notion is a tiling, tabbed window manager for X.

Run notion with xinit.

To start/select Notion with a display manager, a standard .desktop file can be created in the /usr/share/xsessions/ directory. An example notion.desktop file can be found below:

See the display manager article for more details.

You can view Notion's man page during use with the F1 key and pressing the return key, this will tell you the default key bindings for Notion. You can also access the man page for other programs this way by pressing F1, typing in the program's name and pressing return. Besides the manual there is also a getting started tour available for a quick overview of Notion.

Notion can be configured using Lua. To get started, copy /etc/notion/cfg_notion.lua to ~/.notion. For more information, read Configuring and Extending Notion using Lua.

**Examples:**

Example 1 (unknown):
```unknown
/usr/share/xsessions/notion.desktop
```

Example 2 (unknown):
```unknown
/usr/share/xsessions/
```

Example 3 (unknown):
```unknown
notion.desktop
```

Example 4 (unknown):
```unknown
[Desktop Entry]
Name=Notion
Comment=This session logs you into Notion
Exec=/usr/bin/notion
TryExec=/usr/bin/notion
Icon=
Type=XSession
```

---

## Bug reporting guidelines

**URL:** https://wiki.archlinux.org/title/Bug_report

**Contents:**
- Before reporting
  - Search for duplicates
  - Upstream or Arch?
  - Bug or feature?
    - Reasons for not being a bug
    - Reasons for not being a feature
  - Gather useful information
- Opening a bug
  - Creating an account
  - Where to open the bug

Opening (and closing) bug reports on the Arch Linux bug tracker in GitLab is one of many possible ways to help the community. However, poorly-formed bug reports can be counter-productive. When bugs are incorrectly reported, developers waste time investigating and closing invalid reports. This document will guide anyone wanting to help the community by efficiently reporting and hunting bugs.

See also: How to Report Bugs Effectively by Simon Tatham

Preparing a detailed and well-formed bug report is not difficult, but requires an effort on behalf of the reporter. The work done before reporting a bug is arguably the most useful part of the job. Unfortunately, few people take the time to do this work properly.

The following steps will guide you in preparing your bug report or feature request.

If you encounter a problem or desire a new feature, there is a high probability that someone else already had this problem or idea. If so, an appropriate bug report may already exist. In this case, please do not create a duplicate report; see #Following up on bugs instead.

Search thoroughly for existing information, including:

Arch Linux is a GNU/Linux distribution. Arch developers and Package Maintainers are responsible for compiling, packaging, and distributing software from a wide range of sources. Upstream refers to these sources  the original authors or maintainers of software that is distributed in Arch Linux. For example, the popular Firefox web browser is developed by the Mozilla Project.

If Arch is not responsible for a bug, the problem will not be solved by reporting the bug to Arch developers. Responsibility for a bug is said to lie upstream when it is not caused through the distribution's porting and integration efforts.

By reporting bugs upstream, you will not only help Archers and Arch developers, but you will also help other people in the free software community as the solution will trickle down to all distributions.

Once you have reported a bug upstream or have found other relevant information from upstream, it might be helpful to post this in the Arch bug tracker, so both Arch developers and users are made aware of it.

So what is Arch Linux responsible for?

If a bug/feature is not under Arch's responsibility, report it upstream. See also #Reasons for not being a bug.

Here is a list of useful information that should be mentioned in your bug report:

If you have to paste a lot of text, like the output of dmesg, or an Xorg log, is it preferred to save it as a file on your computer and attach it to your bug report. Attaching a file rather than using a pastebin to present relevant information is preferable in general due to the fact that pastebined content may suffer by expired links or any other potential problems. Attaching a file guarantees the provided information will always be available.

When you are sure it is a bug or a feature and you gathered all useful information, then you are ready to open a bug report or feature request.

You have to create an account on Arch's GitLab, which manages its login via Arch Linux SSO.

Once you have determined your feature or bug is related to Arch and not an upstream issue, you will need to file your problem in the correct project. This is most easily done via Add new bug on the respective packages page on archlinux.org.

You can alternatively also get to the correct page using pkgctl from devtools:

Problems with packages in the AUR are not reported in the bug tracker. The AUR allows you to add comments to a package, which you should use to report bugs to the package maintainer.

Please write a concise and descriptive title for your bug/feature request.

Here is a list of recommendations:

This article or section is out of date.

Choosing a critical severity will not help to solve the bug faster. It will only make truly critical problems less visible and probably make the developer assigned to your bug a bit less open to fixing it.

Here is a general usage of severities:

This is maybe one of the most difficult parts of bug reporting. You have to choose from the section #Gather useful information which information you will add to your bug report. This will depend on which your problem is. If you do not know what the relevant pieces of information are, do not be shy: it is better to give more information than needed than not enough.

A good tutorial on reporting bugs can be found at https://www.chiark.greenend.org.uk/~sgtatham/bugs.html.

However, developers or bug hunters will ask you for more information if needed. Fortunately after a few bug reports you will know what information should be given.

Short information can be inlined in your bug report, whereas long information (logs, screenshots...) should be attached.

Do not think the work is done once you have reported a bug!

Developers or other users will probably ask you for more details and give you some potential fixes to try. Without continued feedback, bugs cannot be closed and the end result is both angry users and developers.

You can vote for your favourite bugs via reactions on the issue. The number of reactions indicates to the developers how many people are impacted by the bug without creating too much noise. However, this is not a very effective way of getting the bug solved. Much more important would be posting any additional information you know about the bug if you were not the original reporter.

Watching a bug is important: you will receive an email when new comments are added or the bug status has changed. This can be done via the "..." menu in the upper right corner by toggling the Notification switch there. If you opened a issue or commented on it you will automatically be subscribed to changes.

People will take the time to look at your bug report and will try to help you. You need to continue to help them resolve your bug. Not answering to their questions will keep your bug unresolved and likely hamper enthusiasm to fix it.

Please take the time to give people more information if requested and try the solutions proposed.

Developers or bug hunters will close your bug if you do not answer questions after a few weeks or a month.

Sometimes, a bug is only present in certain version(s) of a given package, and the bug is fixed in a new version of the package. If this is the case, say so in the bug report comments, and request that the bug be closed.

Sometimes people report a bug but do not notify when they have solved it on their own, leaving people searching for a solution that has already been found. Please close the bug if you found a solution, and give the solution in the bug report comments.

During its life, a bug may go through several states:

The Bug Wranglers are responsible for dispatching bugs and setting their status together with the help of the respective maintainers of the package the bug was opened against.

**Examples:**

Example 1 (unknown):
```unknown
pacman -Qi package_name
```

Example 2 (unknown):
```unknown
/var/log/messages
```

Example 3 (unknown):
```unknown
~/.local/share/xorg/Xorg.0.log
```

Example 4 (unknown):
```unknown
/var/log/Xorg.0.log
```

---

## ICC profiles

**URL:** https://wiki.archlinux.org/title/ICC_profiles

**Contents:**
- Utilities
- Profile generation
  - Colorimeter or spectrometer
  - Argyll CMS
  - Monitor calibration and profiling with additional calibration hardware
  - Scanner calibration
  - Printer calibration
  - File transfer
  - Gnome Color Manager
  - ThinkPads

As it pertains to general desktop use, an ICC profile is a binary file which contains precise data regarding the color attributes of an input, or output device. Single, or multiple profiles can be applied across a system and its devices to produce consistent and repeatable results for graphic and document editing and publishing. ICC profiles are typically calibrated with a (tristimulus) colorimeter, or a spectrophotometer when absolute color accuracy is required.

Color management is a workflow of hardware calibration, software profiling and embedding the profile into the picture or video. It is all based on using an ICC profile.

It is highly recommended to use a colorimeter or spectrometer device for hardware-assisted display, printer and scanner calibration. For home use there are several affordable colorimeters available. Some are well- or even better-supported under Linux than on other operating systems. Frequently recommended devices are X-Rite ColorMunki Display and DataColor Spyder5 Express. You can find more Linux-supported devices listed in the AgyllCMS documentation.

The Argyll Color Management System is a complete suite of command-line profile creation and loading tools listed under argyllcms.

Review the official Argyll CMS documentation for details on how to profile selected devices.

There is a GUI frontend for ArgyllCMS called DisplayCal, available as displaycal. In most common cases you will want to use its default settings. It is a common way to calibrate to a daylight color of 6500K and gamma 2,2. Read the DispalGui documentation for more. Many tutorials are available on the net.

Follow the scanner part of the scanner calibration tutorial.

See cups-calibrate(8).

Profile generation on a Windows or macOS system is one of the easiest and most widely recommended methods to obtain a ICC monitor profile. Since ICC color profiles are written to an open specification, they are compatible across operating systems. Transferring profiles from one OS to another can be used as a workaround for the lack of support for certain spectrophotometers or colorimeters under Linux: one can simply produce a profile on a different OS and then use it in a Linux workflow. Note that the system on which the profile is generated must host the exact same video card and monitor for which the profile is to be used. Once generation of an ICC profile, or a series of profiles is complete on a Windows system, copy the file(s) from the default path:

macOS generally stores ICC profiles in:

Once the appropriate .icc/.icm files have been copied, install the device profiles to your desired system. Common installation device profiles directories on Linux include:

On Gnome, an ICC profile can easily be created by using gnome-color-manager. Under Gnome, this is accessible via the Control Center and is pretty straightforward to use. You will need a colorimeter device to use this feature.

See color profiles for IBM/Lenovo ThinkPad notebook monitor profile (generic) support.

The ThinkWiki instructions can be used to extract other ThinkPad driver executables from Lenovo, such as the Monitor INF File for Windows 11 for X1 Carbon Gen 10, X1 Yoga Gen 7, Z13, Z16.

ICC profiles are loaded either by the session daemon or by a dedicated ICC loader. Both Gnome and KDE have daemons capable of loading ICC profiles from colord. If you use colord in combination with either gnome-settings-daemon or colord-kde, the profile will be loaded automatically. If you are not using either Gnome or KDE, you may install an independent daemon, xiccd, which does the same but does not depend on your desktop environment. Do not start two ICC-capable daemons (e.g. gnome-settings-daemon and xiccd) at the same time.

If you are not using any ICC-capable session daemon, make sure you use only one ICC loader - either xcalib, dispwin, dispcalGUI-apply-profiles or others. Otherwise, you can easily end up with an uncontrolled environment. (The most recently run loader sets the calibration, and the earlier loaded calibration is overwritten.)

Before using a particular ICC loader, you should understand that some tools set only the calibration curves (e.g. xcalib), some tools set only the display profile to X.org _ICC_PROFILE atom (e.g. xicc), and other tools do both tasks at once (e.g. dispwin, dispcalGUI-apply-profiles).

xiccd is a simple bridge between colord and X. It allows non-GNOME and non-KDE desktop environment to load and apply icc profiles.

Make sure colord is installed, then install xiccd.

Copy your icc profiles to the profile directory.

Start colord.service.

If colord was already running, you need to restart colord.service, otherwise new profiles will not show up.

Execute xiccd in a terminal as a backend and ignore any verbose messages. Keep xiccd running during the next steps.

This will enumerate displays and register them for colormgr(colord).

Open another terminal and execute colormgr. Note the Device ID of your screen.

Note the Profile ID which you added earlier and want to use.

Add your profile to the display device.

Make the profile as the default to the display device.

Double-check that xiccd installed /etc/xdg/autostart/xiccd.desktop so that it autostarts at system startup.

Close all terminals, and reboot the system to check whether the icc profile is being applied. If colord was already running, you need to restart colord.service.

xcalib is a lightweight monitor calibration loader which can load an ICC monitor profile to be shared across desktop applications. Install the package xcalib.

To load profile P221W-sRGB.icc in /usr/share/color/icc:

This can be autostarted at the start of the Xorg server, the desktop environment or the window manager.

dispwin is a part of argyllcms.

To load the profile 906w-6500K.icc in /home/archie/.color/icc:

This can be autostarted at the start of the Xorg server, the desktop environment or the window manager.

Wayland supports color management through color profiles, but the user interface for managing these profiles is currently not implemented properly. However, you can manually add a color profile through the following steps:

Firstly, copy your .icc color profile file to the /usr/share/color/icc/colord/ directory.

Run colormgr get-profiles to obtain the available color profiles, and colormgr get-devices to obtain the IDs of the attached devices.

To assign a color profile to a device, use the command colormgr device-add-profile Device_ID Profile_ID. The device ID is obtained from the output of colormgr get-devices and the profile ID from colormgr get-profiles.

For example, if your device ID is "DP-3" and the profile ID is "icc-5fb87663ba378cadf463ba64d92dced3", the command would look like:

With these steps, you can manually manage your color profiles in Wayland until the user interface is fully implemented. Once the ICC profile is added with this method, it will show up and work as expected in system settings like Color Manager in the KDE Plasma settings.

**Examples:**

Example 1 (unknown):
```unknown
C:\WINDOWS\System32\spool\drivers\color
```

Example 2 (unknown):
```unknown
/Library/ColorSync/Profiles/Displays/
```

Example 3 (unknown):
```unknown
/System/Library/ColorSync/Profiles/Displays/
```

Example 4 (unknown):
```unknown
/Users/USER_NAME/Library/ColorSync/Profile
```

---

## openresolv

**URL:** https://wiki.archlinux.org/title/Openresolv

**Contents:**
- Installation
- Usage
- Users
- Subscribers
- Tips and tricks
  - Defining multiple values for options

openresolv is a resolvconf implementation, i.e. a resolv.conf management framework.

Although openresolv is most known for allowing multiple applications to modify /etc/resolv.conf, it is currently the only standard way to implement:

Install the openresolv package.

openresolv provides resolvconf(8) and is configured in /etc/resolvconf.conf. See resolvconf.conf(5) for supported options.

Running resolvconf -u will generate /etc/resolv.conf.

This article or section needs expansion.

openresolv can be configured to pass name servers and search domains to DNS resolvers. The supported resolvers are:

See the official documentation for instructions.

The man page does not mention it, but to define multiple values, for options that support it (e.g. name_servers, resolv_conf_options etc.) in /etc/resolvconf.conf, you need to write them space separated inside quotes. E.g.:

**Examples:**

Example 1 (unknown):
```unknown
/etc/resolv.conf
```

Example 2 (unknown):
```unknown
/etc/resolvconf.conf
```

Example 3 (unknown):
```unknown
resolvconf -u
```

Example 4 (unknown):
```unknown
/etc/resolv.conf
```

---

## Router

**URL:** https://wiki.archlinux.org/title/Router

**Contents:**
- Hardware requirements
- Network interface configuration
  - Persistent interface naming
  - IP configuration
    - With netctl
    - With systemd-networkd
  - ADSL connection/PPPoE
    - PPPoE configuration
- DNS and DHCP
  - dnsmasq

This article is a tutorial for turning a computer into an internet gateway/router. To strengthen its security it should not run any services available to the outside world. Towards the LAN, run only gateway specific services; especially do not run httpd, ftpd, samba, nfsd, etc. as those belong on a server in the LAN since they introduce security risks.

This article does not attempt to show how to set up a shared connection between two machines using cross-over cables. For a simple internet sharing solution, see Internet sharing.

systemd automatically chooses unique interface names for all your interfaces. These are persistent and will not change when you reboot. However you might want to rename your interfaces e.g. in order to highlight their different networks to which they connect. Throughout the following sections of this guide, the convention stated below is used:

You may change the assigned names of your devices by following Network configuration#Change interface name. Due to the example-rich nature of this article, you might want to choose the names above.

Now you will need to configure the network interfaces. One way to do so, is using netctl profiles. You will need to create two profiles.

Next, we set up the interfaces with netctl:

A straight-forward and simple way to configure network interfaces is via systemd-networkd.

See systemd-networkd#Configuration for configuration details and an overview of the available options. Run networkctl reload to apply the configuration changes.

Using ppp, we can connect an ADSL modem to the extern0 interface of the firewall and have Arch manage the connection. Make sure to put the modem in bridged mode though (either half-bridge or RFC1483), otherwise, the modem will act as a router too. Install the ppp package.

It should be noted that if you use only PPPoE to connect to the internet (i.e. you do not have another WAN port, except for the one that connects to your modem) you do not need to set up the extern0-profile as the external pseudo-interface will be ppp0.

You can use netctl to setup the PPPoE connection. To get started, do

and start editing. For the interface configuration, choose the interface that connects to the modem. If you only connect to the internet through PPPoE, this will probably be extern0. Fill in the rest of the fields with your ISP information. See the PPPoE section in the netctl.profile(5) man page for more information on the fields.

The following comparison table lists the available DHCP servers and their features:

A comparison of available DNS servers can be found in Domain name resolution#DNS servers.

To use dnsmasq as DNS server, and optionally DHCP server, for the LAN, install the dnsmasq package.

The default configuration already enables its DNS server, see dnsmasq#Configuration for options.

For this router example, dnsmasq can to be configured to be a DHCP server with a configuration similar to the following:

See dnsmasq#DHCP server for other options. For example, you can also add "static" DHCP leases, i.e. assign an IP-address to the MAC-address of a computer on the LAN. This way, whenever the computer requests a new lease, it will get the same IP. That is very useful for network servers with a DNS record. You can also deny certain MACs from obtaining an IP.

Now start and enable the dnsmasq.service.

To use systemd-networkd instead of dnsmasq as DHCP server, add a [DHCPServer] section to the configuration file for the intern0 interface. See systemd-networkd#DHCP server for the available options.

To make systemd-resolved answer DNS requests on the routers interfaces, their addresses have to be added as described in systemd-resolved#Additional listening interfaces.

Time to tie the two network interfaces together.

First of all, we need to allow packets to hop from one network interface to the other. For this, one needs to have packet forwarding enabled in kernel via sysctl(8). See Internet sharing#Enable packet forwarding for details.

Assuming net.**forwarding is set correctly (i.e. is 1), packets still need to be properly sent and received. Hence, it is necessary to translate the IP addresses between the outward facing network and the subnet used locally. The technique is called masquerading . We also need two forwarding rules to keep connections going and enable LAN to WAN forwarding. For this task, we are going to use iptables.

Refer to the section Internet sharing#Enable NAT for how to masquerade the extern0 interface and packages from intern0 to extern0. Afterwards persist the newly added rules via iptables-save -f /etc/iptables/iptables.rules, see iptables#Configuration and usage for details.

Start and enable iptables.service. The router should now be fully functional and route your traffic. Since it is facing the public Internet, it makes sense to additionally secure it using a Simple stateful firewall.

Amend or create the previously discussed network configuration for intern0 to include the IPMasquerade=ipv4 option in the [Network] section. This configuration will implicitly enable packet forwarding on all interfaces, see systemd.network(5). See systemd-networkd#DHCP server for an example configuration.

See Shorewall for a detailed configuration guide.

This article or section is a candidate for merging with IPv6.

Useful reading: IPv6 and the wikipedia:IPv6.

You can use your router in IPv6 mode even if you do not have an IPv6 address from your ISP. Unless you disable IPv6, all interfaces should have been assigned a unique fe80::/10 address.

This article or section needs expansion.

For internal networking the block fc00::/7 has been reserved. These addresses are guaranteed to be unique and non-routable from the open Internet. Addresses that belong to the fc00::/7 block are called Unique Local Addresses. To get started generate a ULA /64 block[dead link 2025-03-15HTTP 404] to use in your network. For this example we will use fd00:aaaa:bbbb:cccc::/64. Firstly, we must assign a static IPv6 on the internal interface. Modify the intern0-profile we created above to include the following line:

This will add the ULA to the internal interface. As far as the router goes, this is all you need to configure.

If your ISP or WAN network can access the IPv6 Internet, you can additionally assign global link addresses to your router and propagate them through SLAAC to your internal network. The global unicast prefix is usually either static or provided through prefix delegation.

If your ISP has provided you with a static prefix, then edit /etc/netctl/extern0-profile and simply add the IPv6 and the IPv6 prefix (usually /64) you have been provided

You can use this in addition to the ULA address described above.

If your ISP handles IPv6 via prefix delegation, then you can follow the instructions in the IPv6#Prefix delegation (DHCPv6-PD) on how to properly configure your router. Following the conventions of this article, the WAN interface is extern0 (or ppp0 if you are connecting through PPPoE) and the LAN interface is intern0.

To properly hand out IPv6s to the network clients, we will need to use an advertising daemon. Follow the details of the main IPv6 article on how to set up radvd. According to this guide's convention, the LAN-facing interface is intern0. You can either advertise all prefixes or choose which prefixes will be assigned to the local network.

This article or section needs expansion.

The above configuration of shorewall does not include UPnP support. Use of UPnP is discouraged as it may make the gateway vulnerable to attacks from within the LAN. However, some applications require this to function correctly.

To enable UPnP on your router, you need to install an UPnP Internet Gateway Device (IGD) protocol daemon. To get it, install the miniupnpd package.

Read the Shorewall guide on UPnP for more information.

OpenSSH can be used to administer your router remotely. This is useful for running it in headless mode (no monitor or input devices).

See Squid for the setup of a web proxy to speed up browsing and/or adding an extra layer of security.

To use the router as a time server, see System time#Time synchronization for available Network Time Protocol (NTP) server implementations.

Then, configure shorewall or iptables to allow NTP traffic in and out.

Install and configure Privoxy if you need a content filtering solution.

Traffic shaping is very useful, especially when you are not the only one on the LAN. The idea is to assign a priority to different types of traffic. Interactive traffic (ssh, online gaming) probably needs the highest priority, while P2P traffic can do with the lowest. Then there is everything in between.

See Shorewall#Traffic shaping.

**Examples:**

Example 1 (unknown):
```unknown
/etc/netctl/extern0-profile
```

Example 2 (unknown):
```unknown
Description='Public Interface.'
Interface=extern0
Connection=ethernet
IP=dhcp
```

Example 3 (unknown):
```unknown
/etc/netctl/intern0-profile
```

Example 4 (unknown):
```unknown
Description='Private Interface'
Interface=intern0
Connection=ethernet
IP=static
Address=('10.0.0.1/24')
```

---

## Device file

**URL:** https://wiki.archlinux.org/title/Block_device

**Contents:**
- Block devices
  - Block device names
    - SCSI
    - NVMe
    - MMC
    - SCSI optical disc drive
    - virtio-blk
    - Partition
  - Utilities
    - lsblk

This article or section needs expansion.

On Linux they are in the /dev directory, according to the Filesystem Hierarchy Standard.

On Arch Linux the device nodes are managed by udev.

A block device is a special file that provides buffered access to a hardware device. For a detailed description and comparison of virtual file system devices, see Wikipedia:Device file#Block devices.

The beginning of the device name specifies the kernel's used driver subsystem to operate the block device.

Storage devices, like hard disks, SSDs and flash drives, that support the SCSI command (SCSI, SAS, UASP), ATA (PATA, SATA) or USB mass storage connection are handled by the kernel's SCSI driver subsystem. They all share the same naming scheme.

The name of these devices starts with sd. It is then followed by a lower-case letter starting from a for the first discovered device (sda), b for the second discovered device (sdb), and so on.

The name of storage devices, like SSDs, that are attached via NVM Express (NVMe) starts with nvme. It is then followed by a number starting from 0 for the device controller, nvme0 for the first discovered NVMe controller, nvme1 for the second, and so on. Next is the letter "n" and a number starting from 1 expressing the namespace on a controller, i.e. nvme0n1 for first discovered namespace on first discovered controller, nvme0n2 for second discovered namespace on first discovered controller, and so on.

This article or section needs expansion.

SD cards, MMC cards and eMMC storage devices are handled by the kernel's mmc driver and name of those devices start with mmcblk. It is then followed by a number starting from 0 for the device, i.e. mmcblk0 for first discovered device, mmcblk1 for second discovered device and so on.

The name of optical disc drives (ODDs), that are attached using one of the interfaces supported by the SCSI driver subsystem, start with sr. The name is then followed by a number starting from 0 for the device, ie. sr0 for the first discovered device, sr1 for the second discovered device, and so on.

Udev also provides /dev/cdrom that is a symbolic link to /dev/sr0. The name will always be cdrom regardless of the drive's supported disc types or the inserted media.

The name of drives attached to a virtio block device (virtio-blk) interface start with vd. It is then followed by a lower-case letter starting from a for the first discovered device (vda), b for the second discovered device (vdb), and so on.

Partition device names are a combination of the drive's device name and the partition number assigned to them in the partition table, i.e. /dev/drivepartition. For drives whose device name ends with a number, the drive name and partition number is separated with the letter "p", i.e. /dev/driveppartition.

The util-linux package provides the lsblk(8) utility which lists block devices, for example:

In the example above, only one device is available (sda), and that device has three partitions (sda1 to sda3), each with a different file system.

You can use the -o/--output option to enable a specific list of output columns:

The above is based on the options provided by the -f/--fs argument with removal of UUID and addition of partition label and disk size, which are useful when identifying multiple disks. See lsblk --help for a full list of supported columns.

This article or section needs expansion.

wipefs can list or erase file system, RAID or partition-table signatures (magic strings) from the specified device to make the signatures invisible for libblkid(3). It does not erase the file systems themselves nor any other data from the device.

See wipefs(8) for more information.

For example, to erase all signatures from the device /dev/sdb and create a signature backup ~/wipefs-sdb-offset.bak file for each signature:

Device nodes that do not have a physical device.

**Examples:**

Example 1 (unknown):
```unknown
/dev/nvme0n1
```

Example 2 (unknown):
```unknown
/dev/nvme2n5
```

Example 3 (unknown):
```unknown
/dev/mmcblkXboot{0,1}
```

Example 4 (unknown):
```unknown
/dev/mmcblkXrpmb
```

---

## Nextcloud

**URL:** https://wiki.archlinux.org/title/OwnCloud

**Contents:**
- Setup overview
- Installation
- Configuration
  - PHP
  - Nextcloud
  - System and environment
- Database
  - MariaDB / MySQL
  - PostgreSQL
- Application server

Nextcloud is a suite of client-server-software that (by means of so-called apps) allows all kinds of sharing, collaboration and communication, e.g.:

Nextcloud is open source and based on open standards. Data sovereignty is a big plus, i.e. with your own instance of Nextcloud, you break free from proprietary (and potentially untrustworthy) services like Dropbox, Office 365, or Google Drive.

Depending on your needs Nextcloud can be deployed from single-board computers (like e.g. a Raspberry Pi) all the way up to full scale data centers serving millions of users. With an elaborated authorization scheme and the option for federation (connecting discrete instances) Nextcloud is well suited for use in enterprise environments.

Nextcloud is a fork of ownCloud. See Wikipedia for the history.

A complete installation of Nextcloud comprises (at least) the following components:

A web server paired with an application server on which runs Nextcloud (i.e. the PHP code) using a database.

This article will cover MariaDB/MySQL and PostgreSQL as databases and the following combinations of web server and application server:

The Nextcloud package complies with the web application package guidelines. Among other things this mandates that the web application be run with a dedicated user - in this case nextcloud. This is one of the reasons why the application server comes into play here. For the very same reason it is not possible anymore to execute Nextcloud's PHP code directly in the Apache process by means of php-apache.

Install the nextcloud package. This will pull in quite a few dependent packages. Most of the required PHP extensions will be taken care of this way.

It is recommended to also install the packages

(preferrably as dependent package). Other optional dependencies will be covered later depending on your concrete setup (e.g. what database you choose).

Mind that some modules (namely bcmath, exif, gmp, intl and sysvsem) come with php-legacy. So no explicit installation is required for those.

This guide does not tamper with PHP's central configuration file /etc/php-legacy/php.ini but instead puts Nextcloud specific PHP configuration in places where it does not potentially interfere with settings for other PHP based applications. These places are:

Make a copy of /etc/php-legacy/php.ini to /etc/webapps/nextcloud (or even better, extract php.ini from the php-legacy package tarball below /var/cache/pacman/pkg). Although not strictly necessary, change ownership of the copy:

Most of the required PHP modules listed in Nextcloud's documentation are already enabled in the just copied bare PHP installation configuration file. Additionally enable the following extensions:

Depending on the database you are going to use enable the corresponding pdo_xxxx module. See Database below.

Set date.timezone to your preferred timezone, e.g.:

Raise PHP's memory limit to at least 512MiB:

Optional: For additional security configure open_basedir. This limits the locations where Nextcloud's PHP code can read and write files. Proven settings are

Depending on what additional extensions you configure you may need to extend this list, e.g. /run/valkey in case you opt for Valkey.

It is not necessary to configure opcache here as this php.ini is only used by the occ command line tool and the background job, i.e. by short running PHP processes.

Add the following entries to Nextcloud's configuration file:

Adapt the given example hostname cloud.mysite.com. In case your Nextcloud installation will be reachable via a subfolder (e.g. https://www.mysite.com/nextcloud) overwrite.cli.url and htaccess.RewriteBase have to be modified accordingly.

To make sure the Nextcloud specific php.ini is used by the occ tool set the environment variable NEXTCLOUD_PHP_CONFIG:

Also add this line to your .bashrc (or .bash_profile ) to make this setting permanent. For users of the fish shell:

But do not add this to your fish.conf.

As a privacy and security precaution create the dedicated directory for session data:

MariaDB/MySQL is the canonical choice for Nextcloud.

Most information concerning databases with Nextcloud deals with MariaDB/MySQL. The Nextcloud developers admit to have less detailed expertise with other databases.

PostgreSQL is said to deliver better performance and overall has fewer quirks compared to MariaDB/MySQL. SQLite is mainly supported for test / development installations and not recommended for production. The list of supported databases also contains Oracle database. This product will not be covered here.

Since MariaDB has been the default MySQL implementation in Arch Linux since 2013[2] this text only mentions MariaDB.

In case you want to run your database on the same host as Nextcloud install, configure and start mariadb-lts (if you have not done so already). See the corresponding article for details. Do not forget to initialize MariaDB with mariadb-install-db. It is recommended for additional security to configure MariaDB to only listen on a local Unix socket:

Nextcloud's own documentation recommends to set the transaction isolation level to READ-COMMITTED. This is especially important when you expect high load with many concurrent transactions.

The other recommendation to set binlog_format=ROW is obsolete. The default MIXED in recent MariaDB versions is at least as good as the recommended ROW. In any case the setting is only relevant when replication is applied.

Start the CLI tool mysql with database user root. (Default password is empty, but hopefully you change it as soon as possible.)

Create the user and database for Nextcloud with

(db-password is a placeholder for the actual password of DB user nextcloud you must choose.) Quit the tool with \q.

So that you have decided to use MariaDB as the database of your Nextcloud installation you have to enable the corresponding PHP extension:

Further configuration (related to MariaDB) is not required (contrary to the information given in Nextcloud's admin manual).

Now setup Nextcloud's database schema with:

Mind the placeholders (e.g. db-password) and replace them with appropriate values. This command assumes that you run your database on the same host as Nextcloud. Enter occ help maintenance:install and see Nextcloud's documentation for other options. See Using the "occ" command line tool for Arch-specific details about this tool.

Consult the corresponding article for detailed information about PostgreSQL. In case you want to run your database on the same host as Nextcloud install, configure and start postgresql (if you have not done so already). For additional security in this scenario it is recommended to configure PostgreSQL to only listen on a local UNIX socket:

Especially do not forget to initialize your database with initdb. After having done so start PostgreSQL's CLI tool psql and create the database user nextcloud and the database of the same name

(db-password is a placeholder for the password of database user nextcloud that you have to choose.)

Install the additional package php-legacy-pgsql (preferrably as dependent package) and enable the corresponding PHP extension in /etc/webapps/nextcloud/php.ini:

Now setup Nextcloud's database schema with:

Mind the placeholders (e.g. db-password) and replace them with appropriate values. This command assumes that you run your database on the same host as Nextcloud. Enter occ help maintenance:install and see Nextcloud's documentation for other options. See Using the "occ" command line tool for Arch-specific details about this tool.

There are two prevalent application servers that can be used to process PHP code: uWSGI or FPM. FPM is specialized on PHP. The protocol used between the web server and FPM is fastcgi. The tool's documentation leaves room for improvement. uWSGI on the other hand can serve code written in a handful of languages by means of language specific plugins. The protocol used is uwsgi (lowercase). The tool is extensively documented - albeit the sheer amount of documentation can become confusing and unwieldy.

uWSGI has its own article. A lot of useful information can be found there. Install uwsgi and the plugin uwsgi-plugin-php-legacy (preferrably as dependent packages). To run Nextcloud's code with (or in) uWSGI you have to configure one uWSGI specific configuration file (nextcloud.ini) and define one systemd service.

The nextcloud package includes a sample configuration file already in the right place /etc/uwsgi/nextcloud.ini. In almost any case you will have to adapt this file to your requirements and setup. Find a version with lots of commented changes (compared to the package's version). It assumes a no-frills Nextcloud installation for private use (i.e. with moderate load).

In general keep the enabled extensions, extension specific settings and open_basedir in sync with /etc/webapps/nextcloud/php.ini (with the exception of opcache).

The package uwsgi provides a template unit file (uwsgi@.service). The instance ID (here nextcloud) is used to pick up the right configuration file. Enable and start uwsgi@nextcloud.service.

In case you have more than a few (e.g. 2) services started like this and get the impression this is a waste of resource you might consider using emperor mode.

In case you opt to use FPM as your application server install php-legacy-fpm (preferrably as dependent package).

Configuration consists of a copy of php.ini relevant for all applications served by FPM and a so-called pool file specific for the application (here Nextcloud). Finally you have to tweak the systemd service file.

As stated earlier this article avoids modifications of PHP's central configuration in /etc/php-legacy/php.ini. Instead create a FPM specific copy.

Make sure it is owned and only writeable by root (-rw-r--r-- 1 root root ... php-fpm.ini). Enable the op-cache, i.e. uncomment the line

and put the following parameters below the existing line [opcache]:

Next you have to create a so called pool file for FPM. It is responsible for spawning dedicated FPM processes for the Nextcloud application. Create a file /etc/php-legacy/php-fpm.d/nextcloud.conf - you may use this functional version as a starting point.

Again make sure this pool file is owned and only writeable by root (i.e. -rw-r--r-- 1 root root ... nextcloud.conf). Depending on whether access log is configured (it is with above sample nextcloud.conf) you may need to create the corresponding directory (here /var/log/php-fpm-legacy/access). Adapt or add settings (especially pm..., php_value[...] and php_flag[...]) to your liking. The settings php_value[...] and php_flag[..] must be consistent with the corresponding settings in /etc/webapps/nextcloud/php.ini (but not /etc/php-legacy/php-fpm.ini).

The settings done by means of php_value[...] and php_flag[...] could instead be specified in php-fpm.ini. But mind that settings in php-fpm.ini apply for all applications served by FPM.

FPM is run as a systemd service. You have to modify the service configuration to be able to run Nextcloud. This is best achieved by means of a drop-in file:

Do not forget to enable and start the service php-fpm-legacy.

The Nextcloud package unconditionally creates the uWSGI configuration file /etc/uwsgi/nextcloud.ini. Of course it is of no use when you run FPM instead of uWSGI (and it does no harm whatsoever). In case you nevertheless want to get rid of it, just add the following NoExtract line to /etc/pacman.conf:

There is an abundance of web servers you can choose from. Whatever option you finally pick you have to keep in mind that the Nextcloud application needs to be run with its own system user nextcloud. So you will need to forward your requests to one of the above mentioned application servers.

Configuration of nginx is way beyond the scope of this article. See the relevant article for further information. Also consult Nextcloud's documentation for an elaborated configuration. It is up to you how to include this snippet into your nginx' configuration. One common approach is to use directories /etc/nginx/sites-available and /etc/nginx/sites-enabled to separate configurations for various servers (aka virtual hosts). See Nginx#Managing server entries for details.

If using the example nginx config from the Nextcloud documentation linked above, the root directory should be changed to:

The usage of the block upstream php-handler { ... } is not necessary. Just specify fastcgi_pass unix:/run/php-fpm-legacy/nextcloud.sock; in the location block that deals with forwarding request with PHP URIs to the application server. When using uWSGI instead of FPM replace this location block with:

Things you might have to adapt (not exhaustive):

There is no need to install any additional modules since nginx natively supports both protocols FastCGI and uwsgi.

Find lots of useful information in the article about the Apache HTTP Server. Nextcloud's documentation has some sample configuration that can also be found in /usr/share/doc/nextcloud/apache.example.conf. Both implicitly rely on mod_php that cannot be used anymore. mod_proxy_fcgi or mod_proxy_uwsgi need to be applied.

Information about how to integrate Apache with FPM can be found here in this wiki. uWSGI's documentation has some information about how to integrate Apache with PHP by means of uWSGI and mod_proxy_uwsgi. Mind that the Apache package comes with both modules mod_proxy_fcgi and mod_proxy_uwsgi. They need to be loaded as required.

The following Apache modules are required to run Nextcloud:

Also uncomment the following directive to pull in TLS configuration parameters:

Consult Mozilla's SSL configurator for details about how to optimize your TLS configuration.

Refer to the following two sample configuration files depending on how you want to access your Nextcloud installation:

Of course you must adapt these sample configuration files to your concrete setup. Replace the SetHandler directive by SetHandler "proxy:unix:/run/uwsgi/nextcloud.sock|uwsgi://nextcloud/" when you use uWSGI.

The Nextcloud package comes with a .htaccess that already takes care of a lot of rewriting and header stuff. Run occ maintenance:update:htaccess to adapt this file. Parameter htaccess.RewriteBase in /etc/webapps/nextcloud/config/config.php is vital for this.

If the user files are stored in the /home partition, access is protected by default since apache 2.4.65 with the setting

Nextcloud requires certain tasks to be run on a scheduled basis. See Nextcloud's documentation for some details. The easiest (and most reliable) way to set up these background jobs is to use the systemd service and timer units that are already installed by nextcloud. The service unit needs some tweaking so that the job uses the correct PHP ini-file (and not the global php.ini). Create a drop-in file and add:

After that enable and start nextcloud-cron.timer (not the service).

As recommended by the documentation add the parameter

to Nextcloud's configuration file. The value is the hour of the day in UTC defining the start of a 4 hours window. Time consuming jobs that need to be run only once a day will be scheduled in this time frame, i.e. outside working hours.

Nextcloud's documentation recommends to apply some kind of in-memory object cache to significantly improve performance.

Install php-legacy-apcu (preferrably as dependent package). Enable the extension in the relevant configuration files. These are

In /etc/webapps/nextcloud/php.ini add the lines

(preferably somewhere below Module Settings).

For the other two files the setting to activate APCu is already in place and only needs to be uncommented. Two other configuration parameters related to APCu are also already there. No need to touch /etc/php-legacy/php.ini or /etc/php-legacy/conf.d/apcu.ini.

Restart your application server (not the web server as Nextcloud's documentation claims). Add the following line to your Nextcloud configuration file:

Valkey was forked from Redis version 7.2.4 in March 2024 due to license issues. Nextcloud's documentation still only mentions Redis. However, one year later Valkey can still be considered a drop-in replacement for Redis. (Future will show how long this will stand.)

Valkey can be run locally (i.e. on the same host as Nextcloud) or on a different machine. (For more information see Nextcloud's documentation.) In any case install php-legacy-igbinary and php-legacy-redis (preferrably as dependent packages).

Enable these extensions in /etc/webapps/nextcloud/php.ini and, depending on the application server you use, in /etc/uwsgi/nextcloud.ini or /etc/php-legacy/php-fpm.d/nextcloud.conf. This can be done by locating the existing sections where other extensions are enabled and adding two additional lines corresponding to igbinary and redis.

Alternatively, you can enable the required extensions igbinary and redis in their respective initialization files in /etc/php-legacy/conf.d/ by uncommenting the extension= lines.

In case you have specified the open_basedir option in the above configuration files and use Valkey locally with a local Unix socket, you have to extend the list of directories where PHP is allowed to read and write files. Locate the relevant lines in the files specified above and add the directory containing the local Unix socket created by Valkey, e.g. /run/valkey.

Extend your Nextcloud configuration as follows:

Again, adapt /run/valkey/valkey.sock as required. dbindex, password and timeout are optional.

In case Valkey runs on a different machine:

valkey-host.mysite.com is just a placeholder. Adapt to your actual setup.

See the Nextcloud documentation and Security. Nextcloud additionally provides a Security scanner.

The official client can be installed with the nextcloud-client package. Alternative versions are available in the AUR: nextcloud-client-gitAUR. Please keep in mind that using owncloud-client with Nextcloud is not supported.

The desktop client basically syncs one or more directories of your desktop computer with corresponding folders in your Nextcloud's file service. It integrates nicely with your desktop's file manager (Dolphin in KDE Plasma, Nautilus in Gnome) displaying overlays representing synchronization and share status. The context menu of each file gets an additional entry Nextcloud to manage sharing of this file and getting the public or internal share link. Nextcloud's documentation has a volume exclusively about the desktop client.

In case the integration does not work as described consult the optional dependencies of package nextcloud-client. E.g. Nautilus requires nautilus-python. Install as dependent package.

By default nextcloud provides a dbus activated service that might be triggered by a lot of randomly seeming things like opening a file dialog. You can add the following to your pacman config to disable it, before installing the client.

Since version 102 Thunderbird fully supports CalDAV and CardDAV - even with auto detection (i.e. you do not have to provide long URLs to access your calendars and address books). See Nextcloud's documentation for details.

If you want to mount your Nextcloud using WebDAV, install davfs2 (as described in davfs2).

To mount your Nextcloud, use:

You can also create an entry for this in /etc/fstab:

You can access the files directly in Nautilus ('+ Other Locations') via the WebDAV protocol. Use the link as shown in your Nextcloud installation Web GUI (e.g. https://cloud.mysite.com/remote.php/webdav/) but replace the protocol name https by davs. Nautilus will ask for user name and password when trying to connect.

Download the official Nextcloud app from Google Play or F-Droid.

To enable contacts and calendar sync (Android 4+):

Download the official Nextcloud app from the App Store.

A useful tool for server administration is occ. Refer to Nextcloud's documentation for details. You can perform many common server operations with occ, such as managing users and configuring apps.

A convenience wrapper around the original /usr/share/webapps/nextcloud/occ is provided with /usr/bin/occ which automatically runs as the default user (nextcloud), using the default PHP executable and PHP configuration file. The environment variables NEXTCLOUD_USER, NEXTCLOUD_PHP and NEXTCLOUD_PHP_CONFIG can be used to specify a non-default user, PHP executable and PHP configuration file (respectively). Especially the latter (using NEXTCLOUD_PHP_CONFIG) is necessary when Nextcloud was setup in a way as described in the sections Configuration and Application server, i.e. using a PHP configuration specific to Nextcloud. In this case put export NEXTCLOUD_PHP_CONFIG=/etc/webapps/nextcloud/php.ini in your .bashrc.

When using package php instead of the recommended package php-legacy you also have to set NEXTCLOUD_PHP, i.e. export NEXTCLOUD_PHP=/usr/bin/php.

The nextcloud package comes with a pacman hook that takes care of automatically upgrading the Nextcloud database after the package has been updated. Take a look /usr/share/doc/nextcloud/nextcloud.hook.

Unfortunately, this hook unconditionally uses the global php.ini when running occ upgrade, i.e. it does not take into account the value of environment variable NEXTCLOUD_PHP_CONFIG as mentioned above in Using the "occ" command line tool.

As a possible workaround make a copy of the delivered hook file in the appropriate location:

and change the line starting with Exec to:

The instructions in section Web server will result in a setup where your Nextcloud installation is reachable via a dedicated server name, e.g. cloud.mysite.com. If you would like to have Nextcloud located in a subdirectory. e.g. www.mysite.com/nextcloud, then:

See the Nextcloud repository on Docker Hub for running Nextcloud in Docker.

There are currently three different solutions for office integration:

All three have in common that a dedicated server is required and your web server needs to be adapted to forward certain requests to the office service. The actual integration with Nextcloud is then accomplished by means of a Nextcloud app specific for one of the above products.

Mind that all three products are aimed at businesses, i.e. you will have to pay for the office service. Only Collabora offers a developers plan (CODE) for free. ONLYOFFICE offers a Home Server plan for a reasonable price.

For installation, setup instructions and integration with Nextcloud consult:

By default, Nextcloud recommends apps to new clients, which can result in a lot of notifications. To disable this, disable the recommendations app using occ app:disable recommendations.

The calcardbackupAUR package can be installed and configured to provide regular backups of the calendar and/or address book databases. Edit /etc/calcardbackup/calcardbackup.conf to your liking and then start and enable calcardbackup.timer.

By default, the logs of the web application are available in /var/log/nextcloud/nextcloud.log. The entries (lines) are in JSON format and can be very long. Readability can be greatly improved by using jq:

With Nextcloud v28.0.4 a regression was introduced that manifests in error messages containing

in the log file /var/log/nextcloud/nextcloud.log. The file path given after this messages is missing one slash at one position, whereas at another position there is one too many, e.g.

This is a known bug that has already been fixed and backported to v28.0.5. For v28.0.4 you may apply the following work-around:

and depending on the application server you use either (FPM)

With v28.0.5 you should be able to roll back this work-around again.

There are two locations where folders with files of Nextcloud's apps can be found:

When files of an app can be found in both directories (especially in different versions) all kind of strange things can happen. In a concrete case the contacts app could be found in both the read-only apps directory and the writeable apps directory. As a result the page with contacts in the GUI did not display. It remained blank white. The browser's Javascript console showed an error message:

Check for apps to be found in both locations. To find out whether to delete the app folder in the read-only directory or in the writeable directory determine whether the app is part of a package. Run

All folders reported with

(and that can also be found in the writeable apps directory) can be safely deleted from /usr/share/webapps/nextcloud/apps. Other double installed apps (i.e. those belonging to a package) should be removed from /var/lib/nextcloud/apps.

MariaDB versions >= 10.6 and < 10.6.6 were not compatible with Nextcloud as the database enforced read-only for compressed InnoDB tables and Nextcloud has been (and still is) using these kind of tables:

Additionally the issue has already been addressed by Nextcloud. Starting with Nextcloud v24 new installations of Nextcloud do not use row format COMPRESSED anymore. It has to be noted that existing (pre v24) installations are not migrated away from row format COMPRESSED automatically.

Bottom line: Probably you are not affected since Arch Linux has been shipping MariaDB v10.6.6 or later since 2022-02-10.

In the unlikely case that you are affected by this issue there are several possible remedies:

As mentioned in a post in the forum, this issue can be fixed by setting correct permissions on the sessions directory. (See Nextcloud's documentation for details.) It is also possible that the sessions directory is missing altogether. The creation of this directory is documented in System and environment.

/var/lib/nextcloud should look like this:

Also, this can be caused by a full system-partition caused by an extensive size of nextcloud.log because of this bug. In this case, truncating the log-file and disabling the preview-generator or setting the log-level to 4 in config.php and restarting Nextcloud helps.

Depending on what application server you use custom environment variables can be provided to Nextcloud's PHP code.

Add one or more lines in /etc/php-legacy/php-fpm.d/nextcloud.conf as per Nextcloud's documentation, e.g.:

Add one or more lines in /etc/uwsgi/nextcloud.ini, e.g.:

Mind there must not be any blanks around the second =.

If you get the following message in your administration settings:

Add the following in your configuration file: [7]

Replace 192.168.1.0 with your public IP.

If Nextcloud reports corrupted indices (e.g. during occ db: commands or in Administration > Logging) you can repair your database by executing:

If the command fails, it still will point out the table TABLE containing a corrupted index. Repair it by logging into mariadb:

Replace TABLE to match your findings.

In case you get responses with:

double-check your config.php that entry overwrite.cli.url has been set as described in section Configuration / Nextcloud.

**Examples:**

Example 1 (unknown):
```unknown
/etc/php-legacy/php.ini
```

Example 2 (unknown):
```unknown
/etc/webapps/nextcloud
```

Example 3 (unknown):
```unknown
/etc/php-legacy/php.ini
```

Example 4 (unknown):
```unknown
/etc/webapps/nextcloud
```

---

## Kernel

**URL:** https://wiki.archlinux.org/title/Vmlinuz

**Contents:**
- Officially supported kernels
- Compilation
  - kernel.org kernels
  - Unofficial kernels
- Troubleshooting
  - Kernel panics
    - Examine panic message
      - QR code on a blue screen
      - Console way
      - Example scenario: bad module

According to Wikipedia:

Arch Linux is based on the Linux kernel. There are various alternative Linux kernels available for Arch Linux in addition to the latest stable kernel. This article lists some of the options available in the repositories with a brief description of each. There is also a description of patches that can be applied to the system's kernel. The article ends with an overview of custom kernel compilation with links to various methods.

Kernel packages are installed under the /usr/lib/modules/ path and subsequently used to copy the vmlinuz executable image to /boot/. [1] When installing a different kernel or switching between multiple kernels, you must configure your boot loader to reflect the changes. For downgrading the kernel to an older version, see Downgrading packages#Downgrading the kernel.

Community support on forum and bug reporting is available for officially supported kernels.

Following methods can be used to compile your own kernel:

Some of the listed packages may also be available as binary packages via Unofficial user repositories.

Many of these unofficial kernels contain features that need to be enabled manually. Try reading the documentation in the patches themselves (many already include changes to the Documentation/ directory in the kernel source) or searching up the name of the patchset on the web.

A kernel panic occurs when the Linux kernel enters an unrecoverable failure state. The state typically originates from buggy hardware drivers resulting in the machine being deadlocked, non-responsive, and requiring a reboot. Just prior to deadlock, a diagnostic message is generated, consisting of: the machine state when the failure occurred, a call trace leading to the kernel function that recognized the failure, and a listing of currently loaded modules. Thankfully, kernel panics do not happen very often using mainline versions of the kernel--such as those supplied by the official repositories--but when they do happen, you need to know how to deal with them.

If a kernel panic occurs very early in the boot process, you may see a message on the console containing Kernel panic - not syncing:, but once systemd is running, kernel messages will typically be captured and written to the system log. However, when a panic occurs, the diagnostic message output by the kernel is almost never written to the log file on disk because the machine deadlocks before system-journald gets the chance.

Since linux 6.10 (for drm_panic), the kernel will display a panic as a QR code (by default) in a blue screen. The stack trace is visible at the URL given by the QR code. For Arch Linux, it is a link to https://panic.archlinux.org. The URL contains various information and the stack trace compressed by gzip and encoded in the URL fragment which is not transferred to the server (it is processed on the client side).

An example panic with a link and screenshot can be seen in a forum post.

You can revert to the old behavior by passing the parameter panic_screen=kmsg to the drm kernel module (or drm.panic_screen=kmsg as kernel parameter) to display the stack trace in a console.

The "old" style way of viewing the crash on the console as it happens is still available (without resorting to setting up a kdump crashkernel). Boot with the following kernel parameters and attempting to reproduce the panic on tty1:

It is possible to make a best guess as to what subsystem or module is causing the panic using the information in the diagnostic message. In this scenario, we have a panic on some imaginary machine during boot. Pay attention to the lines highlighted in bold:

We can surmise then, that the panic occurred during the initialization routine of module firewire_core as it was loaded. (We might assume then, that the machine's firewire hardware is incompatible with this version of the firewire driver module due to a programmer error, and will have to wait for a new release.) In the meantime, the easiest way to get the machine running again is to prevent the module from being loaded. We can do this in one of two ways:

This article or section is out of date.

The factual accuracy of this article or section is disputed.

You will need a root shell to make changes to the system so the panic no longer occurs. If the panic occurs on boot, there are several strategies to obtain a root shell before the machine deadlocks:

See General troubleshooting#Debugging regressions.

Try linux-mainlineAUR to check if the issue is already fixed upstream. The pinned comment also mentions a repository which contains already built kernels, so it may not be necessary to build it manually, which can take some time.

It may also be worth considering trying the LTS kernel (linux-lts) to debug issues which did not appear recently. Older versions of the LTS kernel can be found in the Arch Linux Archive.

If the issue still persists, bisect the linux-gitAUR kernel and report the bug in accordance to the kernel process for reporting regressions. Depending on the Bugtracker (B:) entry in the MAINTAINERS file this then entails opening an issue via the subsystems mailing lists, Kernel Bugzilla, or in other issue trackers like the DRM Gitlab. It is important to try the "vanilla" version without any patches to make sure it is not related to them. If a patch causes the issue, report it to the author of the patch.

You can shorten kernel build times by building only the modules required by the local system using modprobed-db, or by make localmodconfig. Of course you can completely drop irrelevant drivers, for example sound drivers to debug a network problem.

**Examples:**

Example 1 (unknown):
```unknown
/usr/lib/modules/
```

Example 2 (unknown):
```unknown
/proc/config.gz
```

Example 3 (unknown):
```unknown
CONFIG_IKCONFIG_PROC
```

Example 4 (unknown):
```unknown
Documentation/
```

---

## Fluxbox

**URL:** https://wiki.archlinux.org/title/Fluxbox

**Contents:**
- Installation
- Starting
- Configuration
  - Menus
    - Automatic menu generation
      - fluxbox-generate_menu
      - MenuMaker
      - Xdg-menu
    - Other menus
  - Keyboard

Fluxbox is a window manager for X11. It is based on the (now abandoned) Blackbox 0.61.1 code, but with significant enhancements and continued development. Fluxbox provides a number of window management features such as tabbing and grouping and has hundreds of styles (themes) available. All Fluxbox configuration is stored in plaintext files; however, some settings are exposed graphically in the configuration menu.

Install the fluxbox package.

Run startfluxbox with xinit.

System-wide Fluxbox configuration files are in /usr/share/fluxbox while user configuration files are in ~/.fluxbox:

The Fluxbox root menu is defined in ~/.fluxbox/menu and it can be accessed by right clicking on the desktop. As with other lightweight window managers, Fluxbox does not automatically update its menu when you install new applications. Therefore, the menu will need to be regenerated when new applications are installed/uninstalled.

The basic syntax for a menu item to appear is:

...where "name" is the text you wish to appear for that menu item and "command" is the location of the binary, e.g.:

Note that the "<path to icon>" is optional. If you want to create a submenu, the syntax is:

When you have finished editing, save the file and exit. There is no need to restart Fluxbox. For more information, read editing the Fluxbox menu.

There are some programs which can generate either a complete Fluxbox root menu or a submenu of installed applications which can be manually included in an existing root menu definition. These are outlined below.

There is a built-in command provided with Fluxbox:

This command will auto-generate a ~/.fluxbox/menu file based on your installed programs. However, the menu it generates will not be as comprehensive as that generated by MenuMaker.

MenuMaker is a powerful tool that creates XML-based menus for a variety of Window Managers, including Fluxbox. MenuMaker will search your computer for executable programs and create a menu based on the results. It can be configured to exclude Legacy X, GNOME, KDE, or Xfce applications if desired.

Install menumaker, then you can generate a complete menu and overwrite the default one by running:

You can avoid populating your menu with terminal based applications such as alsamixer(1) by running the following switches with the mmaker command: --no-legacy and --no-debian. For example:

To see more MenuMaker options:

You can also generate a menu using Xdg-menu. See the Xdg-menu#FluxBox section.

In addition to the root menu, Fluxbox also provides the following menus:

The Fluxbox hotkey file is located at ~/.fluxbox/keys. The Control key is represented by Control. Mod1 corresponds to the Alt key and Mod4 corresponds to Super (not a standard key but most users map Super to the Win key).

Just add the following line to ~/.fluxbox/startup:

Instead of 'us', you can also pass your language code and remove the variant option (ex.: 'us_intl', which works like the command above in some setups). See setxkbmap(1) for more options.

To make a help function in your menu, just add in ~/.fluxbox/menu:

You can use just about any clipboard manager you like with Fluxbox. The parcellite package works very well with Fluxbox. Simply install parcellite and then add the commands to start parcellite when Fluxbox starts by adding the following in ~/.fluxbox/startup before the call to exec fluxbox:

Fluxbox defaults to having four workspaces. These are accessible using the Ctrl+F1-F4 shortcuts, or by using the left mouse button to click the arrows on the toolbar. You can also access workspaces via a middle mouse button click on desktop which pops up the Workspaces Menu.

With at least two windows visible on your desktop, use Ctrl+left click on the upper window tab of one window and drag it into the other open window. The two windows will now be grouped together with window tabs in the upper window tab bar. You may now perform a window operation that will affect the entire window "group". To reverse the tabbing, use Ctrl+left click on a tab and drag it to an empty space on the desktop.

Fluxbox provides functionality to autostart applications. The ~/.fluxbox/startup file is a script for autostarting applications as well as starting Fluxbox itself. The # symbol denotes a comment. Make sure that any lines starting applications come before the call to start Fluxbox itself. Otherwise, these lines will not be reached until Fluxbox itself terminates.

Fluxbox provides a wrapper script fbsetbg which can help one to set the wallpaper. Please refer to the Fluxbox wiki for details. Alternatively, you can use a wallpaper setter such as feh or Nitrogen independently if you wish. See below.

Place the following submenu in your Fluxbox menu:

Then, put your background images into ~/.fluxbox/backgrounds or any other directory you specify; they will then appear in the same fashion as your styles.

The same applies to a dual screen wallpaper on a system without 'xinerama' (NVidia TwinView for example):

To make Fluxbox load a wallpaper via feh:

To install a Fluxbox theme, extract the theme archive file to a styles directory. The default directories are:

The fluxmod-stylesAUR package contains a number of Fluxbox styles from the (now defunct) fluxmod.dk site.

To create your own Fluxbox styles, please refer to fluxbox-style(5) and tenr.de Fluxbox style guide.

If you use mmaker -f FluxBox to create your menus, you will not see the styles menu selection after you install the styles. To correct this, add the following to ~/.fluxbox/menu after the restart menu item:

Some window managers, such as Fluxbox, Window Maker and Openbox, have a "Slit". This is a dock for any application that can be 'dockable'. A docked application is anchored and appears on every workspace. It cannot be moved freely and is not influenced by any manipulation to windows. It is essentially a small widget. Dock apps that are useful in such a situation tend to be clocks, system monitors, weather apps and so on. Visit dockapps.net to see what dockapps are available.

**Examples:**

Example 1 (unknown):
```unknown
startfluxbox
```

Example 2 (unknown):
```unknown
/usr/share/fluxbox
```

Example 3 (unknown):
```unknown
~/.fluxbox/menu
```

Example 4 (unknown):
```unknown
[exec] (name) {command} <path to icon>
```

---

## man page

**URL:** https://wiki.archlinux.org/title/Man_page

**Contents:**
- Installation
- Accessing man pages
- Searching manuals
- Page width
- Reading local man pages
  - Conversion to HTML
    - mandoc
    - man2html
    - man -H
    - roffit

man pagesabbreviation for "manual pages"are the form of documentation that is available on almost all UNIX-like operating systems, including Arch Linux. The command used to display them is man(1).

In spite of their scope, manual pages are designed to be self-contained documents, consequently limiting themselves to referring to other manual pages when discussing related subjects. This is in sharp contrast with the hyperlink-aware Info documentsGNU attempt at replacing the traditional manual page format.

man-db implements man on Arch Linux, and less is the default pager used with man. mandoc can also be used.

man-pages provides both the Linux and the POSIX.1 man pages [1].

Some localized man pages are also available:

You can also search for all of the available localized man pages on the official repositories and on the AUR.

To read a man page, simply enter:

Manuals are sorted into several sections. Each section has an intro, such as intro(1), intro(2) and so on. For a full listing see man-pages(7)  Sections of the manual pages.

Man pages are usually referred to by their name, followed by their section number in parentheses. Often there are multiple man pages of the same name, such as man(1) and man(7). In this case, give man the section number followed by the name of the man page, for example:

to read the man page on /etc/passwd, rather than the passwd utility.

Or equivalently, the man page followed by the section number, separated by a period:

Man pages can be searched when the exact name of a page is not known using any of the following equivalent commands:

expression is interpreted as a regular expression by default.

To search for keywords in whole page texts, use the -K option instead.

One-line descriptions of man pages can be displayed using the whatis command. For example, for a brief description of the man page sections about ls, type:

The man page width is controlled by the MANWIDTH environment variable.

If the number of columns in the terminal is too small (e.g. the window width is narrow), the line breaks will be wrong. This can be very disturbing for reading. You can fix this by setting the MANWIDTH on man invocation. With Bash, that would be:

You can use some applications to view man pages:

Using browsers such as lynx and Firefox to view man pages allows users to reap info pages' main benefit of hyperlinked text. Alternatives include the following:

Install the mandoc package. To convert a page, for example free(1):

Now open the file called free.html in your favourite browser.

First, install man2html from the official repositories.

Now, convert a man page:

Another use for man2html is exporting to raw, printer-friendly text:

The man-db implementation also has the ability to do this on its own:

This will read your BROWSER environment variable to determine the browser. You can override this by passing the binary to the -H option.

First install roffitAUR.

To convert a man page:

man pages have always been printable: they are written in troff(1), which is fundamentally a typesetting language. Therefore, you can easily convert man pages to any of the formats supported as output devices by groff, which is used by man-db. For a list of output devices, see the -T option in groff(1) (or mandoc(1) if you use the mandoc package).

This will produce a PDF file:

Caveats: Fonts are generally limited to Times at hardcoded sizes. Some man pages were specifically designed for terminal viewing, and will not look right in PS or PDF form.

For an alternative interface for reading manual pages, one that supports modern features such as hyperlinks and history, install qmanAUR or qman-gitAUR. You can now use qman in the place of man:

For more information and troubleshooting, see the project's GitHub page.

Note that while man-pages provides man pages for POSIX.1-2017 (see [2]), an official online reference also exists:

There is also a comparison table of the online databases.

Here follows a non-exhaustive list of noteworthy pages that might help you understand a lot of things more in-depth. Some of them might serve as a good reference (like the ASCII table).

More generally, have a look at category 7 (miscellaneous) pages:

Arch Linux specific pages:

This article or section needs expansion.

**Examples:**

Example 1 (unknown):
```unknown
$ man page_name
```

Example 2 (unknown):
```unknown
$ man 5 passwd
```

Example 3 (unknown):
```unknown
/etc/passwd
```

Example 4 (unknown):
```unknown
$ man passwd.5
```

---

## SHA hashes

**URL:** https://wiki.archlinux.org/title/SHA_password_hashes

**Contents:**
- SHA password hashes
  - Configuration
  - Usage

The Secure Hash Algorithms (SHA) are a set of hash functions, frequently used for a large variety of purposes.

Some examples of usage of the SHA hash functions in Arch Linux are:

By default Arch Linux used SHA-512 for passwords with package shadow releases from 4.1.4.3-3 (see bug 13591) to 4.14.0 (see yescrypt ). It replaced the older MD5 hash function, which has been found compromised by collision vulnerabilities. See wikipedia:Secure Hash Algorithms#Comparison of SHA functions for more information.

The SHA-512 password hashing can be configured with the rounds=N option to improve key strengthening. For example, rounds=65536 means that an attacker has to compute 65536 hashes for each password they test against the hash in the /etc/shadow password file.

Therefore the attacker will be delayed by a factor of 65536. This also means that your computer must compute 65536 hashes every time you log in, but even on slow computers that takes less than one second.

Since pam release 1.6.0 the SHA-512 rounds option can be configured by either editing the /etc/login.defs file and setting an value for the SHA_CRYPT_MAX_ROUNDS parameter, or editing /etc/pam.d/passwd and adding the rounds with an appropriate value.[1]

If you do not use the rounds option, PAM will use a default different to the 5000 rounds example commented in /etc/login.defs.

For a more detailed explanation of the /etc/pam.d/passwd password options check the pam_unix(8) man page.

Even though you have changed the encryption settings, your passwords are not automatically re-hashed. To fix this, you must reset all user passwords so that they can be re-hashed.

As root issue the following command,

where username is the name of the user whose password you are changing. Then re-enter their current password, and it will be re-hashed.

To verify that your passwords have been re-hashed, check the /etc/shadow file as root. Passwords hashed with SHA-256 begin with a $5, passwords hashed with SHA-512 will begin with $6 and yescrypt hashes with $y.

**Examples:**

Example 1 (unknown):
```unknown
rounds=65536
```

Example 2 (unknown):
```unknown
/etc/shadow
```

Example 3 (unknown):
```unknown
/etc/login.defs
```

Example 4 (unknown):
```unknown
SHA_CRYPT_MAX_ROUNDS
```

---

## BeeGFS

**URL:** https://wiki.archlinux.org/title/BeeGFS

**Contents:**
- Terminology
- Installation
  - Example cluster deployment
  - NTP client
  - Management server
  - Monitoring server
    - Configuration of default Grafana panels
    - Accessing Grafana panels
  - Metadata server
  - Storage server

This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.

BeeGFS is a scalable network-storage platform with a focus on being distributed, resilient, highly configurable and having good performance and high reliability. BeeGFS is extremely configurable, with administrators being able to control virtually all aspects of the system. A command line interface is used to monitor and control the cluster.

In addition to the free and open-source packages described here, BeeGFS also offers a number of Enterprise Features and Professional Support, which include:

The following hardware configuration will be used in this example:

Install and run a time synchronization client on all the nodes. See Time synchronization for details.

Install it with the package beegfs-mgmtdAUR on the management node 192.168.0.1.

The management service needs to know where it can store its data. It will only store some node information like connectivity data, so it will not require much storage space and its data access is not performance critical. Thus, this service is typically not running on a dedicated machine.

Start/enable the beegfs-mgmtd@node01.service on the management node.

Install the package beegfs-monAUR on the management/monitoring node 192.168.0.1, which collects statistics from the system and provides them to the user using a time series database InfluxDB. For visualization of the data beegfs-mon provides predefined Grafana panels that can be used out of the box.

Before running beegfs-mon, you need to edit the configuration file /etc/beegfs/beegfs-mon.conf. If you have everything installed on the same host, you only need to specify the management host:

Start/enable the beegfs-mon@node01.service on the management/monitoring node.

You can use the provided installation script for default InfluxDB and Grafana deployments on the same host.

Access the application on localhost, e.g.: http://127.0.0.1:3000 . Refer to Custom Grafana Panel Configuration for non-default installations and for the Reference to All Metrics monitored.

Install the package beegfs-metaAUR on the metadata server(s), i.e. 192.168.0.2.

The metadata service needs to know where it can store its data and where the management service is running. Typically, one will have multiple metadata services running on different machines.

Start/enable the beegfs-meta@node02.service on the metadata node.

Install the package beegfs-storageAUR on the storage server(s), i.e. 192.168.0.3.

The storage service needs to know where it can store its data and how to reach the management server. Typically, one will have multiple storage services running on different machines and/or multiple storage targets (e.g. multiple RAID volumes) per storage service.

Start/enable the beegfs-storage@node03.service on the storage node.

Install the package beegfs-clientAUR on the client node, which will build the client Kernel module.

The client service needs to know where it can reach the management server.

The client service needs to know where it can mount the cluster storage, as well as the location of the client configuration file.

Load the Kernel module and its dependencies.

Start/enable the beegfs-helperd@node04.service on the client node.

Start/enable the beegfs-client.service on the client node.

Install the package beegfs-utilsAUR.

Check the detected network interfaces and transport protocols from a client node with the following commands:

This article or section needs expansion.

**Examples:**

Example 1 (unknown):
```unknown
beegfs-fsck
```

Example 2 (unknown):
```unknown
libbeegfs-ib
```

Example 3 (unknown):
```unknown
192.168.0.1
```

Example 4 (unknown):
```unknown
192.168.0.2
```

---

## xmodmap

**URL:** https://wiki.archlinux.org/title/Xmodmap

**Contents:**
- Introduction
- Installation
- Keymap table
- Custom table
  - Activating the custom table
  - Test changes
- Modifier keys
  - Finding the keysym column modifier keys
  - Reassigning modifiers to keys on your keyboard
- Compose key

This article or section needs expansion.

xmodmap is a utility for modifying keymaps and pointer button mappings in Xorg.

xmodmap is not directly related to X keyboard extension (XKB), as it uses different (pre-XKB) ideas on how keycodes are processed within X. Generally, it is only recommended for the simplest tasks. See X keyboard extension for advanced layout configuration.

There are two types of keyboard values in Xorg: keycodes and keysyms.

xmodmap can be installed through the xorg-xmodmap package.

Optionally, install xkeycaps, which is a graphical front-end to xmodmap.

Print the current keymap table formatted into expressions:

Each keycode is followed by the keysym it is mapped to. The above example indicates that the keycode 57 is mapped to the lowercase n, while the uppercase N is mapped to keycode 57 plus Shift.

Each keysym column in the table corresponds to a particular combination of modifier keys:

Not all keysyms have to be set, but to assign only a latter keysym, use the NoSymbol value.

To see which keycode corresponds to a key, see Keyboard input#Identifying keycodes in Xorg for details on the xev utility which will output relevant keycode/keysym information about a key when you press it.

Note that xmodmap is influenced by xkbd settings, so all eight keysym are available for the US(intl) xkbd layout but not for the default US (it is missing the ralt_switch symbol defined in level3). To have all 8 keysyms available you should configure the (intl) variant of the keyboard. Using US layout as an example, $ setxkbmap -layout 'us(intl)' before calling xmodmap to test your changes in the current X session. To permanently make this change, edit the xorg configuration or your .xprofile or .xinitrc file. See Xorg/Keyboard configuration#Setting keyboard layout for a full explanation.

To create a key map (i.e. ~/.Xmodmap):

With GDM, XDM or LightDM there is no need to source ~/.Xmodmap. For startx, use:

Alternatively, edit the global startup script /etc/X11/xinit/xinitrc.

To make temporary changes:

xmodmap can also be used to override modifier keys, e.g. to swap Control and Super (the Windows key).

Print the current modifier table verbosely (full sample):

This article or section is a candidate for merging with #Keymap table.

Before assignment, the modifier keys need to be cleared. This applies to both modifiers you intend to assign and modifiers on keys that you intend to use. For example, if you intend to assign Caps_Lock to your A key and B to your NumLock key, you need to first clear the modifiers for both Caps_Lock and Num_Lock, then assign the keysyms, and finally add back the modifiers.

! is a comment, so only the modifiers Control and Mod4 get cleared in the following example. Then the keysyms Control_L, Control_R, Super_L and Super_R are assigned to the opposite modifier. Assigning both left and right to the same modifier means that both keys are treated the same way.

The following example modifies CapsLock to Control, and Shift+CapsLock to CapsLock:

This article or section is a candidate for merging with Xorg/Keyboard configuration#Configuring compose key.

A compose key serves to create special characters and symbols that may not be directly accessible on the keyboard. This is especially useful for typing accented letters from non-English languages. For example, pressing Compose e ' in succession will produce . Some characters require more than 2 keys to be pressed after Compose. Usually a modifier key of choice is mapped to Compose. The compose key can be set in the GUI settings of most desktop environments, but these options will not work if a custom key map table is used as described in this article. To set the compose key using Xmodmap, use the Multi_key identifier. For example, to map the right alt key (AltGr) to the compose key:

The keycode number could vary based on keyboard models.

This article or section is a candidate for merging with Mouse buttons.

The natural scrolling feature available in OS X Lion (mimicking smartphone or tablet scrolling) can be replicated with xmodmap. Since the synaptics driver uses the buttons 4/5/6/7 for up/down/left/right scrolling, you simply need to swap the order of how the buttons are declared in ~/.Xmodmap:

This article or section is a candidate for merging with Mouse buttons.

The left, middle and right mouse buttons correspond to buttons 1,2 and 3 respectively in the synaptics driver. To swap left and right mouse buttons, again simply reverse the order in which they are listed in your ~/.Xmodmap:

This should suffice for a simple mouse setup. Again, update xmodmap:

Simplest example of changing CapsLock into Control.

Laptop users may prefer having CapsLock as Control. The Left Control key can be used as a Hyper modifier (an additional modifier for emacs, openbox or i3).

Users who wish to have a Hyper key on full keyboard layout may wish to use the Right Super key as Hyper.

Should work fine for layouts similar to Croatian as well.

**Examples:**

Example 1 (unknown):
```unknown
keysym 061
```

Example 2 (unknown):
```unknown
$ xmodmap -pke
```

Example 3 (unknown):
```unknown
[...]
keycode  57 = n N
[...]
```

Example 4 (unknown):
```unknown
Mode_switch+Key
```

---

## SCP and SFTP

**URL:** https://wiki.archlinux.org/title/SCP_and_SFTP

**Contents:**
- Secure file transfer protocol (SFTP)
- Secure file transfer protocol (SFTP) with a chroot jail
  - Setup the filesystem
  - Create an unprivileged user
  - Setup OpenSSH
- Secure copy protocol (SCP)
  - General Usage
    - Linux to Linux
    - Linux to Windows
  - Scponly

This article or section is a candidate for merging with SFTP chroot.

The SSH file transfer protocol (SFTP) is a protocol to transfer files, relying on a secure shell back-end. The Secure copy (SCP) is an outdated protocol via a Secure Shell connection. Both protocols allow secure file transfers, encrypting passwords and transferred data. The modern SFTP protocol, however, features additional capabilities like, for example, resuming broken transfers or remote file manipulation like deletion.

Install and configure OpenSSH. Once running, SFTP is available by default.

Access files with the sftp program or SSHFS. Many standard FTP programs should work as well.

Sysadmins can jail a subset of users to a chroot jail using openssh thus restricting their access to a particular directory tree. This can be useful to simply share some files without granting full system access or shell access. Users with this type of setup may use SFTP clients such as filezilla to put/get files in the chroot jail.

Create a jail directory:

Optionally, bind mount the filesystem to be shared to this directory. In this example, /mnt/data/share is to be used. It is owned by root and has octal permissions of 755.

Create the share user and setup a good password:

Add the following to the end of /etc/ssh/sshd_config to enable the share and to enforce the restrictions:

Restart sshd.service to re-read the configuration file. See SFTP chroot to configure the keys correctly when using chroot or it will get permission denied.

Test that in fact, the restrictions are enforced by attempting an ssh connection via the shell. The ssh server should return a polite notice of the setup:

Install, configure and start OpenSSH. It contains the scp utility to transfer files.

More features are available by installing additional packages, for example rsshAUR or scponly described below.

Copy file from a remote host to local host SCP example:

Copy file from local host to a remote host SCP example:

Copy directory from a remote host to local host SCP example:

Copy directory from local host to a remote host SCP example:

Copy file from remote host to remote host SCP example:

Use a Windows program such as WinSCP

Scponly is a limited shell for allowing users scp/sftp access and only scp/sftp access. Additionally, one can setup scponly to chroot the user into a particular directory increasing the level of security.

For existing users, simply set the user's shell to scponly:

The package comes with a script to create a chroot. To use it, run:

For security reasons the directory set as the chroot directory must be owned by root with only root having write access to it otherwise sftp/ssh connections will be denied. This of course means regular users cannot upload files to the root directory. In order to get around this while not compromising security you can create a folder inside the chroot directory which the regular user or group has write access to, e.g:

Some applications utilizing SFTP do not allow input of sub-directories when performing operations (e.g. uploading files), and will attempt to upload files to the chroot base directory (which will be denied). In order to force these applications to use a specific sub-directory you can append the following to the "ForceCommand" option:

Users on connect will then have their start directory change to the specified sub-directory (remember to restart the sshd server).

**Examples:**

Example 1 (unknown):
```unknown
# mkdir -p /var/lib/jail
```

Example 2 (unknown):
```unknown
/mnt/data/share
```

Example 3 (unknown):
```unknown
# mount -o bind /mnt/data/share /var/lib/jail
```

Example 4 (unknown):
```unknown
# useradd -g sshusers -d /var/lib/jail foo
# passwd foo
```

---

## KeePass

**URL:** https://wiki.archlinux.org/title/KeePassXC

**Contents:**
- Installation
- Integration
  - Plugin installation in KeePass
  - Browser integration
    - keepassxc-browser for KeePassXC
    - keepassxc-browser for KeePass
    - KeePassRPC and Kee
    - Via autotype feature
  - Nextcloud
  - Yubikey

KeePass is an encrypted password database format. It is an alternative to online password managers and is supported on all major platforms.

There are two versions of the format: KeePass 1.x (Classic) and KeePass 2.x

There are three major implementations of KeePass available in the official repositories:

Other lesser-known alternatives can be found in the AUR:

Many plugins and extensions are available for integrating KeePass to other software. KeePassX and KeePassXC do not have a plugin interface, but KeePassXC has various integrations built-in.

KeePass is by default installed at /usr/share/keepass/. Copy plugin.plgx to a plugins sub-directory under the KeePass installation directory as demonstrated below:

keepassxc-browser is the browser extension of KeePassXCs built-in browser integration using native-messaging and transport encryption using libsodium. It was developed to replace KeePassHTTP, as KeePassHTTPs protocol has fundamental security problems.

The developers provide the browser extension on

Support for Firefox and Chromium forks is available. For librewolfAUR, open KeePassXC, go to Tools > Settings > Browser Integration > Advanced > Config Location:, and add ~/.librewolf/native-messaging-hosts.

The source code and an explanation how it works can be found on GitHub, the KeePassXC developers provide a configuration guide on their website.

This article or section is being considered for removal.

keepassxc-browser can also be used with KeePass through Keepass-natmsg Plugin from AUR (keepass-natmsgAUR) and is recommended as successor of KeePassHTTP.

Kee (GitHub repo) is a browser extension for Firefox and Chromium which integrates KeePass through KeePassRPC, a KeePass plugin from the same developers.

The KeePass plugin is available from GitHub or from the AUR (keepass-plugin-rpcAUR).

The browser extension can be found on GitHub, Firefox Add-ons and the chrome web store.

An alternative to having a direct channel between browser and KeePass(XC) is using the autotype feature.

To enable the autotype feature on Wayland, edit /usr/share/applications/org.keepassxc.KeePassXC.desktop and change the value of Exec to keepassxc -platform xcb. Alternatively, set the QT_QPA_PLATFORM=xcb environment variable before launching KeePassXC. However, native Wayland applications will not work with autotype. For example, autotype works when running Firefox without Wayland, but not with.

There are browser extensions which support this way by putting the page URL into the window name:

This article or section is being considered for removal.

YubiKey can be integrated with KeePass thanks to contributors of KeePass plugins. KeepassXC provides built-in support for Yubikey Challenge-Response without plugins.

KeePassXC offers SSH agent support, a similar feature is also available for KeePass using the KeeAgent plugin.

The feature allows to store SSH keys in KeePass databases, KeePassXC/KeeAgent acts as OpenSSH Client and dynamically adds and removes the key to the Agent.

The feature in KeePassXC is documented in its FAQ. First configure SSH agent to start on login and make sure the SSH_AUTH_SOCK variable is set. Then logout and log back in. Now, in KeePassXC settings, enable SSH agent integration. The SSH_AUTH_SOCK value exposed in the UI should correspond to what you configured earlier.

KeePassXC contains a Freedesktop.org Secret Service integration. It will allow external applications to use KeePassXC as an encrypted database (a.k.a. as a vault, wallet, or keyring) to store user credentials (e.g., messaging applications, games).

It can be enabled by going into the settings (under the Tools menu), and selecting which group(s) you want to share (for each database, open Database > Database Settings..., then go to the Secret Service Integration tab).

KeePassXC will refuse to enable its integration if it detects that another program (such as GNOME/Keyring) is already providing that service. You should first stop that program (for example, by stopping gnome-keyring-daemon.service user unit for gnome-keyring). Note that you will likely want to disable the program permanently, otherwise KeePassXC's integration will fail on the next reboot (for example, by disabling the gnome-keyring-daemon.socket of a systemd/User for gnome-keyring).

An application that requests access to the database will connect to KeePassXC through D-Bus, where KeePassXC will be "seen" just as GNOME libsecret by the application. The database that will be exposed can be stored anywhere on the disk, just like any other KeePassXC database, and the master password of this database will be the one to type when applications will want to retrieve a credential in the future.

This article or section is being considered for removal.

To ensure also dbus doesn't active gnome-keyring again (bypassing the systemd socket for some odd reason...) delete these files:

and edit your pacman config prevent it from recreating these conflicting files when reinstalling or updating packages:

KeePassXC will not be automatically started when an application requests secrets, which may cause them to break. A D-Bus auto-start file can be created:

If you are an avid user of clipboard managers, you may need to disable your clipboard manager before you launch Keepass and then re-start your clipboard manager afterwards.

KeePassXC implementations has the option to auto-clear the clipboard manager after an amount of time, enough to paste copied items.

To enable the dark theme for KeePass, install keepass-keethemeAUR. After installation, the plugin will get compiled upon starting KeePass. It can then be activated via Tools > Dark Theme, or by pressing Ctrl+t.

Without using specialized plugin, a KeePass database is well-suited to be synchronized through Syncthing. On conflicts, some applications provides a way to resolve them, such as the Merge from database feature of KeePassXC.

If the user interface elements are not scaled properly, see HiDPI#Qt 5 and upstream bug report.

Some options like Start minimized and locked may appear greyed-out. According to a discussion on SourceForge, since version 2.31, KeePass has disabled two options because of their broken behaviors on Mono.

To force these features to be enabled, launch KeePass with the -wa-disable:1418 argument.

In some desktop environments, the tray icon of KeePass may appear too big or too small due to Mono's bug, according to a bug report on SourceForge.

Keebuntu contains three plugins to provide desktop integration:

After installing one of these plugins, it is sometimes necessary to hide the original tray icon to prevent duplicate icons in the system tray.

First, check that the group under which your passwords are stored is exposed; the Tools > Settings menu contains a list of groups enabled for each database. If a database isn't exposing the proper group, select its tab, open Database > Database Settings..., then select the group in the Secret Service Integration tab).

Note that merging a database can cause it to stop exposing any groups.

If you are experiencing graphical glitches, install the qt5-wayland package. KeePassXC (as of version v2.7.7) still uses Qt5.

**Examples:**

Example 1 (unknown):
```unknown
keepassxc-cli
```

Example 2 (unknown):
```unknown
/usr/share/keepass/
```

Example 3 (unknown):
```unknown
plugin.plgx
```

Example 4 (unknown):
```unknown
# mkdir /usr/share/keepass/plugins
# cp plugin.plgx /usr/share/keepass/plugins
```

---

## Node.js

**URL:** https://wiki.archlinux.org/title/Node.js

**Contents:**
- Installation
  - Node Version Manager
- Node Packaged Modules
  - Managing packages with npm
    - Installing packages
      - Allow user-wide installations
    - Updating packages
      - Updating all packages
    - Removing packages
    - Listing packages

The factual accuracy of this article or section is disputed.

Node.js is a JavaScript runtime environment combined with useful libraries. Node.js uses Google V8 engine to execute code outside of the web browser. Due to its event-driven, non-blocking I/O model, it is suitable for real-time web applications.

Install the nodejs package. There are long-term support (LTS) releases, too:

It is not uncommon to need or desire to work in different versions of Node.js. A preferred method among Node.js users is to use Node Version Manager (nvm), which allows for cheap and easy alternative installs. You can set nvm up by adding this to your Command-line shell#Configuration files:

Usage is well documented on the project's GitHub but is as simple as:

If you want to run nvm use automatically every time there is a .nvmrc file on the directory, add this in shell initialization files.

npm is the official package manager for Node.js. It can be installed with the npm package.

This article or section is being considered for removal.

Any package can be installed using:

This command installs the package in the current directory under node_modules and executables under node_modules/.bin.

For a system-wide installation global switch -g can be used:

By default this command installs the package under /usr/lib/node_modules/npm and requires root privileges to do so. (If using a secure umask like umask 0077, you will need to set up a permissive sudo umask for the package to be usable.)

To allow global package installations for the current user, set the npm_config_prefix environment variable. This is used by both npm and yarn.

Re-login or source to update changes.

You can also specify the --prefix parameter for npm install. However, this is not recommended, since you will need to add it every time you install a global package.

Another option is to set prefix field in $HOME/.npmrc. This achieves the same effect as using npm_config_prefix="$HOME/.local" in one's .profile:

Updating packages is as simple as

For the case of globally installed packages (-g)

However, sometimes you may just wish to update all packages, either locally or globally. Leaving off the packageName npm will attempt to update all packages

or add the -g flag to update globally installed packages

To remove a package installed with the -g switch simply use:

to remove a local package drop the switch and run:

To show a tree view of the installed globally packages use:

This tree is often quite deep. To only display the top level packages use:

To display obsolete packages that may need to be updated:

Some Node.js packages can be found in Arch User Repository (AUR) with the name nodejs-package_name.

See the Node.js package guidelines for best practices in packaging Node.js packages for AUR.

Using npm help topic may not display the documentation for topic. Instead, use man npm-topic. For example:

This is a bug with Arch's npm package.

In case of errors like gyp WARN EACCES user "root" does not have permission to access the ... dir, --unsafe-perm option might help:

Since npm 5.x.x. package-lock.json file is generated along with the package.json file. Conflictions may arise when the two files refer to different package versions. A temporary method to solving this problem has been:

However, fixes were made after npm 5.1.0 or above. For further information, see: missing dependencies

**Examples:**

Example 1 (unknown):
```unknown
. /usr/share/nvm/init-nvm.sh
```

Example 2 (unknown):
```unknown
$ nvm install 8.0
Downloading and installing node v8.0.0...
[..]

$ nvm use 8.0
Now using node v8.0.0 (npm v5.0.0)
```

Example 3 (unknown):
```unknown
nodejs-fake
```

Example 4 (unknown):
```unknown
--assume-installed nodejs=<version>
```

---

## Mobile broadband modem

**URL:** https://wiki.archlinux.org/title/Mmcli

**Contents:**
- Device identification
- Mode switching
  - From mass storage mode
  - From router mode
- Modem mode
  - Remove the PIN
    - Using mmcli
    - Using AT commands
  - Connection
    - ModemManager

This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.

A number of mobile phone carriers around the world offer internet access via mobile broadband (e.g. LTE, UMTS, EDGE, GSM, etc.).

This article focuses on mobile broadband modems in the format of portable USB devices and mini PCIe cards. For standalone mobile broadband routers, simply connect to them using an interface they provide (e.g. Ethernet or Wi-Fi).

Examine the output of:

which will show the vendor and product IDs of the device. Note that some devices will show two different product IDs at different times as explained below.

Often these devices will have two modes (1) USB flash memory storage (2) USB modem. The first mode, sometimes known as ZeroCD, is often used to deliver an internet communications program for another operating system and is generally of no interest to Linux users. Additionally some have a slot into which the user can insert an additional flash memory card.

A useful utility for switching these devices into modem mode is usb_modeswitch. It ships with udev rules /usr/lib/udev/rules.d/40-usb_modeswitch.rules that contain entries for many devices, which it will switch to modem mode upon insertion.

When a device is switched, its product ID may change to a different value. The vendor ID will remain unchanged. This can be seen in the output of lsusb.

Some devices are supported in the USB serial kernel module called option (named after the "Option" devices, but not limited to just those) and may be used without usb_modeswitch.

This article or section needs expansion.

Depending on the device, it may expose an Ethernet network interface or provide Wi-Fi. In that case you will need to have the interface up. If the device has a DHCP server, you can use a DHCP client to match it. Otherwise, you will have to have some knowledge about the network the device expects. Such information might be obtained from its behavior in another OS. Or by searching the web. Or from the drivers, and other information, stored in the initial USB flash memory storage (ZeroCD). Some Huawei HiLink devices, for example, sometime operate at 192.168.8.0/24, with a gateway at 192.168.8.1. They also might have a web interface at http://192.168.8.1.

In general, at this point you should note if mode switching left you with additional /dev/ttyUSB* serial device and a ww* network interface. You can do that with journalctl or by shell commands such as:

First of all use your SIM card in a normal phone and disable the PIN request if present. If the SIM card asks the PIN wvdial will not work.

Failing that, you can use mmcli (provided by modemmanager) or AT commands, to unlock the SIM card.

First, list the modems and find the modem's index:

Look for /org/freedesktop/ModemManager1/Modem/MODEM_INDEX.

Find the SIM card index:

Just as with the modem index, look for primary sim path: /org/freedesktop/ModemManager1/SIM/SIM_INDEX.

Remove the requirement for PIN:

Follow the instructions in https://unix.stackexchange.com/a/313878.

To connect to the mobile network, use one of the following methods.

This article or section needs expansion.

ModemManager is a system daemon which controls WWAN devices and connections.

Install the modemmanager and usb_modeswitch packages.

Start and enable ModemManager.service.

Use mmcli(1) to communicate with the modem.

The simplest way to establish a connection is to use mmcli's --simple-connect option.

First, list the modems and find the modem's index:

Look for /org/freedesktop/ModemManager1/Modem/MODEM_INDEX.

Next connect to the mobile network. For example:

Replace internet.myisp.example with your ISP's provided APN. If a user name and password is required, set them accordingly:

To disconnect from the mobile network run:

See also mmcli(1)  EXAMPLES.

NetworkManager uses ModemManager to work with mobile broadband modems. See NetworkManager#Mobile broadband support.

Install libmbim. To bring up the modem you can use mbim-network which is a wrapper for mbimcli calls. First create a profile for mbim-network.

Now connect to the network with:

Then follow Network configuration to bring up the ww* interface and get an IP address using DHCP.

pppd can be used to configure 3g connections. Step by step instruction is available on 3G and GPRS modems with pppd. Optionally, pppconfigAUR can be used to simplify the pppd configuration using dialog interface.

See main article: wvdial

Netctl can be used to establish a connection using a USB modem. An example configuration file provided by netctl is located at /etc/netctl/examples/mobile_ppp. Minimally you will probably have to specify

See the netctl article and netctl.profile(5) for more information.

Some ways to disable usb_modeswitch from operating on a device before the device was inserted, for example to be able to read the initial flash memory (ZeroCD), are:

Masking the udev rule the package is using can be achieved with

There are some useful commands:

Encode *100# to PDU format:

Decode AA180C3602 from PDU format:

Answer decoding (this example is balance response: 151.25):

Some operators return USSD result in PDU encoding, so you should check proper decoding method.

Frequently a 3G connection obtained via a mobile phone operator comes with restricted bandwidth, so that you are only allowed to use a certain bandwidth per time (e.g. 1GB per month). While it is quite straight-forward to know which type of network applications are pretty bandwidth extensive (e.g. video streaming, gaming, torrent, etc.), it may be difficult to keep an overview about overall consumed bandwidth.

A number of tools are available to help with that. Two console tools are vnstat, which allows to keep track of bandwith over time, and iftop to monitor bandwidth of individual sessions.

The internal web server found in some devices, such as some Huawei HiLink, might also show information about bandwidth usage.

This was tested on a Huawei EM770W (GTM382E) 3g card integrated into an Acer Aspire AS3810TG laptop. Install gnokiiAUR, then:

Usually the configuration directory is ~/.config/gnokii.

Edit ~/.config/gnokii/config as follows:

You may have to use a different port depending on your configuration, for example /dev/ttyUSB1 or something else:

You need to be part of the uucp group to use /dev/ttyUSB0.

Click on the "SMS" icon button, a window opens up. Then click: "messages->activate sms reading". Your messages will show up in the window.

A small command line script using gnokii to read SMS on your SIM card (not phone memory) without having to start a GUI:

Granted this does not work very well if your SMS contains the word "Text", but you may adapt the script to your liking.

Another option is to use mmcli, you can use simple bash script like this one[dead link 2024-10-12HTTP 404] that is also used to write messages or this one below:

Some Devices, such as some Huawei HiLink, include an email like web interface for SMS. It is included in the device internal web server, which is used for other purposes too.

You may need give permission by creating file with content like

Unplugging, and plugging, the device is sometimes used to restart the USB device. The following describes how to do that from the shell. Doing that from the shell might be useful, if, for example, the plug is at the rear side of the PC. The method described is not just for USB modems. It should be good for many other USB devices.

The important part is that the requirements are for the USB bus, and the port, the device is attached to. There could be one, or more, sub ports too. Suppose I obtained bus 2 and port 4, without sub ports, for my device from the output of lsusb -t. This information is also recorded in the journal. With

I can verify it is the intended device.

The following sequence will restart the device:

Some more comments are at, for example, https://askubuntu.com/questions/1036341/unplug-and-plug-in-again-a-usb-device-in-the-terminal.

This problem commonly occurs on some modems which locked by a mobile operator. You can successfully connect to the internet but after few minutes connection halts and your modem reboots. That happens because an operator built a some checks into modem firmware so a modem checks if a branded software is running on your pc, but usually that software is Windows-only, and obviously you do not use it. Fix (it works on ZTE-mf190 at least) is simple - send this command through serial port (use minicom or similar soft):

This command will delete a NODOWNLOAD.FLG file in the modem's filesystem - it will disable such checks.

Another possibility for such disconnections is to help the customer save bandwidth, which might be expensive. With Huawei HiLink devices with a web interface, there might be an option there to set a longer period of inactivity before the connection hangs up.

Someone claims that the connection speed under Linux is lower than Windows [3]. This is a short summary for possible solutions which are not fully verified.

In most of conditions, the low speed is caused by bad receiver signals and too many people in cell. But you still could use the following method to try to improve the connection speed:

It is advisable to see the baud rate set by the official modem application for Windows (possibly 9600 on Vista).

If you are getting low quality images while browsing the web over a mobile broadband connection with the hints shift+r improves the quality of this image and shift+a improves the quality of all images on this page, follow these instructions:

Edit /etc/tinyproxy/tinyproxy.conf and insert the following two lines:

Start tinyproxy.service

Configure your browser to use localhost:8888 as a proxy server and you are all done. This is especially useful if you are using, for example, Google Chrome which, unlike Firefox, does not allow you to modify the Pragma and Cache-Control headers.

In case ModemManager does not recognize the modem, check the unit status of ModemManager.service. If you get error messages such as Couldn't check support for device and not supported by any plugin, you may have to whitelist your device using the ModemManager filter rules.

The FCC lock is a software lock integrated in WWAN modules shipped by several different laptop manufacturers like Lenovo, Dell, or HP. This lock prevents the WWAN module from being put online until some specific unlock procedure (usually a magic command sent to the module) is performed.

Since release 1.18.4, the ModemManager daemon no longer automatically performs the FCC unlock procedure [4].

ModemManager will keep on providing support for the known FCC unlock procedures, but no longer automatically: the user must install and select the FCC unlock procedure needed in the specific laptop being used. This applies to: EM7355, MC8805, MC7355, EM7455, SDX55, EM120.

The modemmanager package ships several scripts installed under /usr/share/ModemManager/fcc-unlock.available.d/ and named as vid:pid with either the PCI or USB vendor and product IDs. However, they are not used if they are not in the /etc/ModemManager directory.

For each device the vid:pid can be found in the brackets at the end of the line:

To enable unlock script for the device it must be symlinked like so:

For a Quectel EM120 modem that would be:

See the ModemManager documentation for more information.

If NetworkManager persists on that the device (e.g. /dev/cdc-wdm0) is not available while ModemManager can use it, it could either be, that the device is blocked using a hardware switch, by rfkill or just NetworkManager believes that.

The wwan device should be listed as unblocked for both SOFT & HARD. If it is HARD blocked, a hardware switch blocks the device. If it is SOFT blocked, unblock it using:

If NetworkManager still declares the device not available, it could be that NetworkManager is not synced with rfkill. Check that using:

If WWAN is listed as disabled, enable it using:

This has been reported to happen on some LTE modems with buggy or incompatible firmware versions. In this scenario, when inspecting the bearer with:

It can be seen how the IPv4 configuration section shows no IP address, and may show dhcp as the method despite the associated interface (e.g. wwan0) not being dhcp-capable. In this cases, the modem firmware is not behaving correctly and it should be upgraded.

**Examples:**

Example 1 (unknown):
```unknown
$ lspci -nn
```

Example 2 (unknown):
```unknown
/usr/lib/udev/rules.d/40-usb_modeswitch.rules
```

Example 3 (unknown):
```unknown
/dev/ttyUSB*
```

Example 4 (unknown):
```unknown
$ ls /dev/ttyUSB*
$ ip link
```

---

## Help:Reading

**URL:** https://wiki.archlinux.org/title/Disable

**Contents:**
- Organization
- Formatting
- Root, regular user or another user
- Append, add, create, edit
  - Make executable
- Source
- Installation of packages
  - Official packages
  - Arch User Repository
- Control of systemd units

Because the vast majority of the ArchWiki contains indications that may need clarification for users new to Arch Linux (or GNU/Linux in general), this rundown of basic procedures was written both to avoid confusion in the assimilation of the articles and to deter repetition in the content itself.

Most articles on the ArchWiki do not attempt to provide a holistic introduction to a single topic; they are instead written in adherence to the "Don't Repeat Yourself" principle, under the assumption that the user will seek out and read any supporting material that they do not yet understand. Where possible, such supporting material is indicated in the article via special formatting, see #Formatting.

Because of this organization, it may be necessary to examine several related sources in order to fully understand an ArchWiki article. In particular, users who are new to Arch (or GNU/Linux in general) should expect to end up reading a great number of articles even when solving simple problems. It is especially important to study the supporting material before seeking additional help from other users.

Some lines are written like so:

Others have a different prefix:

The numeral or hash sign (#) indicates that the command needs to be run as root, whereas the dollar sign ($) shows that the command should be run as a regular user.

When the commands need to run as a specific user, they will be prefixed by the username in square brackets, for example:

This means you should use a privilege elevation tool, e.g. with sudo:

A notable exception to watch out for:

In this example, the context surrounding the numeral sign communicates that this is not to be run as a command; it should be edited into a file instead. So in this case, the numeral sign denotes a comment. A comment can be explanatory text that will not be interpreted by the associated program. Bash scripts denotation for comments happens to coincide with the root PS1.

After further examination, "give away" signs include the uppercase character following the # sign. Usually, Unix commands are not written this way and most of the time they are short abbreviations instead of full-blown English words (e.g., Copy becomes cp).

Regardless, most articles make this easy to discern by notifying the reader:

Append to ~/path/to/file:

When prompted to append to, add to, create, or edit one or more files, it is implied that you should use one of the following methods.

To create or modify multiline files, it is suggested to use a text editor. For example, using the nano command to edit the file /etc/bash.bashrc is:

To create or overwrite a file from a string, it may be simpler to use output redirection. The following example creates or overwrites the contents of the file /etc/hostname with the text myhostname.

Output redirection can also be used to append a string to a file. The following example appends the text [custom-repo] to the file /etc/pacman.conf.

When prompted to create directories, use the mkdir command:

After creating a file, if it is meant to be run as a script (whether manually or called by another program), it needs to be set as executable, for example with:

See chmod. Some applications such as file managers may provide graphical interfaces to do the same.

Some applications, notably command-line shells, use scripts for their configuration: after modifying them, they must be sourced in order for the changes to be applied. In the case of bash, for example, this is done by running (you can also replace source with .):

When the wiki suggests modifying such a configuration script, it will not explicitly remind you to source the file, and only in some cases will it point to this section with a reminder link.

When an article invites you to install some packages in the conventional way, it will not indicate the detailed instructions to do so; instead, it will simply mention the names of the packages to be installed.

The subsections below give an overview of the generic installation procedures depending on the package type.

For packages from the official repositories, you will read something like:

This means that you have to run:

The pacman article contains detailed explanations to deal with package management in Arch Linux proficiently.

For packages from the Arch User Repository (AUR), you will read something like:

This means that in general you have to follow the foobarAUR link, download the PKGBUILD archive, extract it, verify the content and finally run, in the same folder:

The Arch User Repository article contains all the detailed explanations and best practices to deal with AUR packages.

When an article invites to start, enable, etc., some systemd unit (e.g. a service), it will not indicate the detailed instructions to do so, but instead you will read something like:

This means that you have to run:

A notable command that does not follow this exact pattern is systemctl daemon-reload which will be called without arguments.

The systemd#Using units section contains structured list of available actions (like start, enable, enable and start, etc.) with their corresponding systemctl commands.

It is important to remember that there are two different kinds of configurations on a GNU/Linux system. System-wide configuration affects all users. Since system-wide settings are generally located in the /etc directory, root privileges are required in order to alter them. For example, to apply a Bash setting that affects all users, /etc/bash.bashrc should be modified.

User-specific configuration affects only a single user. Dotfiles are used for user-specific configuration. For example, the file ~/.bashrc is the user-specific configuration file. The idea is that each user can define their own settings, such as aliases, functions and other interactive features like the prompt, without affecting other users' preferences.

Bash and other Bourne-compatible shells, such as Zsh, also source files depending on whether the shell is a login shell or an interactive shell. See Bash#Configuration files and Zsh#Startup/Shutdown files for details.

Some code blocks may contain so-called pseudo-variables, which, as the name says, are not actual variables used in the code. Instead they are generic placeholders and have to be manually replaced with system-specific configuration items before the code may be run or parsed. Common shells such as bash and zsh provide tab-completion to auto-complete parameters for common commands such as systemctl.

In the articles that comply with Help:Style/Formatting and punctuation, pseudo-variables are formatted in italics. For example:

In this case interface_name is used as a pseudo-variable placeholder in a systemd template unit. All systemd template units, identifiable by the @ sign, require a system-specific configuration item as argument. See systemd#Using units.

In this case the pseudo-variables are used to describe the parameters that must be substituted for them. Details on how to gather them are elaborated on in the section Securely wipe disk#Calculate blocks to wipe manually, which features the command.

This article or section needs expansion.

In case of file examples, pasting pseudo-variables in real configuration files might break the programs that use them.

In most cases, ellipses (...) are not part of the actual file content or code output, and instead represent omitted or optional text that is not relevant for the discussed subject.

For example HOOKS=(... encrypt ... filesystems ...) or:

Be aware though that, in a few instances, ellipses may be a meaningful part of the code syntax: attentive users should be able to recognize these cases by the context.

**Examples:**

Example 1 (unknown):
```unknown
# mkinitcpio -p linux
```

Example 2 (unknown):
```unknown
$ makepkg -s
```

Example 3 (unknown):
```unknown
sudo command
```

Example 4 (unknown):
```unknown
[postgres]$ initdb -D /var/lib/postgres/data
```

---

## File permissions and attributes

**URL:** https://wiki.archlinux.org/title/Permissions

**Contents:**
- Viewing permissions
  - Examples
- Changing permissions
  - Text method
    - Text method shortcuts
    - Copying permissions
  - Numeric method
  - Bulk chmod
- Changing ownership
- Access Control Lists

File systems use permissions and attributes to regulate the level of interaction that system processes can have with files and directories.

Use the ls command's -l option to view the permissions (or file mode) set for the contents of a directory, for example:

The first column is what we must focus on. Taking an example value of drwxrwxrwx+, the meaning of each character is explained in the following tables:

Each of the three permission triads (rwx in the example above) can be made up of the following characters:

See info Coreutils -n "Mode Structure" and chmod(1) for more details.

Let us see some examples to clarify:

Archie has full access to the Documents directory. They can list, create files and rename, delete any file in Documents, regardless of file permissions. Their ability to access a file depends on the file's permissions.

Archie has full access except they can not create, rename, delete any file. They can list the files and (if the file's permissions allow it) may access an existing file in Documents.

Archie can not do ls in the Documents directory but if they know the name of an existing file then they may list, rename, delete or (if the file's permissions allow it) access it. Also, they are able to create new files.

Archie is only capable of (if the file's permissions allow it) accessing those files the Documents directory which they know of. They can not list already existing files or create, rename, delete any of them.

You should keep in mind that we elaborate on directory permissions and it has nothing to do with the individual file permissions. When you create a new file it is the directory that changes. That is why you need write permission to the directory.

Let us look at another example, this time of a file, not a directory:

Here we can see the first letter is not d but -. So we know it is a file, not a directory. Next the owner's permissions are rw- so the owner has the ability to read and write but not execute. This may seem odd that the owner does not have all three permissions, but the x permission is not needed as it is a text/data file, to be read by a text editor such as Gedit, EMACS, or software like R, and not an executable in its own right (if it contained something like python programming code then it very well could be). The group's permissions are set to r--, so the group has the ability to read the file but not write/edit it in any way  it is essentially like setting something to read-only. We can see that the same permissions apply to everyone else as well.

chmod is a command in Linux and other Unix-like operating systems that allows to change the permissions (or access mode) of a file or directory.

To change the permissions  or access mode  of a file, use the chmod command in a terminal. Below is the command's general structure:

Where who is any from a range of letters, each signifying who is being given the permission. They are as follows:

The permissions are the same as discussed in #Viewing permissions (r, w and x).

Now have a look at some examples using this command. Suppose you became very protective of the Documents directory and wanted to deny everybody but yourself, permissions to read, write, and execute (or in this case search/look) in it:

Before: drwxr-xr-x 6 archie web 4096 Jul 5 17:37 Documents

After: drwx------ 6 archie web 4096 Jul 6 17:32 Documents

Here, because you want to deny permissions, you do not put any letters after the = where permissions would be entered. Now you can see that only the owner's permissions are rwx and all other permissions are -.

This can be reverted with:

Before: drwx------ 6 archie web 4096 Jul 6 17:32 Documents

After: drwxr-xr-x 6 archie web 4096 Jul 6 17:32 Documents

In the next example, you want to grant read and execute permissions to the group, and other users, so you put the letters for the permissions (r and x) after the =, with no spaces.

You can simplify this to put more than one who letter in the same command, e.g:

Now let us consider a second example, suppose you want to change a foobar file so that you have read and write permissions, and fellow users in the group web who may be colleagues working on foobar, can also read and write to it, but other users can only read it:

Before: -rw-r--r-- 1 archie web 5120 Jun 27 08:28 foobar

After: -rw-rw-r-- 1 archie web 5120 Jun 27 08:28 foobar

This is exactly like the first example, but with a file, not a directory, and you grant write permission (just so as to give an example of granting every permission).

The chmod command lets add and subtract permissions from an existing set using + or - instead of =. This is different from the above commands, which essentially re-write the permissions (e.g. to change a permission from r-- to rw-, you still need to include r as well as w after the = in the chmod command invocation. If you missed out r, it would take away the r permission as they are being re-written with the =. Using + and - avoids this by adding or taking away from the current set of permissions).

Let us try this + and - method with the previous example of adding write permissions to the group:

Before: -rw-r--r-- 1 archie web 5120 Jun 27 08:28 foobar

After: -rw-rw-r-- 1 archie web 5120 Jun 27 08:28 foobar

Another example, denying write permissions to all (a):

Before: -rw-rw-r-- 1 archie web 5120 Jun 27 08:28 foobar

After: -r--r--r-- 1 archie web 5120 Jun 27 08:28 foobar

A different shortcut is the special X mode: this is not an actual file mode, but it is often used in conjunction with the -R option to set the executable bit only for directories, and leave it unchanged for regular files, for example:

It is possible to tell chmod to copy the permissions from one class, say the owner, and give those same permissions to group or even all. To do this, instead of putting r, w, or x after the =, put another who letter. e.g:

Before: -rw-r--r-- 1 archie web 5120 Jun 27 08:28 foobar

After: -rw-rw-r-- 1 archie web 5120 Jun 27 08:28 foobar

This command essentially translates to "change the permissions of group (g=), to be the same as the owning user (=u). Note that you cannot copy a set of permissions as well as grant new ones e.g.:

In that case chmod throw an error.

chmod can also set permissions using numbers.

Using numbers is another method which allows you to edit the permissions for all three owner, group, and others at the same time, as well as the setuid, setgid, and sticky bits. This basic structure of the code is this:

Where xxx is a 3-digit number where each digit can be anything from 0 to 7. The first digit applies to permissions for owner, the second digit applies to permissions for group, and the third digit applies to permissions for all others.

In this number notation, the values r, w, and x have their own number value:

To come up with a 3-digit number you need to consider what permissions you want owner, group, and all others to have, and then total their values up. For example, if you want to grant the owner of a directory read write and execution permissions, and you want group and everyone else to have just read and execute permissions, you would come up with the numerical values like so:

This is the equivalent of using the following:

To view the existing permissions of a file or directory in numeric form, use the stat(1) command:

Where the %a option specifies output in numeric form.

Most directories are set to 755 to allow reading, writing and execution to the owner, but deny writing to everyone else, and files are normally 644 to allow reading and writing for the owner but just reading for everyone else; refer to the last note on the lack of x permissions with non executable files: it is the same thing here.

To see this in action with examples consider the previous example that has been used but with this numerical method applied instead:

Before: -rw-r--r-- 1 archie web 5120 Jun 27 08:28 foobar

After: -rw-rw-r-- 1 archie web 5120 Jun 27 08:28 foobar

If this were an executable the number would be 774 if you wanted to grant executable permission to the owner and group. Alternatively if you wanted everyone to only have read permission the number would be 444. Treating r as 4, w as 2, and x as 1 is probably the easiest way to work out the numerical values for using chmod xxx filename, but there is also a binary method, where each permission has a binary number, and then that is in turn converted to a number. It is a bit more convoluted, but here included for completeness.

Consider this permission set:

If you put a 1 under each permission granted, and a 0 for every one not granted, the result would be something like this:

You can then convert these binary numbers:

The value of the above would therefore be 775.

Consider we wanted to remove the writable permission from group:

The value would therefore be 755 and you would use chmod 755 filename to remove the writable permission. You will notice you get the same three digit number no matter which method you use. Whether you use text or numbers will depend on personal preference and typing speed. When you want to restore a directory or file to default permissions e.g. read and write (and execute) permission to the owner but deny write permission to everyone else, it may be faster to use chmod 755/644 filename. However if you are changing the permissions to something out of the norm, it may be simpler and quicker to use the text method as opposed to trying to convert it to numbers, which may lead to a mistake. It could be argued that there is not any real significant difference in the speed of either method for a user that only needs to use chmod on occasion.

You can also use the numeric method to set the setuid, setgid, and sticky bits by using four digits.

For example, chmod 2777 filename will set read/write/executable bits for everyone and also enable the setgid bit.

Generally directories and files should not have the same permissions. If it is necessary to bulk modify a directory tree, use find to selectively modify one or the other.

To chmod only directories to 755:

To chmod only files to 644:

chown changes the owner of a file or directory, which is quicker and easier than altering the permissions in some cases.

Consider the following example, making a new partition with GParted for backup data. Gparted does this all as root so everything belongs to root by default. This is all well and good but when it comes to writing data to the mounted partition, permission is denied for regular users.

As you can see the device in /dev is owned by root, as is the mount location (/media/Backup). To change the owner of the mount location one can do the following:

Before: drwxr-xr-x 5 root root 4096 Jul 6 16:01 Backup

After: drwxr-xr-x 5 archie root 4096 Jul 6 16:01 Backup

Now the partition can have data written to it by the new owner, archie, without altering the permissions (as the owner triad already had rwx permissions).

Access Control Lists provides an additional, more flexible permission mechanism for file systems by allowing to set permissions for any user or group to any file.

The umask utility is used to control the file-creation mode mask, which determines the initial value of file permission bits for newly created files.

Apart from the file mode bits that control user and group read, write and execute permissions, several file systems support file attributes that enable further customization of allowable file operations.

The e2fsprogs package contains the programs lsattr(1) and chattr(1) that list and change a file's attributes, respectively.

These are a few useful attributes. Not all filesystems support every attribute.

See chattr(1) for a complete list of attributes and for more info on what each attribute does.

For example, if you want to set the immutable bit on some file, use the following command:

To remove an attribute on a file just change + to -.

See Extended attributes.

Use the --preserve-root flag to prevent chmod from acting recursively on /. This can, for example, prevent one from removing the executable bit systemwide and thus breaking the system. To use this flag every time, set it within an alias. See also [1].

**Examples:**

Example 1 (unknown):
```unknown
$ ls -l /path/to/directory
```

Example 2 (unknown):
```unknown
total 128
drwxr-xr-x 2 archie archie  4096 Jul  5 21:03 Desktop
drwxr-xr-x 6 archie archie  4096 Jul  5 17:37 Documents
drwxr-xr-x 2 archie archie  4096 Jul  5 13:45 Downloads
-rw-rw-r-- 1 archie archie  5120 Jun 27 08:28 customers.ods
-rw-r--r-- 1 archie archie  3339 Jun 27 08:28 todo
-rwxr-xr-x 1 archie archie  2048 Jul  6 12:56 myscript.sh
```

Example 3 (unknown):
```unknown
drwxrwxrwx+
```

Example 4 (unknown):
```unknown
info ls -n "What information is listed"
```

---

## Screen capture

**URL:** https://wiki.archlinux.org/title/Screen_capture

**Contents:**
- Screenshot software
  - Dedicated software
    - Application list
    - Usage
      - maim
      - scrot
  - Desktop environment specific
    - Budgie
    - Cinnamon
    - GNOME

This article lists and describes screenshot and screencast software.

This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.

maim is aimed to be an improved scrot.

Save the full screen to file:

Prompt for selection and save to file:

Save the active window to file, assuming xdotool is installed:

Prompt for selection, save without cursor, and store it to clipboard, assuming xclip is installed:

scrot enables taking screenshots from the CLI and offers features such as a user-definable time delay. Unless instructed otherwise, it saves the file in the current working directory.

The above command saves a dated .png file, along with a thumbnail (20% of original), for Web posting. It provides a 5 second delay before capturing in this instance.

You can also use standard date and time formatting when saving to a file. e.g.,

saves the screenshot in a filename with the current year, month, date, hours, minutes, and seconds to a folder in your home directory called "screenshots"

See scrot(1) for more information.

Budgie ships with its own screenshot utility called BudgieScreenshot.

It allows capturing the whole screen using PrintScreen, the active window or a selected area. See Keyboard Shortcuts for the shortcut to a specific action.

The default installation of Cinnamon does not provide a screenshot utility. Installing gnome-screenshot will enable screenshots through the Menu > Accessories > Screenshot or by pressing PrintScreen.

GNOME users can press PrintScreen or click the camera icon in the system menu. You can also optionally install gnome-screenshot and open it via Apps > Accessories > Take Screenshot.

GNOME features built-in screen recording with the Ctrl+Shift+Alt+r key combination. A red circle is displayed in the bottom right corner of the screen when the recording is in progress. After the recording is finished, a file named Screencast from %d%u-%c.webm is saved in the Videos directory. In order to use the screencast feature the gst-plugin-pipewire and gst-plugins-good packages need to be installed.

If you use KDE, you might want to use Spectacle.

Spectacle is provided by the spectacle package.

If you use Xfce you can install xfce4-screenshooter and then add a keyboard binding:

Xfce Menu > Settings > Keyboard > Application Shortcuts

If you want to skip the Screenshot prompt, type $ xfce4-screenshooter -h in terminal for the options.

This article or section is a candidate for merging with ImageMagick#Screenshot taking.

For other desktop environments such as LXDE or window managers such as Openbox and Compiz, one can add the above commands to the hotkey to take the screenshot. For example:

Note that import is part of the imagemagick package. Adding the above command to the PrintScreen key to Compiz allows to take the screenshot to the Pictures folder according to date and time.

Notice that the rc.xml file in Openbox does not understand commas; so, in order to bind that command to the PrintScreen key in Openbox, you need to add the following to the keyboard section of your rc.xml file:

If the PrintScreen above does not work, see Keyboard input and use different keysym or keycode.

See ImageMagick#Screenshot taking.

You also can take screenshots with GIMP (File > Create > Screenshot...).

imlib2 provides a binary imlib2_grab to take screenshots. To take a screenshot of the full screen, type:

See FFmpeg#Screen capture.

See also FFmpeg#Screen capture and Wikipedia:Comparison of screencasting software.

Screencast utilities allow you to create a video of your desktop or individual windows.

Capturing the screen on Wlroots-based compositor can be done using:

Optionally, slurp can be used to select the part of the screen to capture. If your GPU supports vaapi encoding, wl-screenrecAUR can be a more efficient alternative to wf-recorder.

Take a screenshot of the whole screen:

Take a screenshot of current window in Sway:

Take a screenshot of current window in Hyprland:

Take a screenshot of a part of the screen:

Take a screenshot of a part of the screen and put the output into the clipboard using wl-clipboard:

Take a screenshot of a part of the screen, save to a file, and put the output into the clipboard using wl-clipboard:

Capture a video of the whole screen:

Capture a video of a part of the screen:

green-recorderAUR, obs-gnome-screencastAUR and obs-studio support screen recording on Wayland using GNOME screencast feature.

See PipeWire#WebRTC screen sharing.

Chromium and Firefox should now be able to access the screenshare. You can verify this through Mozilla's getUserMedia / getDisplayMedia Test Page.

The Hyprland window manager allows screen casting and recording with OBS (including selection of individual windows and workspaces) when used with xdg-desktop-portal-hyprland [1].

Configuration for screen sharing with selection on Sway is covered by default in /usr/share/xdg-desktop-portal/sway-portals.conf.

The factual accuracy of this article or section is disputed.

This article or section needs language, wiki syntax or style improvements. See Help:Style for reference.

for those on a different wlroots based compositors, such as dwl: you will need to do the following,

install xdg-desktop-portal-wlr with the gtk version then follow XDG Desktop Portal#Configuration with PipeWire installed then restart

See PipeWire#WebRTC screen sharing.

See v4l2loopback#Casting Wayland using wf-recorder.

Install wf-recorder (or wf-recorder-gitAUR) and v4l2loopback-dkms. Load the v4l2loopback kernel module with the following parameters:

Verify that a new virtual video device VirtualVideoDevice has been created:

Start recording the screen with wf-recorder and feed the output to the new virtual video device VirtualVideoDevice created by v4l2loopback:

The yuv420p colour space is required for the video output to be compatible with Zoom [2].

You can now select the above virtual video device as your "webcam" in video calling/video conferencing applications (the device is called VirtualVideoDevice). You can use ffplay (part of ffmpeg), mpv, or gst-launch (part of gstreamer) to verify that the virtual video device indeed outputs your screenshare:

If Firefox is unable to read the video stream and prints a message like "AbortError: Starting video failed", try preloading v4l2compat.so:

As explained above, wf-recorder is able to record only a portion of the screen by first selecting a region with slurp. To use this functionality for sharing a specific region/application window through a virtual video device, start recording the screen with the following modified command:

After selecting a region of the screen, you will be able to access the video feed through the virtual video device /dev/video2 as above.

You can share native Wayland windows (or the whole screen/workspace) to the X11 application. For this you need to use xwaylandvideobridgeAUR. See Fixing Wayland: Xwayland screen casting for details.

You can use the script(1) command, part of the util-linux package. Just run script and from that moment, all the output is going to be saved to the typescript file, including the ANSI codes.

Once you are done, just run exit and the typescript would ready. The resulting file can be converted to HTML using the ansi2htmlAUR package (not to be confused with ansi2html from python-ansi2html).

To convert the typescript file to typescript.html, do the following:

Actually, some commands can be piped directly to ansi2html:

That does not work on every single case, so in those cases, using script is mandatory.

Install a framebuffer and use fbgrab or fbdumpAUR to take a screenshot.

If you merely want to capture the text in the console and not an actual image, you can use setterm, which is part of the util-linux package. The following command will dump the textual contents of virtual console 1 to a file screen.dump in the current directory:

Root permission is needed because the contents of /dev/vcs1 need to be read.

asciinema allows to record a whole terminal session activity, which is saved in a file in its own (open) format. This file can be played with the same tool or an HTML5 version of the tool, and can be shared on the asciinema.org official web site of the application, or on any own hosted HTML version.

Usage: asciinema(1) or asciinema --help.

Interesting arguments for recording:

Other functions than recording:

See KDE#Spectacle screenshot uses old screen state.

If the nvidia proprietary driver is in use and the screen recording is experiencing background clipping, enable the ForceCompositionPipeline setting. See NVIDIA/Troubleshooting#Avoid screen tearing for details.

**Examples:**

Example 1 (unknown):
```unknown
lximage-qt --screenshot
```

Example 2 (unknown):
```unknown
$ maim filename
```

Example 3 (unknown):
```unknown
$ maim --select filename
```

Example 4 (unknown):
```unknown
$ maim --window $(xdotool getactivewindow) filename
```

---

## List of applications/Other

**URL:** https://wiki.archlinux.org/title/Taskbar

**Contents:**
- Organization
  - CalDAV/CardDAV servers
  - Personal information managers
  - Time management
    - Console
    - Graphical
  - Timers
    - Countdown timers and stopwatch
    - Break timers
    - Pomodoro timers

These applications support time, task and contacts management.

See Wikipedia:Pomodoro Technique for an introduction.

See also Wikipedia:Comparison of accounting software.

See also Wikipedia:Comparison of project management software.

See also List of applications/Science#Navigation and routing.

See also List of games#Education.

See also Wikipedia:List of flashcard software.

See Accessibility for tips on operating the desktop and Category:Accessibility for all available articles. See also On-screen keyboards.

See also Wikipedia:Comparison of speech synthesizers and listening comparison of the different engines.

See also Wikipedia:Speech recognition software for Linux.

See the main article: Display manager#List of display managers.

See the main article: Desktop environment#List of desktop environments.

See also List of applications/Utilities#Terminal multiplexers, which offer some of the functions of window managers for the console.

See the main article: Window manager#List of window managers.

See the main article: Xorg#List of composite managers.

See the main article: Wayland#Compositors.

See also Wikipedia:Taskbar.

Desktop environments typically have their own system tray implementation. E.g. KDE ships with Plasma Panel and Xfce ships with xfce4-panel. For GNOME, see GNOME#AppIndicators/Top bar icons. For dwm, see systray patch.

Desktop-independent tray indicators. Useful for window managers without built-in tray widgets:

See also Wikipedia:Comparison of desktop application launchers.

See also Wikipedia:Wallpaper (computing).

See also Wikipedia:Pager (GUI).

See: Notification servers.

See Clipboard#Managers.

See also Wikipedia:Open-source artificial intelligence, Wikipedia:Lists of open-source artificial intelligence software, Wikipedia:Comparison of deep learning software

**Examples:**

Example 1 (unknown):
```unknown
display -backdrop -background '#3f3f3f' -flatten -window root image
```

---

## Metric-compatible fonts

**URL:** https://wiki.archlinux.org/title/Metric-compatible_fonts

**Contents:**
- List of metric-compatible fonts
- Generic Families
  - PostScript
    - Garamond
  - Microsoft
- Metric-compatible font projects
  - TeX Gyre
  - GNU FreeFont
  - Liberation
  - Ume

The factual accuracy of this article or section is disputed.

Metric-compatible fonts are fonts that match the metrics (i.e. glyph dimensions) of another font (often generics such as Helvetica, Times or Courier). Due to their matching metrics, replacing a font with a metric-compatible alternative does not change the formatting of the document or a web page. Such fonts are often developed for FOSS systems to display pages correctly.

In the following table, commonly-specified families are shown in bold. This table is roughly based on fontconfig's 30-metric-aliases.conf and Wikipedia pages for individual fonts.

The PostScript language defines 35 core fonts in PostScript 2. URW released open-source versions/clones of these 35 fonts for Ghostscript, available as gsfonts. Projects including GUST's TeX Gyre and GNU FreeFont release enhanced versions of these fonts.

PostScript 3 defines an additional 101 fonts, many of which are made available by URW under the AFPL in GhostPDL[dead link 2025-01-22HTTP 404]. The AFPL bars commercial use. Many of the dual font names are caused by a batch update[dead link 2025-01-22HTTP 404].

URW's Garamond No.8 only provides one optical size (8pt). You may use EB Garamond for more OpenType features, including the 12pt size. It is, however, not guaranteed to be metric-identical.

Microsoft bundles a number of fonts with Microsoft Windows and Microsoft Office. While some of these fonts are just a cheaper version (or look-alike) of corresponding PostScript families, Cambria and Calibri (default font since MS Office 2007) are independent from other families. Microsoft used to provide many core fonts in its Core fonts for the Web project. Although this project is later unavailable on Microsoft's site, the license terms that allow these fonts to be distributed from third-party sites make packages like ttf-ms-fontsAUR possible. See also Microsoft fonts.

Prior to the introduction of Arial and Times New Roman, Microsoft used two bitmap fonts called Helv and Tms Rmn in Windows 1.0, each being unlicensed imitations of better-known fonts already covered here. They were later renamed MS Sans Serif and MS Serif starting with Windows 3.1, and MS Sans Serif was eventually vectorized into "Microsoft Sans Serif". Documents using these fonts are rare, but user interfaces that use Microsoft Sans Serif can occasionally be found in Mono libgdiplus applications. It is generally safe to assume these fonts are metric-compatible with Helvetica and Times when trying to replace them.

TeX Gyre (tex-gyre-fonts) is a remake and extension of the 35 base PostScript fonts distributed with Ghostscript 4.00. The project provides TeX support and also the cross-platform OpenType format of the fonts. A related project, TeX Gyre Math, provides corresponding mathematical OpenType fonts.

GNU FreeFont (gnu-free-fonts) is an outline family intended to cover as much of the Universal Character Set as possible. Most of the Latin characters are from URW (Nimbus) fonts. This set of fonts is released under GPL v3+ + FE. Note that some of the glyphs for non-LGC languages may be considered lower quality.

Liberation fonts provides four families Liberation Sans, Liberation Serif, and Liberation Mono, intended to be metric-compatible with common Microsoft Windows fonts. Since version 2.0.0, this set of fonts is released under SIL OFL, and is based on #Chrome OS core fonts. They are available as ttf-liberation. Though metric-compatible, the overall glyph style is rather different from Microsoft and Apple's implementations.

Older, GPL-licensed versions of this font is based on Ascender Corporation's fonts, which is licensed by Red Hat, Inc. These versions of Liberation also includes Liberation Sans Narrow, which corresponds to Arial Narrow. This one font is available as ttf-liberation-sans-narrowAUR.

Ume Fonts (Japanese) (ttf-umeAUR) is a font projects which provides metric-compatible fonts with MS Japanese fonts, such as: Ume Gothic (MS Gothic), Ume UI Gothic (MS UI Gothic), Ume P Gothic (MS PGothic), ...

Google provides a high number of fonts, including different metric-compatible font families.

Gelasio (ttf-gelasioAUR), the Google alternative for Georgia, can be found on FontLibrary under SIL OFL.

Google ships open-source metric-compatible fonts with its operating system, Chrome OS, under the Apache License 2.0. CrOS core (croscore, ttf-croscore) is a collection of Arimo (sans), Tinos (serif) and Cousine (mono), also licensed from Ascender Corporation. A set of extra fonts, CrOS extra (crosextra) provides Carlito (ttf-carlito) and Caladea (ttf-caladea) to match default fonts for Microsoft Word.

Since glyph mappings from Symbol are usually implemented in browsers, Google no longer ships SymbolNeu in croscore > 1.23.0. You can get this font from croscorefonts-1.23.0.tar.gz.

Google's Noto Fonts are available via noto-fonts. They are licensed under SIL OFL. Noto Fonts are designed to supplement glyph coverage for Roboto (ttf-roboto), the standard typeface for Android, and are vertically (i.e. same line height for the same font size) metric-compatible with Roboto.

DMCA Sans Serif (ttf-dmcasansserifAUR) is a general purpose sans serif alternative to Microsoft's Consolas, it uses the same metric (1884/2048 top, 514/2048 bottom, 1126/2048 width) and is in the public domain. Version 9.0 has 3309 characters, which is the Subset3+ character set.

Kissinger 2 is a public domain competitor of Unifont. Unlike Unifont, Kissinger 2 is split into separate halfwidth (816) and fullwidth (1616) fonts, with some characters having glyphs in both widths. Version dev4 has 8450 halfwidth characters, 14724 fullwidth characters, 21911 total, and 1263 overlap, however, users can contribute glyphs by the method described in the official website.

Selawik (ttf-selawikAUR) is Microsoft's open-source alternative to its Segoe UI font. Unfortunately it does not match Segoe UI's kerning parameters. It was developed for use in the WinJS framework, which is now abandoned.

The Wine project developed a metric-compatible font to replace Microsoft's Tahoma, available as ttf-tahomaAUR. Its name in TTF data is simply "Tahoma", so there is no configuration needed.

The factual accuracy of this article or section is disputed.

For font consistency, all applications should be set to use the serif, sans-serif, and monospace aliases, which are mapped to particular fonts by fontconfig. Font configuration#Set default or fallback fonts explains two ways to achieve the configuration, both are covered with an example for metric-compatible fonts below.

The following example configuration uses the #Liberation fonts.

The following example configuration uses the #Chrome OS fonts, adding additional aliases for other fonts frequently required to refer.

**Examples:**

Example 1 (unknown):
```unknown
binding="same"
```

Example 2 (unknown):
```unknown
<alias>...<accept>
```

Example 3 (unknown):
```unknown
/etc/fonts/local.conf
```

Example 4 (unknown):
```unknown
<?xml version="1.0"?>
<!DOCTYPE fontconfig SYSTEM "fonts.dtd">
<fontconfig>
    <match target="pattern">
        <test qual="any" name="family"><string>serif</string></test>
        <edit name="family" mode="assign" binding="same"><string>Liberation Serif</string></edit>
    </match>
    <match target="pattern">
        <test qual="any" name="family"><string>sans-serif</string></test>
        <edit name="family" mode="assign" binding="same"><string>Liberation Sans</string></edit>
    </match>
    <match target="pattern">
        <test qual="any" name="family"><string>monospace</string></test>
        <edit name="family" mode="assign" binding="same"><string>Liberation Mono</string></edit>
    </match>
</fontconfig>
```

---

## Power management/Suspend and hibernate

**URL:** https://wiki.archlinux.org/title/Hibernation

**Contents:**
- Kernel interface (swsusp)
- High level interface (systemd)
- Changing suspend method
- Hibernation
  - About swap partition/file size
  - Configure the initramfs
  - Pass hibernate location to initramfs
    - Manually specify hibernate location
      - Acquire swap file offset
  - Change the image compression algorithm for hibernation

There are multiple methods of suspending available, notably:

The kernel provides basic functionality, and some high level interfaces provide tweaks to handle problematic hardware drivers/kernel modules (e.g. video card re-initialization).

It is possible to directly inform the in-kernel software suspend code (swsusp) to enter a suspended state; the exact method and state depends on the level of hardware support. On modern kernels, writing appropriate strings to /sys/power/state is the primary mechanism to trigger this suspend.

See kernel documentation for details.

systemd provides native commands for suspend, hibernate and a hybrid suspend. This is the default interface used in Arch Linux.

systemctl suspend should work out of the box. For systemctl hibernate to work on your system you might need to follow the instructions at #Hibernation.

There are also two modes combining suspend and hibernate:

See #Sleep hooks for additional information on configuring suspend/hibernate hooks. Also see systemctl(1), systemd-sleep(8), and systemd.special(7).

On systems where S0ix suspension does not provide the same energy savings as the regular S3 sleep, or when conserving energy is preferred to a quick resume time, changing the default suspend method is possible.

Run the following command to see all suspend methods hardware advertises support for (current method is shown in square brackets[1]):

If your hardware does not advertise the deep sleep status, check first if your UEFI advertises some settings for it, generally under Power or Sleep state or similar wording, with options named Windows 10, Windows and Linux or S3/Modern standby support for S0ix, and Legacy, Linux, Linux S3 or S3 enabled for S3 sleep. Failing that, you can keep using s2idle, consider using hibernation or try to patch the DSDT tables (or find a patched version online).

Confirm that your hardware does not exhibit issues with S3 sleep by testing a few sleep cycles after changing the sleep method:

If no issues have been found, you can make the change permanent through the MemorySleepMode directive in systemd-sleep.conf(5):

or through the mem_sleep_default=deep kernel parameter.

In some opposite situations, faulty firmware advertises support for deep sleep, while only s2idle is supported. In this case, an alternative method for using s2idle is available through the SuspendState setting:

In order to use hibernation, you must create a swap partition or file, configure the initramfs so that the resume process will be initiated in early userspace, and specify the location of the swap space in a way that is available to the initramfs, e.g. HibernateLocation EFI variable defined by systemd or resume= kernel parameter. These three steps are described in detail below.

Even if your swap partition is smaller than RAM, you still have a good chance of hibernating successfully. See "image_size" in the kernel documentation for information on the image_size sysfs(5) pseudo-file.

You may either decrease the value of /sys/power/image_size to make the suspend image as small as possible (for small swap partitions), or increase it to possibly speed up the hibernation process. For systems with a large amount of RAM, smaller values may drastically increase the speed of resuming a hibernating system. systemd#systemd-tmpfiles - temporary files can be used to make this change persistent:

The suspend image cannot span multiple swap partitions and/or swap files. It must fully fit in one swap partition or one swap file.[2]

When the system hibernates, the memory image is dumped to the swap space, which also includes the state of mounted file systems. Therefore, the hibernate location must be made available to the initramfs, i.e. before the root file system is mounted for resuming from hibernate to work.

When the system is running on UEFI, systemd-sleep(8) will automatically pick a suitable swap space to hibernate into, and the information of the used swap space is stored in HibernateLocation EFI variable. Upon next boot, systemd-hibernate-resume(8) reads the location off the EFI variable and the system resumes. This means the following steps are not necessary unless the system is using legacy BIOS or you want to choose a different swap space from the automatically-selected one.

The kernel parameter resume=swap_device can be used, where swap_device follows the persistent block device naming. For example:

The kernel parameters will only take effect after rebooting. To hibernate right away, obtain the volume's major and minor device numbers from lsblk and echo them in format major:minor to /sys/power/resume.

For example, if the swap device is 8:3:

If using a swap file, additionally follow the procedures in #Acquire swap file offset.

When using a swap file for hibernation, the block device on which the file system lies should be specified in resume=, and additionally the physical offset of swap file must be specified through resume_offset= kernel parameter. [3]

On file systems other than Btrfs, the value of resume_offset= can be obtained by running filefrag -v swap_file. The output is in a table format and the required value is in the first row of the physical_offset column.

In the example the value of resume_offset= is the first 38912.

Alternatively, to directly acquire the offset value:

For Btrfs, do not try to use the filefrag tool, since the "physical" offset you get from filefrag is not the real physical offset on disk; there is a virtual disk address space in order to support multiple devices.[4] Instead, use the btrfs-inspect-internal(8) command. E.g.:

In this example, the kernel parameter would be resume_offset=198122980.

To apply the change immediately (without rebooting), echo the resume offset to /sys/power/resume_offset. For example, if the offset is 38912:

Starting with Linux 6.9[5], the image compression algorithm for hibernation can be changed. The default compression algorithm is selected based on the compile time option CONFIG_HIBERNATION_DEF_COMP, but it can be overridden at boot time and runtime.

Different compression algorithms have different characteristics and hibernation may benefit when it uses any of these algorithms, especially when a secondary algorithm (LZ4) offers better decompression speeds over a default algorithm (LZO), which in turn reduces hibernation image restore time.

You can override the default algorithm in two ways:

1) Passing hibernate.compressor as a kernel parameter:

2) Specifying the algorithm at runtime:

Currently lzo and lz4 are the supported algorithms with LZO being the default.

It is possible to solve the hibernation problem with zram RAM-only swap by maintaining two or more swap spaces at the same time. systemd will always ignore zram block devices before triggering hibernation [6], therefore keeping both spaces enabled should work without further intervention.

After configuring the swap file, follow the zram page. Make sure zram has the higher swap priority (e.g. pri=100).

Hibernation into a thinly-provisioned LVM volume is possible, but you have to make sure that the volume is fully allocated. Otherwise resuming from it will fail, see FS#50703.

You can fully allocate the LVM volume by simply filling it with zeros. E.g.:

To verify the volume is fully allocated, you can use:

A fully allocated volume will show up as having 100% data usage.

In Linux 6.8, zswap gained a per-cgroup option to disable writeback. By using systemd unit setting MemoryZSwapWriteback (see systemd.resource-control(5)  Memory Accounting and Control) in all possible unit types, zswap writeback can be effectively disabled entirely. This allows to use zswap just like zram with the added benefit of supporting hibernation.

To avoid having to manually create twelve top level per-type drop-in files (for system and user scope, service, slice, socket, mount, swap units types), install zswap-disable-writebackAUR. Enable zswap and reboot for the settings to take effect.

Try to perform memory intensive tasks and confirm that zswap has not written anything to disk:

systemd starts suspend.target, hibernate.target, hybrid-sleep.target, or suspend-then-hibernate.target for each sleep state, respectively. All the aforementioned targets pull in sleep.target. Any of the targets can be used to invoke custom units before or after suspend/hibernate. Separate files should be created for user actions and root/system actions. Examples:

Enable user-suspend@user.service and/or user-resume@user.service for the change to take effect.

For root/system actions:

With the combined unit file, a single hook does all the work for different phases (sleep/resume) and for different targets.

Example and explanation:

systemd-sleep runs all executables in /usr/lib/systemd/system-sleep/, passing two arguments to each of them:

An environment variable called SYSTEMD_SLEEP_ACTION will be set and contain the sleep action that is processing. This is primarily helpful for suspend-then-hibernate where the value of the variable will be suspend, hibernate, or suspend-after-failed-hibernate in cases where hibernation has failed.

The output of any custom script will be logged by systemd-suspend.service, systemd-hibernate.service or systemd-hybrid-sleep.service. You can see its output in systemd's journalctl:

An example of a custom sleep script:

Do not forget to make your script executable.

When resuming, you can automatically unlock your system if it is connected to certain devices or trusted Wi-Fi networks.

Configure your desktop environment so that it locks on resume, and then create a sleep hook that runs the above script after resuming. You also need to install wireless_tools to read the connected Wi-Fi SSID. If you also want to test for connected USB devices, uncomment the lsusb -d ... line in the script and fill in the ID of your trusted device. You can get the ID of your device by running lsusb.

When using a device as e.g a server, suspending/hibernating might not be needed or it could even be undesired. Each sleep state can be disabled through systemd-sleep.conf(5):

Intel Rapid Start Technology is a firmware method of hibernation that allows hibernating from sleep after a predefined interval or according to battery state. This should be faster and more reliable than regular hibernation as it is done by firmware instead of at the operating system level. Generally it must enabled in the firmware, and the firmware also provides support for setting the duration after suspend/battery event triggering hibernation. However, some devicesdespite supporting IRST in the firmwareonly allow it to be configured via Intel's Windows drivers. In such cases the intel-rst kernel module described below should be able to configure the events under Linux.

With Intel Rapid Start Technology (IRST) enabled, resuming from a deep sleep takes "a few seconds longer than resuming from S3 but is far faster than resuming from hibernation".

Many Intel-based systems have firmware support for IRST but require a special partition on an SSD (rather than an HDD). OEM deployments of Windows may have a pre-existing IRST partition which can be retained during the Arch Linux installation process (rather than wiping and re-partitioning the whole SSD). It should show up as an unformatted partition equal in size to the system's RAM.

If you intend to wipe and re-partition the whole drive (or have already done so), then the IRST partition must be recreated if you also plan on using the technology. This can be done by creating an empty partition equal in size to the system's RAM and by setting its partition type to GUID D3BFE2DE-3DAF-11DF-BA40-E3A556D89593 for a GPT partition or ID 0x84 for an MBR partition. You may also need to enable support for IRST in your system's firmware settings.

The duration of the IRST hibernation process (i.e., copying the "entire contents of RAM to a special partition") is dependent on the system's RAM size and SSD speed and can thus take 2060 seconds. Some systems may indicate the process's completion with an LED indicator, e.g., when it stops blinking.

Configuring IRST hibernation events in the Linux kernel requires CONFIG_INTEL_RST built-in or as a module. Once loaded via modprobe intel_rst, it should create the files wakeup_events and wakeup_time under /sys/bus/acpi/drivers/intel_rapid_start/*/ that can be used for further configuration. This module is tersely documented, see the source drivers/platform/x86/intel/rst.c for more details.

See also the general Q&A and user guides for Intel Rapid Start Technology.

To measure power consumption in suspend states use Batenergy script to log battery changes to the system journal. This allows to compare power consumption in S3 / S0x states or check after BIOS and kernel updates for regressions and fixes. The script needs bc to be installed for calculation.

You might want to tweak your DSDT table to make it work. See DSDT.

The factual accuracy of this article or section is disputed.

There have been many reports about the screen going black without easily viewable errors or the ability to do anything when going into and coming back from suspend and/or hibernate. These problems have been seen on both laptops and desktops. This is not an official solution, but switching to an older kernel, especially the LTS-kernel, will probably fix this.

A problem may arise when using the hardware watchdog timer (disabled by default, see RuntimeWatchdogSec= in systemd-system.conf(5)  OPTIONS). A buggy watchdog timer may reset the computer before the system finishes creating the hibernation image.

Sometimes the screen goes black due to device initialization from within the initramfs. Removing any modules you might have in Mkinitcpio#MODULES, removing the kms hook and rebuilding the initramfs can possibly solve this issue, in particular with graphics drivers for early KMS. Initializing such devices before resuming can cause inconsistencies that prevents the system resuming from hibernation. This does not affect resuming from RAM. Also, check the blog article best practices to debug suspend issues.

Moving from the ATI video driver to the newer AMDGPU driver could also help to make the hibernation and awakening process successful.

With NVIDIA cards, the VRAM contents are saved to disk when suspending.[8] Make sure that you have enough disk space, otherwise you might get a blank screen when resuming. Another cause for this could be fixed by blacklisting the module nvidiafb. [9]

Laptops with an Intel CPU that load the intel_lpss_pci module for a touchpad may face kernel panic on resume (blinking caps lock) [10]. The module needs to be added to initramfs as:

Then regenerate the initramfs.

System may fail to suspend because of a USB device. You might see the following error:

lspci may give you more information on the failing device:

Try disconnecting devices on that port.

If Wake-on-LAN is active, the network interface card will consume power even if the computer is hibernated.

See Wakeup triggers#Instantaneous wakeup after suspending.

When you hibernate your system, the system should power off (after saving the state on the disk). On some firmware the S4 sleeping state does not work reliably. For example, instead of powering off, the system might reboot or stay on but unresponsive. If that happens, it might be instructive to set the HibernateMode to shutdown in sleep.conf.d(5):

With the above configuration, if everything else is set up correctly, on invocation of a systemctl hibernate the machine will shut down, saving state to disk as it does so.

This can happen when the boot disk is an external disk, and seems to be caused by a BIOS/firmware limitation. The BIOS/firmware tries to boot from an internal disk, while hibernation was done from an OS on an external (or other) disk.

Set HibernateMode=shutdown as shown in #System does not power off when hibernating to solve the problem permanently. If you have already locked yourself out, you can try rebooting your system 4 times (wait for the error to appear each time), which on some BIOS'es forces a normal boot procedure.

If the swap file is in /home/, systemd-logind will not be able to access it, giving the Call to Hibernate failed: No such file or directory warning message and resulting in a need for authentication on systemctl hibernate. This setup should be avoided, as it is considered unsupported upstream. See systemd issue 15354 for two workarounds.

On some motherboards with A520i and B550i chipsets, the system will not completely enter the sleep state or come out of it. Symptoms include the system entering sleep and the monitor turning off while internal LEDs on the motherboard or the power LED stay on. Subsequently, the system will not come back from this state and require a hard power off. If you have similar issues with AMD, first make sure your system is fully updated and check whether the AMD microcode package is installed.

Verify the line starting with GPP0 has the enabled status:

If that is enabled, you can run the following command to disable it:

Now test by running systemctl suspend and let the system go to sleep. Then try to wake the system after a few seconds. If it works, you can make the workaround permanent. Create a systemd unit file:

Do a daemon-reload and start/enable the newly created unit.

Alternatively, you can create a udev rule. Assuming GPP0s sysfs node is pci:0000:00:01.1 like in the example, run udevadm info -a -p /sys/bus/pci/devices/0000\:00\:01.1 to get the relevant information and create a udev rule like this one:

The udev daemon is already watching for changes in your system by default. If needed you can reload the rules manually.

If, regardless of the setting in logind.conf, the sleep button does not work (pressing it does not even produce a message in syslog), then logind is probably not watching the keyboard device. [11] Do:

You might see something like this:

Notice no keyboard device. List keyboard devices as follows:

Now obtain ATTRS{name} for the parent keyboard device [12]. As an example, on the above list this keyboard device has event6 as device input event, it can be used to search its respective attribute name:

Now write a custom udev rule to add the "power-switch" tag:

After reloading the udev rules and restarting systemd-logind.service, you should see Watching system buttons on /dev/input/event6 in the journal of logind.

Since systemd v256, systemd freezes user.slice before sleeping. This process can fail due to kernel bugs, particularly when KVM is in use.[13][14]

Messages in the logs will contain Failed to freeze unit 'user.slice' before sleep. When such an issue occurs, trying to login (start another session) would fail with:

To temporarily revert back to the old behavior, edit systemd-suspend.service, systemd-hibernate.service, systemd-hybrid-sleep.service, and systemd-suspend-then-hibernate.service with the following drop-in:

However, this drop-in can itself prevent the system from going to sleep.[15]

If you are running a multi boot system (including but not limited to dual boot with Windows) and want to be able to boot into your other system while your main Arch Linux is hibernated, you must take extra caution not to mount filesystems that are still in use by the hibernated system. Before attempting to mount such filesystem within another system, you must make sure to unmount this filesystem before hibernating the system. This can be achieved with sleep hooks.

This issue is particularly relevant for the EFI system partition, because the ESP is expected to be shared across multiple systems. Check the matching section in EFI system partition for mitigation strategies, which can be adapted to other filesystems as well.

**Examples:**

Example 1 (unknown):
```unknown
/sys/power/state
```

Example 2 (unknown):
```unknown
systemctl suspend
```

Example 3 (unknown):
```unknown
systemctl hibernate
```

Example 4 (unknown):
```unknown
systemctl hybrid-sleep
```

---

## fscrypt

**URL:** https://wiki.archlinux.org/title/Fscrypt

**Contents:**
- Alternatives to consider
- Preparations
  - Kernel
  - File system
    - ext4
    - F2FS
  - Userspace tool
  - PAM module
- Encrypt a directory
- Lock/unlock a directory

fscrypt is a tool for managing the native file encryption support of the ext4, F2FS, UBIFS, CephFS and Lustre file systems.

The underlying encryption mechanism in the kernel, which is integrated into the above file systems, is also sometimes called "fscrypt". To avoid ambiguity, this article calls the kernel feature "Linux native file encryption". With Linux native file encryption, different directories can use different encryption keys. In an encrypted directory, all file contents, filenames, and symlinks are encrypted. All subdirectories are encrypted too. Non-filename metadata, such as timestamps, the sizes and number of files, and extended attributes, is not encrypted.

As this article assumes the use of the fscrypt tool (and optionally pam_fscrypt, which goes along with fscrypt), most of it is not applicable to other userspace tools that can set up Linux native file encryption, for example systemd-homed.

This article or section needs expansion.

To protect an entire file system with one password, block device encryption with dm-crypt (LUKS) is generally a better option, as it ensures that all files on the file system are encrypted, and also that all file system metadata is encrypted. fscrypt is most useful to encrypt specific directories, or to enable different encrypted directories to be unlockable independentlyfor example, per-user encrypted home directories.

Compared to eCryptfs, the Linux native file encryption controlled by fscrypt does not use file system stacking, which makes it more memory-efficient. It also uses more up-to-date cryptography and does not require root privileges to set up, which avoids the need for setuid binaries. eCryptfs is also no longer being actively developed, and its largest users (Ubuntu and Chrome OS) have migrated to other solutions.

See data-at-rest encryption for more information about other encryption solutions, and about what encryption does and does not do.

All officially supported kernels support native file encryption on ext4, F2FS, UBIFS and CephFS.

Users of custom kernels, make sure CONFIG_FS_ENCRYPTION=y is set.

For ext4, the file system on which encryption is to be used must have the encrypt feature flag enabled. To enable it, run:

For F2FS, use mkfs.f2fs -O encrypt when creating the file system or fsck.f2fs -O encrypt at a later time.

Install the fscrypt package. Then run:

This creates the file /etc/fscrypt.conf and the directory /.fscrypt.

Then, if the file system on which encryption is to be used is not the root file system, also run:

where mountpoint is where the file system is mounted, e.g. /home.

This creates the directory mountpoint/.fscrypt to store fscrypt policies and protectors.

To unlock login passphrase-protected directories automatically at login, and to keep login passphrase-protected directories in sync with changes to the login passphrase, adjust the system PAM configuration to enable pam_fscrypt.

Append the following line to the auth section in /etc/pam.d/system-login:

Insert the following lines before session include system-auth in the session section:

Finally, append the following line to /etc/pam.d/passwd:

To encrypt an empty directory, run:

Follow the prompts to create or choose a "protector". A protector is the secret or information that protects the directory's encryption key. The types of protectors include:

In both cases, the passphrase can be changed later, or the directory can be re-protected with another method.

Example for custom passphrase:

Example for PAM passphrase:

To unlock an encrypted directory, run:

fscrypt will prompt for the passphrase.

To lock an encrypted directory, run:

To encrypt a user's home directory, first ensure that all preparations have been completed, including enabling pam_fscrypt.

Then, create a new encrypted directory for the user:

Select the option to protect the directory with the user's login passphrase.

Then copy the contents of the user's old home directory into the encrypted directory:

If the cp method was used, check whether the directory is being automatically unlocked on login before actually switching to using it. The simplest way to do this is to reboot and log in as that user. Afterwards, run:

If it says Unlocked: No instead, then something is wrong with the PAM configuration, or the incorrect type of protector was selected.

Otherwise, replace the home directory:

If everything is working as expected, delete the old home directory:

Support to use fscrypt inside Linux Containers (lxc), or more generally in mount_namespaces(7) where the file system's root directory is not visible has been added in v0.2.8.

A systemd/User unit within the container can lock an encrypted directory when the container is stopped:

See https://github.com/google/fscrypt/blob/master/README.md#troubleshooting for solutions to some common problems and also the open issues on Github.

Encrypted regular files from an unlocked fscrypt can be moved into an unencrypted directory. While possible, it is not well supported, normally not desired and could led to later issues: It may not be noticed by the user, unless the according protector is locked or worse: deleted. https://github.com/google/fscrypt/issues/393

If you want to decrypt files permanently, copy them from the unlocked fscrypt directory into a regular unencrypted directory.[1]

**Examples:**

Example 1 (unknown):
```unknown
CONFIG_FS_ENCRYPTION=y
```

Example 2 (unknown):
```unknown
# tune2fs -O encrypt /dev/device
```

Example 3 (unknown):
```unknown
mkfs.ext4 -O encrypt
```

Example 4 (unknown):
```unknown
mkfs.f2fs -O encrypt
```

---
